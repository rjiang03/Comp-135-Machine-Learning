{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn.linear_model \n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "from LRGradientDescent import LogisticRegressionGradientDescent\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGradientDescent():\n",
    "    ''' Logistic Regression binary classifier trainable via gradient descent.\n",
    "\n",
    "    Object that implements the standard sklearn binary classifier API:\n",
    "    * fit : train the model and set internal trainable attributes\n",
    "    * predict : produce hard binary predictions\n",
    "    * predict_proba : produce probabilistic predictions for both labels (0 and 1)\n",
    "\n",
    "    Attributes set by calling __init__()\n",
    "    ------------------------------------\n",
    "    alpha : float\n",
    "    step_size : float\n",
    "    num_iterations : int\n",
    "\n",
    "    Attributes set only by calling fit()\n",
    "    ------------------------------------\n",
    "    w_G : 1D array, size G\n",
    "        estimated weight vector\n",
    "    trace_steps : list\n",
    "    trace_loss : list\n",
    "    trace_L1_norm_of_grad : list\n",
    "\n",
    "    Training Objective\n",
    "    ------------------\n",
    "    Find w_G that minimizes calc_loss(w_G, xbias_NG, y_N)\n",
    "\n",
    "    In math, the loss is defined as:\n",
    "        L(w) = \\frac{1}{N \\log 2} (\\sum_n log_loss(w, x_n, y_n) + \\alpha w^T w)\n",
    "\n",
    "    We can directly interpret L(w) as an upper bound on the error rate\n",
    "    on the training data, because:\n",
    "    * log_loss is an upperbound on zero-one loss when done in base 2\n",
    "    * the extra L2 penalty term will only ever add to the loss\n",
    "\n",
    "    Example Usage\n",
    "    -------------\n",
    "    >>> x_N1 = np.hstack([np.linspace(-2, -1, 3), np.linspace(1, 2, 3)])[:,np.newaxis]\n",
    "    >>> y_N = np.hstack([np.zeros(3), np.ones(3)])\n",
    "\n",
    "    >>> clf = LogisticRegressionGradientDescent(\n",
    "    ...     alpha=1.0, step_size=0.1, verbose=False)\n",
    "\n",
    "    ### Shouldn't have any weights if we haven't trained yet\n",
    "    >>> assert not hasattr(clf, 'w_G')\n",
    "\n",
    "    ### After training, should have some weights\n",
    "    >>> clf.fit(x_N1, y_N)\n",
    "    >>> assert hasattr(clf, 'w_G')\n",
    "\n",
    "    ### Show the positive-class probability\n",
    "    >>> proba1_N = clf.predict_proba(x_N1)[:,1]\n",
    "    >>> print([\"%.2f\" % phat for phat in proba1_N])\n",
    "    ['0.08', '0.14', '0.23', '0.77', '0.86', '0.92']\n",
    "\n",
    "    ### Show the hard binary predictions\n",
    "    >>> clf.predict(x_N1).tolist()\n",
    "    [0, 0, 0, 1, 1, 1]\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=1.0,\n",
    "            step_size=0.001,\n",
    "            num_iterations=30000,\n",
    "            loss_converge_thr=0.00001,\n",
    "            grad_norm_converge_thr=0.001,\n",
    "            param_converge_thr=0.001,\n",
    "            verbose=True,\n",
    "            init_w_recipe='zeros',\n",
    "            random_state=0,\n",
    "            proba_to_binary_threshold=0.5,\n",
    "            ):\n",
    "        ''' Construct instance and set its attributes\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        alpha : float\n",
    "        step_size : float\n",
    "        num_iterations : int\n",
    "        loss_converge_thr : float\n",
    "        grad_norm_converge_thr : float\n",
    "        verbose : bool\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        New instance of this class\n",
    "        '''\n",
    "        self.alpha = float(alpha)\n",
    "        self.num_iterations = int(num_iterations)\n",
    "        self.step_size = float(step_size)\n",
    "        self.loss_converge_thr = float(loss_converge_thr)\n",
    "        self.grad_norm_converge_thr = float(grad_norm_converge_thr)\n",
    "        self.param_converge_thr = float(param_converge_thr)\n",
    "        self.verbose = bool(verbose)\n",
    "        self.proba_to_binary_threshold = float(proba_to_binary_threshold)\n",
    "        self.init_w_recipe = str(init_w_recipe)\n",
    "        if isinstance(random_state, int):\n",
    "            self.random_state = np.random.RandomState(random_state)\n",
    "        else:\n",
    "            self.random_state = random_state\n",
    "\n",
    "    ### Load and save to disk\n",
    "    def sigmoid_function(self, z):\n",
    "        yprob = [0, 0]\n",
    "        if z < 0:\n",
    "            yprob[1] = np.exp(z) / (np.exp(z) + 1.0)\n",
    "            yprob[0] = 1 - yprob[1]\n",
    "        else:\n",
    "            yprob[0] = np.exp(-z) / (np.exp(-z) + 1.0)\n",
    "            yprob[1] = 1  - yprob[0]\n",
    "\n",
    "\n",
    "        return yprob\n",
    "\n",
    "\n",
    "    def write_to_txt_file(self, txtfile):\n",
    "        np.savetxt(txtfile, self.w_G, fmt='%.18e', delimiter=' ')\n",
    "\n",
    "    def load_from_txt_file(self, txtfile):\n",
    "        self.w_G = np.loadtxt(txtfile, delimiter=' ')\n",
    "    \n",
    "    ### Prediction API methods\n",
    "\n",
    "    def predict_proba(self, x_NF):\n",
    "        ''' Produce soft probabilistic predictions for provided input features\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x_NF : 2D array, size N x F (n_examples x n_features_excluding_bias)\n",
    "            Input features (one row per example).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        yproba_N2 : 2D array, size N x 2\n",
    "            First column gives probability of zero label (negative)\n",
    "            Second column gives probability of one label (positive)\n",
    "            Each entry is a non-negative probability value within (0.0, 1.0)\n",
    "            Each row sums to one\n",
    "        '''\n",
    "        #N = x_NF.shape[0]\n",
    "        N, F = x_NF.shape\n",
    "        b = np.ones(N)\n",
    "        b = b * self.w_G[-1]\n",
    "        Y_r = self.w_G[0:F] * x_NF\n",
    "        Y_r = np.insert(Y_r, -1, values=b, axis=1)\n",
    "        yproba_N2 = np.zeros((N, 2))\n",
    "\n",
    "\n",
    "\n",
    "        ## TODO write code to do prediction for logistic regression!\n",
    "        # Hint: Be sure to use a numerically stable logistic_sigmoid function\n",
    "\n",
    "        # TODO replace the placeholder code below\n",
    "        # Which just predicts 100% probability that class is 0\n",
    "        Y_r_1 = np.sum(Y_r, axis=1)\n",
    "\n",
    "        yproba_N21 = np.exp(Y_r_1) / (np.exp(Y_r_1) + 1.0)\n",
    "        yproba_N22 = 1 - yproba_N21\n",
    "        yproba_N2 = np.column_stack((yproba_N22, yproba_N21))\n",
    "\n",
    "\n",
    "        return yproba_N2\n",
    "\n",
    "    def predict(self, x_NF):\n",
    "        ''' Produce hard binary predictions for provided input features\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x_NF : 2D array, size N x F (n_examples x n_features_excluding_bias)\n",
    "            Input features (one row per example).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        yhat_N : 1D array, size N\n",
    "            Each entry is a binary value (either 0 or 1)\n",
    "        '''\n",
    "        proba_N2 = self.predict_proba(x_NF)\n",
    "        return np.asarray(\n",
    "            proba_N2[:,1] >= self.proba_to_binary_threshold,\n",
    "            dtype=np.int32)\n",
    "\n",
    "    ### Method for training\n",
    "\n",
    "    def fit(self, x_NF, y_N):\n",
    "        ''' Fit logistic regression model to provided training data\n",
    "\n",
    "        Will minimize the loss function defined by calc_loss\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing. Only internal instance attributes updated.\n",
    "\n",
    "        Post Condition\n",
    "        --------------\n",
    "        Internal attributes are updated:\n",
    "        * w_G contains the optimal weights\n",
    "        * trace_loss contains loss at every step of gradient descent\n",
    "        * trace_L1_norm_of_grad contains L1 norm of grad after every step\n",
    "        '''\n",
    "        self.did_diverge = False\n",
    "        self.did_converge = False\n",
    "        self.trace_steps = list()\n",
    "        self.trace_loss = list()\n",
    "        self.trace_L1_norm_of_grad = list()\n",
    "        self.trace_w = list()\n",
    "\n",
    "        ## Setup dimension attributes\n",
    "        # F : num features excluding bias\n",
    "        # G : num features including bias \n",
    "        self.num_features_excluding_bias = x_NF.shape[1]\n",
    "        self.F = x_NF.shape[1]\n",
    "        self.G = self.F + 1\n",
    "\n",
    "        ## Setup input features with additional 'all ones' column\n",
    "        xbias_NG = self.insert_final_col_of_all_ones(x_NF)\n",
    "\n",
    "        ## Initialize w_G according to the selected recipe\n",
    "        if self.verbose:\n",
    "            print(\"Initializing w_G with %d features using recipe: %s\" % (\n",
    "                self.G, self.init_w_recipe))\n",
    "        w_G = self.initialize_w_G(xbias_NG, y_N)\n",
    "\n",
    "\n",
    "\n",
    "        ## Run gradient descent!\n",
    "        # Loop over iterations 0, 1, ..., num_iterations -1, num_iterations\n",
    "        # We don't do a parameter update on iteration 0, just use the initial w\n",
    "        if self.verbose:\n",
    "            print(\"Running up to %d iters of gradient descent with step_size %.3g\" % (\n",
    "                self.num_iterations, self.step_size))\n",
    "        for iter_id in range(self.num_iterations + 1):\n",
    "            if iter_id > 0:\n",
    "                # TODO update parameter: w_G = ...\n",
    "                w_G = w_G - self.step_size * self.calc_grad(w_G, xbias_NG, y_N)\n",
    "\n",
    "            loss = self.calc_loss(w_G, xbias_NG, y_N)\n",
    "            grad_G = self.calc_grad(w_G, xbias_NG, y_N)\n",
    "            avg_L1_norm_of_grad = np.mean(np.abs(grad_G))\n",
    "\n",
    "            ## Print information to stdout\n",
    "            if self.verbose:\n",
    "                if iter_id < 20 or (iter_id % 20 == 0) or (iter_id % 20 == 1):\n",
    "                    print('iter %4d/%d  loss % 16.6f  avg_L1_norm_grad % 16.6f  w[0] % 8.3f bias % 8.3f' % (\n",
    "                        iter_id, self.num_iterations, loss, avg_L1_norm_of_grad,\n",
    "                        w_G[0], w_G[-1]))\n",
    "\n",
    "            ## Record information\n",
    "            self.trace_steps.append(iter_id)\n",
    "            self.trace_loss.append(loss)\n",
    "            self.trace_L1_norm_of_grad.append(avg_L1_norm_of_grad)\n",
    "            self.trace_w.append(w_G)\n",
    "\n",
    "            ## Assess divergence and raise ValueError as soon as it happens\n",
    "            self.raise_error_if_diverging(\n",
    "                self.trace_steps, self.trace_loss, self.trace_L1_norm_of_grad)\n",
    "\n",
    "            ## Assess convergence and break early if happens\n",
    "            self.did_converge = self.check_convergence(\n",
    "                self.trace_steps, self.trace_loss,\n",
    "                self.trace_L1_norm_of_grad, self.trace_w)\n",
    "            if self.did_converge:\n",
    "                break\n",
    "\n",
    "        if not self.did_diverge:\n",
    "            self.w_G = w_G\n",
    "        if self.verbose:\n",
    "            if self.did_converge:\n",
    "                print(\"Done. Converged after %d iterations.\" % self.trace_steps[-1])\n",
    "            else:\n",
    "                print(\"Done. Did NOT converge.\")\n",
    "        # Done with `fit`.\n",
    "\n",
    "\n",
    "    ### Methods for gradient descent: calc_loss and calc_grad\n",
    "\n",
    "    def calc_loss(self, w_G, xbias_NG, y_N):\n",
    "        ''' Compute total loss for used to train logistic regression.\n",
    "\n",
    "        Sum of log loss over training examples plus L2 penalty term.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        w_G : 1D array, size G \n",
    "            Combined vector of weights and bias\n",
    "        xbias_NG : 2D array, size N x G (n_examples x n_features+1)\n",
    "            Input features, with last column of all ones\n",
    "        y_N : 1D array, size N\n",
    "            Binary labels for each example (either 0 or 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            Scalar loss. Lower is better.\n",
    "        '''\n",
    "        G = w_G.size\n",
    "        N = float(y_N.size)\n",
    "        denom = N * np.log(2)\n",
    "\n",
    "        ## First term: Calc loss due to L2 penalty on weights\n",
    "        L2_loss = 1/2 * np.sum(w_G * w_G) * self.alpha # TODO calc L2 penalty term\n",
    "\n",
    "\n",
    "        ## Second term: Calc log loss by summing over examples\n",
    "        #prob = self.sigmoid_function(w_G * xbias_NG)\n",
    "        z = np.dot(xbias_NG, w_G)\n",
    "        #log_loss = np.sum(y_N * z) - np.sum(np.log(np.exp(z)+1))\n",
    "        c = np.zeros(int(N))\n",
    "        d = np.column_stack((z,c))\n",
    "        log_loss = np.sum(y_N * z) - np.sum(logsumexp(d, axis = 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ## Add two terms together and return sum divided by N * np.log(2)\n",
    "        return (L2_loss - log_loss) / denom\n",
    "        \n",
    "    def calc_grad(self, w_G, xbias_NG, y_N):\n",
    "        ''' Compute gradient of total loss for training logistic regression.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        w_G : 1D array, size G (G = n_features_including_bias)\n",
    "            Combined vector of weights and bias\n",
    "        xbias_NG : 2D array, size N x G (n_examples x n_features_including_bias)\n",
    "            Input features, with last column of all ones\n",
    "        y_N : 1D array, size N\n",
    "            Binary labels for each example (either 0 or 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grad_wrt_w_G : 1D array, size G\n",
    "            Entry g contains derivative of loss with respect to w_G[g]\n",
    "        '''\n",
    "        G = w_G.size\n",
    "        N = float(y_N.size)\n",
    "        denom = N * np.log(2)\n",
    "\n",
    "        # TODO calc gradient of L2 penalty term\n",
    "        grad_L2_wrt_w_G = np.zeros(G) + w_G * self.alpha\n",
    "        #grad_L2_wrt_w_G = w_G * self.alpha\n",
    "        # TODO calc gradient of log loss term\n",
    "\n",
    "        z = np.dot(xbias_NG, w_G)\n",
    "        xbias_NG = xbias_NG.T\n",
    "        grad_logloss_wrt_w_G = - np.sum((y_N - np.exp(z) / (np.exp(z) + 1.0)) * xbias_NG, axis=1)\n",
    "        grad_logloss_wrt_w_G = grad_logloss_wrt_w_G.T\n",
    "\n",
    "        return (grad_L2_wrt_w_G + grad_logloss_wrt_w_G) / denom\n",
    "\n",
    "    ### Helper methods\n",
    "\n",
    "    def insert_final_col_of_all_ones(self, x_NF):\n",
    "        ''' Append a column of all ones to provided array.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x_NF : 2D array, size N x F\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xbias_NG : 2D array, size N x G, where G = F+1\n",
    "            First F columns will be same as input array x_NF\n",
    "            Final column will be equal to all ones.\n",
    "        '''\n",
    "        N = x_NF.shape[0]\n",
    "        return np.hstack([x_NF, np.ones((N,1))])\n",
    "\n",
    "    def initialize_w_G(self, xbias_NG, y_N):\n",
    "        ''' Initialize weight vectors according to this instance's recipe\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        xbias_NG : 2D array, size N x G (n_examples x n_features_including_bias)\n",
    "            Input features, with last column of all ones\n",
    "        y_N : 1D array, size N\n",
    "            Binary labels for each example (either 0 or 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        w_G : 1D array, size G (n_features_including_bias)\n",
    "            Weight vector, where final entry is the bias\n",
    "        '''\n",
    "        F = self.num_features_excluding_bias\n",
    "        G = F + 1\n",
    "        if self.init_w_recipe == 'uniform_-1_to_1':\n",
    "            w_G = self.random_state.uniform(-1, 1, size=G)\n",
    "        elif self.init_w_recipe == 'zeros':\n",
    "            w_G = np.zeros(G)\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized init_w_recipe: %s\" % init_w_recipe)\n",
    "        return w_G\n",
    "\n",
    "    def check_convergence(self, trace_steps, trace_loss,\n",
    "                          trace_L1_norm_of_grad, trace_w):\n",
    "        ''' Assess if current gradient descent run has converged\n",
    "\n",
    "        We assume that at least 100 iters are needed to verify convergence.\n",
    "        This might be abundantly cautious, but we'd rather be sure.\n",
    "\n",
    "        Convergence is assessed on three criteria:\n",
    "        * loss has stopped changing meaningfully over last 100 iters\n",
    "            Measured by difference of mean loss from recent iters 100-50 to 50-0.\n",
    "            Compared against the threshold attribute 'loss_converge_thr'\n",
    "        * gradient is close enough to zero vector\n",
    "            Measured by the L1 norm of the gradient vector at latest iteration.\n",
    "            Compared against the threshold attribute 'grad_norm_converge_thr'\n",
    "        * weights have not changed significantly over last 100 iters\n",
    "            Compared against the threshold attribute 'param_converge_thr'\n",
    "\n",
    "        If all 3 criteria are satisified, we return True.\n",
    "        Otherwise, we return False.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        trace_steps : list of int\n",
    "            Each entry is an iteration number\n",
    "            Counts up from 0, 1, 2, ...\n",
    "        trace_loss : list of scalars\n",
    "            Each entry is the value of the loss at an iteration.\n",
    "            Should be generally going down\n",
    "        trace_L1_norm_of_grad : list of scalars\n",
    "            Each entry is the L1 gradient norm at an iteration.\n",
    "            Should be generally going down and approaching zero.\n",
    "        trace_w : list of 1D arrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        did_converge : bool\n",
    "            Boolean flag that indicates if the run has converged.\n",
    "        '''\n",
    "        iter_id = trace_steps[-1]\n",
    "        ## Assess convergence\n",
    "        if iter_id < 100:\n",
    "            is_loss_converged = False\n",
    "            is_grad_converged = False\n",
    "            is_param_converged = False\n",
    "        else:\n",
    "            ## Criteria 1/3: has the loss stopped changing?\n",
    "            # Calc average loss from 100-50 steps ago\n",
    "            old_avg_loss = np.mean(trace_loss[-100:-50])\n",
    "            # Calc average loss from 50-0 steps ago\n",
    "            new_avg_loss = np.mean(trace_loss[-50:])\n",
    "            loss_diff = np.abs(old_avg_loss - new_avg_loss)\n",
    "            is_loss_converged = loss_diff < self.loss_converge_thr\n",
    "\n",
    "            ## Criteria 2/3: is the gradient close to zero?\n",
    "            # Check if gradient is small enough\n",
    "            is_grad_converged = trace_L1_norm_of_grad[-1] < self.grad_norm_converge_thr\n",
    "\n",
    "            ## Criteria 3/3: have weight vector parameters stopped changing?\n",
    "            # Check if max L1 diff across all weight values is small enough\n",
    "            max_param_diff = np.max(np.abs(trace_w[-100] - trace_w[-1]))\n",
    "            is_param_converged = max_param_diff < self.param_converge_thr\n",
    "                \n",
    "        did_converge = is_param_converged and is_loss_converged and is_grad_converged\n",
    "        return did_converge\n",
    "\n",
    "    def raise_error_if_diverging(\n",
    "            self, trace_steps, trace_loss, trace_L1_norm_of_grad):\n",
    "        ''' Raise error if current gradient descent run is diverging\n",
    "\n",
    "        Will assess current trace and raise ValueError only if diverging.\n",
    "\n",
    "        Divergence occurs when:\n",
    "        * loss is going UP consistently over 10 iterations, when should go DOWN.\n",
    "        * loss is NaN or infinite\n",
    "        * any entry of the gradient is NaN or infinite\n",
    "\n",
    "        Divergence happens in gradient descent when step_size is set too large.\n",
    "        If divergence is detected, we recommend using a smaller step_size.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        trace_steps : list of trace step numbers\n",
    "            Counts up from 0, 1, 2, ...\n",
    "        trace_loss : list of loss values\n",
    "            Should be generally going down\n",
    "        trace_L1_norm_of_grad : list of L1 gradient norms\n",
    "            Should be generally going down\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing\n",
    "\n",
    "        Post Condition\n",
    "        --------------\n",
    "        Internal attribute `did_diverge` is set to True or False, as needed. \n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError if divergence is detected.\n",
    "        '''\n",
    "        n_completed_iters = len(trace_loss)\n",
    "        loss = trace_loss[-1]\n",
    "        L1_norm_grad = trace_L1_norm_of_grad[-1]\n",
    "        did_diverge = False\n",
    "        if np.isnan(loss):\n",
    "            did_diverge = True\n",
    "            reason_str = 'Loss should never be NaN'            \n",
    "        elif not np.isfinite(loss):\n",
    "            did_diverge = True\n",
    "            reason_str = 'Loss should never be infinite'\n",
    "        elif np.isnan(L1_norm_grad):\n",
    "            did_diverge = True\n",
    "            reason_str = 'Grad should never be NaN'\n",
    "        elif not np.isfinite(L1_norm_grad):\n",
    "            did_diverge = True\n",
    "            reason_str = 'Grad should never be infinite'\n",
    "\n",
    "        # We need at least 10 completed steps to verify diverging...\n",
    "        elif n_completed_iters >= 10:\n",
    "            # Let's look at the 10 most recent steps we took, and compare:\n",
    "            # * the average loss on steps 10-5\n",
    "            # * the average loss on steps 5-0\n",
    "            # If the loss is moving in wrong direction by significant amount,\n",
    "            # We mark run as diverging and exit early.\n",
    "            old_loss = np.median(trace_loss[-10:-5])\n",
    "            new_loss = np.median(trace_loss[-5:])\n",
    "            perc_change_last10 = (new_loss - old_loss) / (1e-10 + np.abs(old_loss))\n",
    "\n",
    "            oldnew_loss = np.median(trace_loss[-6:-3])\n",
    "            newnew_loss = np.median(trace_loss[-3:])\n",
    "            perc_change_last6 = (newnew_loss - oldnew_loss) / (1e-10 + np.abs(oldnew_loss))\n",
    "\n",
    "            if perc_change_last10 > 0.50 and perc_change_last6 > 0.50:\n",
    "                did_diverge = True\n",
    "                reason_str = 'Loss is increasing but should be decreasing!'\n",
    "\n",
    "        self.did_diverge = did_diverge\n",
    "        if did_diverge:\n",
    "            hint_str = \"Try a smaller step_size than current value %.3e\" % (\n",
    "                self.step_size)\n",
    "            print(\"ALERT! Divergence detected. %s\" % reason_str)\n",
    "            print(\"Recent history of loss values:\")\n",
    "            M = np.minimum(10, n_completed_iters)\n",
    "            for ii in range(M):\n",
    "                print(\"iter %4d  loss % 16.6f\" % (\n",
    "                    trace_steps[-M+ii], trace_loss[-M+ii]))\n",
    "            raise ValueError(\"Divergence detected. %s. %s.\" % (\n",
    "                reason_str, hint_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking loss and grad at all zeros w vector\n",
      "w_G = [0. 0.]\n",
      "loss(w_G) = 1.000\n",
      "grad(w_G) = [-1.08202128  0.        ]\n",
      "Trying gradient descent\n",
      "Initializing w_G with 2 features using recipe: zeros\n",
      "Running up to 10000 iters of gradient descent with step_size 0.01\n",
      "iter    0/10000  loss         1.000000  avg_L1_norm_grad         0.541011  w[0]    0.000 bias    0.000\n",
      "iter    1/10000  loss         0.988343  avg_L1_norm_grad         0.536298  w[0]    0.011 bias    0.000\n",
      "iter    2/10000  loss         0.976889  avg_L1_norm_grad         0.531628  w[0]    0.022 bias   -0.000\n",
      "iter    3/10000  loss         0.965633  avg_L1_norm_grad         0.527000  w[0]    0.032 bias   -0.000\n",
      "iter    4/10000  loss         0.954572  avg_L1_norm_grad         0.522414  w[0]    0.043 bias   -0.000\n",
      "iter    5/10000  loss         0.943703  avg_L1_norm_grad         0.517871  w[0]    0.053 bias   -0.000\n",
      "iter    6/10000  loss         0.933022  avg_L1_norm_grad         0.513371  w[0]    0.064 bias   -0.000\n",
      "iter    7/10000  loss         0.922526  avg_L1_norm_grad         0.508914  w[0]    0.074 bias   -0.000\n",
      "iter    8/10000  loss         0.912211  avg_L1_norm_grad         0.504501  w[0]    0.084 bias   -0.000\n",
      "iter    9/10000  loss         0.902074  avg_L1_norm_grad         0.500131  w[0]    0.094 bias   -0.000\n",
      "iter   10/10000  loss         0.892112  avg_L1_norm_grad         0.495804  w[0]    0.104 bias   -0.000\n",
      "iter   11/10000  loss         0.882322  avg_L1_norm_grad         0.491521  w[0]    0.114 bias   -0.000\n",
      "iter   12/10000  loss         0.872700  avg_L1_norm_grad         0.487282  w[0]    0.124 bias   -0.000\n",
      "iter   13/10000  loss         0.863243  avg_L1_norm_grad         0.483087  w[0]    0.134 bias   -0.000\n",
      "iter   14/10000  loss         0.853948  avg_L1_norm_grad         0.478935  w[0]    0.143 bias   -0.000\n",
      "iter   15/10000  loss         0.844812  avg_L1_norm_grad         0.474827  w[0]    0.153 bias   -0.000\n",
      "iter   16/10000  loss         0.835832  avg_L1_norm_grad         0.470762  w[0]    0.162 bias   -0.000\n",
      "iter   17/10000  loss         0.827006  avg_L1_norm_grad         0.466741  w[0]    0.172 bias   -0.000\n",
      "iter   18/10000  loss         0.818329  avg_L1_norm_grad         0.462762  w[0]    0.181 bias   -0.000\n",
      "iter   19/10000  loss         0.809799  avg_L1_norm_grad         0.458827  w[0]    0.190 bias   -0.000\n",
      "iter   20/10000  loss         0.801414  avg_L1_norm_grad         0.454935  w[0]    0.199 bias   -0.000\n",
      "iter   21/10000  loss         0.793171  avg_L1_norm_grad         0.451086  w[0]    0.209 bias   -0.000\n",
      "iter   40/10000  loss         0.660070  avg_L1_norm_grad         0.385700  w[0]    0.368 bias    0.000\n",
      "iter   41/10000  loss         0.654143  avg_L1_norm_grad         0.382643  w[0]    0.375 bias    0.000\n",
      "iter   60/10000  loss         0.557382  avg_L1_norm_grad         0.330931  w[0]    0.511 bias    0.000\n",
      "iter   61/10000  loss         0.553017  avg_L1_norm_grad         0.328516  w[0]    0.518 bias    0.000\n",
      "iter   80/10000  loss         0.480890  avg_L1_norm_grad         0.287567  w[0]    0.635 bias    0.000\n",
      "iter   81/10000  loss         0.477593  avg_L1_norm_grad         0.285648  w[0]    0.641 bias    0.000\n",
      "iter  100/10000  loss         0.422470  avg_L1_norm_grad         0.252912  w[0]    0.743 bias   -0.000\n",
      "iter  101/10000  loss         0.419919  avg_L1_norm_grad         0.251368  w[0]    0.748 bias   -0.000\n",
      "iter  120/10000  loss         0.376812  avg_L1_norm_grad         0.224860  w[0]    0.839 bias   -0.000\n",
      "iter  121/10000  loss         0.374796  avg_L1_norm_grad         0.223601  w[0]    0.843 bias   -0.000\n",
      "iter  140/10000  loss         0.340389  avg_L1_norm_grad         0.201840  w[0]    0.924 bias    0.000\n",
      "iter  141/10000  loss         0.338763  avg_L1_norm_grad         0.200799  w[0]    0.928 bias    0.000\n",
      "iter  160/10000  loss         0.310804  avg_L1_norm_grad         0.182697  w[0]    1.001 bias    0.000\n",
      "iter  161/10000  loss         0.309472  avg_L1_norm_grad         0.181826  w[0]    1.005 bias    0.000\n",
      "iter  180/10000  loss         0.286394  avg_L1_norm_grad         0.166579  w[0]    1.071 bias   -0.000\n",
      "iter  181/10000  loss         0.285286  avg_L1_norm_grad         0.165841  w[0]    1.075 bias   -0.000\n",
      "iter  200/10000  loss         0.265975  avg_L1_norm_grad         0.152853  w[0]    1.135 bias   -0.000\n",
      "iter  201/10000  loss         0.265043  avg_L1_norm_grad         0.152220  w[0]    1.138 bias   -0.000\n",
      "iter  220/10000  loss         0.248690  avg_L1_norm_grad         0.141043  w[0]    1.194 bias   -0.000\n",
      "iter  221/10000  loss         0.247896  avg_L1_norm_grad         0.140495  w[0]    1.197 bias   -0.000\n",
      "iter  240/10000  loss         0.233902  avg_L1_norm_grad         0.130786  w[0]    1.248 bias   -0.000\n",
      "iter  241/10000  loss         0.233219  avg_L1_norm_grad         0.130309  w[0]    1.251 bias   -0.000\n",
      "iter  260/10000  loss         0.221133  avg_L1_norm_grad         0.121804  w[0]    1.299 bias   -0.000\n",
      "iter  261/10000  loss         0.220541  avg_L1_norm_grad         0.121384  w[0]    1.301 bias   -0.000\n",
      "iter  280/10000  loss         0.210016  avg_L1_norm_grad         0.113878  w[0]    1.346 bias   -0.000\n",
      "iter  281/10000  loss         0.209499  avg_L1_norm_grad         0.113506  w[0]    1.348 bias   -0.000\n",
      "iter  300/10000  loss         0.200267  avg_L1_norm_grad         0.106836  w[0]    1.390 bias   -0.000\n",
      "iter  301/10000  loss         0.199811  avg_L1_norm_grad         0.106505  w[0]    1.392 bias   -0.000\n",
      "iter  320/10000  loss         0.191659  avg_L1_norm_grad         0.100541  w[0]    1.432 bias   -0.000\n",
      "iter  321/10000  loss         0.191256  avg_L1_norm_grad         0.100244  w[0]    1.434 bias   -0.000\n",
      "iter  340/10000  loss         0.184016  avg_L1_norm_grad         0.094882  w[0]    1.471 bias   -0.000\n",
      "iter  341/10000  loss         0.183656  avg_L1_norm_grad         0.094614  w[0]    1.473 bias   -0.000\n",
      "iter  360/10000  loss         0.177192  avg_L1_norm_grad         0.089769  w[0]    1.508 bias   -0.000\n",
      "iter  361/10000  loss         0.176870  avg_L1_norm_grad         0.089526  w[0]    1.510 bias   -0.000\n",
      "iter  380/10000  loss         0.171069  avg_L1_norm_grad         0.085127  w[0]    1.543 bias   -0.000\n",
      "iter  381/10000  loss         0.170780  avg_L1_norm_grad         0.084906  w[0]    1.545 bias   -0.000\n",
      "iter  400/10000  loss         0.165553  avg_L1_norm_grad         0.080894  w[0]    1.576 bias   -0.000\n",
      "iter  401/10000  loss         0.165291  avg_L1_norm_grad         0.080693  w[0]    1.578 bias   -0.000\n",
      "iter  420/10000  loss         0.160562  avg_L1_norm_grad         0.077020  w[0]    1.608 bias   -0.000\n",
      "iter  421/10000  loss         0.160325  avg_L1_norm_grad         0.076835  w[0]    1.609 bias   -0.000\n",
      "iter  440/10000  loss         0.156030  avg_L1_norm_grad         0.073462  w[0]    1.638 bias   -0.000\n",
      "iter  441/10000  loss         0.155814  avg_L1_norm_grad         0.073291  w[0]    1.639 bias   -0.000\n",
      "iter  460/10000  loss         0.151900  avg_L1_norm_grad         0.070181  w[0]    1.667 bias   -0.000\n",
      "iter  461/10000  loss         0.151703  avg_L1_norm_grad         0.070024  w[0]    1.668 bias   -0.000\n",
      "iter  480/10000  loss         0.148125  avg_L1_norm_grad         0.067148  w[0]    1.694 bias   -0.000\n",
      "iter  481/10000  loss         0.147945  avg_L1_norm_grad         0.067002  w[0]    1.696 bias   -0.000\n",
      "iter  500/10000  loss         0.144665  avg_L1_norm_grad         0.064335  w[0]    1.720 bias   -0.000\n",
      "iter  501/10000  loss         0.144500  avg_L1_norm_grad         0.064200  w[0]    1.722 bias   -0.000\n",
      "iter  520/10000  loss         0.141485  avg_L1_norm_grad         0.061720  w[0]    1.746 bias   -0.000\n",
      "iter  521/10000  loss         0.141333  avg_L1_norm_grad         0.061594  w[0]    1.747 bias   -0.000\n",
      "iter  540/10000  loss         0.138555  avg_L1_norm_grad         0.059283  w[0]    1.770 bias   -0.000\n",
      "iter  541/10000  loss         0.138415  avg_L1_norm_grad         0.059165  w[0]    1.771 bias   -0.000\n",
      "iter  560/10000  loss         0.135849  avg_L1_norm_grad         0.057005  w[0]    1.793 bias   -0.000\n",
      "iter  561/10000  loss         0.135719  avg_L1_norm_grad         0.056895  w[0]    1.794 bias   -0.000\n",
      "iter  580/10000  loss         0.133344  avg_L1_norm_grad         0.054873  w[0]    1.816 bias   -0.000\n",
      "iter  581/10000  loss         0.133223  avg_L1_norm_grad         0.054770  w[0]    1.817 bias   -0.000\n",
      "iter  600/10000  loss         0.131020  avg_L1_norm_grad         0.052872  w[0]    1.837 bias   -0.000\n",
      "iter  601/10000  loss         0.130909  avg_L1_norm_grad         0.052775  w[0]    1.838 bias   -0.000\n",
      "iter  620/10000  loss         0.128861  avg_L1_norm_grad         0.050991  w[0]    1.858 bias   -0.000\n",
      "iter  621/10000  loss         0.128757  avg_L1_norm_grad         0.050900  w[0]    1.859 bias   -0.000\n",
      "iter  640/10000  loss         0.126852  avg_L1_norm_grad         0.049220  w[0]    1.878 bias   -0.000\n",
      "iter  641/10000  loss         0.126755  avg_L1_norm_grad         0.049134  w[0]    1.879 bias   -0.000\n",
      "iter  660/10000  loss         0.124978  avg_L1_norm_grad         0.047549  w[0]    1.897 bias   -0.000\n",
      "iter  661/10000  loss         0.124887  avg_L1_norm_grad         0.047468  w[0]    1.898 bias   -0.000\n",
      "iter  680/10000  loss         0.123227  avg_L1_norm_grad         0.045970  w[0]    1.916 bias   -0.000\n",
      "iter  681/10000  loss         0.123143  avg_L1_norm_grad         0.045893  w[0]    1.917 bias   -0.000\n",
      "iter  700/10000  loss         0.121590  avg_L1_norm_grad         0.044476  w[0]    1.934 bias   -0.000\n",
      "iter  701/10000  loss         0.121511  avg_L1_norm_grad         0.044403  w[0]    1.935 bias   -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  720/10000  loss         0.120057  avg_L1_norm_grad         0.043060  w[0]    1.952 bias   -0.000\n",
      "iter  721/10000  loss         0.119983  avg_L1_norm_grad         0.042991  w[0]    1.953 bias   -0.000\n",
      "iter  740/10000  loss         0.118618  avg_L1_norm_grad         0.041716  w[0]    1.969 bias   -0.000\n",
      "iter  741/10000  loss         0.118549  avg_L1_norm_grad         0.041651  w[0]    1.970 bias   -0.000\n",
      "iter  760/10000  loss         0.117268  avg_L1_norm_grad         0.040439  w[0]    1.985 bias   -0.000\n",
      "iter  761/10000  loss         0.117202  avg_L1_norm_grad         0.040377  w[0]    1.986 bias   -0.000\n",
      "iter  780/10000  loss         0.115998  avg_L1_norm_grad         0.039224  w[0]    2.001 bias   -0.000\n",
      "iter  781/10000  loss         0.115936  avg_L1_norm_grad         0.039165  w[0]    2.002 bias   -0.000\n",
      "iter  800/10000  loss         0.114802  avg_L1_norm_grad         0.038067  w[0]    2.017 bias   -0.000\n",
      "iter  801/10000  loss         0.114744  avg_L1_norm_grad         0.038010  w[0]    2.017 bias   -0.000\n",
      "iter  820/10000  loss         0.113676  avg_L1_norm_grad         0.036963  w[0]    2.032 bias   -0.000\n",
      "iter  821/10000  loss         0.113621  avg_L1_norm_grad         0.036909  w[0]    2.032 bias   -0.000\n",
      "iter  840/10000  loss         0.112613  avg_L1_norm_grad         0.035910  w[0]    2.046 bias   -0.000\n",
      "iter  841/10000  loss         0.112562  avg_L1_norm_grad         0.035858  w[0]    2.047 bias   -0.000\n",
      "iter  860/10000  loss         0.111610  avg_L1_norm_grad         0.034903  w[0]    2.060 bias   -0.000\n",
      "iter  861/10000  loss         0.111561  avg_L1_norm_grad         0.034854  w[0]    2.061 bias   -0.000\n",
      "iter  880/10000  loss         0.110661  avg_L1_norm_grad         0.033940  w[0]    2.074 bias   -0.000\n",
      "iter  881/10000  loss         0.110615  avg_L1_norm_grad         0.033893  w[0]    2.075 bias   -0.000\n",
      "iter  900/10000  loss         0.109764  avg_L1_norm_grad         0.033018  w[0]    2.087 bias   -0.000\n",
      "iter  901/10000  loss         0.109720  avg_L1_norm_grad         0.032973  w[0]    2.088 bias   -0.000\n",
      "iter  920/10000  loss         0.108915  avg_L1_norm_grad         0.032135  w[0]    2.101 bias   -0.000\n",
      "iter  921/10000  loss         0.108873  avg_L1_norm_grad         0.032092  w[0]    2.101 bias   -0.000\n",
      "iter  940/10000  loss         0.108110  avg_L1_norm_grad         0.031288  w[0]    2.113 bias   -0.000\n",
      "iter  941/10000  loss         0.108071  avg_L1_norm_grad         0.031246  w[0]    2.114 bias   -0.000\n",
      "iter  960/10000  loss         0.107346  avg_L1_norm_grad         0.030475  w[0]    2.126 bias   -0.000\n",
      "iter  961/10000  loss         0.107309  avg_L1_norm_grad         0.030435  w[0]    2.126 bias   -0.000\n",
      "iter  980/10000  loss         0.106622  avg_L1_norm_grad         0.029694  w[0]    2.138 bias   -0.000\n",
      "iter  981/10000  loss         0.106587  avg_L1_norm_grad         0.029655  w[0]    2.138 bias   -0.000\n",
      "iter 1000/10000  loss         0.105934  avg_L1_norm_grad         0.028943  w[0]    2.149 bias   -0.000\n",
      "iter 1001/10000  loss         0.105900  avg_L1_norm_grad         0.028906  w[0]    2.150 bias   -0.000\n",
      "iter 1020/10000  loss         0.105280  avg_L1_norm_grad         0.028221  w[0]    2.161 bias   -0.000\n",
      "iter 1021/10000  loss         0.105248  avg_L1_norm_grad         0.028185  w[0]    2.161 bias   -0.000\n",
      "iter 1040/10000  loss         0.104658  avg_L1_norm_grad         0.027526  w[0]    2.172 bias   -0.000\n",
      "iter 1041/10000  loss         0.104628  avg_L1_norm_grad         0.027491  w[0]    2.172 bias   -0.000\n",
      "iter 1060/10000  loss         0.104066  avg_L1_norm_grad         0.026856  w[0]    2.183 bias   -0.000\n",
      "iter 1061/10000  loss         0.104038  avg_L1_norm_grad         0.026823  w[0]    2.183 bias   -0.000\n",
      "iter 1080/10000  loss         0.103503  avg_L1_norm_grad         0.026210  w[0]    2.193 bias   -0.000\n",
      "iter 1081/10000  loss         0.103475  avg_L1_norm_grad         0.026179  w[0]    2.194 bias   -0.000\n",
      "iter 1100/10000  loss         0.102966  avg_L1_norm_grad         0.025588  w[0]    2.204 bias   -0.000\n",
      "iter 1101/10000  loss         0.102940  avg_L1_norm_grad         0.025557  w[0]    2.204 bias   -0.000\n",
      "iter 1120/10000  loss         0.102454  avg_L1_norm_grad         0.024987  w[0]    2.214 bias   -0.000\n",
      "iter 1121/10000  loss         0.102429  avg_L1_norm_grad         0.024958  w[0]    2.214 bias   -0.000\n",
      "iter 1140/10000  loss         0.101966  avg_L1_norm_grad         0.024407  w[0]    2.224 bias   -0.000\n",
      "iter 1141/10000  loss         0.101942  avg_L1_norm_grad         0.024379  w[0]    2.224 bias   -0.000\n",
      "iter 1160/10000  loss         0.101500  avg_L1_norm_grad         0.023847  w[0]    2.233 bias   -0.000\n",
      "iter 1161/10000  loss         0.101477  avg_L1_norm_grad         0.023819  w[0]    2.234 bias   -0.000\n",
      "iter 1180/10000  loss         0.101055  avg_L1_norm_grad         0.023305  w[0]    2.243 bias   -0.000\n",
      "iter 1181/10000  loss         0.101033  avg_L1_norm_grad         0.023278  w[0]    2.243 bias   -0.000\n",
      "iter 1200/10000  loss         0.100630  avg_L1_norm_grad         0.022781  w[0]    2.252 bias   -0.000\n",
      "iter 1201/10000  loss         0.100609  avg_L1_norm_grad         0.022755  w[0]    2.253 bias   -0.000\n",
      "iter 1220/10000  loss         0.100224  avg_L1_norm_grad         0.022274  w[0]    2.261 bias   -0.000\n",
      "iter 1221/10000  loss         0.100204  avg_L1_norm_grad         0.022249  w[0]    2.262 bias   -0.000\n",
      "iter 1240/10000  loss         0.099836  avg_L1_norm_grad         0.021783  w[0]    2.270 bias   -0.000\n",
      "iter 1241/10000  loss         0.099817  avg_L1_norm_grad         0.021759  w[0]    2.270 bias   -0.000\n",
      "iter 1260/10000  loss         0.099464  avg_L1_norm_grad         0.021308  w[0]    2.279 bias   -0.000\n",
      "iter 1261/10000  loss         0.099446  avg_L1_norm_grad         0.021285  w[0]    2.279 bias   -0.000\n",
      "iter 1280/10000  loss         0.099108  avg_L1_norm_grad         0.020848  w[0]    2.287 bias   -0.000\n",
      "iter 1281/10000  loss         0.099091  avg_L1_norm_grad         0.020825  w[0]    2.287 bias   -0.000\n",
      "iter 1300/10000  loss         0.098768  avg_L1_norm_grad         0.020401  w[0]    2.295 bias   -0.000\n",
      "iter 1301/10000  loss         0.098751  avg_L1_norm_grad         0.020379  w[0]    2.296 bias   -0.000\n",
      "iter 1320/10000  loss         0.098442  avg_L1_norm_grad         0.019969  w[0]    2.303 bias   -0.000\n",
      "iter 1321/10000  loss         0.098426  avg_L1_norm_grad         0.019947  w[0]    2.304 bias   -0.000\n",
      "iter 1340/10000  loss         0.098129  avg_L1_norm_grad         0.019549  w[0]    2.311 bias    0.000\n",
      "iter 1341/10000  loss         0.098114  avg_L1_norm_grad         0.019528  w[0]    2.312 bias    0.000\n",
      "iter 1360/10000  loss         0.097830  avg_L1_norm_grad         0.019141  w[0]    2.319 bias    0.000\n",
      "iter 1361/10000  loss         0.097815  avg_L1_norm_grad         0.019121  w[0]    2.319 bias    0.000\n",
      "iter 1380/10000  loss         0.097543  avg_L1_norm_grad         0.018745  w[0]    2.327 bias    0.000\n",
      "iter 1381/10000  loss         0.097529  avg_L1_norm_grad         0.018726  w[0]    2.327 bias    0.000\n",
      "iter 1400/10000  loss         0.097267  avg_L1_norm_grad         0.018361  w[0]    2.334 bias    0.000\n",
      "iter 1401/10000  loss         0.097254  avg_L1_norm_grad         0.018342  w[0]    2.334 bias    0.000\n",
      "iter 1420/10000  loss         0.097003  avg_L1_norm_grad         0.017988  w[0]    2.341 bias    0.000\n",
      "iter 1421/10000  loss         0.096990  avg_L1_norm_grad         0.017969  w[0]    2.342 bias    0.000\n",
      "iter 1440/10000  loss         0.096749  avg_L1_norm_grad         0.017625  w[0]    2.348 bias    0.000\n",
      "iter 1441/10000  loss         0.096737  avg_L1_norm_grad         0.017607  w[0]    2.349 bias    0.000\n",
      "iter 1460/10000  loss         0.096505  avg_L1_norm_grad         0.017272  w[0]    2.355 bias    0.000\n",
      "iter 1461/10000  loss         0.096493  avg_L1_norm_grad         0.017255  w[0]    2.356 bias    0.000\n",
      "iter 1480/10000  loss         0.096271  avg_L1_norm_grad         0.016929  w[0]    2.362 bias    0.000\n",
      "iter 1481/10000  loss         0.096260  avg_L1_norm_grad         0.016912  w[0]    2.363 bias    0.000\n",
      "iter 1500/10000  loss         0.096046  avg_L1_norm_grad         0.016595  w[0]    2.369 bias    0.000\n",
      "iter 1501/10000  loss         0.096035  avg_L1_norm_grad         0.016579  w[0]    2.369 bias    0.000\n",
      "iter 1520/10000  loss         0.095830  avg_L1_norm_grad         0.016270  w[0]    2.375 bias    0.000\n",
      "iter 1521/10000  loss         0.095820  avg_L1_norm_grad         0.016254  w[0]    2.376 bias    0.000\n",
      "iter 1540/10000  loss         0.095623  avg_L1_norm_grad         0.015954  w[0]    2.382 bias    0.000\n",
      "iter 1541/10000  loss         0.095612  avg_L1_norm_grad         0.015939  w[0]    2.382 bias    0.000\n",
      "iter 1560/10000  loss         0.095423  avg_L1_norm_grad         0.015646  w[0]    2.388 bias    0.000\n",
      "iter 1561/10000  loss         0.095413  avg_L1_norm_grad         0.015631  w[0]    2.389 bias    0.000\n",
      "iter 1580/10000  loss         0.095231  avg_L1_norm_grad         0.015346  w[0]    2.394 bias    0.000\n",
      "iter 1581/10000  loss         0.095221  avg_L1_norm_grad         0.015331  w[0]    2.395 bias    0.000\n",
      "iter 1600/10000  loss         0.095046  avg_L1_norm_grad         0.015054  w[0]    2.401 bias    0.000\n",
      "iter 1601/10000  loss         0.095037  avg_L1_norm_grad         0.015040  w[0]    2.401 bias    0.000\n",
      "iter 1620/10000  loss         0.094868  avg_L1_norm_grad         0.014769  w[0]    2.407 bias    0.000\n",
      "iter 1621/10000  loss         0.094859  avg_L1_norm_grad         0.014755  w[0]    2.407 bias    0.000\n",
      "iter 1640/10000  loss         0.094696  avg_L1_norm_grad         0.014492  w[0]    2.412 bias    0.000\n",
      "iter 1641/10000  loss         0.094688  avg_L1_norm_grad         0.014478  w[0]    2.413 bias    0.000\n",
      "iter 1660/10000  loss         0.094531  avg_L1_norm_grad         0.014221  w[0]    2.418 bias    0.000\n",
      "iter 1661/10000  loss         0.094523  avg_L1_norm_grad         0.014208  w[0]    2.418 bias    0.000\n",
      "iter 1680/10000  loss         0.094373  avg_L1_norm_grad         0.013957  w[0]    2.424 bias    0.000\n",
      "iter 1681/10000  loss         0.094365  avg_L1_norm_grad         0.013944  w[0]    2.424 bias    0.000\n",
      "iter 1700/10000  loss         0.094220  avg_L1_norm_grad         0.013700  w[0]    2.429 bias    0.000\n",
      "iter 1701/10000  loss         0.094212  avg_L1_norm_grad         0.013687  w[0]    2.430 bias    0.000\n",
      "iter 1720/10000  loss         0.094072  avg_L1_norm_grad         0.013448  w[0]    2.435 bias    0.000\n",
      "iter 1721/10000  loss         0.094065  avg_L1_norm_grad         0.013436  w[0]    2.435 bias    0.000\n",
      "iter 1740/10000  loss         0.093930  avg_L1_norm_grad         0.013203  w[0]    2.440 bias    0.000\n",
      "iter 1741/10000  loss         0.093923  avg_L1_norm_grad         0.013191  w[0]    2.440 bias    0.000\n",
      "iter 1760/10000  loss         0.093793  avg_L1_norm_grad         0.012964  w[0]    2.445 bias    0.000\n",
      "iter 1761/10000  loss         0.093786  avg_L1_norm_grad         0.012952  w[0]    2.446 bias    0.000\n",
      "iter 1780/10000  loss         0.093661  avg_L1_norm_grad         0.012730  w[0]    2.450 bias    0.000\n",
      "iter 1781/10000  loss         0.093654  avg_L1_norm_grad         0.012719  w[0]    2.451 bias    0.000\n",
      "iter 1800/10000  loss         0.093533  avg_L1_norm_grad         0.012502  w[0]    2.455 bias    0.000\n",
      "iter 1801/10000  loss         0.093527  avg_L1_norm_grad         0.012491  w[0]    2.456 bias    0.000\n",
      "iter 1820/10000  loss         0.093411  avg_L1_norm_grad         0.012279  w[0]    2.460 bias    0.000\n",
      "iter 1821/10000  loss         0.093405  avg_L1_norm_grad         0.012268  w[0]    2.461 bias    0.000\n",
      "iter 1840/10000  loss         0.093292  avg_L1_norm_grad         0.012061  w[0]    2.465 bias    0.000\n",
      "iter 1841/10000  loss         0.093286  avg_L1_norm_grad         0.012050  w[0]    2.466 bias    0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1860/10000  loss         0.093178  avg_L1_norm_grad         0.011848  w[0]    2.470 bias    0.000\n",
      "iter 1861/10000  loss         0.093172  avg_L1_norm_grad         0.011838  w[0]    2.470 bias    0.000\n",
      "iter 1880/10000  loss         0.093067  avg_L1_norm_grad         0.011640  w[0]    2.475 bias    0.000\n",
      "iter 1881/10000  loss         0.093062  avg_L1_norm_grad         0.011630  w[0]    2.475 bias    0.000\n",
      "iter 1900/10000  loss         0.092961  avg_L1_norm_grad         0.011437  w[0]    2.479 bias    0.000\n",
      "iter 1901/10000  loss         0.092956  avg_L1_norm_grad         0.011427  w[0]    2.480 bias    0.000\n",
      "iter 1920/10000  loss         0.092858  avg_L1_norm_grad         0.011238  w[0]    2.484 bias    0.000\n",
      "iter 1921/10000  loss         0.092853  avg_L1_norm_grad         0.011228  w[0]    2.484 bias    0.000\n",
      "iter 1940/10000  loss         0.092759  avg_L1_norm_grad         0.011044  w[0]    2.488 bias    0.000\n",
      "iter 1941/10000  loss         0.092754  avg_L1_norm_grad         0.011034  w[0]    2.489 bias    0.000\n",
      "iter 1960/10000  loss         0.092663  avg_L1_norm_grad         0.010854  w[0]    2.493 bias    0.000\n",
      "iter 1961/10000  loss         0.092658  avg_L1_norm_grad         0.010844  w[0]    2.493 bias    0.000\n",
      "iter 1980/10000  loss         0.092570  avg_L1_norm_grad         0.010668  w[0]    2.497 bias    0.000\n",
      "iter 1981/10000  loss         0.092565  avg_L1_norm_grad         0.010658  w[0]    2.497 bias    0.000\n",
      "iter 2000/10000  loss         0.092480  avg_L1_norm_grad         0.010486  w[0]    2.501 bias    0.000\n",
      "iter 2001/10000  loss         0.092476  avg_L1_norm_grad         0.010477  w[0]    2.502 bias   -0.000\n",
      "iter 2020/10000  loss         0.092394  avg_L1_norm_grad         0.010308  w[0]    2.505 bias    0.000\n",
      "iter 2021/10000  loss         0.092390  avg_L1_norm_grad         0.010299  w[0]    2.506 bias    0.000\n",
      "iter 2040/10000  loss         0.092310  avg_L1_norm_grad         0.010133  w[0]    2.510 bias    0.000\n",
      "iter 2041/10000  loss         0.092306  avg_L1_norm_grad         0.010125  w[0]    2.510 bias    0.000\n",
      "iter 2060/10000  loss         0.092230  avg_L1_norm_grad         0.009963  w[0]    2.514 bias    0.000\n",
      "iter 2061/10000  loss         0.092226  avg_L1_norm_grad         0.009954  w[0]    2.514 bias    0.000\n",
      "iter 2080/10000  loss         0.092151  avg_L1_norm_grad         0.009796  w[0]    2.518 bias    0.000\n",
      "iter 2081/10000  loss         0.092148  avg_L1_norm_grad         0.009787  w[0]    2.518 bias    0.000\n",
      "iter 2100/10000  loss         0.092076  avg_L1_norm_grad         0.009632  w[0]    2.521 bias    0.000\n",
      "iter 2101/10000  loss         0.092072  avg_L1_norm_grad         0.009624  w[0]    2.522 bias    0.000\n",
      "iter 2120/10000  loss         0.092003  avg_L1_norm_grad         0.009472  w[0]    2.525 bias    0.000\n",
      "iter 2121/10000  loss         0.091999  avg_L1_norm_grad         0.009464  w[0]    2.525 bias    0.000\n",
      "iter 2140/10000  loss         0.091932  avg_L1_norm_grad         0.009315  w[0]    2.529 bias    0.000\n",
      "iter 2141/10000  loss         0.091929  avg_L1_norm_grad         0.009307  w[0]    2.529 bias    0.000\n",
      "iter 2160/10000  loss         0.091864  avg_L1_norm_grad         0.009161  w[0]    2.533 bias   -0.000\n",
      "iter 2161/10000  loss         0.091861  avg_L1_norm_grad         0.009154  w[0]    2.533 bias   -0.000\n",
      "iter 2180/10000  loss         0.091798  avg_L1_norm_grad         0.009011  w[0]    2.536 bias   -0.000\n",
      "iter 2181/10000  loss         0.091795  avg_L1_norm_grad         0.009003  w[0]    2.537 bias   -0.000\n",
      "iter 2200/10000  loss         0.091734  avg_L1_norm_grad         0.008863  w[0]    2.540 bias   -0.000\n",
      "iter 2201/10000  loss         0.091731  avg_L1_norm_grad         0.008856  w[0]    2.540 bias   -0.000\n",
      "iter 2220/10000  loss         0.091672  avg_L1_norm_grad         0.008719  w[0]    2.543 bias   -0.000\n",
      "iter 2221/10000  loss         0.091669  avg_L1_norm_grad         0.008712  w[0]    2.544 bias   -0.000\n",
      "iter 2240/10000  loss         0.091612  avg_L1_norm_grad         0.008577  w[0]    2.547 bias   -0.000\n",
      "iter 2241/10000  loss         0.091609  avg_L1_norm_grad         0.008570  w[0]    2.547 bias   -0.000\n",
      "iter 2260/10000  loss         0.091554  avg_L1_norm_grad         0.008438  w[0]    2.550 bias   -0.000\n",
      "iter 2261/10000  loss         0.091551  avg_L1_norm_grad         0.008431  w[0]    2.550 bias   -0.000\n",
      "iter 2280/10000  loss         0.091498  avg_L1_norm_grad         0.008302  w[0]    2.554 bias   -0.000\n",
      "iter 2281/10000  loss         0.091495  avg_L1_norm_grad         0.008295  w[0]    2.554 bias   -0.000\n",
      "iter 2300/10000  loss         0.091444  avg_L1_norm_grad         0.008169  w[0]    2.557 bias   -0.000\n",
      "iter 2301/10000  loss         0.091441  avg_L1_norm_grad         0.008162  w[0]    2.557 bias   -0.000\n",
      "iter 2320/10000  loss         0.091391  avg_L1_norm_grad         0.008038  w[0]    2.560 bias   -0.000\n",
      "iter 2321/10000  loss         0.091389  avg_L1_norm_grad         0.008031  w[0]    2.560 bias   -0.000\n",
      "iter 2340/10000  loss         0.091341  avg_L1_norm_grad         0.007910  w[0]    2.563 bias   -0.000\n",
      "iter 2341/10000  loss         0.091338  avg_L1_norm_grad         0.007903  w[0]    2.564 bias   -0.000\n",
      "iter 2360/10000  loss         0.091291  avg_L1_norm_grad         0.007784  w[0]    2.567 bias   -0.000\n",
      "iter 2361/10000  loss         0.091289  avg_L1_norm_grad         0.007777  w[0]    2.567 bias   -0.000\n",
      "iter 2380/10000  loss         0.091244  avg_L1_norm_grad         0.007660  w[0]    2.570 bias   -0.000\n",
      "iter 2381/10000  loss         0.091241  avg_L1_norm_grad         0.007654  w[0]    2.570 bias   -0.000\n",
      "iter 2400/10000  loss         0.091197  avg_L1_norm_grad         0.007539  w[0]    2.573 bias   -0.000\n",
      "iter 2401/10000  loss         0.091195  avg_L1_norm_grad         0.007533  w[0]    2.573 bias   -0.000\n",
      "iter 2420/10000  loss         0.091153  avg_L1_norm_grad         0.007420  w[0]    2.576 bias   -0.000\n",
      "iter 2421/10000  loss         0.091150  avg_L1_norm_grad         0.007414  w[0]    2.576 bias   -0.000\n",
      "iter 2440/10000  loss         0.091109  avg_L1_norm_grad         0.007304  w[0]    2.579 bias   -0.000\n",
      "iter 2441/10000  loss         0.091107  avg_L1_norm_grad         0.007298  w[0]    2.579 bias   -0.000\n",
      "iter 2460/10000  loss         0.091067  avg_L1_norm_grad         0.007189  w[0]    2.581 bias   -0.000\n",
      "iter 2461/10000  loss         0.091065  avg_L1_norm_grad         0.007184  w[0]    2.582 bias   -0.000\n",
      "iter 2480/10000  loss         0.091026  avg_L1_norm_grad         0.007077  w[0]    2.584 bias   -0.000\n",
      "iter 2481/10000  loss         0.091024  avg_L1_norm_grad         0.007072  w[0]    2.584 bias   -0.000\n",
      "iter 2500/10000  loss         0.090987  avg_L1_norm_grad         0.006967  w[0]    2.587 bias   -0.000\n",
      "iter 2501/10000  loss         0.090985  avg_L1_norm_grad         0.006961  w[0]    2.587 bias   -0.000\n",
      "iter 2520/10000  loss         0.090949  avg_L1_norm_grad         0.006859  w[0]    2.590 bias   -0.000\n",
      "iter 2521/10000  loss         0.090947  avg_L1_norm_grad         0.006853  w[0]    2.590 bias   -0.000\n",
      "iter 2540/10000  loss         0.090912  avg_L1_norm_grad         0.006753  w[0]    2.593 bias   -0.000\n",
      "iter 2541/10000  loss         0.090910  avg_L1_norm_grad         0.006747  w[0]    2.593 bias   -0.000\n",
      "iter 2560/10000  loss         0.090876  avg_L1_norm_grad         0.006648  w[0]    2.595 bias   -0.000\n",
      "iter 2561/10000  loss         0.090874  avg_L1_norm_grad         0.006643  w[0]    2.595 bias   -0.000\n",
      "iter 2580/10000  loss         0.090841  avg_L1_norm_grad         0.006546  w[0]    2.598 bias   -0.000\n",
      "iter 2581/10000  loss         0.090839  avg_L1_norm_grad         0.006541  w[0]    2.598 bias   -0.000\n",
      "iter 2600/10000  loss         0.090807  avg_L1_norm_grad         0.006446  w[0]    2.601 bias   -0.000\n",
      "iter 2601/10000  loss         0.090805  avg_L1_norm_grad         0.006441  w[0]    2.601 bias   -0.000\n",
      "iter 2620/10000  loss         0.090774  avg_L1_norm_grad         0.006347  w[0]    2.603 bias   -0.000\n",
      "iter 2621/10000  loss         0.090773  avg_L1_norm_grad         0.006342  w[0]    2.603 bias   -0.000\n",
      "iter 2640/10000  loss         0.090743  avg_L1_norm_grad         0.006250  w[0]    2.606 bias   -0.000\n",
      "iter 2641/10000  loss         0.090741  avg_L1_norm_grad         0.006245  w[0]    2.606 bias   -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2660/10000  loss         0.090712  avg_L1_norm_grad         0.006155  w[0]    2.608 bias   -0.000\n",
      "iter 2661/10000  loss         0.090710  avg_L1_norm_grad         0.006150  w[0]    2.608 bias   -0.000\n",
      "iter 2680/10000  loss         0.090682  avg_L1_norm_grad         0.006061  w[0]    2.611 bias    0.000\n",
      "iter 2681/10000  loss         0.090681  avg_L1_norm_grad         0.006057  w[0]    2.611 bias    0.000\n",
      "iter 2700/10000  loss         0.090653  avg_L1_norm_grad         0.005969  w[0]    2.613 bias    0.000\n",
      "iter 2701/10000  loss         0.090652  avg_L1_norm_grad         0.005965  w[0]    2.613 bias    0.000\n",
      "iter 2720/10000  loss         0.090625  avg_L1_norm_grad         0.005879  w[0]    2.615 bias   -0.000\n",
      "iter 2721/10000  loss         0.090624  avg_L1_norm_grad         0.005875  w[0]    2.615 bias   -0.000\n",
      "iter 2740/10000  loss         0.090598  avg_L1_norm_grad         0.005791  w[0]    2.618 bias    0.000\n",
      "iter 2741/10000  loss         0.090596  avg_L1_norm_grad         0.005786  w[0]    2.618 bias    0.000\n",
      "iter 2760/10000  loss         0.090571  avg_L1_norm_grad         0.005703  w[0]    2.620 bias    0.000\n",
      "iter 2761/10000  loss         0.090570  avg_L1_norm_grad         0.005699  w[0]    2.620 bias    0.000\n",
      "iter 2780/10000  loss         0.090546  avg_L1_norm_grad         0.005618  w[0]    2.622 bias   -0.000\n",
      "iter 2781/10000  loss         0.090544  avg_L1_norm_grad         0.005614  w[0]    2.622 bias   -0.000\n",
      "iter 2800/10000  loss         0.090521  avg_L1_norm_grad         0.005534  w[0]    2.624 bias   -0.000\n",
      "iter 2801/10000  loss         0.090520  avg_L1_norm_grad         0.005530  w[0]    2.625 bias   -0.000\n",
      "iter 2820/10000  loss         0.090497  avg_L1_norm_grad         0.005451  w[0]    2.627 bias    0.000\n",
      "iter 2821/10000  loss         0.090495  avg_L1_norm_grad         0.005447  w[0]    2.627 bias    0.000\n",
      "iter 2840/10000  loss         0.090473  avg_L1_norm_grad         0.005370  w[0]    2.629 bias    0.000\n",
      "iter 2841/10000  loss         0.090472  avg_L1_norm_grad         0.005366  w[0]    2.629 bias    0.000\n",
      "iter 2860/10000  loss         0.090450  avg_L1_norm_grad         0.005290  w[0]    2.631 bias   -0.000\n",
      "iter 2861/10000  loss         0.090449  avg_L1_norm_grad         0.005286  w[0]    2.631 bias    0.000\n",
      "iter 2880/10000  loss         0.090428  avg_L1_norm_grad         0.005211  w[0]    2.633 bias   -0.000\n",
      "iter 2881/10000  loss         0.090427  avg_L1_norm_grad         0.005208  w[0]    2.633 bias   -0.000\n",
      "iter 2900/10000  loss         0.090407  avg_L1_norm_grad         0.005134  w[0]    2.635 bias   -0.000\n",
      "iter 2901/10000  loss         0.090406  avg_L1_norm_grad         0.005130  w[0]    2.635 bias   -0.000\n",
      "iter 2920/10000  loss         0.090386  avg_L1_norm_grad         0.005058  w[0]    2.637 bias   -0.000\n",
      "iter 2921/10000  loss         0.090385  avg_L1_norm_grad         0.005055  w[0]    2.637 bias   -0.000\n",
      "iter 2940/10000  loss         0.090366  avg_L1_norm_grad         0.004984  w[0]    2.639 bias    0.000\n",
      "iter 2941/10000  loss         0.090365  avg_L1_norm_grad         0.004980  w[0]    2.639 bias    0.000\n",
      "iter 2960/10000  loss         0.090346  avg_L1_norm_grad         0.004911  w[0]    2.641 bias    0.000\n",
      "iter 2961/10000  loss         0.090345  avg_L1_norm_grad         0.004907  w[0]    2.641 bias    0.000\n",
      "iter 2980/10000  loss         0.090327  avg_L1_norm_grad         0.004838  w[0]    2.643 bias    0.000\n",
      "iter 2981/10000  loss         0.090326  avg_L1_norm_grad         0.004835  w[0]    2.643 bias    0.000\n",
      "iter 3000/10000  loss         0.090309  avg_L1_norm_grad         0.004767  w[0]    2.645 bias    0.000\n",
      "iter 3001/10000  loss         0.090308  avg_L1_norm_grad         0.004764  w[0]    2.645 bias    0.000\n",
      "iter 3020/10000  loss         0.090291  avg_L1_norm_grad         0.004698  w[0]    2.647 bias    0.000\n",
      "iter 3021/10000  loss         0.090290  avg_L1_norm_grad         0.004694  w[0]    2.647 bias    0.000\n",
      "iter 3040/10000  loss         0.090274  avg_L1_norm_grad         0.004629  w[0]    2.649 bias    0.000\n",
      "iter 3041/10000  loss         0.090273  avg_L1_norm_grad         0.004626  w[0]    2.649 bias    0.000\n",
      "iter 3060/10000  loss         0.090257  avg_L1_norm_grad         0.004562  w[0]    2.651 bias    0.000\n",
      "iter 3061/10000  loss         0.090256  avg_L1_norm_grad         0.004558  w[0]    2.651 bias    0.000\n",
      "iter 3080/10000  loss         0.090240  avg_L1_norm_grad         0.004495  w[0]    2.652 bias    0.000\n",
      "iter 3081/10000  loss         0.090239  avg_L1_norm_grad         0.004492  w[0]    2.653 bias    0.000\n",
      "iter 3100/10000  loss         0.090224  avg_L1_norm_grad         0.004430  w[0]    2.654 bias    0.000\n",
      "iter 3101/10000  loss         0.090224  avg_L1_norm_grad         0.004427  w[0]    2.654 bias    0.000\n",
      "iter 3120/10000  loss         0.090209  avg_L1_norm_grad         0.004366  w[0]    2.656 bias    0.000\n",
      "iter 3121/10000  loss         0.090208  avg_L1_norm_grad         0.004363  w[0]    2.656 bias    0.000\n",
      "iter 3140/10000  loss         0.090194  avg_L1_norm_grad         0.004303  w[0]    2.658 bias    0.000\n",
      "iter 3141/10000  loss         0.090193  avg_L1_norm_grad         0.004300  w[0]    2.658 bias    0.000\n",
      "iter 3160/10000  loss         0.090179  avg_L1_norm_grad         0.004241  w[0]    2.659 bias    0.000\n",
      "iter 3161/10000  loss         0.090179  avg_L1_norm_grad         0.004238  w[0]    2.660 bias    0.000\n",
      "iter 3180/10000  loss         0.090165  avg_L1_norm_grad         0.004180  w[0]    2.661 bias    0.000\n",
      "iter 3181/10000  loss         0.090164  avg_L1_norm_grad         0.004177  w[0]    2.661 bias    0.000\n",
      "iter 3200/10000  loss         0.090151  avg_L1_norm_grad         0.004119  w[0]    2.663 bias    0.000\n",
      "iter 3201/10000  loss         0.090151  avg_L1_norm_grad         0.004116  w[0]    2.663 bias    0.000\n",
      "iter 3220/10000  loss         0.090138  avg_L1_norm_grad         0.004060  w[0]    2.664 bias    0.000\n",
      "iter 3221/10000  loss         0.090137  avg_L1_norm_grad         0.004057  w[0]    2.665 bias    0.000\n",
      "iter 3240/10000  loss         0.090125  avg_L1_norm_grad         0.004002  w[0]    2.666 bias    0.000\n",
      "iter 3241/10000  loss         0.090124  avg_L1_norm_grad         0.003999  w[0]    2.666 bias    0.000\n",
      "iter 3260/10000  loss         0.090112  avg_L1_norm_grad         0.003945  w[0]    2.668 bias    0.000\n",
      "iter 3261/10000  loss         0.090112  avg_L1_norm_grad         0.003942  w[0]    2.668 bias    0.000\n",
      "iter 3280/10000  loss         0.090100  avg_L1_norm_grad         0.003888  w[0]    2.669 bias    0.000\n",
      "iter 3281/10000  loss         0.090099  avg_L1_norm_grad         0.003886  w[0]    2.669 bias    0.000\n",
      "iter 3300/10000  loss         0.090088  avg_L1_norm_grad         0.003833  w[0]    2.671 bias   -0.000\n",
      "iter 3301/10000  loss         0.090087  avg_L1_norm_grad         0.003830  w[0]    2.671 bias   -0.000\n",
      "iter 3320/10000  loss         0.090076  avg_L1_norm_grad         0.003778  w[0]    2.672 bias   -0.000\n",
      "iter 3321/10000  loss         0.090076  avg_L1_norm_grad         0.003776  w[0]    2.672 bias   -0.000\n",
      "iter 3340/10000  loss         0.090065  avg_L1_norm_grad         0.003724  w[0]    2.674 bias    0.000\n",
      "iter 3341/10000  loss         0.090065  avg_L1_norm_grad         0.003722  w[0]    2.674 bias    0.000\n",
      "iter 3360/10000  loss         0.090054  avg_L1_norm_grad         0.003672  w[0]    2.675 bias   -0.000\n",
      "iter 3361/10000  loss         0.090054  avg_L1_norm_grad         0.003669  w[0]    2.675 bias   -0.000\n",
      "iter 3380/10000  loss         0.090044  avg_L1_norm_grad         0.003619  w[0]    2.677 bias   -0.000\n",
      "iter 3381/10000  loss         0.090043  avg_L1_norm_grad         0.003617  w[0]    2.677 bias   -0.000\n",
      "iter 3400/10000  loss         0.090033  avg_L1_norm_grad         0.003568  w[0]    2.678 bias   -0.000\n",
      "iter 3401/10000  loss         0.090033  avg_L1_norm_grad         0.003566  w[0]    2.678 bias   -0.000\n",
      "iter 3420/10000  loss         0.090023  avg_L1_norm_grad         0.003518  w[0]    2.680 bias   -0.000\n",
      "iter 3421/10000  loss         0.090023  avg_L1_norm_grad         0.003515  w[0]    2.680 bias   -0.000\n",
      "iter 3440/10000  loss         0.090013  avg_L1_norm_grad         0.003468  w[0]    2.681 bias   -0.000\n",
      "iter 3441/10000  loss         0.090013  avg_L1_norm_grad         0.003466  w[0]    2.681 bias   -0.000\n",
      "iter 3460/10000  loss         0.090004  avg_L1_norm_grad         0.003419  w[0]    2.682 bias   -0.000\n",
      "iter 3461/10000  loss         0.090004  avg_L1_norm_grad         0.003417  w[0]    2.682 bias   -0.000\n",
      "iter 3480/10000  loss         0.089995  avg_L1_norm_grad         0.003371  w[0]    2.684 bias   -0.000\n",
      "iter 3481/10000  loss         0.089994  avg_L1_norm_grad         0.003369  w[0]    2.684 bias   -0.000\n",
      "iter 3500/10000  loss         0.089986  avg_L1_norm_grad         0.003324  w[0]    2.685 bias   -0.000\n",
      "iter 3501/10000  loss         0.089985  avg_L1_norm_grad         0.003321  w[0]    2.685 bias   -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3520/10000  loss         0.089977  avg_L1_norm_grad         0.003277  w[0]    2.686 bias   -0.000\n",
      "iter 3521/10000  loss         0.089977  avg_L1_norm_grad         0.003275  w[0]    2.686 bias   -0.000\n",
      "iter 3540/10000  loss         0.089969  avg_L1_norm_grad         0.003231  w[0]    2.688 bias   -0.000\n",
      "iter 3541/10000  loss         0.089968  avg_L1_norm_grad         0.003229  w[0]    2.688 bias   -0.000\n",
      "iter 3560/10000  loss         0.089960  avg_L1_norm_grad         0.003186  w[0]    2.689 bias   -0.000\n",
      "iter 3561/10000  loss         0.089960  avg_L1_norm_grad         0.003183  w[0]    2.689 bias   -0.000\n",
      "iter 3580/10000  loss         0.089952  avg_L1_norm_grad         0.003141  w[0]    2.690 bias   -0.000\n",
      "iter 3581/10000  loss         0.089952  avg_L1_norm_grad         0.003139  w[0]    2.690 bias   -0.000\n",
      "iter 3600/10000  loss         0.089945  avg_L1_norm_grad         0.003097  w[0]    2.691 bias   -0.000\n",
      "iter 3601/10000  loss         0.089944  avg_L1_norm_grad         0.003095  w[0]    2.692 bias   -0.000\n",
      "iter 3620/10000  loss         0.089937  avg_L1_norm_grad         0.003054  w[0]    2.693 bias   -0.000\n",
      "iter 3621/10000  loss         0.089937  avg_L1_norm_grad         0.003052  w[0]    2.693 bias   -0.000\n",
      "iter 3640/10000  loss         0.089930  avg_L1_norm_grad         0.003011  w[0]    2.694 bias   -0.000\n",
      "iter 3641/10000  loss         0.089929  avg_L1_norm_grad         0.003009  w[0]    2.694 bias   -0.000\n",
      "iter 3660/10000  loss         0.089922  avg_L1_norm_grad         0.002970  w[0]    2.695 bias   -0.000\n",
      "iter 3661/10000  loss         0.089922  avg_L1_norm_grad         0.002967  w[0]    2.695 bias   -0.000\n",
      "iter 3680/10000  loss         0.089916  avg_L1_norm_grad         0.002928  w[0]    2.696 bias   -0.000\n",
      "iter 3681/10000  loss         0.089915  avg_L1_norm_grad         0.002926  w[0]    2.696 bias   -0.000\n",
      "iter 3700/10000  loss         0.089909  avg_L1_norm_grad         0.002888  w[0]    2.697 bias   -0.000\n",
      "iter 3701/10000  loss         0.089908  avg_L1_norm_grad         0.002886  w[0]    2.697 bias   -0.000\n",
      "iter 3720/10000  loss         0.089902  avg_L1_norm_grad         0.002848  w[0]    2.699 bias   -0.000\n",
      "iter 3721/10000  loss         0.089902  avg_L1_norm_grad         0.002846  w[0]    2.699 bias   -0.000\n",
      "iter 3740/10000  loss         0.089896  avg_L1_norm_grad         0.002808  w[0]    2.700 bias   -0.000\n",
      "iter 3741/10000  loss         0.089895  avg_L1_norm_grad         0.002806  w[0]    2.700 bias   -0.000\n",
      "iter 3760/10000  loss         0.089890  avg_L1_norm_grad         0.002769  w[0]    2.701 bias   -0.000\n",
      "iter 3761/10000  loss         0.089889  avg_L1_norm_grad         0.002767  w[0]    2.701 bias   -0.000\n",
      "iter 3780/10000  loss         0.089884  avg_L1_norm_grad         0.002731  w[0]    2.702 bias   -0.000\n",
      "iter 3781/10000  loss         0.089883  avg_L1_norm_grad         0.002729  w[0]    2.702 bias   -0.000\n",
      "iter 3800/10000  loss         0.089878  avg_L1_norm_grad         0.002693  w[0]    2.703 bias   -0.000\n",
      "iter 3801/10000  loss         0.089877  avg_L1_norm_grad         0.002691  w[0]    2.703 bias   -0.000\n",
      "iter 3820/10000  loss         0.089872  avg_L1_norm_grad         0.002656  w[0]    2.704 bias   -0.000\n",
      "iter 3821/10000  loss         0.089872  avg_L1_norm_grad         0.002654  w[0]    2.704 bias   -0.000\n",
      "iter 3840/10000  loss         0.089866  avg_L1_norm_grad         0.002620  w[0]    2.705 bias   -0.000\n",
      "iter 3841/10000  loss         0.089866  avg_L1_norm_grad         0.002618  w[0]    2.705 bias   -0.000\n",
      "iter 3860/10000  loss         0.089861  avg_L1_norm_grad         0.002583  w[0]    2.706 bias   -0.000\n",
      "iter 3861/10000  loss         0.089861  avg_L1_norm_grad         0.002582  w[0]    2.706 bias   -0.000\n",
      "iter 3880/10000  loss         0.089856  avg_L1_norm_grad         0.002548  w[0]    2.707 bias   -0.000\n",
      "iter 3881/10000  loss         0.089855  avg_L1_norm_grad         0.002546  w[0]    2.707 bias   -0.000\n",
      "iter 3900/10000  loss         0.089851  avg_L1_norm_grad         0.002513  w[0]    2.708 bias   -0.000\n",
      "iter 3901/10000  loss         0.089850  avg_L1_norm_grad         0.002511  w[0]    2.708 bias   -0.000\n",
      "iter 3920/10000  loss         0.089846  avg_L1_norm_grad         0.002478  w[0]    2.709 bias   -0.000\n",
      "iter 3921/10000  loss         0.089845  avg_L1_norm_grad         0.002477  w[0]    2.709 bias   -0.000\n",
      "iter 3940/10000  loss         0.089841  avg_L1_norm_grad         0.002444  w[0]    2.710 bias   -0.000\n",
      "iter 3941/10000  loss         0.089840  avg_L1_norm_grad         0.002443  w[0]    2.710 bias   -0.000\n",
      "iter 3960/10000  loss         0.089836  avg_L1_norm_grad         0.002411  w[0]    2.711 bias   -0.000\n",
      "iter 3961/10000  loss         0.089836  avg_L1_norm_grad         0.002409  w[0]    2.711 bias   -0.000\n",
      "iter 3980/10000  loss         0.089831  avg_L1_norm_grad         0.002378  w[0]    2.712 bias   -0.000\n",
      "iter 3981/10000  loss         0.089831  avg_L1_norm_grad         0.002376  w[0]    2.712 bias   -0.000\n",
      "iter 4000/10000  loss         0.089827  avg_L1_norm_grad         0.002346  w[0]    2.713 bias   -0.000\n",
      "iter 4001/10000  loss         0.089827  avg_L1_norm_grad         0.002344  w[0]    2.713 bias   -0.000\n",
      "iter 4020/10000  loss         0.089823  avg_L1_norm_grad         0.002314  w[0]    2.714 bias   -0.000\n",
      "iter 4021/10000  loss         0.089822  avg_L1_norm_grad         0.002312  w[0]    2.714 bias   -0.000\n",
      "iter 4040/10000  loss         0.089818  avg_L1_norm_grad         0.002282  w[0]    2.715 bias   -0.000\n",
      "iter 4041/10000  loss         0.089818  avg_L1_norm_grad         0.002280  w[0]    2.715 bias   -0.000\n",
      "iter 4060/10000  loss         0.089814  avg_L1_norm_grad         0.002251  w[0]    2.716 bias   -0.000\n",
      "iter 4061/10000  loss         0.089814  avg_L1_norm_grad         0.002249  w[0]    2.716 bias   -0.000\n",
      "iter 4080/10000  loss         0.089810  avg_L1_norm_grad         0.002220  w[0]    2.717 bias   -0.000\n",
      "iter 4081/10000  loss         0.089810  avg_L1_norm_grad         0.002219  w[0]    2.717 bias   -0.000\n",
      "iter 4100/10000  loss         0.089806  avg_L1_norm_grad         0.002190  w[0]    2.718 bias   -0.000\n",
      "iter 4101/10000  loss         0.089806  avg_L1_norm_grad         0.002189  w[0]    2.718 bias   -0.000\n",
      "iter 4120/10000  loss         0.089803  avg_L1_norm_grad         0.002160  w[0]    2.718 bias   -0.000\n",
      "iter 4121/10000  loss         0.089802  avg_L1_norm_grad         0.002159  w[0]    2.719 bias   -0.000\n",
      "iter 4140/10000  loss         0.089799  avg_L1_norm_grad         0.002131  w[0]    2.719 bias   -0.000\n",
      "iter 4141/10000  loss         0.089799  avg_L1_norm_grad         0.002129  w[0]    2.719 bias   -0.000\n",
      "iter 4160/10000  loss         0.089795  avg_L1_norm_grad         0.002102  w[0]    2.720 bias   -0.000\n",
      "iter 4161/10000  loss         0.089795  avg_L1_norm_grad         0.002101  w[0]    2.720 bias   -0.000\n",
      "iter 4180/10000  loss         0.089792  avg_L1_norm_grad         0.002073  w[0]    2.721 bias   -0.000\n",
      "iter 4181/10000  loss         0.089792  avg_L1_norm_grad         0.002072  w[0]    2.721 bias   -0.000\n",
      "iter 4200/10000  loss         0.089788  avg_L1_norm_grad         0.002045  w[0]    2.722 bias   -0.000\n",
      "iter 4201/10000  loss         0.089788  avg_L1_norm_grad         0.002044  w[0]    2.722 bias   -0.000\n",
      "iter 4220/10000  loss         0.089785  avg_L1_norm_grad         0.002018  w[0]    2.723 bias   -0.000\n",
      "iter 4221/10000  loss         0.089785  avg_L1_norm_grad         0.002016  w[0]    2.723 bias   -0.000\n",
      "iter 4240/10000  loss         0.089782  avg_L1_norm_grad         0.001990  w[0]    2.723 bias   -0.000\n",
      "iter 4241/10000  loss         0.089782  avg_L1_norm_grad         0.001989  w[0]    2.724 bias   -0.000\n",
      "iter 4260/10000  loss         0.089779  avg_L1_norm_grad         0.001964  w[0]    2.724 bias   -0.000\n",
      "iter 4261/10000  loss         0.089779  avg_L1_norm_grad         0.001962  w[0]    2.724 bias   -0.000\n",
      "iter 4280/10000  loss         0.089776  avg_L1_norm_grad         0.001937  w[0]    2.725 bias   -0.000\n",
      "iter 4281/10000  loss         0.089776  avg_L1_norm_grad         0.001936  w[0]    2.725 bias   -0.000\n",
      "iter 4300/10000  loss         0.089773  avg_L1_norm_grad         0.001911  w[0]    2.726 bias   -0.000\n",
      "iter 4301/10000  loss         0.089773  avg_L1_norm_grad         0.001910  w[0]    2.726 bias   -0.000\n",
      "iter 4320/10000  loss         0.089770  avg_L1_norm_grad         0.001885  w[0]    2.727 bias   -0.000\n",
      "iter 4321/10000  loss         0.089770  avg_L1_norm_grad         0.001884  w[0]    2.727 bias   -0.000\n",
      "iter 4340/10000  loss         0.089767  avg_L1_norm_grad         0.001860  w[0]    2.727 bias   -0.000\n",
      "iter 4341/10000  loss         0.089767  avg_L1_norm_grad         0.001858  w[0]    2.727 bias   -0.000\n",
      "iter 4360/10000  loss         0.089764  avg_L1_norm_grad         0.001835  w[0]    2.728 bias   -0.000\n",
      "iter 4361/10000  loss         0.089764  avg_L1_norm_grad         0.001833  w[0]    2.728 bias   -0.000\n",
      "iter 4380/10000  loss         0.089762  avg_L1_norm_grad         0.001810  w[0]    2.729 bias   -0.000\n",
      "iter 4381/10000  loss         0.089762  avg_L1_norm_grad         0.001809  w[0]    2.729 bias   -0.000\n",
      "iter 4400/10000  loss         0.089759  avg_L1_norm_grad         0.001786  w[0]    2.730 bias   -0.000\n",
      "iter 4401/10000  loss         0.089759  avg_L1_norm_grad         0.001784  w[0]    2.730 bias   -0.000\n",
      "iter 4420/10000  loss         0.089757  avg_L1_norm_grad         0.001762  w[0]    2.730 bias   -0.000\n",
      "iter 4421/10000  loss         0.089756  avg_L1_norm_grad         0.001761  w[0]    2.730 bias   -0.000\n",
      "iter 4440/10000  loss         0.089754  avg_L1_norm_grad         0.001738  w[0]    2.731 bias   -0.000\n",
      "iter 4441/10000  loss         0.089754  avg_L1_norm_grad         0.001737  w[0]    2.731 bias   -0.000\n",
      "iter 4460/10000  loss         0.089752  avg_L1_norm_grad         0.001715  w[0]    2.732 bias   -0.000\n",
      "iter 4461/10000  loss         0.089752  avg_L1_norm_grad         0.001714  w[0]    2.732 bias   -0.000\n",
      "iter 4480/10000  loss         0.089749  avg_L1_norm_grad         0.001692  w[0]    2.732 bias   -0.000\n",
      "iter 4481/10000  loss         0.089749  avg_L1_norm_grad         0.001691  w[0]    2.732 bias   -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4500/10000  loss         0.089747  avg_L1_norm_grad         0.001669  w[0]    2.733 bias   -0.000\n",
      "iter 4501/10000  loss         0.089747  avg_L1_norm_grad         0.001668  w[0]    2.733 bias   -0.000\n",
      "iter 4520/10000  loss         0.089745  avg_L1_norm_grad         0.001647  w[0]    2.734 bias   -0.000\n",
      "iter 4521/10000  loss         0.089745  avg_L1_norm_grad         0.001646  w[0]    2.734 bias   -0.000\n",
      "iter 4540/10000  loss         0.089743  avg_L1_norm_grad         0.001625  w[0]    2.734 bias   -0.000\n",
      "iter 4541/10000  loss         0.089743  avg_L1_norm_grad         0.001624  w[0]    2.734 bias   -0.000\n",
      "iter 4560/10000  loss         0.089741  avg_L1_norm_grad         0.001603  w[0]    2.735 bias   -0.000\n",
      "iter 4561/10000  loss         0.089741  avg_L1_norm_grad         0.001602  w[0]    2.735 bias   -0.000\n",
      "iter 4580/10000  loss         0.089739  avg_L1_norm_grad         0.001582  w[0]    2.736 bias   -0.000\n",
      "iter 4581/10000  loss         0.089739  avg_L1_norm_grad         0.001581  w[0]    2.736 bias   -0.000\n",
      "iter 4600/10000  loss         0.089737  avg_L1_norm_grad         0.001560  w[0]    2.736 bias   -0.000\n",
      "iter 4601/10000  loss         0.089737  avg_L1_norm_grad         0.001559  w[0]    2.736 bias   -0.000\n",
      "iter 4620/10000  loss         0.089735  avg_L1_norm_grad         0.001540  w[0]    2.737 bias   -0.000\n",
      "iter 4621/10000  loss         0.089735  avg_L1_norm_grad         0.001539  w[0]    2.737 bias   -0.000\n",
      "iter 4640/10000  loss         0.089733  avg_L1_norm_grad         0.001519  w[0]    2.737 bias   -0.000\n",
      "iter 4641/10000  loss         0.089733  avg_L1_norm_grad         0.001518  w[0]    2.737 bias   -0.000\n",
      "iter 4660/10000  loss         0.089731  avg_L1_norm_grad         0.001499  w[0]    2.738 bias   -0.000\n",
      "iter 4661/10000  loss         0.089731  avg_L1_norm_grad         0.001498  w[0]    2.738 bias   -0.000\n",
      "iter 4680/10000  loss         0.089729  avg_L1_norm_grad         0.001479  w[0]    2.739 bias   -0.000\n",
      "iter 4681/10000  loss         0.089729  avg_L1_norm_grad         0.001478  w[0]    2.739 bias   -0.000\n",
      "iter 4700/10000  loss         0.089728  avg_L1_norm_grad         0.001459  w[0]    2.739 bias   -0.000\n",
      "iter 4701/10000  loss         0.089728  avg_L1_norm_grad         0.001458  w[0]    2.739 bias   -0.000\n",
      "iter 4720/10000  loss         0.089726  avg_L1_norm_grad         0.001440  w[0]    2.740 bias   -0.000\n",
      "iter 4721/10000  loss         0.089726  avg_L1_norm_grad         0.001439  w[0]    2.740 bias   -0.000\n",
      "iter 4740/10000  loss         0.089724  avg_L1_norm_grad         0.001421  w[0]    2.740 bias   -0.000\n",
      "iter 4741/10000  loss         0.089724  avg_L1_norm_grad         0.001420  w[0]    2.740 bias   -0.000\n",
      "iter 4760/10000  loss         0.089723  avg_L1_norm_grad         0.001402  w[0]    2.741 bias   -0.000\n",
      "iter 4761/10000  loss         0.089723  avg_L1_norm_grad         0.001401  w[0]    2.741 bias   -0.000\n",
      "iter 4780/10000  loss         0.089721  avg_L1_norm_grad         0.001383  w[0]    2.741 bias   -0.000\n",
      "iter 4781/10000  loss         0.089721  avg_L1_norm_grad         0.001382  w[0]    2.742 bias   -0.000\n",
      "iter 4800/10000  loss         0.089720  avg_L1_norm_grad         0.001365  w[0]    2.742 bias   -0.000\n",
      "iter 4801/10000  loss         0.089720  avg_L1_norm_grad         0.001364  w[0]    2.742 bias   -0.000\n",
      "iter 4820/10000  loss         0.089718  avg_L1_norm_grad         0.001347  w[0]    2.743 bias   -0.000\n",
      "iter 4821/10000  loss         0.089718  avg_L1_norm_grad         0.001346  w[0]    2.743 bias   -0.000\n",
      "iter 4840/10000  loss         0.089717  avg_L1_norm_grad         0.001329  w[0]    2.743 bias   -0.000\n",
      "iter 4841/10000  loss         0.089717  avg_L1_norm_grad         0.001328  w[0]    2.743 bias   -0.000\n",
      "iter 4860/10000  loss         0.089715  avg_L1_norm_grad         0.001311  w[0]    2.744 bias   -0.000\n",
      "iter 4861/10000  loss         0.089715  avg_L1_norm_grad         0.001310  w[0]    2.744 bias   -0.000\n",
      "iter 4880/10000  loss         0.089714  avg_L1_norm_grad         0.001294  w[0]    2.744 bias   -0.000\n",
      "iter 4881/10000  loss         0.089714  avg_L1_norm_grad         0.001293  w[0]    2.744 bias   -0.000\n",
      "iter 4900/10000  loss         0.089713  avg_L1_norm_grad         0.001277  w[0]    2.745 bias   -0.000\n",
      "iter 4901/10000  loss         0.089713  avg_L1_norm_grad         0.001276  w[0]    2.745 bias   -0.000\n",
      "iter 4920/10000  loss         0.089711  avg_L1_norm_grad         0.001260  w[0]    2.745 bias   -0.000\n",
      "iter 4921/10000  loss         0.089711  avg_L1_norm_grad         0.001259  w[0]    2.745 bias   -0.000\n",
      "iter 4940/10000  loss         0.089710  avg_L1_norm_grad         0.001243  w[0]    2.746 bias   -0.000\n",
      "iter 4941/10000  loss         0.089710  avg_L1_norm_grad         0.001242  w[0]    2.746 bias   -0.000\n",
      "iter 4960/10000  loss         0.089709  avg_L1_norm_grad         0.001227  w[0]    2.746 bias   -0.000\n",
      "iter 4961/10000  loss         0.089709  avg_L1_norm_grad         0.001226  w[0]    2.746 bias   -0.000\n",
      "iter 4980/10000  loss         0.089708  avg_L1_norm_grad         0.001211  w[0]    2.747 bias   -0.000\n",
      "iter 4981/10000  loss         0.089708  avg_L1_norm_grad         0.001210  w[0]    2.747 bias   -0.000\n",
      "iter 5000/10000  loss         0.089707  avg_L1_norm_grad         0.001195  w[0]    2.747 bias   -0.000\n",
      "iter 5001/10000  loss         0.089707  avg_L1_norm_grad         0.001194  w[0]    2.747 bias   -0.000\n",
      "iter 5020/10000  loss         0.089705  avg_L1_norm_grad         0.001179  w[0]    2.748 bias   -0.000\n",
      "iter 5021/10000  loss         0.089705  avg_L1_norm_grad         0.001178  w[0]    2.748 bias   -0.000\n",
      "iter 5040/10000  loss         0.089704  avg_L1_norm_grad         0.001163  w[0]    2.748 bias   -0.000\n",
      "iter 5041/10000  loss         0.089704  avg_L1_norm_grad         0.001163  w[0]    2.748 bias   -0.000\n",
      "iter 5060/10000  loss         0.089703  avg_L1_norm_grad         0.001148  w[0]    2.749 bias   -0.000\n",
      "iter 5061/10000  loss         0.089703  avg_L1_norm_grad         0.001147  w[0]    2.749 bias   -0.000\n",
      "iter 5080/10000  loss         0.089702  avg_L1_norm_grad         0.001133  w[0]    2.749 bias   -0.000\n",
      "iter 5081/10000  loss         0.089702  avg_L1_norm_grad         0.001132  w[0]    2.749 bias   -0.000\n",
      "iter 5100/10000  loss         0.089701  avg_L1_norm_grad         0.001118  w[0]    2.749 bias   -0.000\n",
      "iter 5101/10000  loss         0.089701  avg_L1_norm_grad         0.001117  w[0]    2.749 bias   -0.000\n",
      "iter 5120/10000  loss         0.089700  avg_L1_norm_grad         0.001103  w[0]    2.750 bias   -0.000\n",
      "iter 5121/10000  loss         0.089700  avg_L1_norm_grad         0.001102  w[0]    2.750 bias   -0.000\n",
      "iter 5140/10000  loss         0.089699  avg_L1_norm_grad         0.001089  w[0]    2.750 bias   -0.000\n",
      "iter 5141/10000  loss         0.089699  avg_L1_norm_grad         0.001088  w[0]    2.750 bias   -0.000\n",
      "iter 5160/10000  loss         0.089698  avg_L1_norm_grad         0.001074  w[0]    2.751 bias   -0.000\n",
      "iter 5161/10000  loss         0.089698  avg_L1_norm_grad         0.001074  w[0]    2.751 bias   -0.000\n",
      "iter 5180/10000  loss         0.089697  avg_L1_norm_grad         0.001060  w[0]    2.751 bias   -0.000\n",
      "iter 5181/10000  loss         0.089697  avg_L1_norm_grad         0.001059  w[0]    2.751 bias   -0.000\n",
      "iter 5200/10000  loss         0.089697  avg_L1_norm_grad         0.001046  w[0]    2.752 bias   -0.000\n",
      "iter 5201/10000  loss         0.089696  avg_L1_norm_grad         0.001046  w[0]    2.752 bias   -0.000\n",
      "iter 5220/10000  loss         0.089696  avg_L1_norm_grad         0.001033  w[0]    2.752 bias   -0.000\n",
      "iter 5221/10000  loss         0.089696  avg_L1_norm_grad         0.001032  w[0]    2.752 bias   -0.000\n",
      "iter 5240/10000  loss         0.089695  avg_L1_norm_grad         0.001019  w[0]    2.752 bias   -0.000\n",
      "iter 5241/10000  loss         0.089695  avg_L1_norm_grad         0.001018  w[0]    2.752 bias   -0.000\n",
      "iter 5260/10000  loss         0.089694  avg_L1_norm_grad         0.001006  w[0]    2.753 bias   -0.000\n",
      "iter 5261/10000  loss         0.089694  avg_L1_norm_grad         0.001005  w[0]    2.753 bias   -0.000\n",
      "iter 5280/10000  loss         0.089693  avg_L1_norm_grad         0.000992  w[0]    2.753 bias   -0.000\n",
      "iter 5281/10000  loss         0.089693  avg_L1_norm_grad         0.000992  w[0]    2.753 bias   -0.000\n",
      "iter 5300/10000  loss         0.089692  avg_L1_norm_grad         0.000979  w[0]    2.754 bias   -0.000\n",
      "iter 5301/10000  loss         0.089692  avg_L1_norm_grad         0.000979  w[0]    2.754 bias   -0.000\n",
      "iter 5320/10000  loss         0.089692  avg_L1_norm_grad         0.000967  w[0]    2.754 bias   -0.000\n",
      "iter 5321/10000  loss         0.089692  avg_L1_norm_grad         0.000966  w[0]    2.754 bias   -0.000\n",
      "iter 5340/10000  loss         0.089691  avg_L1_norm_grad         0.000954  w[0]    2.754 bias   -0.000\n",
      "iter 5341/10000  loss         0.089691  avg_L1_norm_grad         0.000953  w[0]    2.754 bias   -0.000\n",
      "iter 5360/10000  loss         0.089690  avg_L1_norm_grad         0.000941  w[0]    2.755 bias   -0.000\n",
      "iter 5361/10000  loss         0.089690  avg_L1_norm_grad         0.000941  w[0]    2.755 bias   -0.000\n",
      "iter 5380/10000  loss         0.089690  avg_L1_norm_grad         0.000929  w[0]    2.755 bias   -0.000\n",
      "iter 5381/10000  loss         0.089689  avg_L1_norm_grad         0.000928  w[0]    2.755 bias   -0.000\n",
      "iter 5400/10000  loss         0.089689  avg_L1_norm_grad         0.000917  w[0]    2.756 bias   -0.000\n",
      "iter 5401/10000  loss         0.089689  avg_L1_norm_grad         0.000916  w[0]    2.756 bias   -0.000\n",
      "iter 5420/10000  loss         0.089688  avg_L1_norm_grad         0.000905  w[0]    2.756 bias   -0.000\n",
      "iter 5421/10000  loss         0.089688  avg_L1_norm_grad         0.000904  w[0]    2.756 bias   -0.000\n",
      "iter 5440/10000  loss         0.089688  avg_L1_norm_grad         0.000893  w[0]    2.756 bias   -0.000\n",
      "iter 5441/10000  loss         0.089687  avg_L1_norm_grad         0.000892  w[0]    2.756 bias   -0.000\n",
      "iter 5460/10000  loss         0.089687  avg_L1_norm_grad         0.000881  w[0]    2.757 bias   -0.000\n",
      "iter 5461/10000  loss         0.089687  avg_L1_norm_grad         0.000881  w[0]    2.757 bias   -0.000\n",
      "iter 5480/10000  loss         0.089686  avg_L1_norm_grad         0.000870  w[0]    2.757 bias   -0.000\n",
      "iter 5481/10000  loss         0.089686  avg_L1_norm_grad         0.000869  w[0]    2.757 bias   -0.000\n",
      "iter 5500/10000  loss         0.089686  avg_L1_norm_grad         0.000858  w[0]    2.757 bias   -0.000\n",
      "iter 5501/10000  loss         0.089686  avg_L1_norm_grad         0.000858  w[0]    2.757 bias   -0.000\n",
      "iter 5520/10000  loss         0.089685  avg_L1_norm_grad         0.000847  w[0]    2.758 bias   -0.000\n",
      "iter 5521/10000  loss         0.089685  avg_L1_norm_grad         0.000847  w[0]    2.758 bias   -0.000\n",
      "iter 5540/10000  loss         0.089685  avg_L1_norm_grad         0.000836  w[0]    2.758 bias   -0.000\n",
      "iter 5541/10000  loss         0.089685  avg_L1_norm_grad         0.000836  w[0]    2.758 bias   -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5560/10000  loss         0.089684  avg_L1_norm_grad         0.000825  w[0]    2.758 bias   -0.000\n",
      "iter 5561/10000  loss         0.089684  avg_L1_norm_grad         0.000825  w[0]    2.758 bias   -0.000\n",
      "iter 5580/10000  loss         0.089683  avg_L1_norm_grad         0.000814  w[0]    2.759 bias   -0.000\n",
      "iter 5581/10000  loss         0.089683  avg_L1_norm_grad         0.000814  w[0]    2.759 bias   -0.000\n",
      "iter 5600/10000  loss         0.089683  avg_L1_norm_grad         0.000804  w[0]    2.759 bias   -0.000\n",
      "iter 5601/10000  loss         0.089683  avg_L1_norm_grad         0.000803  w[0]    2.759 bias   -0.000\n",
      "iter 5620/10000  loss         0.089682  avg_L1_norm_grad         0.000793  w[0]    2.759 bias   -0.000\n",
      "iter 5621/10000  loss         0.089682  avg_L1_norm_grad         0.000793  w[0]    2.759 bias   -0.000\n",
      "iter 5640/10000  loss         0.089682  avg_L1_norm_grad         0.000783  w[0]    2.760 bias   -0.000\n",
      "iter 5641/10000  loss         0.089682  avg_L1_norm_grad         0.000782  w[0]    2.760 bias   -0.000\n",
      "iter 5660/10000  loss         0.089681  avg_L1_norm_grad         0.000773  w[0]    2.760 bias   -0.000\n",
      "iter 5661/10000  loss         0.089681  avg_L1_norm_grad         0.000772  w[0]    2.760 bias   -0.000\n",
      "iter 5680/10000  loss         0.089681  avg_L1_norm_grad         0.000763  w[0]    2.760 bias   -0.000\n",
      "iter 5681/10000  loss         0.089681  avg_L1_norm_grad         0.000762  w[0]    2.760 bias   -0.000\n",
      "iter 5700/10000  loss         0.089680  avg_L1_norm_grad         0.000753  w[0]    2.761 bias   -0.000\n",
      "iter 5701/10000  loss         0.089680  avg_L1_norm_grad         0.000752  w[0]    2.761 bias   -0.000\n",
      "iter 5720/10000  loss         0.089680  avg_L1_norm_grad         0.000743  w[0]    2.761 bias   -0.000\n",
      "iter 5721/10000  loss         0.089680  avg_L1_norm_grad         0.000743  w[0]    2.761 bias   -0.000\n",
      "iter 5740/10000  loss         0.089680  avg_L1_norm_grad         0.000733  w[0]    2.761 bias   -0.000\n",
      "iter 5741/10000  loss         0.089680  avg_L1_norm_grad         0.000733  w[0]    2.761 bias   -0.000\n",
      "iter 5760/10000  loss         0.089679  avg_L1_norm_grad         0.000724  w[0]    2.761 bias   -0.000\n",
      "iter 5761/10000  loss         0.089679  avg_L1_norm_grad         0.000723  w[0]    2.761 bias   -0.000\n",
      "iter 5780/10000  loss         0.089679  avg_L1_norm_grad         0.000714  w[0]    2.762 bias   -0.000\n",
      "iter 5781/10000  loss         0.089679  avg_L1_norm_grad         0.000714  w[0]    2.762 bias   -0.000\n",
      "iter 5800/10000  loss         0.089678  avg_L1_norm_grad         0.000705  w[0]    2.762 bias   -0.000\n",
      "iter 5801/10000  loss         0.089678  avg_L1_norm_grad         0.000705  w[0]    2.762 bias   -0.000\n",
      "iter 5820/10000  loss         0.089678  avg_L1_norm_grad         0.000696  w[0]    2.762 bias   -0.000\n",
      "iter 5821/10000  loss         0.089678  avg_L1_norm_grad         0.000695  w[0]    2.762 bias   -0.000\n",
      "iter 5840/10000  loss         0.089678  avg_L1_norm_grad         0.000687  w[0]    2.763 bias   -0.000\n",
      "iter 5841/10000  loss         0.089678  avg_L1_norm_grad         0.000686  w[0]    2.763 bias   -0.000\n",
      "iter 5860/10000  loss         0.089677  avg_L1_norm_grad         0.000678  w[0]    2.763 bias   -0.000\n",
      "iter 5861/10000  loss         0.089677  avg_L1_norm_grad         0.000677  w[0]    2.763 bias   -0.000\n",
      "iter 5880/10000  loss         0.089677  avg_L1_norm_grad         0.000669  w[0]    2.763 bias   -0.000\n",
      "iter 5881/10000  loss         0.089677  avg_L1_norm_grad         0.000669  w[0]    2.763 bias   -0.000\n",
      "iter 5900/10000  loss         0.089677  avg_L1_norm_grad         0.000660  w[0]    2.763 bias   -0.000\n",
      "iter 5901/10000  loss         0.089676  avg_L1_norm_grad         0.000660  w[0]    2.763 bias   -0.000\n",
      "iter 5920/10000  loss         0.089676  avg_L1_norm_grad         0.000652  w[0]    2.764 bias   -0.000\n",
      "iter 5921/10000  loss         0.089676  avg_L1_norm_grad         0.000651  w[0]    2.764 bias   -0.000\n",
      "iter 5940/10000  loss         0.089676  avg_L1_norm_grad         0.000643  w[0]    2.764 bias   -0.000\n",
      "iter 5941/10000  loss         0.089676  avg_L1_norm_grad         0.000643  w[0]    2.764 bias   -0.000\n",
      "iter 5960/10000  loss         0.089675  avg_L1_norm_grad         0.000635  w[0]    2.764 bias   -0.000\n",
      "iter 5961/10000  loss         0.089675  avg_L1_norm_grad         0.000635  w[0]    2.764 bias   -0.000\n",
      "iter 5980/10000  loss         0.089675  avg_L1_norm_grad         0.000627  w[0]    2.764 bias   -0.000\n",
      "iter 5981/10000  loss         0.089675  avg_L1_norm_grad         0.000626  w[0]    2.764 bias   -0.000\n",
      "iter 6000/10000  loss         0.089675  avg_L1_norm_grad         0.000619  w[0]    2.765 bias   -0.000\n",
      "iter 6001/10000  loss         0.089675  avg_L1_norm_grad         0.000618  w[0]    2.765 bias   -0.000\n",
      "iter 6020/10000  loss         0.089675  avg_L1_norm_grad         0.000611  w[0]    2.765 bias   -0.000\n",
      "iter 6021/10000  loss         0.089675  avg_L1_norm_grad         0.000610  w[0]    2.765 bias   -0.000\n",
      "iter 6040/10000  loss         0.089674  avg_L1_norm_grad         0.000603  w[0]    2.765 bias   -0.000\n",
      "iter 6041/10000  loss         0.089674  avg_L1_norm_grad         0.000602  w[0]    2.765 bias   -0.000\n",
      "iter 6060/10000  loss         0.089674  avg_L1_norm_grad         0.000595  w[0]    2.765 bias   -0.000\n",
      "iter 6061/10000  loss         0.089674  avg_L1_norm_grad         0.000595  w[0]    2.765 bias   -0.000\n",
      "iter 6080/10000  loss         0.089674  avg_L1_norm_grad         0.000587  w[0]    2.766 bias   -0.000\n",
      "iter 6081/10000  loss         0.089674  avg_L1_norm_grad         0.000587  w[0]    2.766 bias   -0.000\n",
      "iter 6100/10000  loss         0.089673  avg_L1_norm_grad         0.000580  w[0]    2.766 bias   -0.000\n",
      "iter 6101/10000  loss         0.089673  avg_L1_norm_grad         0.000579  w[0]    2.766 bias   -0.000\n",
      "iter 6120/10000  loss         0.089673  avg_L1_norm_grad         0.000572  w[0]    2.766 bias   -0.000\n",
      "iter 6121/10000  loss         0.089673  avg_L1_norm_grad         0.000572  w[0]    2.766 bias   -0.000\n",
      "iter 6140/10000  loss         0.089673  avg_L1_norm_grad         0.000565  w[0]    2.766 bias   -0.000\n",
      "iter 6141/10000  loss         0.089673  avg_L1_norm_grad         0.000564  w[0]    2.766 bias   -0.000\n",
      "iter 6160/10000  loss         0.089673  avg_L1_norm_grad         0.000557  w[0]    2.767 bias   -0.000\n",
      "iter 6161/10000  loss         0.089673  avg_L1_norm_grad         0.000557  w[0]    2.767 bias   -0.000\n",
      "iter 6180/10000  loss         0.089672  avg_L1_norm_grad         0.000550  w[0]    2.767 bias   -0.000\n",
      "iter 6181/10000  loss         0.089672  avg_L1_norm_grad         0.000550  w[0]    2.767 bias   -0.000\n",
      "iter 6200/10000  loss         0.089672  avg_L1_norm_grad         0.000543  w[0]    2.767 bias   -0.000\n",
      "iter 6201/10000  loss         0.089672  avg_L1_norm_grad         0.000543  w[0]    2.767 bias   -0.000\n",
      "iter 6220/10000  loss         0.089672  avg_L1_norm_grad         0.000536  w[0]    2.767 bias   -0.000\n",
      "iter 6221/10000  loss         0.089672  avg_L1_norm_grad         0.000536  w[0]    2.767 bias   -0.000\n",
      "iter 6240/10000  loss         0.089672  avg_L1_norm_grad         0.000529  w[0]    2.767 bias   -0.000\n",
      "iter 6241/10000  loss         0.089672  avg_L1_norm_grad         0.000529  w[0]    2.767 bias   -0.000\n",
      "iter 6260/10000  loss         0.089671  avg_L1_norm_grad         0.000522  w[0]    2.768 bias   -0.000\n",
      "iter 6261/10000  loss         0.089671  avg_L1_norm_grad         0.000522  w[0]    2.768 bias   -0.000\n",
      "iter 6280/10000  loss         0.089671  avg_L1_norm_grad         0.000515  w[0]    2.768 bias   -0.000\n",
      "iter 6281/10000  loss         0.089671  avg_L1_norm_grad         0.000515  w[0]    2.768 bias   -0.000\n",
      "iter 6300/10000  loss         0.089671  avg_L1_norm_grad         0.000509  w[0]    2.768 bias   -0.000\n",
      "iter 6301/10000  loss         0.089671  avg_L1_norm_grad         0.000508  w[0]    2.768 bias   -0.000\n",
      "iter 6320/10000  loss         0.089671  avg_L1_norm_grad         0.000502  w[0]    2.768 bias   -0.000\n",
      "iter 6321/10000  loss         0.089671  avg_L1_norm_grad         0.000502  w[0]    2.768 bias   -0.000\n",
      "iter 6340/10000  loss         0.089671  avg_L1_norm_grad         0.000496  w[0]    2.768 bias   -0.000\n",
      "iter 6341/10000  loss         0.089671  avg_L1_norm_grad         0.000495  w[0]    2.768 bias   -0.000\n",
      "iter 6360/10000  loss         0.089670  avg_L1_norm_grad         0.000489  w[0]    2.769 bias   -0.000\n",
      "iter 6361/10000  loss         0.089670  avg_L1_norm_grad         0.000489  w[0]    2.769 bias   -0.000\n",
      "Done. Converged after 6362 iterations.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ## Toy problem \n",
    "    #\n",
    "    # Logistic regression should be able to perfectly predict all 10 examples\n",
    "    # five examples have x values within (-2, -1) and are labeled 0\n",
    "    # five examples have x values within (+1, +2) and are labeled 1\n",
    "    N = 10\n",
    "    x_NF = np.hstack([np.linspace(-2, -1, 5), np.linspace(1,2, 5)])[:,np.newaxis]\n",
    "    y_N = np.hstack([np.zeros(5), 1.0 * np.ones(5)])\n",
    "\n",
    "    lr = LogisticRegressionGradientDescent(\n",
    "        alpha=0.1, step_size=0.01, init_w_recipe='zeros')\n",
    "\n",
    "    # Prepare features by inserting column of all 1\n",
    "    xbias_NG = lr.insert_final_col_of_all_ones(x_NF)\n",
    "\n",
    "    print(\"Checking loss and grad at all zeros w vector\")\n",
    "    w_G = np.zeros(2)\n",
    "    print(\"w_G = %s\" % str(w_G))\n",
    "    print(\"loss(w_G) = %.3f\" % lr.calc_loss(w_G, xbias_NG, y_N))\n",
    "    print(\"grad(w_G) = %s\" % str(lr.calc_grad(w_G, xbias_NG, y_N)))\n",
    "\n",
    "    print(\"Trying gradient descent\")\n",
    "    lr.fit(x_NF, y_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = LogisticRegressionGradientDescent(alpha=0.1, step_size=0.01, init_w_recipe='zeros')\n",
    "w_G0 = np.arange(-2,12,0.14)\n",
    "W_G1 = np.zeros(100)\n",
    "W_G = np.column_stack((w_G0,W_G1))\n",
    "loss = []\n",
    "grad = []\n",
    "for i in range(100):\n",
    "    loss.append(a.calc_loss(W_G[i], xbias_NG, y_N))\n",
    "    grad.append(a.calc_grad(W_G[i], xbias_NG, y_N)[0])\n",
    "#a.calc_grad( w_G, xbias_NG, y_N)\n",
    "\n",
    "plt.plot(w_G0, loss)\n",
    "plt.xlabel('w_G0');\n",
    "plt.ylabel('loss');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w_G0, grad)\n",
    "plt.xlabel('w_G0');\n",
    "plt.ylabel('grad_W_G0');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a23940710>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = [a.trace_w[i][0] for i in range(a.trace_steps[-1])]\n",
    "w2 = [a.trace_w[i][-1] for i in range(a.trace_steps[-1])]\n",
    "plt.plot(w1, label='w_0')\n",
    "plt.plot(w2,label='bias')\n",
    "plt.ylabel('weight')\n",
    "plt.xlabel('trace_steps')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Yes. Because when w1 is about 2.7, the loss is lowest, so when doing grad decent, the w1 keeps getting close to 2.7. And the bias is always 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2262ccc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = [a2.trace_w[i][0] for i in range(a2.trace_steps[-1])]\n",
    "w2 = [a2.trace_w[i][-1] for i in range(a2.trace_steps[-1])]\n",
    "plt.plot(w1, label='w_0')\n",
    "plt.plot(w2,label='bias')\n",
    "plt.ylabel('weight')\n",
    "plt.xlabel('trace_steps')    \n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = np.loadtxt('data_digits_8_vs_9_noisy/x_train.csv', delimiter=',', skiprows=1)\n",
    "y_tr = np.loadtxt('data_digits_8_vs_9_noisy/y_train.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_va = x_tr[0:2000]\n",
    "y_va = y_tr[0:2000]\n",
    "x_tra = x_tr[2000:11800]\n",
    "y_tra = y_tr[2000:11800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "w154_1 = [lr_2a1.trace_w[i][153] for i in range(lr_2a1.trace_steps[-1])]\n",
    "wb_1 = [lr_2a1.trace_w[i][-1] for i in range(lr_2a1.trace_steps[-1])]\n",
    "w154_2 = [lr_2a2.trace_w[i][153] for i in range(lr_2a2.trace_steps[-1])]\n",
    "wb_2 = [lr_2a2.trace_w[i][-1] for i in range(lr_2a2.trace_steps[-1])]\n",
    "w154_3 = [lr_2a3.trace_w[i][153] for i in range(lr_2a3.trace_steps[-1])]\n",
    "wb_3 = [lr_2a3.trace_w[i][-1] for i in range(lr_2a3.trace_steps[-1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.5\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.024676  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         0.652834  avg_L1_norm_grad         0.058458  w[0]   -0.001 bias    0.002\n",
      "iter    2/30000  loss         4.480393  avg_L1_norm_grad         0.167414  w[0]    0.012 bias    0.145\n",
      "iter    3/30000  loss         9.642480  avg_L1_norm_grad         0.151994  w[0]   -0.025 bias   -0.212\n",
      "iter    4/30000  loss         1.741418  avg_L1_norm_grad         0.129421  w[0]    0.010 bias    0.151\n",
      "iter    5/30000  loss         6.221672  avg_L1_norm_grad         0.151786  w[0]   -0.018 bias   -0.121\n",
      "iter    6/30000  loss         2.955412  avg_L1_norm_grad         0.146658  w[0]    0.016 bias    0.241\n",
      "iter    7/30000  loss         4.446966  avg_L1_norm_grad         0.147311  w[0]   -0.016 bias   -0.069\n",
      "iter    8/30000  loss         2.279642  avg_L1_norm_grad         0.115011  w[0]    0.018 bias    0.282\n",
      "iter    9/30000  loss         1.348115  avg_L1_norm_grad         0.071511  w[0]   -0.008 bias    0.041\n",
      "iter   10/30000  loss         0.421200  avg_L1_norm_grad         0.021189  w[0]    0.009 bias    0.212\n",
      "iter   11/30000  loss         0.285284  avg_L1_norm_grad         0.003384  w[0]    0.004 bias    0.169\n",
      "iter   12/30000  loss         0.276035  avg_L1_norm_grad         0.002788  w[0]    0.004 bias    0.177\n",
      "iter   13/30000  loss         0.268467  avg_L1_norm_grad         0.002686  w[0]    0.004 bias    0.181\n",
      "iter   14/30000  loss         0.261457  avg_L1_norm_grad         0.002595  w[0]    0.003 bias    0.184\n",
      "iter   15/30000  loss         0.254948  avg_L1_norm_grad         0.002512  w[0]    0.003 bias    0.187\n",
      "iter   16/30000  loss         0.248890  avg_L1_norm_grad         0.002435  w[0]    0.003 bias    0.191\n",
      "iter   17/30000  loss         0.243236  avg_L1_norm_grad         0.002362  w[0]    0.002 bias    0.194\n",
      "iter   18/30000  loss         0.237947  avg_L1_norm_grad         0.002294  w[0]    0.002 bias    0.197\n",
      "iter   19/30000  loss         0.232988  avg_L1_norm_grad         0.002230  w[0]    0.001 bias    0.200\n",
      "iter   20/30000  loss         0.228328  avg_L1_norm_grad         0.002171  w[0]    0.001 bias    0.203\n",
      "iter   21/30000  loss         0.223939  avg_L1_norm_grad         0.002115  w[0]    0.000 bias    0.206\n",
      "iter   40/30000  loss         0.171064  avg_L1_norm_grad         0.001441  w[0]   -0.009 bias    0.256\n",
      "iter   41/30000  loss         0.169277  avg_L1_norm_grad         0.001417  w[0]   -0.009 bias    0.259\n",
      "iter   60/30000  loss         0.145200  avg_L1_norm_grad         0.001048  w[0]   -0.018 bias    0.298\n",
      "iter   61/30000  loss         0.144328  avg_L1_norm_grad         0.001033  w[0]   -0.018 bias    0.300\n",
      "iter   80/30000  loss         0.132288  avg_L1_norm_grad         0.000789  w[0]   -0.025 bias    0.334\n",
      "iter   81/30000  loss         0.131836  avg_L1_norm_grad         0.000778  w[0]   -0.025 bias    0.335\n",
      "iter  100/30000  loss         0.125286  avg_L1_norm_grad         0.000618  w[0]   -0.030 bias    0.364\n",
      "iter  101/30000  loss         0.125023  avg_L1_norm_grad         0.000611  w[0]   -0.030 bias    0.366\n",
      "iter  120/30000  loss         0.120979  avg_L1_norm_grad         0.000510  w[0]   -0.035 bias    0.392\n",
      "iter  121/30000  loss         0.120806  avg_L1_norm_grad         0.000506  w[0]   -0.035 bias    0.393\n",
      "iter  140/30000  loss         0.118004  avg_L1_norm_grad         0.000438  w[0]   -0.038 bias    0.417\n",
      "iter  141/30000  loss         0.117878  avg_L1_norm_grad         0.000435  w[0]   -0.039 bias    0.418\n",
      "iter  160/30000  loss         0.115776  avg_L1_norm_grad         0.000387  w[0]   -0.042 bias    0.440\n",
      "iter  161/30000  loss         0.115678  avg_L1_norm_grad         0.000385  w[0]   -0.042 bias    0.441\n",
      "iter  180/30000  loss         0.114017  avg_L1_norm_grad         0.000348  w[0]   -0.044 bias    0.462\n",
      "iter  181/30000  loss         0.113939  avg_L1_norm_grad         0.000346  w[0]   -0.044 bias    0.463\n",
      "iter  200/30000  loss         0.112579  avg_L1_norm_grad         0.000318  w[0]   -0.047 bias    0.482\n",
      "iter  201/30000  loss         0.112513  avg_L1_norm_grad         0.000317  w[0]   -0.047 bias    0.483\n",
      "iter  220/30000  loss         0.111371  avg_L1_norm_grad         0.000294  w[0]   -0.049 bias    0.501\n",
      "iter  221/30000  loss         0.111316  avg_L1_norm_grad         0.000293  w[0]   -0.049 bias    0.502\n",
      "iter  240/30000  loss         0.110339  avg_L1_norm_grad         0.000273  w[0]   -0.050 bias    0.519\n",
      "iter  241/30000  loss         0.110291  avg_L1_norm_grad         0.000272  w[0]   -0.050 bias    0.520\n",
      "iter  260/30000  loss         0.109443  avg_L1_norm_grad         0.000255  w[0]   -0.052 bias    0.536\n",
      "iter  261/30000  loss         0.109401  avg_L1_norm_grad         0.000255  w[0]   -0.052 bias    0.537\n",
      "iter  280/30000  loss         0.108657  avg_L1_norm_grad         0.000240  w[0]   -0.053 bias    0.553\n",
      "iter  281/30000  loss         0.108620  avg_L1_norm_grad         0.000239  w[0]   -0.053 bias    0.554\n",
      "iter  300/30000  loss         0.107961  avg_L1_norm_grad         0.000226  w[0]   -0.054 bias    0.569\n",
      "iter  301/30000  loss         0.107928  avg_L1_norm_grad         0.000225  w[0]   -0.054 bias    0.569\n",
      "iter  320/30000  loss         0.107341  avg_L1_norm_grad         0.000214  w[0]   -0.055 bias    0.584\n",
      "iter  321/30000  loss         0.107312  avg_L1_norm_grad         0.000213  w[0]   -0.055 bias    0.584\n",
      "iter  340/30000  loss         0.106786  avg_L1_norm_grad         0.000203  w[0]   -0.056 bias    0.598\n",
      "iter  341/30000  loss         0.106759  avg_L1_norm_grad         0.000202  w[0]   -0.056 bias    0.599\n",
      "iter  360/30000  loss         0.106286  avg_L1_norm_grad         0.000192  w[0]   -0.056 bias    0.612\n",
      "iter  361/30000  loss         0.106262  avg_L1_norm_grad         0.000192  w[0]   -0.056 bias    0.613\n",
      "iter  380/30000  loss         0.105834  avg_L1_norm_grad         0.000183  w[0]   -0.057 bias    0.626\n",
      "iter  381/30000  loss         0.105812  avg_L1_norm_grad         0.000183  w[0]   -0.057 bias    0.627\n",
      "iter  400/30000  loss         0.105424  avg_L1_norm_grad         0.000174  w[0]   -0.057 bias    0.639\n",
      "iter  401/30000  loss         0.105405  avg_L1_norm_grad         0.000174  w[0]   -0.057 bias    0.640\n",
      "iter  420/30000  loss         0.105052  avg_L1_norm_grad         0.000166  w[0]   -0.058 bias    0.652\n",
      "iter  421/30000  loss         0.105034  avg_L1_norm_grad         0.000166  w[0]   -0.058 bias    0.653\n",
      "iter  440/30000  loss         0.104712  avg_L1_norm_grad         0.000159  w[0]   -0.058 bias    0.664\n",
      "iter  441/30000  loss         0.104696  avg_L1_norm_grad         0.000159  w[0]   -0.058 bias    0.665\n",
      "iter  460/30000  loss         0.104402  avg_L1_norm_grad         0.000152  w[0]   -0.058 bias    0.676\n",
      "iter  461/30000  loss         0.104388  avg_L1_norm_grad         0.000152  w[0]   -0.058 bias    0.677\n",
      "iter  480/30000  loss         0.104118  avg_L1_norm_grad         0.000145  w[0]   -0.058 bias    0.688\n",
      "iter  481/30000  loss         0.104105  avg_L1_norm_grad         0.000145  w[0]   -0.058 bias    0.688\n",
      "iter  500/30000  loss         0.103858  avg_L1_norm_grad         0.000139  w[0]   -0.058 bias    0.699\n",
      "iter  501/30000  loss         0.103845  avg_L1_norm_grad         0.000139  w[0]   -0.058 bias    0.700\n",
      "iter  520/30000  loss         0.103619  avg_L1_norm_grad         0.000133  w[0]   -0.058 bias    0.710\n",
      "iter  521/30000  loss         0.103607  avg_L1_norm_grad         0.000133  w[0]   -0.058 bias    0.711\n",
      "iter  540/30000  loss         0.103398  avg_L1_norm_grad         0.000128  w[0]   -0.058 bias    0.721\n",
      "iter  541/30000  loss         0.103388  avg_L1_norm_grad         0.000128  w[0]   -0.058 bias    0.721\n",
      "iter  560/30000  loss         0.103195  avg_L1_norm_grad         0.000123  w[0]   -0.058 bias    0.731\n",
      "iter  561/30000  loss         0.103186  avg_L1_norm_grad         0.000123  w[0]   -0.058 bias    0.731\n",
      "iter  580/30000  loss         0.103008  avg_L1_norm_grad         0.000118  w[0]   -0.058 bias    0.741\n",
      "iter  581/30000  loss         0.102999  avg_L1_norm_grad         0.000118  w[0]   -0.058 bias    0.741\n",
      "iter  600/30000  loss         0.102835  avg_L1_norm_grad         0.000113  w[0]   -0.058 bias    0.751\n",
      "iter  601/30000  loss         0.102827  avg_L1_norm_grad         0.000113  w[0]   -0.058 bias    0.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.102675  avg_L1_norm_grad         0.000109  w[0]   -0.058 bias    0.760\n",
      "iter  621/30000  loss         0.102667  avg_L1_norm_grad         0.000109  w[0]   -0.058 bias    0.761\n",
      "iter  640/30000  loss         0.102527  avg_L1_norm_grad         0.000105  w[0]   -0.058 bias    0.770\n",
      "iter  641/30000  loss         0.102520  avg_L1_norm_grad         0.000105  w[0]   -0.058 bias    0.770\n",
      "iter  660/30000  loss         0.102389  avg_L1_norm_grad         0.000101  w[0]   -0.058 bias    0.779\n",
      "iter  661/30000  loss         0.102383  avg_L1_norm_grad         0.000101  w[0]   -0.058 bias    0.779\n",
      "iter  680/30000  loss         0.102262  avg_L1_norm_grad         0.000097  w[0]   -0.057 bias    0.787\n",
      "iter  681/30000  loss         0.102256  avg_L1_norm_grad         0.000097  w[0]   -0.057 bias    0.788\n",
      "iter  700/30000  loss         0.102143  avg_L1_norm_grad         0.000094  w[0]   -0.057 bias    0.796\n",
      "iter  701/30000  loss         0.102138  avg_L1_norm_grad         0.000094  w[0]   -0.057 bias    0.796\n",
      "iter  720/30000  loss         0.102033  avg_L1_norm_grad         0.000090  w[0]   -0.057 bias    0.804\n",
      "iter  721/30000  loss         0.102028  avg_L1_norm_grad         0.000090  w[0]   -0.057 bias    0.805\n",
      "iter  740/30000  loss         0.101931  avg_L1_norm_grad         0.000087  w[0]   -0.057 bias    0.812\n",
      "iter  741/30000  loss         0.101926  avg_L1_norm_grad         0.000087  w[0]   -0.057 bias    0.813\n",
      "iter  760/30000  loss         0.101836  avg_L1_norm_grad         0.000084  w[0]   -0.057 bias    0.820\n",
      "iter  761/30000  loss         0.101831  avg_L1_norm_grad         0.000084  w[0]   -0.057 bias    0.821\n",
      "iter  780/30000  loss         0.101747  avg_L1_norm_grad         0.000081  w[0]   -0.056 bias    0.828\n",
      "iter  781/30000  loss         0.101742  avg_L1_norm_grad         0.000081  w[0]   -0.056 bias    0.828\n",
      "iter  800/30000  loss         0.101664  avg_L1_norm_grad         0.000078  w[0]   -0.056 bias    0.836\n",
      "iter  801/30000  loss         0.101660  avg_L1_norm_grad         0.000078  w[0]   -0.056 bias    0.836\n",
      "iter  820/30000  loss         0.101587  avg_L1_norm_grad         0.000075  w[0]   -0.056 bias    0.843\n",
      "iter  821/30000  loss         0.101583  avg_L1_norm_grad         0.000075  w[0]   -0.056 bias    0.843\n",
      "iter  840/30000  loss         0.101514  avg_L1_norm_grad         0.000073  w[0]   -0.056 bias    0.850\n",
      "iter  841/30000  loss         0.101511  avg_L1_norm_grad         0.000073  w[0]   -0.056 bias    0.850\n",
      "iter  860/30000  loss         0.101447  avg_L1_norm_grad         0.000070  w[0]   -0.056 bias    0.857\n",
      "iter  861/30000  loss         0.101444  avg_L1_norm_grad         0.000070  w[0]   -0.056 bias    0.857\n",
      "iter  880/30000  loss         0.101384  avg_L1_norm_grad         0.000068  w[0]   -0.055 bias    0.864\n",
      "iter  881/30000  loss         0.101381  avg_L1_norm_grad         0.000068  w[0]   -0.055 bias    0.864\n",
      "iter  900/30000  loss         0.101325  avg_L1_norm_grad         0.000066  w[0]   -0.055 bias    0.871\n",
      "iter  901/30000  loss         0.101322  avg_L1_norm_grad         0.000066  w[0]   -0.055 bias    0.871\n",
      "iter  920/30000  loss         0.101270  avg_L1_norm_grad         0.000063  w[0]   -0.055 bias    0.877\n",
      "iter  921/30000  loss         0.101267  avg_L1_norm_grad         0.000063  w[0]   -0.055 bias    0.877\n",
      "iter  940/30000  loss         0.101218  avg_L1_norm_grad         0.000061  w[0]   -0.055 bias    0.884\n",
      "iter  941/30000  loss         0.101216  avg_L1_norm_grad         0.000061  w[0]   -0.055 bias    0.884\n",
      "iter  960/30000  loss         0.101170  avg_L1_norm_grad         0.000059  w[0]   -0.054 bias    0.890\n",
      "iter  961/30000  loss         0.101168  avg_L1_norm_grad         0.000059  w[0]   -0.054 bias    0.890\n",
      "iter  980/30000  loss         0.101125  avg_L1_norm_grad         0.000057  w[0]   -0.054 bias    0.896\n",
      "iter  981/30000  loss         0.101122  avg_L1_norm_grad         0.000057  w[0]   -0.054 bias    0.896\n",
      "iter 1000/30000  loss         0.101082  avg_L1_norm_grad         0.000056  w[0]   -0.054 bias    0.902\n",
      "iter 1001/30000  loss         0.101080  avg_L1_norm_grad         0.000055  w[0]   -0.054 bias    0.902\n",
      "iter 1020/30000  loss         0.101042  avg_L1_norm_grad         0.000054  w[0]   -0.054 bias    0.908\n",
      "iter 1021/30000  loss         0.101040  avg_L1_norm_grad         0.000054  w[0]   -0.054 bias    0.908\n",
      "iter 1040/30000  loss         0.101005  avg_L1_norm_grad         0.000052  w[0]   -0.054 bias    0.913\n",
      "iter 1041/30000  loss         0.101003  avg_L1_norm_grad         0.000052  w[0]   -0.054 bias    0.914\n",
      "iter 1060/30000  loss         0.100970  avg_L1_norm_grad         0.000050  w[0]   -0.053 bias    0.919\n",
      "iter 1061/30000  loss         0.100968  avg_L1_norm_grad         0.000050  w[0]   -0.053 bias    0.919\n",
      "iter 1080/30000  loss         0.100936  avg_L1_norm_grad         0.000049  w[0]   -0.053 bias    0.924\n",
      "iter 1081/30000  loss         0.100935  avg_L1_norm_grad         0.000049  w[0]   -0.053 bias    0.925\n",
      "iter 1100/30000  loss         0.100905  avg_L1_norm_grad         0.000047  w[0]   -0.053 bias    0.930\n",
      "iter 1101/30000  loss         0.100904  avg_L1_norm_grad         0.000047  w[0]   -0.053 bias    0.930\n",
      "iter 1120/30000  loss         0.100876  avg_L1_norm_grad         0.000046  w[0]   -0.053 bias    0.935\n",
      "iter 1121/30000  loss         0.100875  avg_L1_norm_grad         0.000046  w[0]   -0.053 bias    0.935\n",
      "iter 1140/30000  loss         0.100849  avg_L1_norm_grad         0.000044  w[0]   -0.053 bias    0.940\n",
      "iter 1141/30000  loss         0.100848  avg_L1_norm_grad         0.000044  w[0]   -0.053 bias    0.940\n",
      "iter 1160/30000  loss         0.100823  avg_L1_norm_grad         0.000043  w[0]   -0.053 bias    0.945\n",
      "iter 1161/30000  loss         0.100822  avg_L1_norm_grad         0.000043  w[0]   -0.053 bias    0.945\n",
      "iter 1180/30000  loss         0.100799  avg_L1_norm_grad         0.000042  w[0]   -0.052 bias    0.950\n",
      "iter 1181/30000  loss         0.100798  avg_L1_norm_grad         0.000042  w[0]   -0.052 bias    0.950\n",
      "iter 1200/30000  loss         0.100776  avg_L1_norm_grad         0.000040  w[0]   -0.052 bias    0.954\n",
      "iter 1201/30000  loss         0.100775  avg_L1_norm_grad         0.000040  w[0]   -0.052 bias    0.955\n",
      "iter 1220/30000  loss         0.100754  avg_L1_norm_grad         0.000039  w[0]   -0.052 bias    0.959\n",
      "iter 1221/30000  loss         0.100753  avg_L1_norm_grad         0.000039  w[0]   -0.052 bias    0.959\n",
      "iter 1240/30000  loss         0.100734  avg_L1_norm_grad         0.000038  w[0]   -0.052 bias    0.964\n",
      "iter 1241/30000  loss         0.100733  avg_L1_norm_grad         0.000038  w[0]   -0.052 bias    0.964\n",
      "iter 1260/30000  loss         0.100715  avg_L1_norm_grad         0.000037  w[0]   -0.052 bias    0.968\n",
      "iter 1261/30000  loss         0.100714  avg_L1_norm_grad         0.000037  w[0]   -0.052 bias    0.968\n",
      "iter 1280/30000  loss         0.100697  avg_L1_norm_grad         0.000036  w[0]   -0.052 bias    0.972\n",
      "iter 1281/30000  loss         0.100696  avg_L1_norm_grad         0.000036  w[0]   -0.052 bias    0.973\n",
      "iter 1300/30000  loss         0.100680  avg_L1_norm_grad         0.000035  w[0]   -0.052 bias    0.977\n",
      "iter 1301/30000  loss         0.100679  avg_L1_norm_grad         0.000035  w[0]   -0.052 bias    0.977\n",
      "iter 1320/30000  loss         0.100664  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.981\n",
      "iter 1321/30000  loss         0.100663  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.981\n",
      "iter 1340/30000  loss         0.100649  avg_L1_norm_grad         0.000033  w[0]   -0.051 bias    0.985\n",
      "iter 1341/30000  loss         0.100648  avg_L1_norm_grad         0.000033  w[0]   -0.051 bias    0.985\n",
      "iter 1360/30000  loss         0.100635  avg_L1_norm_grad         0.000032  w[0]   -0.051 bias    0.989\n",
      "iter 1361/30000  loss         0.100634  avg_L1_norm_grad         0.000032  w[0]   -0.051 bias    0.989\n",
      "iter 1380/30000  loss         0.100621  avg_L1_norm_grad         0.000031  w[0]   -0.051 bias    0.993\n",
      "iter 1381/30000  loss         0.100621  avg_L1_norm_grad         0.000031  w[0]   -0.051 bias    0.993\n",
      "iter 1400/30000  loss         0.100608  avg_L1_norm_grad         0.000030  w[0]   -0.051 bias    0.997\n",
      "iter 1401/30000  loss         0.100608  avg_L1_norm_grad         0.000030  w[0]   -0.051 bias    0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.100596  avg_L1_norm_grad         0.000029  w[0]   -0.051 bias    1.000\n",
      "iter 1421/30000  loss         0.100596  avg_L1_norm_grad         0.000029  w[0]   -0.051 bias    1.000\n",
      "iter 1440/30000  loss         0.100585  avg_L1_norm_grad         0.000028  w[0]   -0.051 bias    1.004\n",
      "iter 1441/30000  loss         0.100585  avg_L1_norm_grad         0.000028  w[0]   -0.051 bias    1.004\n",
      "iter 1460/30000  loss         0.100574  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.008\n",
      "iter 1461/30000  loss         0.100574  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.008\n",
      "iter 1480/30000  loss         0.100564  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.011\n",
      "iter 1481/30000  loss         0.100564  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.011\n",
      "iter 1500/30000  loss         0.100555  avg_L1_norm_grad         0.000026  w[0]   -0.051 bias    1.014\n",
      "iter 1501/30000  loss         0.100554  avg_L1_norm_grad         0.000026  w[0]   -0.051 bias    1.015\n",
      "iter 1520/30000  loss         0.100546  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.018\n",
      "iter 1521/30000  loss         0.100545  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.018\n",
      "iter 1540/30000  loss         0.100537  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.021\n",
      "iter 1541/30000  loss         0.100537  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.021\n",
      "iter 1560/30000  loss         0.100529  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.024\n",
      "iter 1561/30000  loss         0.100529  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.025\n",
      "iter 1580/30000  loss         0.100521  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.028\n",
      "iter 1581/30000  loss         0.100521  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.028\n",
      "iter 1600/30000  loss         0.100514  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.031\n",
      "iter 1601/30000  loss         0.100514  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.031\n",
      "iter 1620/30000  loss         0.100507  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.034\n",
      "iter 1621/30000  loss         0.100507  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.034\n",
      "iter 1640/30000  loss         0.100501  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.037\n",
      "iter 1641/30000  loss         0.100500  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.037\n",
      "iter 1660/30000  loss         0.100495  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.040\n",
      "iter 1661/30000  loss         0.100494  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.040\n",
      "iter 1680/30000  loss         0.100489  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.042\n",
      "iter 1681/30000  loss         0.100488  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.043\n",
      "iter 1700/30000  loss         0.100483  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.045\n",
      "iter 1701/30000  loss         0.100483  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.045\n",
      "iter 1720/30000  loss         0.100478  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.048\n",
      "iter 1721/30000  loss         0.100478  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.048\n",
      "iter 1740/30000  loss         0.100473  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.051\n",
      "iter 1741/30000  loss         0.100473  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.051\n",
      "iter 1760/30000  loss         0.100468  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.053\n",
      "iter 1761/30000  loss         0.100468  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.053\n",
      "iter 1780/30000  loss         0.100464  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.056\n",
      "iter 1781/30000  loss         0.100464  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.056\n",
      "iter 1800/30000  loss         0.100460  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.058\n",
      "iter 1801/30000  loss         0.100459  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.059\n",
      "iter 1820/30000  loss         0.100456  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.061\n",
      "iter 1821/30000  loss         0.100455  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.061\n",
      "iter 1840/30000  loss         0.100452  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.063\n",
      "iter 1841/30000  loss         0.100452  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.063\n",
      "iter 1860/30000  loss         0.100448  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.066\n",
      "iter 1861/30000  loss         0.100448  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.066\n",
      "iter 1880/30000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.068\n",
      "iter 1881/30000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.068\n",
      "iter 1900/30000  loss         0.100441  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.070\n",
      "iter 1901/30000  loss         0.100441  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.071\n",
      "iter 1920/30000  loss         0.100438  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.073\n",
      "iter 1921/30000  loss         0.100438  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.073\n",
      "iter 1940/30000  loss         0.100435  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.075\n",
      "iter 1941/30000  loss         0.100435  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.075\n",
      "iter 1960/30000  loss         0.100433  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.077\n",
      "iter 1961/30000  loss         0.100433  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.077\n",
      "iter 1980/30000  loss         0.100430  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.079\n",
      "iter 1981/30000  loss         0.100430  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.079\n",
      "iter 2000/30000  loss         0.100428  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.081\n",
      "iter 2001/30000  loss         0.100427  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.081\n",
      "iter 2020/30000  loss         0.100425  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.083\n",
      "iter 2021/30000  loss         0.100425  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.083\n",
      "iter 2040/30000  loss         0.100423  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.085\n",
      "iter 2041/30000  loss         0.100423  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.085\n",
      "iter 2060/30000  loss         0.100421  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.087\n",
      "iter 2061/30000  loss         0.100421  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.087\n",
      "iter 2080/30000  loss         0.100419  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.089\n",
      "iter 2081/30000  loss         0.100419  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.089\n",
      "iter 2100/30000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.091\n",
      "iter 2101/30000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.091\n",
      "iter 2120/30000  loss         0.100415  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.093\n",
      "iter 2121/30000  loss         0.100415  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.093\n",
      "iter 2140/30000  loss         0.100413  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.095\n",
      "iter 2141/30000  loss         0.100413  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.095\n",
      "iter 2160/30000  loss         0.100411  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.096\n",
      "iter 2161/30000  loss         0.100411  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.097\n",
      "iter 2180/30000  loss         0.100410  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.098\n",
      "iter 2181/30000  loss         0.100410  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.098\n",
      "iter 2200/30000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.100\n",
      "iter 2201/30000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.100407  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.102\n",
      "iter 2221/30000  loss         0.100407  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.102\n",
      "iter 2240/30000  loss         0.100406  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.103\n",
      "iter 2241/30000  loss         0.100405  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.103\n",
      "iter 2260/30000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.105\n",
      "iter 2261/30000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.105\n",
      "iter 2280/30000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.106\n",
      "iter 2281/30000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.106\n",
      "iter 2300/30000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.108\n",
      "iter 2301/30000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.108\n",
      "iter 2320/30000  loss         0.100401  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.109\n",
      "iter 2321/30000  loss         0.100401  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.110\n",
      "iter 2340/30000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.111\n",
      "iter 2341/30000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.111\n",
      "iter 2360/30000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.112\n",
      "iter 2361/30000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.113\n",
      "iter 2380/30000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.114\n",
      "iter 2381/30000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.114\n",
      "iter 2400/30000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.115\n",
      "iter 2401/30000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.115\n",
      "iter 2420/30000  loss         0.100396  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.117\n",
      "iter 2421/30000  loss         0.100396  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.117\n",
      "iter 2440/30000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.118\n",
      "iter 2441/30000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.118\n",
      "iter 2460/30000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.119\n",
      "iter 2461/30000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.119\n",
      "iter 2480/30000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.121\n",
      "iter 2481/30000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.121\n",
      "iter 2500/30000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.122\n",
      "iter 2501/30000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.122\n",
      "iter 2520/30000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.123\n",
      "iter 2521/30000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.123\n",
      "iter 2540/30000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.124\n",
      "iter 2541/30000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.125\n",
      "iter 2560/30000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.126\n",
      "iter 2561/30000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.126\n",
      "iter 2580/30000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.127\n",
      "iter 2581/30000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.127\n",
      "iter 2600/30000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.128\n",
      "iter 2601/30000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.128\n",
      "iter 2620/30000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.129\n",
      "iter 2621/30000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.129\n",
      "iter 2640/30000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.130\n",
      "iter 2641/30000  loss         0.100388  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.130\n",
      "iter 2660/30000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.131\n",
      "iter 2661/30000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.131\n",
      "iter 2680/30000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.132\n",
      "iter 2681/30000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.133\n",
      "iter 2700/30000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 2701/30000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 2720/30000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 2721/30000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 2740/30000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 2741/30000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 2760/30000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 2761/30000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 2780/30000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 2781/30000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 2800/30000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 2801/30000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 2820/30000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 2821/30000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 2840/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 2841/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 2860/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 2861/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 2880/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 2881/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 2900/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 2901/30000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 2920/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 2921/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 2940/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 2941/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 2960/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 2961/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 2980/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 2981/30000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 3000/30000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 3001/30000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.148\n",
      "iter 3021/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.148\n",
      "iter 3040/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.149\n",
      "iter 3041/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.149\n",
      "iter 3060/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3061/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3080/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3081/30000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3100/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 3101/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 3120/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 3121/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 3140/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3141/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3160/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3161/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3180/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 3181/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 3200/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3201/30000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3220/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3221/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3240/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 3241/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 3260/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3261/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3280/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3281/30000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3300/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3301/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3320/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3321/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3340/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 3341/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 3360/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3361/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3380/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3381/30000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3400/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3401/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3420/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3421/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3440/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3441/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3460/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3461/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3480/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3481/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3500/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3501/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3520/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3521/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3540/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3541/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3560/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3561/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3580/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3581/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3600/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3601/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3620/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3621/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3640/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3641/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3660/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3661/30000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3680/30000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3681/30000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3700/30000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3701/30000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3720/30000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3721/30000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3740/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3741/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3760/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3761/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3780/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3781/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3800/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3801/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3821/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3840/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3841/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3860/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3861/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3880/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3881/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3900/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3901/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3920/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3921/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3940/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.173\n",
      "iter 3941/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.173\n",
      "iter 3960/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 3961/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 3980/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 3981/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 4000/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 4001/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4020/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4021/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4040/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4041/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4060/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4061/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4080/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4081/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4100/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4101/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4120/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4121/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4140/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4141/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4160/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4161/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4180/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4181/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4200/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4201/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4220/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4221/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4240/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4241/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4260/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4261/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4280/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4281/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4300/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4301/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4320/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4321/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4340/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4341/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4360/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4361/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4380/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4381/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4400/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4401/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4420/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4421/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4440/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4441/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4460/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4461/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4480/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4481/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4500/30000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4501/30000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4520/30000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4521/30000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4540/30000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4541/30000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4560/30000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 4561/30000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "Done. Converged after 4577 iterations.\n"
     ]
    }
   ],
   "source": [
    "lr_2a2 = LogisticRegressionGradientDescent(alpha=10, step_size=0.5, init_w_recipe='zeros')\n",
    "lr_2a2.fit(x_tra, y_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a9ef3a400>"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_2a1.trace_loss, label='trace_loss')\n",
    "plt.plot(lr_2a1.trace_L1_norm_of_grad, label='trace_L1_norm_of_grad')\n",
    "plt.plot(w154_1, label='w_154')\n",
    "plt.plot(wb_1, label='w_bias')\n",
    "\n",
    "plt.xlim([0, 200]);\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.xlabel('step_size=0.16')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a4e051748>"
      ]
     },
     "execution_count": 1066,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_2a2.trace_loss, label='trace_loss')\n",
    "plt.plot(lr_2a2.trace_L1_norm_of_grad, label='trace_L1_norm_of_grad')\n",
    "plt.plot(w154_2, label='W_154')\n",
    "plt.plot(wb_2, label='W_bias')\n",
    "\n",
    "plt.xlim([0, 200]);\n",
    "\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.xlabel('step_size=0.165')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'loss')"
      ]
     },
     "execution_count": 1084,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_2a1.trace_loss, label='0.16 trace_loss')\n",
    "plt.plot(lr_2a2.trace_loss, label='0.5 trace_loss')\n",
    "plt.plot(lr_2a3.trace_loss, label='0.75 trace_loss')\n",
    "plt.xlim([0, 100]);\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a5c2a8f60>"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_2a3.trace_loss, label='trace_loss')\n",
    "plt.plot(lr_2a3.trace_L1_norm_of_grad, label='trace_L1_norm_of_grad')\n",
    "plt.plot(w154_3, label='W_154')\n",
    "plt.plot(wb_3, label='W_bias')\n",
    "plt.xlim([0, 200]);\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.xlabel('step_size=0.75')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=0.0001\n",
    "Did not find a step_size can converge within 30000 iterations.\n",
    "So I choosed 1.81 as the stepsize\n",
    "The first slipt error rate was 0.0524. The second slipt error rate was 0.0478. The third slipt error rate was 0.0483.\n",
    "\n",
    "=0.001\n",
    "Did not find a step_size can converge within 30000 iterations.\n",
    "So I choosed 1.7 as the stepsize\n",
    "The first slipt error rate was 0.0524. The second slipt error rate was 0.0475. The third slipt error rate was 0.0481.\n",
    "\n",
    "=1\n",
    "The step_size I choosed was 0.8, it converged for the data.\n",
    "The first slipt error rate was 0.0386. The second slipt error rate was 0.0389. The third slipt error rate was 0.0384.\n",
    "\n",
    "=100\n",
    "The step_size I choosed was 0.1, it converged for the data.\n",
    "The first slipt error rate was 0.0333. The second slipt error rate was 0.0290. The third slipt error rate was 0.0315.\n",
    "\n",
    "=10000\n",
    "The step_size I choosed was 0.01, it converged for the data.\n",
    "The first slipt error rate was 0.0666. The second slipt error rate was 0.0592. The third slipt error rate was 0.0519.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'error rate')"
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "er1 = np.array([0.0524, 0.0478, 0.0483])\n",
    "er2 = np.array([0.0524, 0.0475, 0.0481])\n",
    "er3 = np.array([0.0386, 0.0389, 0.0384])\n",
    "er4 = np.array([0.0333, 0.0290, 0.0315])\n",
    "er5 = np.array([0.0666, 0.0592, 0.0519])\n",
    "er = np.array([np.mean(er1), np.mean(er2), np.mean(er3), np.mean(er4), np.mean(er5)])\n",
    "alpah = np.array([0.0001, 0.001, 1, 100, 10000])\n",
    "\n",
    "plt.plot(er, '-rs')\n",
    "plt.xticks(range(5), alpah)\n",
    "plt.xlabel('error rate')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=100\n",
    "It has the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_va = x_tr[0:3934]\n",
    "y_va = y_tr[0:3934]\n",
    "x_tra = x_tr[3934:11800]\n",
    "y_tra = y_tr[3934:11800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lr2c.predict(x_va)\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "FP_list = list()\n",
    "FN_list = list()\n",
    "\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i] == y_va[i]  == 1:\n",
    "        TP = TP + 1\n",
    "    elif y_hat[i] == y_va[i]  == 0:\n",
    "        TN = TN + 1\n",
    "    elif (y_hat[i] == 1) & (y_va[i]  == 0):\n",
    "        FP = FP + 1\n",
    "        FP_list.append(i)\n",
    "    else:\n",
    "        FN = FN + 1\n",
    "        FN_list.append(i)\n",
    "     \n",
    "    \n",
    "TP = TP/len(y_hat)\n",
    "TN = TN/len(y_hat)\n",
    "FP = FP/len(y_hat)\n",
    "FN = FN/len(y_hat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1860</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>69</td>\n",
       "      <td>1943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        1860    62\n",
       "1.0          69  1943"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actu = pd.Series(y_va, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_NF = np.loadtxt('data_digits_8_vs_9_noisy/x_test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "yproba1_test_N = lr.predict_proba(x_test_NF)[:, 1]\n",
    "\n",
    "np.savetxt('yproba3_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(X, y):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9,9))\n",
    "    for ii in range(9):\n",
    "        cur_ax = axes.flatten()[ii]\n",
    "        cur_ax.imshow(X[ii].reshape(28,28), interpolation='nearest', vmin=0, vmax=1, cmap='gray')\n",
    "        cur_ax.set_xticks([])\n",
    "        cur_ax.set_yticks([])\n",
    "        cur_ax.set_title('y=%d' % y[ii])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_FP = [x_va[i] for i in FP_list]\n",
    "y_FP = [y_va[i] for i in FP_list]\n",
    "x_FP = x_FP[25:46]\n",
    "\n",
    "x_FN = [x_va[i] for i in FN_list]\n",
    "y_FN = [y_va[i] for i in FN_list]\n",
    "\n",
    "x_FN = x_FN[23:43]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_FP, y_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_FN, y_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for the false positive, most of their's lower circle is not very obvious.\n",
    "The lower circle is some kind like a straight line.\n",
    "\n",
    "And for the false negative, most of their's lower straight line is not straight, which is bending, looks like a circle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'alpha is best')"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(lr.w_G[0:784].reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.colorbar()\n",
    "plt.title('alpha is best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the pixels associated with a digit '8' gathers on the pixels which is difference parts of '8' and '9', and the pixels associated with a digit '9' gathers on the parts same partes of '8' and '9'.\n",
    "\n",
    "The lower and upper left arc pixels are associated with a digit '8'.\n",
    "\n",
    "The middle arc pixels are associated with a digit '8'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1a512efac8>"
      ]
     },
     "execution_count": 1043,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(lr_2d1.w_G[0:784].reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1a5157a898>"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAD8CAYAAAD9uIjPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAF6BJREFUeJzt3X+sZGd93/H3586uQcLGsbPr9co/uoRYqlwrtZsbVxVt4gqIlijKghSonTZZq5ClSqwEJZGwTAWIqJJLAwSpiGbBK9YJ4UcClFXqBhznB43UuL44COy4xI61sRdv1rsBmV8x9r330z9m1r1zf5zn3Hvmzjlz7ucljXZmzq9nzsx+73Oe53ueR7aJiOiTubYLEBExaQlsEdE7CWwR0TsJbBHROwlsEdE7CWwR0TsJbBHROwlsEdE7CWwR0Tu7pnmwPXsu9oEDl0/zkBE7ysmTf8e5c8+oyT5e+n3XevH579Ra9x+++8TnbB9scrzt0CiwSToIvB8YAB+2fWfV+gcOXM4DD/y3ijVSgYxo4kd+5EjjfSw+/x3+8XVvrbXuX/6fX9zT+IDbYMuBTdIA+ADwauAU8ICkE7b/alKFi4jpk0BzjSp9rWtSY7sReMz24wCSPg4cAhLYImaZQLsHbZeikSbXflcAT654fWr03hhJRyQtSFo4e/aZBoeLiOkQGtR7dFWTwLbep1ozBpLto7bnbc/v3Xtxg8NFxFSMLkXrPLqqyaXoKeCqFa+vBJ5qVpyI6IIu18bqaBLYHgCukfQy4GvAzcDPTKRUEdEeAXOznaGw5cBme1HSbcDnGKZ7HLP9cHnL2T5hEe1b3ta9i263n9XRKI/N9j3APRMqS0R0gWBu92xXQKZ650FEzIAdnscWEX0kwSA1tojomdTYIqJXpJ2d7hERfSSl8yAiemin5rFFl1TlNc32DzTWs/3fadrYIqJfetDGlj/nETFG1LsBvm6tTtJBSV+V9Jik2yvW+2lJljTf9DOkxhYR48TE8tjqDkgr6SLgl4D7J3Hc1NgiYtzolqo6jxpeGJDW9nPA+QFpV/t14N3As5P4CAlsEbHKRC9FiwPSSroBuMr2H0zqE+RSNCLGba7zYI+khRWvj9o+Or63NV4YkFbSHPA+4NbNFrNKAttElIaR2e6KcSreMTkCVD+P7Zztqsb+0oC0FwHXAX8qCeBy4ISkn7K9MmBuSgJbRIwTzE0uj61yQFrbzwAvTOEn6U+BX2sS1CCBLSJWkcRgQrdUbTQgraR3AQu2T0zkQKsksEXEGhOssa07IK3tt2+w7k2TOGYCW0SMm+ylaCsS2CJizHAulwS2iOgVMZfRPSKiTySYm/Gb4BPYJqLtv24VeXRuOFXbcsPtq46vwnkr1RpK2zfSdN9t5zY2IBjs6nD5akhgi4gxQmlji4ieSa9oRPSNSBtbRPRNamwR0T9J94iInlF6RSOij3Z0G5ukk8C3gCVgsTAu0w5WyGlqmmu2tFix7PmtbwuwXFjepOylPLTS8l0XVC+fG1QsK/z0W82hK2n4eylQ2tgA+Ne2z01gPxHRCclji4ieSY1tOHb55yUZ+K1VY51HxCwSDHZXXMbPgKaB7RW2n5J0GXCvpP9r+wsrV5B0BDgCcPXV+xoeLiK2Wx9uqWrUAmr7qdG/TwOfYTiH4Op1jtqetz2/d+/FTQ4XEdMwuhSt8+iqLQc2SS8Zzd6MpJcAPw48NKmCRUR7Zj2wNbkU3Qd8ZjRl1i7gd23/4URKFRGtETCn7gatOrYc2Gw/DvzTCZalhqr8ne3OK2qQO1TK9SqNeVbKJVte2lx5VirlY+1+8db3XdJ0rLem21dx4bwMCv91Oj1WXEF6RSOib4TYnVuqIqJPpB18KRoR/ZVL0YjonQS2iOiV4S1VbZeimQS2iFhFDNLGNk0d/TPSNJ2j6bBFu1608bLCn94lqtM5llyxb8CFtAhp48820Pcqtx3wbOVyFp+rXl51XkvnvJSuUfpOSz/VVoc9qibBrvSKRkSf7OgE3Yjor3QeRES/qNv3gdaRwBYRYwQMEtgiok8k2DVI50FE9EwuRSOiV5Q2tkkr5XPNdvV4Qw2nmVvyxsuXvbty22VX/wSWqN6+mMdW8Z2Wti193YO5hsM9VWma51a09SG4XLl8MgFpkukekg4C7wcGwIdt37lq+a8AbwIWgbPAv7f9t02O2dNIERFbJSY3gq6kAfAB4DXAtcAtkq5dtdpfAvO2fwj4feDdTT9DAltEjNOwV7TOo4YbgcdsP277OeDjwKGVK9j+E9vfHb38C+DKph+hY5eiEdE2oUn2il4BPLni9Sngn1es/0bgfzY9aAJbRIzb3OgeeyQtrHh9dNX8wutV67zuYaV/B8wDP1b76BtIYIuIMZu8V/Sc7fmK5aeAq1a8vhJ4as0xpVcBbwN+zHb16Ag1JLBFxBoTzPZ4ALhG0suArwE3Az+zcgVJNwC/BRwczVHcWAJbRIyZ5Ogethcl3QZ8jmG6xzHbD0t6F7Bg+wTwX4ALgd8bTef5hO2fanLcjgW2Ge2kLeU0FRpiXfgalgq5ZqVctSpzqs71mqOQC1b4/VfnXDVUagiqypNrOgZei6pyAzdovtq0Seax2b4HuGfVe29f8fxVEzvYSMcCW0S0TYLdufMgIvpmxuNaAltEjBPKCLoR0T8JbBHRK8OZ4NsuRTMJbBGxxq4Zj2wJbBExZkfMUiXpGPCTwNO2rxu9dynwCeAAcBJ4g+1vbF8xu2DjnKhSrtZyYdyxJnloAHN6fsNlu1Sae7OUp1bKQyvMW1qRg9f0c6PCz7diTtNt1+F5Q8tmv/Ogztn/CHBw1Xu3A/fZvga4b/Q6InrgfBtbnUdXFQOb7S8AX1/19iHg+Oj5ceC1Ey5XRLRoTqr16KqttrHts30awPZpSZdNsEwR0bIu18bq2PbOA0lHgCMAV1+9b7sPFxENidm/pWqrLZxnJO0HGP274VAjto/anrc9v3fvxVs8XERMy7CNbbYvRbca2E4Ah0fPDwOfnUxxIqILZj2w1Un3+BhwE8MhgE8B7wDuBD4p6Y3AE8Drt7OQETE9wzy2tkvRTDGw2b5lg0WvnHBZWtVk3LCm46WV5tcsjZm2i+9uvPC5Zyu3LWo45+lcxfbLhaHDmua5DQYVeWyl8diWC8s3MSnA7Ol2bayO3HkQEWOk3FIVEb3jwii93ZfAFhFrqM3b0SYggS0ixggzx1LbxWgkgS0i1kiNLSJ6Jm1sM6PpNHBVQw+V0hJK6SBmULl8oMLE2IsVQxM9X9i2mM5RSLkopUVUfLTSd1JKgymZq6h1qJBCU5oyscbRG27fHmEGFUNhzYIdE9gioiblUjQieiiXohHRM06NLSL6R0n3iIg+Ea7seJkFCWwRsYqLAy90XQJbRIwRuRTtjSZT5JXy1Jb8osrlrfZAlYbvcWFsoYKqc7NYOC+lPLbdhculqu900Pr0eFVlb7ts6TyIiB6aS7pHRPRN8tgiolckMzfjt1S1fTEfEZ0zTPeo86hD0kFJX5X0mKTb11n+IkmfGC2/X9KBpp8ggS0i1hDLtR7F/UgD4APAa4BrgVskXbtqtTcC37D9g8D7gP/ctPwJbBExRqNe0TqPGm4EHrP9uO3ngI8Dh1atcwg4Pnr++8ArpWazySSwRcQaYqnWo4YrgCdXvD41em/ddWwvAs8A39+k/Dum86Bcbd6+GF/KxyqNS7a0XJ3vtWv3iyt23jBPrfSHc1DI4VvauOxLy9VT9zXNpdre24Ka/p66XKfYVB7bHkkLK14ftX10xev1fkCrf3R11tmUHRPYIqI+Lde+peqc7fmK5aeAq1a8vhJ4aoN1TknaBVwMfL1uAdbT5T8bEdEKD2v6dR5lDwDXSHqZpAuAm4ETq9Y5ARwePf9p4I/tZre8pMYWEeNM41vpXtiVvSjpNuBzDAeKP2b7YUnvAhZsnwDuAn5b0mMMa2o3Nz1uAltErFWvNlZvV/Y9wD2r3nv7iufPAq+f2AFJYIuINVyepKfjEtgiYq0J1tjakMAWEeNsqN8r2knFwCbpGPCTwNO2rxu9907g54Gzo9XuGF1HN9RerlkTpSFeSqORlvLYlkvzb1Z8jZorfMVLTW92LuTgsfE4dqXPtYvq81a6UXt7R6jo5m9xYmb8UrTOt/MR4OA677/P9vWjxwSCWkR0g4e1tjqPjirW2Gx/YRJ320fEjDAz38bWpD59m6QvSzom6ZKJlSgi2je5BN1WbDWwfRB4OXA9cBp4z0YrSjoiaUHSwtmzz2zxcBExNTZeWqz16KotBTbbZ2wv2V4GPsRwaJKN1j1qe972/N69F2+1nBExNRO9paoVWwpskvavePk64KHJFCciOqHvnQeSPgbcxHB4klPAO4CbJF3PsJnxJPDmbSxjRExbh2tjddTpFb1lnbfv2oay0GZuUHnsrq3ne5Xmvywp5mv5uY0XlvLUSvlKu6vHglt09ZhqVWPRDQr5fbvn/qFyeWn7ak1zJmcz57Ie9z+wRcQOY2Y+QTeBLSJW2QG3VEXEDtThjoE6EtgiYpwzbFFE9FE6DyKidxLY+qE0xM2gImWjWdoB4ML2ixXpHACLFSkdpUbgXdXpGqXly8sbD0sE1VPo7eJ7ldsOVL1chWGNKpX+4xan653ldI6CXIpGRP8YFtMrGhF9YmA5vaIR0Te5FI2IXkkbW0T0UgJbRPSK03kQEX2UzoO+aFD1LuVENclDA1gqbF91/EEpT600LNGLqw9dMSwRwKBiuKddTYclajWJtMfDFmV0j4jon3QeREQPObdURUSvJN0jInppcantEjSSwBYR41Jji4heSmCLiF5JuscOUtVLtFQaT62Qp7b4bPXy0o9sUPE17qoeL62U59ZkvDWoHlOtnKdWWF46L3MznEtWabuDzuxfivb1m4+IrTp/S1WdRwOSLpV0r6RHR/9ess4610v635IelvRlSf+mzr4T2CJirWXXezRzO3Cf7WuA+0avV/su8HO2/wlwEPhNSd9X2nECW0SMO9/GVufRzCHg+Oj5ceC1a4pi/7XtR0fPnwKeBvaWdpw2tohYZWptbPtsnwawfVrSZVUrS7oRuAD4m9KOE9giYq36gW2PpIUVr4/aPnr+haQ/Ai5fZ7u3baY4kvYDvw0cdo37vRLYImKcwUu128/O2Z7fcFf2qzZaJumMpP2j2tp+hpeZ6633UuB/AP/R9l/UKVTa2CJinA3PL9d7NHMCODx6fhj47OoVJF0AfAa42/bv1d1xscYm6SrgbobVyWWGVc33S7oU+ARwADgJvMH2N+oeuHNKtduqqnlp7s4m+65DFX+f5qq/Yhf+tpXmW51TdY5eZa5a0zy1HWt76yMGPJ2BJu8EPinpjcATwOsBJM0D/8H2m4A3AD8KfL+kW0fb3Wr7S1U7rnMpugj8qu0HJV0EfFHSvcCtDLtq75R0O8Ou2rdu+qNFRLcYqH8puvXD2H8PvHKd9xeAN42e/w7wO5vddzH02z5t+8HR828BjwBXUKOrNiJmkIGl5XqPjtpU54GkA8ANwP1ssqs2ImaFp3Upum1qBzZJFwKfAt5i+5uS6m53BDgCcPXV+7ZSxoiYpildim6nWq2QknYzDGoftf3p0dtnRl20VHXV2j5qe972/N69F0+izBGxnQx+frnWo6uKgU3DqtldwCO237tiUbGrNiJmkad1S9W2qXMp+grgZ4GvSDrfxXoHG3TVzqyqlAkopyY00XR4naqyF1JRRPXy3XMNU1maNDCX9l36zrZVj1NAe3ApWgxstv8c2KhBbU1XbUTMvh3TeRARO8ROqLFFxA5jd7pjoI4EtohYq8PJt3UksEXEGDttbBHRO04bW0T0jJnEfAatSmCrq2qKu6b5VsUcutKwRxW5ZqX0O3+nsEJDc4ONlzU9L6X8v8rtS3los93G1NQmBprspAS2iBhnw/NLbZeikQS2iBiXzoOI6KVcikZEr6TGFhF9lM6DiOgVeweNoBsRO4RhKfeKTlLpZFblHjXZtsbyqpHQC1PcFc9yKR9rqeGYaE32XcolG1TkqUH1uSnmoW3v1IHVejzeWsFw+r0EtojoEzttbBHRP2lji4h+cXpFI6JnbFhaTBtbRPRK2tgiom9y50FE9FEC20Q1yR3a7ryjiv1XjdVWZ3mTPLSmGs/NWb39kjf+7KU8NAr/t0p5anMVuYfNctz6zek8iIj+cRJ0I6JnDMu5pSoi+sSG5bSxRUTfzHob28690zci1jcatqjOowlJl0q6V9Kjo38vqVj3pZK+Jum/1tl3AltErOEl13o0dDtwn+1rgPtGrzfy68Cf1d1xAltEjBsl6G53jQ04BBwfPT8OvHa9lST9MLAP+HzdHRfb2CRdBdwNXM5w0LOjtt8v6Z3AzwNnR6veYfueugful+q/D6V8reXScu9utP/Cxo2U8sGqylbaVmpvCrjtHeut2zy9XtF9tk8Pj+nTki5bvYKkOeA9wM8Cr6y74zqdB4vAr9p+UNJFwBcl3Tta9j7bv1H3YBExC7yZXtE9khZWvD5q++j5F5L+iGGlaLW31dz/LwD32H5SqhrtdVwxsI0i6vmo+i1JjwBX1D5CRMwUA5vIzz1ne37Dfdmv2miZpDOS9o9qa/uBp9dZ7V8A/0rSLwAXAhdI+rbtqva4zV3DSDoA3ADcP3rrNklflnRsox4NSUckLUhaOHv2mc0cLiLa4GFgq/No6ARwePT8MPDZNUWx/63tq20fAH4NuLsU1GATgU3ShcCngLfY/ibwQeDlwPUMa3TvWW8720dtz9ue37v34rqHi4gWTSmw3Qm8WtKjwKtHr5E0L+nDTXZcK0FX0m6GQe2jtj8NYPvMiuUfAv6gSUEiohtsWJxCv43tv2edDgHbC8Cb1nn/I8BH6uy7Tq+ogLuAR2y/d8X7+8/3aACvAx6qc8CI6LZNtrF1Up0a2ysYdrV+RdKXRu/dAdwi6XqG5+Ek8OZtKeGYqrPd3ZS8JsPrDJd/b8vHXnaz8zKn7v7Cm6SaNN13WdPpIFvkHRDYbP8568+quUNz1iL6r/eBLSJ2lp1yKRoRO8lOuBSNiJ3FhsXFtkvRTAJbRKxhz/Z4bAlsETEmbWwR0T9pY5u2rub+NMtZ2s4hcAYdzkPbbu0OLdTV32o9CWwR0Su5FI2I3kmvaET0T9rYIqKPZnxa0QS2iBiXNraI6J9cikZE35jZ7zzQNG+dkHQW+NsVb+0Bzk2tAJvT1bJ1tVyQsm3VJMv2j2zvbbIDSX/IsEx1nLN9sMnxtsNUA9uag0sLVTPctKmrZetquSBl26oul21WzXZ6dETEOhLYIqJ32g5sR8urtKarZetquSBl26oul20mtdrGFhGxHdqusUVETFwrgU3SQUlflfSYpOJ09dMk6aSkr0j6kqSFlstyTNLTkh5a8d6lku6V9Ojo30s6VLZ3Svra6Nx9SdJPtFS2qyT9iaRHJD0s6ZdH77d67irK1Ynz1idTvxSVNAD+muGU9qeAB4BbbP/VVAuyAUkngXnbrec8SfpR4NvA3bavG733buDrtu8c/VG4xPZbO1K2dwLftv0b0y7PqrLtB/bbflDSRcAXgdcCt9Liuaso1xvowHnrkzZqbDcCj9l+3PZzwMeBQy2Uo/NsfwH4+qq3DwHHR8+PM/yPMXUblK0TbJ+2/eDo+beAR4AraPncVZQrJqyNwHYF8OSK16fo1pdr4POSvijpSNuFWcc+26dh+B8FuKzl8qx2m6Qvjy5VW7lMXknSAeAG4H46dO5WlQs6dt5mXRuBbb1Z5bvUNfsK2/8MeA3wi6NLrqjng8DLgeuB08B72iyMpAuBTwFvsf3NNsuy0jrl6tR564M2Atsp4KoVr68EnmqhHOuy/dTo36eBzzC8dO6SM6O2mvNtNk+3XJ4X2D5je8n2MvAhWjx3knYzDB4ftf3p0dutn7v1ytWl89YXbQS2B4BrJL1M0gXAzcCJFsqxhqSXjBp1kfQS4MeBh6q3mroTwOHR88PAZ1ssy5jzQWPkdbR07iQJuAt4xPZ7Vyxq9dxtVK6unLc+aSVBd9Sd/ZvAADhm+z9NvRDrkPQDDGtpMBzS6XfbLJukjwE3MRxp4QzwDuC/A58ErgaeAF5ve+qN+BuU7SaGl1MGTgJvPt+mNeWy/UvgfwFf4f9PIXYHw/as1s5dRbluoQPnrU9y50FE9E7uPIiI3klgi4jeSWCLiN5JYIuI3klgi4jeSWCLiN5JYIuI3klgi4je+X/Jx3gdmPs4KwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(lr_2d2.w_G[0:784].reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate is 0.033787191124558746\n",
    "The mean error rate of the 3-fold validation 0.031\n",
    "They are match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = np.loadtxt('data_sneaker_vs_sandal/x_train.csv', delimiter=',', skiprows=1)\n",
    "y_tr = np.loadtxt('data_sneaker_vs_sandal/y_train.csv', delimiter=',', skiprows=1)\n",
    "x_te = np.loadtxt('data_sneaker_vs_sandal/x_test.csv', delimiter=',', skiprows=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I checked the data set, I noticed most of the picture are like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is not very clear, and it seems has some noise on these pictures, especially on the outlier of the shoes, it doesnt affect the judgement of the classifier of the shoes, but it actually has some very little value that will affect the result.\n",
    "For human, what really affect the result is the profile of the picture. Like whether there is some hole in the shoes or whether it is very thick or thin.\n",
    "So I was thinking whether there is some ways I can delete the irrelevant values and the noise, and just reserve the value of the profile. So the contrast of the image will be enhanced, then wo can clearly determine which part of the image is shoes and which part is hole.\n",
    "So I made some try, I tried some threshold, the pixels which are larger than the threshold will be turned to 1 and the pixels which are smaller than the threshold will be turned to 0. After several times of trying, I noticed that 0.095 was the best threshold. In this situation, the image was the most clear one and I can distinguish the image in a very high accuracy.\n",
    "The image in the new training set is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_trbw, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trbw = x_tr.copy()\n",
    "x_tebw = x_te.copy()\n",
    "for i in range(12000):\n",
    "    x_trbw[i] = np.where(x_tr[i] < 0.1, x_trbw[i], 1)\n",
    "    x_trbw[i] = np.where(x_tr[i] > 0.1, x_trbw[i], 0)\n",
    "\n",
    "for i in range(2000):\n",
    "    x_tebw[i] = np.where(x_te[i] < 0.1, x_tebw[i], 1)\n",
    "    x_tebw[i] = np.where(x_te[i] > 0.1, x_tebw[i], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.1\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.048868  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         1.004322  avg_L1_norm_grad         0.095532  w[0]    0.000 bias    0.000\n",
      "iter    2/30000  loss         1.976237  avg_L1_norm_grad         0.179454  w[0]    0.000 bias    0.040\n",
      "iter    3/30000  loss         3.361643  avg_L1_norm_grad         0.175121  w[0]    0.000 bias   -0.019\n",
      "iter    4/30000  loss         0.881007  avg_L1_norm_grad         0.082850  w[0]    0.000 bias    0.050\n",
      "iter    5/30000  loss         1.359259  avg_L1_norm_grad         0.114673  w[0]    0.000 bias    0.028\n",
      "iter    6/30000  loss         1.440445  avg_L1_norm_grad         0.153459  w[0]    0.000 bias    0.077\n",
      "iter    7/30000  loss         2.370728  avg_L1_norm_grad         0.147700  w[0]    0.000 bias    0.028\n",
      "iter    8/30000  loss         0.843519  avg_L1_norm_grad         0.084200  w[0]    0.000 bias    0.088\n",
      "iter    9/30000  loss         1.145308  avg_L1_norm_grad         0.094212  w[0]    0.000 bias    0.065\n",
      "iter   10/30000  loss         0.998070  avg_L1_norm_grad         0.116419  w[0]    0.000 bias    0.107\n",
      "iter   11/30000  loss         1.550149  avg_L1_norm_grad         0.117403  w[0]    0.000 bias    0.072\n",
      "iter   12/30000  loss         0.852444  avg_L1_norm_grad         0.099775  w[0]    0.000 bias    0.122\n",
      "iter   13/30000  loss         1.234108  avg_L1_norm_grad         0.102147  w[0]    0.000 bias    0.093\n",
      "iter   14/30000  loss         0.821150  avg_L1_norm_grad         0.100026  w[0]    0.000 bias    0.137\n",
      "iter   15/30000  loss         1.186955  avg_L1_norm_grad         0.100797  w[0]    0.000 bias    0.107\n",
      "iter   16/30000  loss         0.760244  avg_L1_norm_grad         0.094802  w[0]    0.000 bias    0.151\n",
      "iter   17/30000  loss         1.074105  avg_L1_norm_grad         0.095293  w[0]    0.000 bias    0.123\n",
      "iter   18/30000  loss         0.711368  avg_L1_norm_grad         0.090772  w[0]    0.000 bias    0.164\n",
      "iter   19/30000  loss         0.987273  avg_L1_norm_grad         0.090875  w[0]    0.000 bias    0.137\n",
      "iter   20/30000  loss         0.665237  avg_L1_norm_grad         0.086370  w[0]    0.000 bias    0.176\n",
      "iter   21/30000  loss         0.904284  avg_L1_norm_grad         0.086160  w[0]    0.000 bias    0.151\n",
      "iter   40/30000  loss         0.376130  avg_L1_norm_grad         0.038626  w[0]    0.000 bias    0.267\n",
      "iter   41/30000  loss         0.395685  avg_L1_norm_grad         0.037041  w[0]    0.000 bias    0.257\n",
      "iter   60/30000  loss         0.291476  avg_L1_norm_grad         0.007397  w[0]    0.000 bias    0.328\n",
      "iter   61/30000  loss         0.290076  avg_L1_norm_grad         0.006578  w[0]    0.000 bias    0.328\n",
      "iter   80/30000  loss         0.271437  avg_L1_norm_grad         0.001914  w[0]    0.000 bias    0.383\n",
      "iter   81/30000  loss         0.270686  avg_L1_norm_grad         0.001895  w[0]    0.000 bias    0.386\n",
      "iter  100/30000  loss         0.258137  avg_L1_norm_grad         0.001672  w[0]    0.000 bias    0.435\n",
      "iter  101/30000  loss         0.257554  avg_L1_norm_grad         0.001662  w[0]    0.000 bias    0.437\n",
      "iter  120/30000  loss         0.247619  avg_L1_norm_grad         0.001494  w[0]    0.000 bias    0.483\n",
      "iter  121/30000  loss         0.247148  avg_L1_norm_grad         0.001487  w[0]    0.000 bias    0.485\n",
      "iter  140/30000  loss         0.239013  avg_L1_norm_grad         0.001355  w[0]    0.000 bias    0.528\n",
      "iter  141/30000  loss         0.238622  avg_L1_norm_grad         0.001349  w[0]    0.000 bias    0.530\n",
      "iter  160/30000  loss         0.231791  avg_L1_norm_grad         0.001243  w[0]    0.000 bias    0.571\n",
      "iter  161/30000  loss         0.231459  avg_L1_norm_grad         0.001238  w[0]    0.000 bias    0.573\n",
      "iter  180/30000  loss         0.225609  avg_L1_norm_grad         0.001151  w[0]    0.000 bias    0.611\n",
      "iter  181/30000  loss         0.225322  avg_L1_norm_grad         0.001147  w[0]    0.000 bias    0.613\n",
      "iter  200/30000  loss         0.220232  avg_L1_norm_grad         0.001074  w[0]    0.000 bias    0.650\n",
      "iter  201/30000  loss         0.219981  avg_L1_norm_grad         0.001071  w[0]    0.000 bias    0.652\n",
      "iter  220/30000  loss         0.215494  avg_L1_norm_grad         0.001009  w[0]    0.000 bias    0.688\n",
      "iter  221/30000  loss         0.215271  avg_L1_norm_grad         0.001006  w[0]    0.000 bias    0.689\n",
      "iter  240/30000  loss         0.211272  avg_L1_norm_grad         0.000953  w[0]    0.000 bias    0.724\n",
      "iter  241/30000  loss         0.211073  avg_L1_norm_grad         0.000951  w[0]    0.000 bias    0.725\n",
      "iter  260/30000  loss         0.207477  avg_L1_norm_grad         0.000905  w[0]    0.000 bias    0.758\n",
      "iter  261/30000  loss         0.207297  avg_L1_norm_grad         0.000903  w[0]    0.000 bias    0.760\n",
      "iter  280/30000  loss         0.204036  avg_L1_norm_grad         0.000862  w[0]    0.000 bias    0.792\n",
      "iter  281/30000  loss         0.203872  avg_L1_norm_grad         0.000860  w[0]    0.000 bias    0.794\n",
      "iter  300/30000  loss         0.200897  avg_L1_norm_grad         0.000824  w[0]    0.000 bias    0.825\n",
      "iter  301/30000  loss         0.200747  avg_L1_norm_grad         0.000822  w[0]    0.000 bias    0.826\n",
      "iter  320/30000  loss         0.198015  avg_L1_norm_grad         0.000790  w[0]    0.000 bias    0.856\n",
      "iter  321/30000  loss         0.197877  avg_L1_norm_grad         0.000788  w[0]    0.000 bias    0.858\n",
      "iter  340/30000  loss         0.195355  avg_L1_norm_grad         0.000758  w[0]    0.000 bias    0.887\n",
      "iter  341/30000  loss         0.195227  avg_L1_norm_grad         0.000757  w[0]    0.000 bias    0.889\n",
      "iter  360/30000  loss         0.192890  avg_L1_norm_grad         0.000730  w[0]    0.000 bias    0.917\n",
      "iter  361/30000  loss         0.192771  avg_L1_norm_grad         0.000729  w[0]    0.000 bias    0.919\n",
      "iter  380/30000  loss         0.190594  avg_L1_norm_grad         0.000705  w[0]    0.000 bias    0.947\n",
      "iter  381/30000  loss         0.190484  avg_L1_norm_grad         0.000704  w[0]    0.000 bias    0.948\n",
      "iter  400/30000  loss         0.188450  avg_L1_norm_grad         0.000682  w[0]    0.000 bias    0.976\n",
      "iter  401/30000  loss         0.188347  avg_L1_norm_grad         0.000680  w[0]    0.000 bias    0.977\n",
      "iter  420/30000  loss         0.186440  avg_L1_norm_grad         0.000660  w[0]    0.000 bias    1.004\n",
      "iter  421/30000  loss         0.186343  avg_L1_norm_grad         0.000659  w[0]    0.000 bias    1.005\n",
      "iter  440/30000  loss         0.184550  avg_L1_norm_grad         0.000640  w[0]    0.000 bias    1.032\n",
      "iter  441/30000  loss         0.184459  avg_L1_norm_grad         0.000639  w[0]    0.000 bias    1.033\n",
      "iter  460/30000  loss         0.182769  avg_L1_norm_grad         0.000622  w[0]    0.000 bias    1.059\n",
      "iter  461/30000  loss         0.182682  avg_L1_norm_grad         0.000621  w[0]    0.000 bias    1.060\n",
      "iter  480/30000  loss         0.181085  avg_L1_norm_grad         0.000605  w[0]    0.000 bias    1.086\n",
      "iter  481/30000  loss         0.181003  avg_L1_norm_grad         0.000604  w[0]    0.000 bias    1.087\n",
      "iter  500/30000  loss         0.179490  avg_L1_norm_grad         0.000589  w[0]    0.000 bias    1.112\n",
      "iter  501/30000  loss         0.179412  avg_L1_norm_grad         0.000588  w[0]    0.000 bias    1.113\n",
      "iter  520/30000  loss         0.177976  avg_L1_norm_grad         0.000574  w[0]    0.000 bias    1.138\n",
      "iter  521/30000  loss         0.177902  avg_L1_norm_grad         0.000573  w[0]    0.000 bias    1.139\n",
      "iter  540/30000  loss         0.176536  avg_L1_norm_grad         0.000560  w[0]    0.000 bias    1.163\n",
      "iter  541/30000  loss         0.176465  avg_L1_norm_grad         0.000559  w[0]    0.000 bias    1.164\n",
      "iter  560/30000  loss         0.175164  avg_L1_norm_grad         0.000546  w[0]    0.000 bias    1.188\n",
      "iter  561/30000  loss         0.175097  avg_L1_norm_grad         0.000546  w[0]    0.000 bias    1.189\n",
      "iter  580/30000  loss         0.173854  avg_L1_norm_grad         0.000534  w[0]    0.000 bias    1.213\n",
      "iter  581/30000  loss         0.173790  avg_L1_norm_grad         0.000533  w[0]    0.000 bias    1.214\n",
      "iter  600/30000  loss         0.172602  avg_L1_norm_grad         0.000522  w[0]    0.000 bias    1.237\n",
      "iter  601/30000  loss         0.172541  avg_L1_norm_grad         0.000521  w[0]    0.000 bias    1.238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.171404  avg_L1_norm_grad         0.000510  w[0]    0.000 bias    1.261\n",
      "iter  621/30000  loss         0.171346  avg_L1_norm_grad         0.000510  w[0]    0.000 bias    1.262\n",
      "iter  640/30000  loss         0.170256  avg_L1_norm_grad         0.000500  w[0]    0.000 bias    1.284\n",
      "iter  641/30000  loss         0.170200  avg_L1_norm_grad         0.000499  w[0]    0.000 bias    1.285\n",
      "iter  660/30000  loss         0.169154  avg_L1_norm_grad         0.000489  w[0]    0.000 bias    1.308\n",
      "iter  661/30000  loss         0.169100  avg_L1_norm_grad         0.000489  w[0]    0.000 bias    1.309\n",
      "iter  680/30000  loss         0.168094  avg_L1_norm_grad         0.000480  w[0]    0.000 bias    1.331\n",
      "iter  681/30000  loss         0.168042  avg_L1_norm_grad         0.000479  w[0]    0.000 bias    1.332\n",
      "iter  700/30000  loss         0.167075  avg_L1_norm_grad         0.000470  w[0]    0.000 bias    1.353\n",
      "iter  701/30000  loss         0.167025  avg_L1_norm_grad         0.000470  w[0]    0.000 bias    1.354\n",
      "iter  720/30000  loss         0.166094  avg_L1_norm_grad         0.000461  w[0]    0.000 bias    1.376\n",
      "iter  721/30000  loss         0.166046  avg_L1_norm_grad         0.000461  w[0]    0.000 bias    1.377\n",
      "iter  740/30000  loss         0.165148  avg_L1_norm_grad         0.000453  w[0]    0.000 bias    1.398\n",
      "iter  741/30000  loss         0.165102  avg_L1_norm_grad         0.000453  w[0]    0.000 bias    1.399\n",
      "iter  760/30000  loss         0.164235  avg_L1_norm_grad         0.000445  w[0]    0.000 bias    1.420\n",
      "iter  761/30000  loss         0.164190  avg_L1_norm_grad         0.000445  w[0]    0.000 bias    1.421\n",
      "iter  780/30000  loss         0.163353  avg_L1_norm_grad         0.000437  w[0]    0.000 bias    1.441\n",
      "iter  781/30000  loss         0.163310  avg_L1_norm_grad         0.000437  w[0]    0.000 bias    1.442\n",
      "iter  800/30000  loss         0.162501  avg_L1_norm_grad         0.000430  w[0]    0.000 bias    1.463\n",
      "iter  801/30000  loss         0.162459  avg_L1_norm_grad         0.000429  w[0]    0.000 bias    1.464\n",
      "iter  820/30000  loss         0.161676  avg_L1_norm_grad         0.000423  w[0]    0.000 bias    1.484\n",
      "iter  821/30000  loss         0.161636  avg_L1_norm_grad         0.000422  w[0]    0.000 bias    1.485\n",
      "iter  840/30000  loss         0.160878  avg_L1_norm_grad         0.000416  w[0]    0.000 bias    1.505\n",
      "iter  841/30000  loss         0.160839  avg_L1_norm_grad         0.000416  w[0]    0.000 bias    1.506\n",
      "iter  860/30000  loss         0.160104  avg_L1_norm_grad         0.000409  w[0]    0.000 bias    1.526\n",
      "iter  861/30000  loss         0.160066  avg_L1_norm_grad         0.000409  w[0]    0.000 bias    1.527\n",
      "iter  880/30000  loss         0.159354  avg_L1_norm_grad         0.000403  w[0]    0.000 bias    1.546\n",
      "iter  881/30000  loss         0.159317  avg_L1_norm_grad         0.000403  w[0]    0.000 bias    1.547\n",
      "iter  900/30000  loss         0.158626  avg_L1_norm_grad         0.000397  w[0]    0.000 bias    1.566\n",
      "iter  901/30000  loss         0.158590  avg_L1_norm_grad         0.000397  w[0]    0.000 bias    1.567\n",
      "iter  920/30000  loss         0.157920  avg_L1_norm_grad         0.000391  w[0]    0.000 bias    1.586\n",
      "iter  921/30000  loss         0.157885  avg_L1_norm_grad         0.000391  w[0]    0.000 bias    1.587\n",
      "iter  940/30000  loss         0.157233  avg_L1_norm_grad         0.000385  w[0]    0.000 bias    1.606\n",
      "iter  941/30000  loss         0.157199  avg_L1_norm_grad         0.000385  w[0]    0.000 bias    1.607\n",
      "iter  960/30000  loss         0.156565  avg_L1_norm_grad         0.000380  w[0]    0.000 bias    1.626\n",
      "iter  961/30000  loss         0.156532  avg_L1_norm_grad         0.000379  w[0]    0.000 bias    1.627\n",
      "iter  980/30000  loss         0.155916  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.646\n",
      "iter  981/30000  loss         0.155884  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.647\n",
      "iter 1000/30000  loss         0.155284  avg_L1_norm_grad         0.000369  w[0]    0.000 bias    1.665\n",
      "iter 1001/30000  loss         0.155253  avg_L1_norm_grad         0.000369  w[0]    0.000 bias    1.666\n",
      "iter 1020/30000  loss         0.154668  avg_L1_norm_grad         0.000364  w[0]    0.000 bias    1.684\n",
      "iter 1021/30000  loss         0.154638  avg_L1_norm_grad         0.000364  w[0]    0.000 bias    1.685\n",
      "iter 1040/30000  loss         0.154069  avg_L1_norm_grad         0.000359  w[0]    0.000 bias    1.703\n",
      "iter 1041/30000  loss         0.154039  avg_L1_norm_grad         0.000359  w[0]    0.000 bias    1.704\n",
      "iter 1060/30000  loss         0.153484  avg_L1_norm_grad         0.000355  w[0]    0.000 bias    1.722\n",
      "iter 1061/30000  loss         0.153455  avg_L1_norm_grad         0.000355  w[0]    0.000 bias    1.723\n",
      "iter 1080/30000  loss         0.152914  avg_L1_norm_grad         0.000350  w[0]    0.000 bias    1.741\n",
      "iter 1081/30000  loss         0.152886  avg_L1_norm_grad         0.000350  w[0]    0.000 bias    1.742\n",
      "iter 1100/30000  loss         0.152358  avg_L1_norm_grad         0.000346  w[0]    0.000 bias    1.759\n",
      "iter 1101/30000  loss         0.152330  avg_L1_norm_grad         0.000346  w[0]    0.000 bias    1.760\n",
      "iter 1120/30000  loss         0.151815  avg_L1_norm_grad         0.000342  w[0]    0.000 bias    1.778\n",
      "iter 1121/30000  loss         0.151788  avg_L1_norm_grad         0.000342  w[0]    0.000 bias    1.779\n",
      "iter 1140/30000  loss         0.151284  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    1.796\n",
      "iter 1141/30000  loss         0.151258  avg_L1_norm_grad         0.000337  w[0]    0.000 bias    1.797\n",
      "iter 1160/30000  loss         0.150766  avg_L1_norm_grad         0.000334  w[0]    0.000 bias    1.814\n",
      "iter 1161/30000  loss         0.150741  avg_L1_norm_grad         0.000333  w[0]    0.000 bias    1.815\n",
      "iter 1180/30000  loss         0.150260  avg_L1_norm_grad         0.000330  w[0]    0.000 bias    1.832\n",
      "iter 1181/30000  loss         0.150235  avg_L1_norm_grad         0.000330  w[0]    0.000 bias    1.833\n",
      "iter 1200/30000  loss         0.149765  avg_L1_norm_grad         0.000326  w[0]    0.000 bias    1.850\n",
      "iter 1201/30000  loss         0.149740  avg_L1_norm_grad         0.000326  w[0]    0.000 bias    1.851\n",
      "iter 1220/30000  loss         0.149281  avg_L1_norm_grad         0.000322  w[0]    0.000 bias    1.868\n",
      "iter 1221/30000  loss         0.149257  avg_L1_norm_grad         0.000322  w[0]    0.000 bias    1.869\n",
      "iter 1240/30000  loss         0.148807  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    1.885\n",
      "iter 1241/30000  loss         0.148783  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    1.886\n",
      "iter 1260/30000  loss         0.148343  avg_L1_norm_grad         0.000315  w[0]    0.000 bias    1.903\n",
      "iter 1261/30000  loss         0.148320  avg_L1_norm_grad         0.000315  w[0]    0.000 bias    1.904\n",
      "iter 1280/30000  loss         0.147888  avg_L1_norm_grad         0.000312  w[0]    0.000 bias    1.920\n",
      "iter 1281/30000  loss         0.147866  avg_L1_norm_grad         0.000312  w[0]    0.000 bias    1.921\n",
      "iter 1300/30000  loss         0.147444  avg_L1_norm_grad         0.000309  w[0]    0.000 bias    1.937\n",
      "iter 1301/30000  loss         0.147422  avg_L1_norm_grad         0.000309  w[0]    0.000 bias    1.938\n",
      "iter 1320/30000  loss         0.147008  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    1.955\n",
      "iter 1321/30000  loss         0.146986  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    1.955\n",
      "iter 1340/30000  loss         0.146580  avg_L1_norm_grad         0.000302  w[0]    0.000 bias    1.971\n",
      "iter 1341/30000  loss         0.146559  avg_L1_norm_grad         0.000302  w[0]    0.000 bias    1.972\n",
      "iter 1360/30000  loss         0.146161  avg_L1_norm_grad         0.000299  w[0]    0.000 bias    1.988\n",
      "iter 1361/30000  loss         0.146141  avg_L1_norm_grad         0.000299  w[0]    0.000 bias    1.989\n",
      "iter 1380/30000  loss         0.145750  avg_L1_norm_grad         0.000296  w[0]    0.000 bias    2.005\n",
      "iter 1381/30000  loss         0.145730  avg_L1_norm_grad         0.000296  w[0]    0.000 bias    2.006\n",
      "iter 1400/30000  loss         0.145347  avg_L1_norm_grad         0.000294  w[0]    0.000 bias    2.022\n",
      "iter 1401/30000  loss         0.145327  avg_L1_norm_grad         0.000293  w[0]    0.000 bias    2.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.144952  avg_L1_norm_grad         0.000291  w[0]    0.000 bias    2.038\n",
      "iter 1421/30000  loss         0.144932  avg_L1_norm_grad         0.000291  w[0]    0.000 bias    2.039\n",
      "iter 1440/30000  loss         0.144563  avg_L1_norm_grad         0.000288  w[0]    0.000 bias    2.055\n",
      "iter 1441/30000  loss         0.144544  avg_L1_norm_grad         0.000288  w[0]    0.000 bias    2.055\n",
      "iter 1460/30000  loss         0.144182  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    2.071\n",
      "iter 1461/30000  loss         0.144163  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    2.072\n",
      "iter 1480/30000  loss         0.143808  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    2.087\n",
      "iter 1481/30000  loss         0.143789  avg_L1_norm_grad         0.000282  w[0]    0.000 bias    2.088\n",
      "iter 1500/30000  loss         0.143440  avg_L1_norm_grad         0.000280  w[0]    0.000 bias    2.103\n",
      "iter 1501/30000  loss         0.143422  avg_L1_norm_grad         0.000280  w[0]    0.000 bias    2.104\n",
      "iter 1520/30000  loss         0.143079  avg_L1_norm_grad         0.000277  w[0]    0.000 bias    2.119\n",
      "iter 1521/30000  loss         0.143061  avg_L1_norm_grad         0.000277  w[0]    0.000 bias    2.120\n",
      "iter 1540/30000  loss         0.142723  avg_L1_norm_grad         0.000275  w[0]    0.000 bias    2.135\n",
      "iter 1541/30000  loss         0.142706  avg_L1_norm_grad         0.000275  w[0]    0.000 bias    2.136\n",
      "iter 1560/30000  loss         0.142374  avg_L1_norm_grad         0.000273  w[0]    0.000 bias    2.151\n",
      "iter 1561/30000  loss         0.142357  avg_L1_norm_grad         0.000272  w[0]    0.000 bias    2.152\n",
      "iter 1580/30000  loss         0.142031  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    2.167\n",
      "iter 1581/30000  loss         0.142014  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    2.167\n",
      "iter 1600/30000  loss         0.141693  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    2.182\n",
      "iter 1601/30000  loss         0.141676  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    2.183\n",
      "iter 1620/30000  loss         0.141361  avg_L1_norm_grad         0.000266  w[0]    0.000 bias    2.198\n",
      "iter 1621/30000  loss         0.141345  avg_L1_norm_grad         0.000266  w[0]    0.000 bias    2.199\n",
      "iter 1640/30000  loss         0.141034  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    2.213\n",
      "iter 1641/30000  loss         0.141018  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    2.214\n",
      "iter 1660/30000  loss         0.140713  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    2.229\n",
      "iter 1661/30000  loss         0.140697  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    2.229\n",
      "iter 1680/30000  loss         0.140396  avg_L1_norm_grad         0.000259  w[0]    0.000 bias    2.244\n",
      "iter 1681/30000  loss         0.140380  avg_L1_norm_grad         0.000259  w[0]    0.000 bias    2.245\n",
      "iter 1700/30000  loss         0.140084  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    2.259\n",
      "iter 1701/30000  loss         0.140069  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    2.260\n",
      "iter 1720/30000  loss         0.139777  avg_L1_norm_grad         0.000255  w[0]    0.000 bias    2.274\n",
      "iter 1721/30000  loss         0.139762  avg_L1_norm_grad         0.000255  w[0]    0.000 bias    2.275\n",
      "iter 1740/30000  loss         0.139475  avg_L1_norm_grad         0.000253  w[0]    0.000 bias    2.289\n",
      "iter 1741/30000  loss         0.139460  avg_L1_norm_grad         0.000253  w[0]    0.000 bias    2.290\n",
      "iter 1760/30000  loss         0.139177  avg_L1_norm_grad         0.000251  w[0]    0.000 bias    2.304\n",
      "iter 1761/30000  loss         0.139163  avg_L1_norm_grad         0.000251  w[0]    0.000 bias    2.305\n",
      "iter 1780/30000  loss         0.138884  avg_L1_norm_grad         0.000249  w[0]    0.000 bias    2.319\n",
      "iter 1781/30000  loss         0.138870  avg_L1_norm_grad         0.000249  w[0]    0.000 bias    2.320\n",
      "iter 1800/30000  loss         0.138595  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    2.334\n",
      "iter 1801/30000  loss         0.138581  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    2.334\n",
      "iter 1820/30000  loss         0.138310  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.348\n",
      "iter 1821/30000  loss         0.138296  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.349\n",
      "iter 1840/30000  loss         0.138029  avg_L1_norm_grad         0.000244  w[0]    0.000 bias    2.363\n",
      "iter 1841/30000  loss         0.138015  avg_L1_norm_grad         0.000243  w[0]    0.000 bias    2.364\n",
      "iter 1860/30000  loss         0.137752  avg_L1_norm_grad         0.000242  w[0]    0.000 bias    2.377\n",
      "iter 1861/30000  loss         0.137739  avg_L1_norm_grad         0.000242  w[0]    0.000 bias    2.378\n",
      "iter 1880/30000  loss         0.137479  avg_L1_norm_grad         0.000240  w[0]    0.000 bias    2.392\n",
      "iter 1881/30000  loss         0.137466  avg_L1_norm_grad         0.000240  w[0]    0.000 bias    2.393\n",
      "iter 1900/30000  loss         0.137210  avg_L1_norm_grad         0.000238  w[0]    0.000 bias    2.406\n",
      "iter 1901/30000  loss         0.137197  avg_L1_norm_grad         0.000238  w[0]    0.000 bias    2.407\n",
      "iter 1920/30000  loss         0.136945  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    2.421\n",
      "iter 1921/30000  loss         0.136931  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    2.421\n",
      "iter 1940/30000  loss         0.136682  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    2.435\n",
      "iter 1941/30000  loss         0.136669  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    2.436\n",
      "iter 1960/30000  loss         0.136424  avg_L1_norm_grad         0.000233  w[0]    0.000 bias    2.449\n",
      "iter 1961/30000  loss         0.136411  avg_L1_norm_grad         0.000233  w[0]    0.000 bias    2.450\n",
      "iter 1980/30000  loss         0.136169  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    2.463\n",
      "iter 1981/30000  loss         0.136156  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    2.464\n",
      "iter 2000/30000  loss         0.135917  avg_L1_norm_grad         0.000230  w[0]    0.000 bias    2.477\n",
      "iter 2001/30000  loss         0.135905  avg_L1_norm_grad         0.000230  w[0]    0.000 bias    2.478\n",
      "iter 2020/30000  loss         0.135669  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.491\n",
      "iter 2021/30000  loss         0.135656  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.492\n",
      "iter 2040/30000  loss         0.135423  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    2.505\n",
      "iter 2041/30000  loss         0.135411  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    2.506\n",
      "iter 2060/30000  loss         0.135181  avg_L1_norm_grad         0.000225  w[0]    0.000 bias    2.519\n",
      "iter 2061/30000  loss         0.135169  avg_L1_norm_grad         0.000225  w[0]    0.000 bias    2.519\n",
      "iter 2080/30000  loss         0.134942  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    2.532\n",
      "iter 2081/30000  loss         0.134930  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    2.533\n",
      "iter 2100/30000  loss         0.134706  avg_L1_norm_grad         0.000222  w[0]    0.000 bias    2.546\n",
      "iter 2101/30000  loss         0.134694  avg_L1_norm_grad         0.000222  w[0]    0.000 bias    2.547\n",
      "iter 2120/30000  loss         0.134472  avg_L1_norm_grad         0.000221  w[0]    0.000 bias    2.560\n",
      "iter 2121/30000  loss         0.134461  avg_L1_norm_grad         0.000221  w[0]    0.000 bias    2.560\n",
      "iter 2140/30000  loss         0.134242  avg_L1_norm_grad         0.000219  w[0]    0.000 bias    2.573\n",
      "iter 2141/30000  loss         0.134230  avg_L1_norm_grad         0.000219  w[0]    0.000 bias    2.574\n",
      "iter 2160/30000  loss         0.134014  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.587\n",
      "iter 2161/30000  loss         0.134003  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.588\n",
      "iter 2180/30000  loss         0.133789  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    2.600\n",
      "iter 2181/30000  loss         0.133778  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    2.601\n",
      "iter 2200/30000  loss         0.133567  avg_L1_norm_grad         0.000215  w[0]    0.000 bias    2.614\n",
      "iter 2201/30000  loss         0.133556  avg_L1_norm_grad         0.000215  w[0]    0.000 bias    2.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.133347  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    2.627\n",
      "iter 2221/30000  loss         0.133336  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    2.628\n",
      "iter 2240/30000  loss         0.133130  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.640\n",
      "iter 2241/30000  loss         0.133119  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.641\n",
      "iter 2260/30000  loss         0.132915  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.653\n",
      "iter 2261/30000  loss         0.132904  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.654\n",
      "iter 2280/30000  loss         0.132703  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    2.667\n",
      "iter 2281/30000  loss         0.132692  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    2.667\n",
      "iter 2300/30000  loss         0.132493  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    2.680\n",
      "iter 2301/30000  loss         0.132482  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    2.680\n",
      "iter 2320/30000  loss         0.132285  avg_L1_norm_grad         0.000208  w[0]    0.000 bias    2.693\n",
      "iter 2321/30000  loss         0.132275  avg_L1_norm_grad         0.000208  w[0]    0.000 bias    2.693\n",
      "iter 2340/30000  loss         0.132080  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    2.706\n",
      "iter 2341/30000  loss         0.132070  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    2.706\n",
      "iter 2360/30000  loss         0.131877  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    2.719\n",
      "iter 2361/30000  loss         0.131867  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    2.719\n",
      "iter 2380/30000  loss         0.131676  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    2.731\n",
      "iter 2381/30000  loss         0.131666  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    2.732\n",
      "iter 2400/30000  loss         0.131478  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    2.744\n",
      "iter 2401/30000  loss         0.131468  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    2.745\n",
      "iter 2420/30000  loss         0.131281  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    2.757\n",
      "iter 2421/30000  loss         0.131272  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    2.758\n",
      "iter 2440/30000  loss         0.131087  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    2.770\n",
      "iter 2441/30000  loss         0.131077  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    2.770\n",
      "iter 2460/30000  loss         0.130895  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    2.782\n",
      "iter 2461/30000  loss         0.130885  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    2.783\n",
      "iter 2480/30000  loss         0.130704  avg_L1_norm_grad         0.000198  w[0]    0.000 bias    2.795\n",
      "iter 2481/30000  loss         0.130695  avg_L1_norm_grad         0.000198  w[0]    0.000 bias    2.795\n",
      "iter 2500/30000  loss         0.130516  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    2.807\n",
      "iter 2501/30000  loss         0.130507  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    2.808\n",
      "iter 2520/30000  loss         0.130330  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    2.820\n",
      "iter 2521/30000  loss         0.130320  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    2.820\n",
      "iter 2540/30000  loss         0.130145  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    2.832\n",
      "iter 2541/30000  loss         0.130136  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    2.833\n",
      "iter 2560/30000  loss         0.129962  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.845\n",
      "iter 2561/30000  loss         0.129953  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.845\n",
      "iter 2580/30000  loss         0.129781  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.857\n",
      "iter 2581/30000  loss         0.129772  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.858\n",
      "iter 2600/30000  loss         0.129602  avg_L1_norm_grad         0.000192  w[0]    0.000 bias    2.869\n",
      "iter 2601/30000  loss         0.129594  avg_L1_norm_grad         0.000192  w[0]    0.000 bias    2.870\n",
      "iter 2620/30000  loss         0.129425  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.882\n",
      "iter 2621/30000  loss         0.129416  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.882\n",
      "iter 2640/30000  loss         0.129250  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.894\n",
      "iter 2641/30000  loss         0.129241  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.894\n",
      "iter 2660/30000  loss         0.129076  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.906\n",
      "iter 2661/30000  loss         0.129067  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.906\n",
      "iter 2680/30000  loss         0.128904  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.918\n",
      "iter 2681/30000  loss         0.128895  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.919\n",
      "iter 2700/30000  loss         0.128733  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.930\n",
      "iter 2701/30000  loss         0.128725  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.931\n",
      "iter 2720/30000  loss         0.128564  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.942\n",
      "iter 2721/30000  loss         0.128556  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.943\n",
      "iter 2740/30000  loss         0.128397  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.954\n",
      "iter 2741/30000  loss         0.128389  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.955\n",
      "iter 2760/30000  loss         0.128231  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.966\n",
      "iter 2761/30000  loss         0.128223  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.966\n",
      "iter 2780/30000  loss         0.128067  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.978\n",
      "iter 2781/30000  loss         0.128059  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.978\n",
      "iter 2800/30000  loss         0.127904  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.990\n",
      "iter 2801/30000  loss         0.127896  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.990\n",
      "iter 2820/30000  loss         0.127743  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    3.001\n",
      "iter 2821/30000  loss         0.127735  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    3.002\n",
      "iter 2840/30000  loss         0.127583  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    3.013\n",
      "iter 2841/30000  loss         0.127575  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    3.014\n",
      "iter 2860/30000  loss         0.127425  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    3.025\n",
      "iter 2861/30000  loss         0.127417  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    3.025\n",
      "iter 2880/30000  loss         0.127268  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    3.036\n",
      "iter 2881/30000  loss         0.127260  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    3.037\n",
      "iter 2900/30000  loss         0.127113  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    3.048\n",
      "iter 2901/30000  loss         0.127105  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    3.048\n",
      "iter 2920/30000  loss         0.126959  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    3.059\n",
      "iter 2921/30000  loss         0.126951  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    3.060\n",
      "iter 2940/30000  loss         0.126806  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    3.071\n",
      "iter 2941/30000  loss         0.126798  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    3.072\n",
      "iter 2960/30000  loss         0.126654  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    3.082\n",
      "iter 2961/30000  loss         0.126647  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    3.083\n",
      "iter 2980/30000  loss         0.126504  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    3.094\n",
      "iter 2981/30000  loss         0.126497  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    3.094\n",
      "iter 3000/30000  loss         0.126356  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    3.105\n",
      "iter 3001/30000  loss         0.126348  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    3.106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.126208  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    3.117\n",
      "iter 3021/30000  loss         0.126201  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    3.117\n",
      "iter 3040/30000  loss         0.126062  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.128\n",
      "iter 3041/30000  loss         0.126054  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.128\n",
      "iter 3060/30000  loss         0.125917  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.139\n",
      "iter 3061/30000  loss         0.125909  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.140\n",
      "iter 3080/30000  loss         0.125773  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.150\n",
      "iter 3081/30000  loss         0.125766  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.151\n",
      "iter 3100/30000  loss         0.125630  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    3.162\n",
      "iter 3101/30000  loss         0.125623  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    3.162\n",
      "iter 3120/30000  loss         0.125489  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.173\n",
      "iter 3121/30000  loss         0.125482  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.173\n",
      "iter 3140/30000  loss         0.125348  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.184\n",
      "iter 3141/30000  loss         0.125341  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.184\n",
      "iter 3160/30000  loss         0.125209  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.195\n",
      "iter 3161/30000  loss         0.125202  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.195\n",
      "iter 3180/30000  loss         0.125071  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    3.206\n",
      "iter 3181/30000  loss         0.125064  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    3.207\n",
      "iter 3200/30000  loss         0.124934  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.217\n",
      "iter 3201/30000  loss         0.124928  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.218\n",
      "iter 3220/30000  loss         0.124799  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.228\n",
      "iter 3221/30000  loss         0.124792  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    3.228\n",
      "iter 3240/30000  loss         0.124664  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    3.239\n",
      "iter 3241/30000  loss         0.124657  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    3.239\n",
      "iter 3260/30000  loss         0.124530  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.250\n",
      "iter 3261/30000  loss         0.124524  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.250\n",
      "iter 3280/30000  loss         0.124398  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.261\n",
      "iter 3281/30000  loss         0.124391  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.261\n",
      "iter 3300/30000  loss         0.124266  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.271\n",
      "iter 3301/30000  loss         0.124260  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.272\n",
      "iter 3320/30000  loss         0.124136  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    3.282\n",
      "iter 3321/30000  loss         0.124129  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    3.283\n",
      "iter 3340/30000  loss         0.124006  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.293\n",
      "iter 3341/30000  loss         0.124000  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.293\n",
      "iter 3360/30000  loss         0.123877  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.304\n",
      "iter 3361/30000  loss         0.123871  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.304\n",
      "iter 3380/30000  loss         0.123750  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.314\n",
      "iter 3381/30000  loss         0.123744  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.315\n",
      "iter 3400/30000  loss         0.123623  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.325\n",
      "iter 3401/30000  loss         0.123617  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.325\n",
      "iter 3420/30000  loss         0.123498  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.335\n",
      "iter 3421/30000  loss         0.123492  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.336\n",
      "iter 3440/30000  loss         0.123373  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.346\n",
      "iter 3441/30000  loss         0.123367  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.347\n",
      "iter 3460/30000  loss         0.123249  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.357\n",
      "iter 3461/30000  loss         0.123243  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.357\n",
      "iter 3480/30000  loss         0.123127  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.367\n",
      "iter 3481/30000  loss         0.123120  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.368\n",
      "iter 3500/30000  loss         0.123005  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.378\n",
      "iter 3501/30000  loss         0.122999  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.378\n",
      "iter 3520/30000  loss         0.122884  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.388\n",
      "iter 3521/30000  loss         0.122878  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.388\n",
      "iter 3540/30000  loss         0.122764  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.398\n",
      "iter 3541/30000  loss         0.122758  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.399\n",
      "iter 3560/30000  loss         0.122644  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.409\n",
      "iter 3561/30000  loss         0.122638  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.409\n",
      "iter 3580/30000  loss         0.122526  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.419\n",
      "iter 3581/30000  loss         0.122520  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.420\n",
      "iter 3600/30000  loss         0.122408  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.429\n",
      "iter 3601/30000  loss         0.122403  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.430\n",
      "iter 3620/30000  loss         0.122292  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.440\n",
      "iter 3621/30000  loss         0.122286  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.440\n",
      "iter 3640/30000  loss         0.122176  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.450\n",
      "iter 3641/30000  loss         0.122170  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.450\n",
      "iter 3660/30000  loss         0.122061  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.460\n",
      "iter 3661/30000  loss         0.122055  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.461\n",
      "iter 3680/30000  loss         0.121947  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.470\n",
      "iter 3681/30000  loss         0.121941  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.471\n",
      "iter 3700/30000  loss         0.121833  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.480\n",
      "iter 3701/30000  loss         0.121828  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.481\n",
      "iter 3720/30000  loss         0.121721  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.490\n",
      "iter 3721/30000  loss         0.121715  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.491\n",
      "iter 3740/30000  loss         0.121609  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.501\n",
      "iter 3741/30000  loss         0.121603  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.501\n",
      "iter 3760/30000  loss         0.121498  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.511\n",
      "iter 3761/30000  loss         0.121492  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.511\n",
      "iter 3780/30000  loss         0.121388  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.521\n",
      "iter 3781/30000  loss         0.121382  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.521\n",
      "iter 3800/30000  loss         0.121278  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.531\n",
      "iter 3801/30000  loss         0.121273  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.121169  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.540\n",
      "iter 3821/30000  loss         0.121164  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.541\n",
      "iter 3840/30000  loss         0.121061  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.550\n",
      "iter 3841/30000  loss         0.121056  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.551\n",
      "iter 3860/30000  loss         0.120954  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.560\n",
      "iter 3861/30000  loss         0.120949  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.561\n",
      "iter 3880/30000  loss         0.120848  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.570\n",
      "iter 3881/30000  loss         0.120842  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.571\n",
      "iter 3900/30000  loss         0.120742  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.580\n",
      "iter 3901/30000  loss         0.120736  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.580\n",
      "iter 3920/30000  loss         0.120637  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.590\n",
      "iter 3921/30000  loss         0.120631  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.590\n",
      "iter 3940/30000  loss         0.120532  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.600\n",
      "iter 3941/30000  loss         0.120527  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.600\n",
      "iter 3960/30000  loss         0.120428  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.609\n",
      "iter 3961/30000  loss         0.120423  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.610\n",
      "iter 3980/30000  loss         0.120325  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.619\n",
      "iter 3981/30000  loss         0.120320  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.620\n",
      "iter 4000/30000  loss         0.120223  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.629\n",
      "iter 4001/30000  loss         0.120218  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.629\n",
      "iter 4020/30000  loss         0.120121  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.638\n",
      "iter 4021/30000  loss         0.120116  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.639\n",
      "iter 4040/30000  loss         0.120020  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.648\n",
      "iter 4041/30000  loss         0.120015  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.648\n",
      "iter 4060/30000  loss         0.119920  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.658\n",
      "iter 4061/30000  loss         0.119915  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.658\n",
      "iter 4080/30000  loss         0.119820  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.667\n",
      "iter 4081/30000  loss         0.119815  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.668\n",
      "iter 4100/30000  loss         0.119721  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.677\n",
      "iter 4101/30000  loss         0.119716  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.677\n",
      "iter 4120/30000  loss         0.119622  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.686\n",
      "iter 4121/30000  loss         0.119617  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.687\n",
      "iter 4140/30000  loss         0.119524  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.696\n",
      "iter 4141/30000  loss         0.119519  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.696\n",
      "iter 4160/30000  loss         0.119427  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.705\n",
      "iter 4161/30000  loss         0.119422  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.706\n",
      "iter 4180/30000  loss         0.119330  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.715\n",
      "iter 4181/30000  loss         0.119326  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.715\n",
      "iter 4200/30000  loss         0.119234  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.724\n",
      "iter 4201/30000  loss         0.119230  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.725\n",
      "iter 4220/30000  loss         0.119139  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.733\n",
      "iter 4221/30000  loss         0.119134  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.734\n",
      "iter 4240/30000  loss         0.119044  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.743\n",
      "iter 4241/30000  loss         0.119039  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.743\n",
      "iter 4260/30000  loss         0.118950  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.752\n",
      "iter 4261/30000  loss         0.118945  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.753\n",
      "iter 4280/30000  loss         0.118856  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.761\n",
      "iter 4281/30000  loss         0.118852  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.762\n",
      "iter 4300/30000  loss         0.118763  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.771\n",
      "iter 4301/30000  loss         0.118758  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.771\n",
      "iter 4320/30000  loss         0.118671  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.780\n",
      "iter 4321/30000  loss         0.118666  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.780\n",
      "iter 4340/30000  loss         0.118579  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.789\n",
      "iter 4341/30000  loss         0.118574  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.790\n",
      "iter 4360/30000  loss         0.118487  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.798\n",
      "iter 4361/30000  loss         0.118483  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.799\n",
      "iter 4380/30000  loss         0.118396  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.808\n",
      "iter 4381/30000  loss         0.118392  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.808\n",
      "iter 4400/30000  loss         0.118306  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.817\n",
      "iter 4401/30000  loss         0.118302  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.817\n",
      "iter 4420/30000  loss         0.118216  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.826\n",
      "iter 4421/30000  loss         0.118212  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.826\n",
      "iter 4440/30000  loss         0.118127  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.835\n",
      "iter 4441/30000  loss         0.118123  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.835\n",
      "iter 4460/30000  loss         0.118038  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.844\n",
      "iter 4461/30000  loss         0.118034  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.845\n",
      "iter 4480/30000  loss         0.117950  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.853\n",
      "iter 4481/30000  loss         0.117946  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.854\n",
      "iter 4500/30000  loss         0.117863  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.862\n",
      "iter 4501/30000  loss         0.117858  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.863\n",
      "iter 4520/30000  loss         0.117775  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.871\n",
      "iter 4521/30000  loss         0.117771  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.872\n",
      "iter 4540/30000  loss         0.117689  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.880\n",
      "iter 4541/30000  loss         0.117685  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.881\n",
      "iter 4560/30000  loss         0.117603  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.889\n",
      "iter 4561/30000  loss         0.117598  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.890\n",
      "iter 4580/30000  loss         0.117517  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.898\n",
      "iter 4581/30000  loss         0.117513  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.898\n",
      "iter 4600/30000  loss         0.117432  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.907\n",
      "iter 4601/30000  loss         0.117428  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4620/30000  loss         0.117347  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.916\n",
      "iter 4621/30000  loss         0.117343  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.916\n",
      "iter 4640/30000  loss         0.117263  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.925\n",
      "iter 4641/30000  loss         0.117259  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.925\n",
      "iter 4660/30000  loss         0.117180  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.933\n",
      "iter 4661/30000  loss         0.117175  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.934\n",
      "iter 4680/30000  loss         0.117096  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.942\n",
      "iter 4681/30000  loss         0.117092  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.943\n",
      "iter 4700/30000  loss         0.117014  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.951\n",
      "iter 4701/30000  loss         0.117010  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.952\n",
      "iter 4720/30000  loss         0.116932  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.960\n",
      "iter 4721/30000  loss         0.116927  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.960\n",
      "iter 4740/30000  loss         0.116850  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.969\n",
      "iter 4741/30000  loss         0.116846  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.969\n",
      "iter 4760/30000  loss         0.116768  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.977\n",
      "iter 4761/30000  loss         0.116764  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.978\n",
      "iter 4780/30000  loss         0.116688  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.986\n",
      "iter 4781/30000  loss         0.116684  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.986\n",
      "iter 4800/30000  loss         0.116607  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.995\n",
      "iter 4801/30000  loss         0.116603  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.995\n",
      "iter 4820/30000  loss         0.116527  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    4.003\n",
      "iter 4821/30000  loss         0.116523  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    4.004\n",
      "iter 4840/30000  loss         0.116448  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    4.012\n",
      "iter 4841/30000  loss         0.116444  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    4.012\n",
      "iter 4860/30000  loss         0.116369  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    4.021\n",
      "iter 4861/30000  loss         0.116365  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    4.021\n",
      "iter 4880/30000  loss         0.116290  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    4.029\n",
      "iter 4881/30000  loss         0.116286  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    4.030\n",
      "iter 4900/30000  loss         0.116212  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    4.038\n",
      "iter 4901/30000  loss         0.116208  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    4.038\n",
      "iter 4920/30000  loss         0.116134  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    4.046\n",
      "iter 4921/30000  loss         0.116130  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    4.047\n",
      "iter 4940/30000  loss         0.116057  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    4.055\n",
      "iter 4941/30000  loss         0.116053  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    4.055\n",
      "iter 4960/30000  loss         0.115980  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    4.063\n",
      "iter 4961/30000  loss         0.115976  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    4.064\n",
      "iter 4980/30000  loss         0.115903  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    4.072\n",
      "iter 4981/30000  loss         0.115900  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    4.072\n",
      "iter 5000/30000  loss         0.115827  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    4.080\n",
      "iter 5001/30000  loss         0.115824  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    4.081\n",
      "iter 5020/30000  loss         0.115752  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    4.089\n",
      "iter 5021/30000  loss         0.115748  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    4.089\n",
      "iter 5040/30000  loss         0.115676  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    4.097\n",
      "iter 5041/30000  loss         0.115673  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    4.097\n",
      "iter 5060/30000  loss         0.115602  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    4.105\n",
      "iter 5061/30000  loss         0.115598  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    4.106\n",
      "iter 5080/30000  loss         0.115527  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    4.114\n",
      "iter 5081/30000  loss         0.115523  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    4.114\n",
      "iter 5100/30000  loss         0.115453  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    4.122\n",
      "iter 5101/30000  loss         0.115449  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    4.123\n",
      "iter 5120/30000  loss         0.115379  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    4.130\n",
      "iter 5121/30000  loss         0.115376  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    4.131\n",
      "iter 5140/30000  loss         0.115306  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    4.139\n",
      "iter 5141/30000  loss         0.115302  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    4.139\n",
      "iter 5160/30000  loss         0.115233  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    4.147\n",
      "iter 5161/30000  loss         0.115230  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    4.148\n",
      "iter 5180/30000  loss         0.115161  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    4.155\n",
      "iter 5181/30000  loss         0.115157  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.156\n",
      "iter 5200/30000  loss         0.115089  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.164\n",
      "iter 5201/30000  loss         0.115085  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.164\n",
      "iter 5220/30000  loss         0.115017  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.172\n",
      "iter 5221/30000  loss         0.115013  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.172\n",
      "iter 5240/30000  loss         0.114945  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.180\n",
      "iter 5241/30000  loss         0.114942  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.180\n",
      "iter 5260/30000  loss         0.114874  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.188\n",
      "iter 5261/30000  loss         0.114871  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.189\n",
      "iter 5280/30000  loss         0.114804  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.196\n",
      "iter 5281/30000  loss         0.114800  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.197\n",
      "iter 5300/30000  loss         0.114733  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.205\n",
      "iter 5301/30000  loss         0.114730  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.205\n",
      "iter 5320/30000  loss         0.114664  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.213\n",
      "iter 5321/30000  loss         0.114660  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.213\n",
      "iter 5340/30000  loss         0.114594  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.221\n",
      "iter 5341/30000  loss         0.114591  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.221\n",
      "iter 5360/30000  loss         0.114525  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.229\n",
      "iter 5361/30000  loss         0.114521  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.229\n",
      "iter 5380/30000  loss         0.114456  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.237\n",
      "iter 5381/30000  loss         0.114452  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.237\n",
      "iter 5400/30000  loss         0.114387  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.245\n",
      "iter 5401/30000  loss         0.114384  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5420/30000  loss         0.114319  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.253\n",
      "iter 5421/30000  loss         0.114316  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.253\n",
      "iter 5440/30000  loss         0.114251  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.261\n",
      "iter 5441/30000  loss         0.114248  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.261\n",
      "iter 5460/30000  loss         0.114184  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.269\n",
      "iter 5461/30000  loss         0.114181  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.269\n",
      "iter 5480/30000  loss         0.114117  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.277\n",
      "iter 5481/30000  loss         0.114114  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.277\n",
      "iter 5500/30000  loss         0.114050  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.285\n",
      "iter 5501/30000  loss         0.114047  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.285\n",
      "iter 5520/30000  loss         0.113984  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.293\n",
      "iter 5521/30000  loss         0.113980  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.293\n",
      "iter 5540/30000  loss         0.113918  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.301\n",
      "iter 5541/30000  loss         0.113914  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.301\n",
      "iter 5560/30000  loss         0.113852  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.309\n",
      "iter 5561/30000  loss         0.113848  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.309\n",
      "iter 5580/30000  loss         0.113786  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.316\n",
      "iter 5581/30000  loss         0.113783  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.317\n",
      "iter 5600/30000  loss         0.113721  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.324\n",
      "iter 5601/30000  loss         0.113718  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.325\n",
      "iter 5620/30000  loss         0.113656  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.332\n",
      "iter 5621/30000  loss         0.113653  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.333\n",
      "iter 5640/30000  loss         0.113592  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.340\n",
      "iter 5641/30000  loss         0.113589  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.340\n",
      "iter 5660/30000  loss         0.113528  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.348\n",
      "iter 5661/30000  loss         0.113524  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.348\n",
      "iter 5680/30000  loss         0.113464  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.356\n",
      "iter 5681/30000  loss         0.113461  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.356\n",
      "iter 5700/30000  loss         0.113400  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.363\n",
      "iter 5701/30000  loss         0.113397  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.364\n",
      "iter 5720/30000  loss         0.113337  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.371\n",
      "iter 5721/30000  loss         0.113334  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.371\n",
      "iter 5740/30000  loss         0.113274  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.379\n",
      "iter 5741/30000  loss         0.113271  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.379\n",
      "iter 5760/30000  loss         0.113211  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.387\n",
      "iter 5761/30000  loss         0.113208  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.387\n",
      "iter 5780/30000  loss         0.113149  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.394\n",
      "iter 5781/30000  loss         0.113146  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.395\n",
      "iter 5800/30000  loss         0.113087  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.402\n",
      "iter 5801/30000  loss         0.113084  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.402\n",
      "iter 5820/30000  loss         0.113025  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.410\n",
      "iter 5821/30000  loss         0.113022  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.410\n",
      "iter 5840/30000  loss         0.112964  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.417\n",
      "iter 5841/30000  loss         0.112961  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.418\n",
      "iter 5860/30000  loss         0.112903  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.425\n",
      "iter 5861/30000  loss         0.112900  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.425\n",
      "iter 5880/30000  loss         0.112842  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.432\n",
      "iter 5881/30000  loss         0.112839  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.433\n",
      "iter 5900/30000  loss         0.112781  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.440\n",
      "iter 5901/30000  loss         0.112778  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.440\n",
      "iter 5920/30000  loss         0.112721  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.448\n",
      "iter 5921/30000  loss         0.112718  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.448\n",
      "iter 5940/30000  loss         0.112661  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.455\n",
      "iter 5941/30000  loss         0.112658  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.455\n",
      "iter 5960/30000  loss         0.112601  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.463\n",
      "iter 5961/30000  loss         0.112598  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.463\n",
      "iter 5980/30000  loss         0.112542  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.470\n",
      "iter 5981/30000  loss         0.112539  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.471\n",
      "iter 6000/30000  loss         0.112483  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.478\n",
      "iter 6001/30000  loss         0.112480  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.478\n",
      "iter 6020/30000  loss         0.112424  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.485\n",
      "iter 6021/30000  loss         0.112421  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.485\n",
      "iter 6040/30000  loss         0.112365  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.493\n",
      "iter 6041/30000  loss         0.112362  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.493\n",
      "iter 6060/30000  loss         0.112307  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.500\n",
      "iter 6061/30000  loss         0.112304  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.500\n",
      "iter 6080/30000  loss         0.112249  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.507\n",
      "iter 6081/30000  loss         0.112246  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.508\n",
      "iter 6100/30000  loss         0.112191  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.515\n",
      "iter 6101/30000  loss         0.112188  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.515\n",
      "iter 6120/30000  loss         0.112134  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.522\n",
      "iter 6121/30000  loss         0.112131  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.523\n",
      "iter 6140/30000  loss         0.112076  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.530\n",
      "iter 6141/30000  loss         0.112074  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.530\n",
      "iter 6160/30000  loss         0.112019  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.537\n",
      "iter 6161/30000  loss         0.112017  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.537\n",
      "iter 6180/30000  loss         0.111963  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.544\n",
      "iter 6181/30000  loss         0.111960  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.545\n",
      "iter 6200/30000  loss         0.111906  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.552\n",
      "iter 6201/30000  loss         0.111903  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6220/30000  loss         0.111850  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.559\n",
      "iter 6221/30000  loss         0.111847  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.559\n",
      "iter 6240/30000  loss         0.111794  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.566\n",
      "iter 6241/30000  loss         0.111791  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.567\n",
      "iter 6260/30000  loss         0.111738  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.574\n",
      "iter 6261/30000  loss         0.111736  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.574\n",
      "iter 6280/30000  loss         0.111683  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.581\n",
      "iter 6281/30000  loss         0.111680  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.581\n",
      "iter 6300/30000  loss         0.111628  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.588\n",
      "iter 6301/30000  loss         0.111625  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.588\n",
      "iter 6320/30000  loss         0.111573  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.595\n",
      "iter 6321/30000  loss         0.111570  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.596\n",
      "iter 6340/30000  loss         0.111518  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.603\n",
      "iter 6341/30000  loss         0.111515  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.603\n",
      "iter 6360/30000  loss         0.111464  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.610\n",
      "iter 6361/30000  loss         0.111461  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.610\n",
      "iter 6380/30000  loss         0.111410  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.617\n",
      "iter 6381/30000  loss         0.111407  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.617\n",
      "iter 6400/30000  loss         0.111356  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.624\n",
      "iter 6401/30000  loss         0.111353  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.624\n",
      "iter 6420/30000  loss         0.111302  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.631\n",
      "iter 6421/30000  loss         0.111299  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.632\n",
      "iter 6440/30000  loss         0.111248  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.638\n",
      "iter 6441/30000  loss         0.111246  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.639\n",
      "iter 6460/30000  loss         0.111195  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.646\n",
      "iter 6461/30000  loss         0.111193  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.646\n",
      "iter 6480/30000  loss         0.111142  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.653\n",
      "iter 6481/30000  loss         0.111140  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.653\n",
      "iter 6500/30000  loss         0.111089  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.660\n",
      "iter 6501/30000  loss         0.111087  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.660\n",
      "iter 6520/30000  loss         0.111037  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.667\n",
      "iter 6521/30000  loss         0.111034  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.667\n",
      "iter 6540/30000  loss         0.110985  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.674\n",
      "iter 6541/30000  loss         0.110982  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.674\n",
      "iter 6560/30000  loss         0.110933  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.681\n",
      "iter 6561/30000  loss         0.110930  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.681\n",
      "iter 6580/30000  loss         0.110881  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.688\n",
      "iter 6581/30000  loss         0.110878  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.688\n",
      "iter 6600/30000  loss         0.110829  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.695\n",
      "iter 6601/30000  loss         0.110827  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.695\n",
      "iter 6620/30000  loss         0.110778  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.702\n",
      "iter 6621/30000  loss         0.110775  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.702\n",
      "iter 6640/30000  loss         0.110727  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.709\n",
      "iter 6641/30000  loss         0.110724  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.709\n",
      "iter 6660/30000  loss         0.110676  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.716\n",
      "iter 6661/30000  loss         0.110673  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.716\n",
      "iter 6680/30000  loss         0.110625  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.723\n",
      "iter 6681/30000  loss         0.110622  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.723\n",
      "iter 6700/30000  loss         0.110574  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.730\n",
      "iter 6701/30000  loss         0.110572  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.730\n",
      "iter 6720/30000  loss         0.110524  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.737\n",
      "iter 6721/30000  loss         0.110522  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.737\n",
      "iter 6740/30000  loss         0.110474  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.744\n",
      "iter 6741/30000  loss         0.110472  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.744\n",
      "iter 6760/30000  loss         0.110424  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.751\n",
      "iter 6761/30000  loss         0.110422  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.751\n",
      "iter 6780/30000  loss         0.110375  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.757\n",
      "iter 6781/30000  loss         0.110372  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.758\n",
      "iter 6800/30000  loss         0.110325  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.764\n",
      "iter 6801/30000  loss         0.110323  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.765\n",
      "iter 6820/30000  loss         0.110276  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.771\n",
      "iter 6821/30000  loss         0.110274  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.772\n",
      "iter 6840/30000  loss         0.110227  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.778\n",
      "iter 6841/30000  loss         0.110225  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.778\n",
      "iter 6860/30000  loss         0.110178  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.785\n",
      "iter 6861/30000  loss         0.110176  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.785\n",
      "iter 6880/30000  loss         0.110130  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.792\n",
      "iter 6881/30000  loss         0.110127  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.792\n",
      "iter 6900/30000  loss         0.110081  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.798\n",
      "iter 6901/30000  loss         0.110079  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.799\n",
      "iter 6920/30000  loss         0.110033  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.805\n",
      "iter 6921/30000  loss         0.110031  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.806\n",
      "iter 6940/30000  loss         0.109985  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.812\n",
      "iter 6941/30000  loss         0.109983  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.812\n",
      "iter 6960/30000  loss         0.109938  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.819\n",
      "iter 6961/30000  loss         0.109935  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.819\n",
      "iter 6980/30000  loss         0.109890  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.826\n",
      "iter 6981/30000  loss         0.109888  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.826\n",
      "iter 7000/30000  loss         0.109843  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.832\n",
      "iter 7001/30000  loss         0.109840  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7020/30000  loss         0.109795  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.839\n",
      "iter 7021/30000  loss         0.109793  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.839\n",
      "iter 7040/30000  loss         0.109749  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.846\n",
      "iter 7041/30000  loss         0.109746  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.846\n",
      "iter 7060/30000  loss         0.109702  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.852\n",
      "iter 7061/30000  loss         0.109699  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.853\n",
      "iter 7080/30000  loss         0.109655  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.859\n",
      "iter 7081/30000  loss         0.109653  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.859\n",
      "iter 7100/30000  loss         0.109609  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.866\n",
      "iter 7101/30000  loss         0.109607  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.866\n",
      "iter 7120/30000  loss         0.109563  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.872\n",
      "iter 7121/30000  loss         0.109560  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.873\n",
      "iter 7140/30000  loss         0.109517  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.879\n",
      "iter 7141/30000  loss         0.109514  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.879\n",
      "iter 7160/30000  loss         0.109471  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.886\n",
      "iter 7161/30000  loss         0.109469  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.886\n",
      "iter 7180/30000  loss         0.109425  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.892\n",
      "iter 7181/30000  loss         0.109423  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.892\n",
      "iter 7200/30000  loss         0.109380  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.899\n",
      "iter 7201/30000  loss         0.109378  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.899\n",
      "iter 7220/30000  loss         0.109335  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.905\n",
      "iter 7221/30000  loss         0.109332  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.906\n",
      "iter 7240/30000  loss         0.109290  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.912\n",
      "iter 7241/30000  loss         0.109287  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.912\n",
      "iter 7260/30000  loss         0.109245  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.918\n",
      "iter 7261/30000  loss         0.109243  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.919\n",
      "iter 7280/30000  loss         0.109200  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.925\n",
      "iter 7281/30000  loss         0.109198  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.925\n",
      "iter 7300/30000  loss         0.109156  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.931\n",
      "iter 7301/30000  loss         0.109153  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.932\n",
      "iter 7320/30000  loss         0.109111  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.938\n",
      "iter 7321/30000  loss         0.109109  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.938\n",
      "iter 7340/30000  loss         0.109067  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.944\n",
      "iter 7341/30000  loss         0.109065  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.945\n",
      "iter 7360/30000  loss         0.109023  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.951\n",
      "iter 7361/30000  loss         0.109021  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.951\n",
      "iter 7380/30000  loss         0.108980  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.957\n",
      "iter 7381/30000  loss         0.108977  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.958\n",
      "iter 7400/30000  loss         0.108936  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.964\n",
      "iter 7401/30000  loss         0.108934  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.964\n",
      "iter 7420/30000  loss         0.108893  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.970\n",
      "iter 7421/30000  loss         0.108890  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.971\n",
      "iter 7440/30000  loss         0.108849  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.977\n",
      "iter 7441/30000  loss         0.108847  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.977\n",
      "iter 7460/30000  loss         0.108806  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.983\n",
      "iter 7461/30000  loss         0.108804  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.983\n",
      "iter 7480/30000  loss         0.108763  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.990\n",
      "iter 7481/30000  loss         0.108761  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.990\n",
      "iter 7500/30000  loss         0.108721  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.996\n",
      "iter 7501/30000  loss         0.108719  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.996\n",
      "iter 7520/30000  loss         0.108678  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.002\n",
      "iter 7521/30000  loss         0.108676  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.003\n",
      "iter 7540/30000  loss         0.108636  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.009\n",
      "iter 7541/30000  loss         0.108634  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.009\n",
      "iter 7560/30000  loss         0.108594  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.015\n",
      "iter 7561/30000  loss         0.108591  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.015\n",
      "iter 7580/30000  loss         0.108552  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.021\n",
      "iter 7581/30000  loss         0.108549  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.022\n",
      "iter 7600/30000  loss         0.108510  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.028\n",
      "iter 7601/30000  loss         0.108508  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.028\n",
      "iter 7620/30000  loss         0.108468  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.034\n",
      "iter 7621/30000  loss         0.108466  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    5.034\n",
      "iter 7640/30000  loss         0.108426  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.040\n",
      "iter 7641/30000  loss         0.108424  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.041\n",
      "iter 7660/30000  loss         0.108385  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.047\n",
      "iter 7661/30000  loss         0.108383  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.047\n",
      "iter 7680/30000  loss         0.108344  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.053\n",
      "iter 7681/30000  loss         0.108342  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.053\n",
      "iter 7700/30000  loss         0.108303  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.059\n",
      "iter 7701/30000  loss         0.108301  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.059\n",
      "iter 7720/30000  loss         0.108262  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.065\n",
      "iter 7721/30000  loss         0.108260  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.066\n",
      "iter 7740/30000  loss         0.108221  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.072\n",
      "iter 7741/30000  loss         0.108219  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    5.072\n",
      "iter 7760/30000  loss         0.108181  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.078\n",
      "iter 7761/30000  loss         0.108179  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.078\n",
      "iter 7780/30000  loss         0.108140  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.084\n",
      "iter 7781/30000  loss         0.108138  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.084\n",
      "iter 7800/30000  loss         0.108100  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.090\n",
      "iter 7801/30000  loss         0.108098  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7820/30000  loss         0.108060  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.096\n",
      "iter 7821/30000  loss         0.108058  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.097\n",
      "iter 7840/30000  loss         0.108020  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.103\n",
      "iter 7841/30000  loss         0.108018  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    5.103\n",
      "iter 7860/30000  loss         0.107980  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.109\n",
      "iter 7861/30000  loss         0.107978  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.109\n",
      "iter 7880/30000  loss         0.107941  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.115\n",
      "iter 7881/30000  loss         0.107939  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.115\n",
      "iter 7900/30000  loss         0.107901  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.121\n",
      "iter 7901/30000  loss         0.107899  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.121\n",
      "iter 7920/30000  loss         0.107862  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.127\n",
      "iter 7921/30000  loss         0.107860  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.128\n",
      "iter 7940/30000  loss         0.107823  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.133\n",
      "iter 7941/30000  loss         0.107821  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.134\n",
      "iter 7960/30000  loss         0.107784  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.139\n",
      "iter 7961/30000  loss         0.107782  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    5.140\n",
      "iter 7980/30000  loss         0.107745  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.146\n",
      "iter 7981/30000  loss         0.107743  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.146\n",
      "iter 8000/30000  loss         0.107706  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.152\n",
      "iter 8001/30000  loss         0.107704  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.152\n",
      "iter 8020/30000  loss         0.107668  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.158\n",
      "iter 8021/30000  loss         0.107666  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.158\n",
      "iter 8040/30000  loss         0.107629  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.164\n",
      "iter 8041/30000  loss         0.107627  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.164\n",
      "iter 8060/30000  loss         0.107591  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.170\n",
      "iter 8061/30000  loss         0.107589  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.170\n",
      "iter 8080/30000  loss         0.107553  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.176\n",
      "iter 8081/30000  loss         0.107551  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    5.176\n",
      "iter 8100/30000  loss         0.107515  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.182\n",
      "iter 8101/30000  loss         0.107513  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.182\n",
      "iter 8120/30000  loss         0.107477  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.188\n",
      "iter 8121/30000  loss         0.107475  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.188\n",
      "iter 8140/30000  loss         0.107439  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.194\n",
      "iter 8141/30000  loss         0.107438  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.194\n",
      "iter 8160/30000  loss         0.107402  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.200\n",
      "iter 8161/30000  loss         0.107400  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.200\n",
      "iter 8180/30000  loss         0.107364  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.206\n",
      "iter 8181/30000  loss         0.107363  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.206\n",
      "iter 8200/30000  loss         0.107327  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.212\n",
      "iter 8201/30000  loss         0.107325  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.212\n",
      "iter 8220/30000  loss         0.107290  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.218\n",
      "iter 8221/30000  loss         0.107288  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.218\n",
      "iter 8240/30000  loss         0.107253  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.224\n",
      "iter 8241/30000  loss         0.107251  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.224\n",
      "iter 8260/30000  loss         0.107216  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.230\n",
      "iter 8261/30000  loss         0.107214  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.230\n",
      "iter 8280/30000  loss         0.107180  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.236\n",
      "iter 8281/30000  loss         0.107178  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.236\n",
      "iter 8300/30000  loss         0.107143  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.241\n",
      "iter 8301/30000  loss         0.107141  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.242\n",
      "iter 8320/30000  loss         0.107107  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.247\n",
      "iter 8321/30000  loss         0.107105  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.248\n",
      "iter 8340/30000  loss         0.107070  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.253\n",
      "iter 8341/30000  loss         0.107069  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.254\n",
      "iter 8360/30000  loss         0.107034  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.259\n",
      "iter 8361/30000  loss         0.107032  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.259\n",
      "iter 8380/30000  loss         0.106998  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.265\n",
      "iter 8381/30000  loss         0.106996  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.265\n",
      "iter 8400/30000  loss         0.106962  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.271\n",
      "iter 8401/30000  loss         0.106961  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.271\n",
      "iter 8420/30000  loss         0.106927  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.277\n",
      "iter 8421/30000  loss         0.106925  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.277\n",
      "iter 8440/30000  loss         0.106891  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.283\n",
      "iter 8441/30000  loss         0.106889  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.283\n",
      "iter 8460/30000  loss         0.106856  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.288\n",
      "iter 8461/30000  loss         0.106854  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.289\n",
      "iter 8480/30000  loss         0.106820  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.294\n",
      "iter 8481/30000  loss         0.106818  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.294\n",
      "iter 8500/30000  loss         0.106785  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.300\n",
      "iter 8501/30000  loss         0.106783  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.300\n",
      "iter 8520/30000  loss         0.106750  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.306\n",
      "iter 8521/30000  loss         0.106748  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.306\n",
      "iter 8540/30000  loss         0.106715  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.312\n",
      "iter 8541/30000  loss         0.106713  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.312\n",
      "iter 8560/30000  loss         0.106680  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.317\n",
      "iter 8561/30000  loss         0.106678  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.318\n",
      "iter 8580/30000  loss         0.106645  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.323\n",
      "iter 8581/30000  loss         0.106644  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.323\n",
      "iter 8600/30000  loss         0.106611  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.329\n",
      "iter 8601/30000  loss         0.106609  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8620/30000  loss         0.106576  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.334\n",
      "iter 8621/30000  loss         0.106575  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.335\n",
      "iter 8640/30000  loss         0.106542  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.340\n",
      "iter 8641/30000  loss         0.106540  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.341\n",
      "iter 8660/30000  loss         0.106508  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.346\n",
      "iter 8661/30000  loss         0.106506  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.346\n",
      "iter 8680/30000  loss         0.106474  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.352\n",
      "iter 8681/30000  loss         0.106472  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.352\n",
      "iter 8700/30000  loss         0.106440  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.357\n",
      "iter 8701/30000  loss         0.106438  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.358\n",
      "iter 8720/30000  loss         0.106406  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.363\n",
      "iter 8721/30000  loss         0.106404  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.363\n",
      "iter 8740/30000  loss         0.106372  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.369\n",
      "iter 8741/30000  loss         0.106371  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.369\n",
      "iter 8760/30000  loss         0.106339  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.374\n",
      "iter 8761/30000  loss         0.106337  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.375\n",
      "iter 8780/30000  loss         0.106305  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.380\n",
      "iter 8781/30000  loss         0.106304  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.380\n",
      "iter 8800/30000  loss         0.106272  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.386\n",
      "iter 8801/30000  loss         0.106270  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.386\n",
      "iter 8820/30000  loss         0.106239  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.391\n",
      "iter 8821/30000  loss         0.106237  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.392\n",
      "iter 8840/30000  loss         0.106206  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.397\n",
      "iter 8841/30000  loss         0.106204  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.397\n",
      "iter 8860/30000  loss         0.106173  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.402\n",
      "iter 8861/30000  loss         0.106171  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.403\n",
      "iter 8880/30000  loss         0.106140  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.408\n",
      "iter 8881/30000  loss         0.106138  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.408\n",
      "iter 8900/30000  loss         0.106107  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.414\n",
      "iter 8901/30000  loss         0.106106  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.414\n",
      "iter 8920/30000  loss         0.106075  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.419\n",
      "iter 8921/30000  loss         0.106073  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.419\n",
      "iter 8940/30000  loss         0.106042  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.425\n",
      "iter 8941/30000  loss         0.106041  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.425\n",
      "iter 8960/30000  loss         0.106010  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.430\n",
      "iter 8961/30000  loss         0.106008  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.431\n",
      "iter 8980/30000  loss         0.105978  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.436\n",
      "iter 8981/30000  loss         0.105976  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.436\n",
      "iter 9000/30000  loss         0.105945  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.441\n",
      "iter 9001/30000  loss         0.105944  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.442\n",
      "iter 9020/30000  loss         0.105913  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.447\n",
      "iter 9021/30000  loss         0.105912  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.447\n",
      "iter 9040/30000  loss         0.105881  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.452\n",
      "iter 9041/30000  loss         0.105880  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.453\n",
      "iter 9060/30000  loss         0.105850  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.458\n",
      "iter 9061/30000  loss         0.105848  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.458\n",
      "iter 9080/30000  loss         0.105818  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.463\n",
      "iter 9081/30000  loss         0.105816  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.464\n",
      "iter 9100/30000  loss         0.105786  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.469\n",
      "iter 9101/30000  loss         0.105785  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.469\n",
      "iter 9120/30000  loss         0.105755  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.474\n",
      "iter 9121/30000  loss         0.105753  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.475\n",
      "iter 9140/30000  loss         0.105724  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.480\n",
      "iter 9141/30000  loss         0.105722  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.480\n",
      "iter 9160/30000  loss         0.105692  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.485\n",
      "iter 9161/30000  loss         0.105691  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.486\n",
      "iter 9180/30000  loss         0.105661  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.491\n",
      "iter 9181/30000  loss         0.105660  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.491\n",
      "iter 9200/30000  loss         0.105630  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.496\n",
      "iter 9201/30000  loss         0.105629  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.496\n",
      "iter 9220/30000  loss         0.105599  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.502\n",
      "iter 9221/30000  loss         0.105598  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.502\n",
      "iter 9240/30000  loss         0.105568  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.507\n",
      "iter 9241/30000  loss         0.105567  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.507\n",
      "iter 9260/30000  loss         0.105538  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.512\n",
      "iter 9261/30000  loss         0.105536  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.513\n",
      "iter 9280/30000  loss         0.105507  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.518\n",
      "iter 9281/30000  loss         0.105506  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.518\n",
      "iter 9300/30000  loss         0.105477  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.523\n",
      "iter 9301/30000  loss         0.105475  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.524\n",
      "iter 9320/30000  loss         0.105446  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.529\n",
      "iter 9321/30000  loss         0.105445  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.529\n",
      "iter 9340/30000  loss         0.105416  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.534\n",
      "iter 9341/30000  loss         0.105415  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.534\n",
      "iter 9360/30000  loss         0.105386  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.539\n",
      "iter 9361/30000  loss         0.105384  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.540\n",
      "iter 9380/30000  loss         0.105356  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.545\n",
      "iter 9381/30000  loss         0.105354  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.545\n",
      "iter 9400/30000  loss         0.105326  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.550\n",
      "iter 9401/30000  loss         0.105324  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9420/30000  loss         0.105296  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.555\n",
      "iter 9421/30000  loss         0.105295  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.556\n",
      "iter 9440/30000  loss         0.105266  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.561\n",
      "iter 9441/30000  loss         0.105265  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.561\n",
      "iter 9460/30000  loss         0.105237  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.566\n",
      "iter 9461/30000  loss         0.105235  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.566\n",
      "iter 9480/30000  loss         0.105207  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.571\n",
      "iter 9481/30000  loss         0.105206  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.572\n",
      "iter 9500/30000  loss         0.105178  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.577\n",
      "iter 9501/30000  loss         0.105176  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.577\n",
      "iter 9520/30000  loss         0.105148  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.582\n",
      "iter 9521/30000  loss         0.105147  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.582\n",
      "iter 9540/30000  loss         0.105119  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.587\n",
      "iter 9541/30000  loss         0.105118  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.587\n",
      "iter 9560/30000  loss         0.105090  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.592\n",
      "iter 9561/30000  loss         0.105088  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.593\n",
      "iter 9580/30000  loss         0.105061  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.598\n",
      "iter 9581/30000  loss         0.105059  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.598\n",
      "iter 9600/30000  loss         0.105032  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.603\n",
      "iter 9601/30000  loss         0.105030  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.603\n",
      "iter 9620/30000  loss         0.105003  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.608\n",
      "iter 9621/30000  loss         0.105002  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.608\n",
      "iter 9640/30000  loss         0.104974  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.613\n",
      "iter 9641/30000  loss         0.104973  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.614\n",
      "iter 9660/30000  loss         0.104946  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.619\n",
      "iter 9661/30000  loss         0.104944  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.619\n",
      "iter 9680/30000  loss         0.104917  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.624\n",
      "iter 9681/30000  loss         0.104916  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.624\n",
      "iter 9700/30000  loss         0.104889  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.629\n",
      "iter 9701/30000  loss         0.104887  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.629\n",
      "iter 9720/30000  loss         0.104860  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.634\n",
      "iter 9721/30000  loss         0.104859  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.634\n",
      "iter 9740/30000  loss         0.104832  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.639\n",
      "iter 9741/30000  loss         0.104831  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.640\n",
      "iter 9760/30000  loss         0.104804  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.644\n",
      "iter 9761/30000  loss         0.104802  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.645\n",
      "iter 9780/30000  loss         0.104776  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.650\n",
      "iter 9781/30000  loss         0.104774  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.650\n",
      "iter 9800/30000  loss         0.104748  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.655\n",
      "iter 9801/30000  loss         0.104746  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.655\n",
      "iter 9820/30000  loss         0.104720  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.660\n",
      "iter 9821/30000  loss         0.104719  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.660\n",
      "iter 9840/30000  loss         0.104692  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.665\n",
      "iter 9841/30000  loss         0.104691  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.665\n",
      "iter 9860/30000  loss         0.104664  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.670\n",
      "iter 9861/30000  loss         0.104663  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.670\n",
      "iter 9880/30000  loss         0.104637  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.675\n",
      "iter 9881/30000  loss         0.104635  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.676\n",
      "iter 9900/30000  loss         0.104609  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.680\n",
      "iter 9901/30000  loss         0.104608  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.681\n",
      "iter 9920/30000  loss         0.104582  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.686\n",
      "iter 9921/30000  loss         0.104581  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.686\n",
      "iter 9940/30000  loss         0.104555  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.691\n",
      "iter 9941/30000  loss         0.104553  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.691\n",
      "iter 9960/30000  loss         0.104527  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.696\n",
      "iter 9961/30000  loss         0.104526  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.696\n",
      "iter 9980/30000  loss         0.104500  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.701\n",
      "iter 9981/30000  loss         0.104499  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.701\n",
      "iter 10000/30000  loss         0.104473  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.706\n",
      "iter 10001/30000  loss         0.104472  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.706\n",
      "iter 10020/30000  loss         0.104446  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.711\n",
      "iter 10021/30000  loss         0.104445  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.711\n",
      "iter 10040/30000  loss         0.104419  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.716\n",
      "iter 10041/30000  loss         0.104418  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.716\n",
      "iter 10060/30000  loss         0.104393  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.721\n",
      "iter 10061/30000  loss         0.104391  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.721\n",
      "iter 10080/30000  loss         0.104366  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.726\n",
      "iter 10081/30000  loss         0.104365  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.726\n",
      "iter 10100/30000  loss         0.104339  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.731\n",
      "iter 10101/30000  loss         0.104338  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.731\n",
      "iter 10120/30000  loss         0.104313  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.736\n",
      "iter 10121/30000  loss         0.104311  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.736\n",
      "iter 10140/30000  loss         0.104286  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.741\n",
      "iter 10141/30000  loss         0.104285  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.741\n",
      "iter 10160/30000  loss         0.104260  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.746\n",
      "iter 10161/30000  loss         0.104259  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.746\n",
      "iter 10180/30000  loss         0.104234  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.751\n",
      "iter 10181/30000  loss         0.104232  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.751\n",
      "iter 10200/30000  loss         0.104208  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.756\n",
      "iter 10201/30000  loss         0.104206  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10220/30000  loss         0.104182  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.761\n",
      "iter 10221/30000  loss         0.104180  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.761\n",
      "iter 10240/30000  loss         0.104156  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.766\n",
      "iter 10241/30000  loss         0.104154  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.766\n",
      "iter 10260/30000  loss         0.104130  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.771\n",
      "iter 10261/30000  loss         0.104128  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.771\n",
      "iter 10280/30000  loss         0.104104  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.776\n",
      "iter 10281/30000  loss         0.104102  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.776\n",
      "iter 10300/30000  loss         0.104078  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.781\n",
      "iter 10301/30000  loss         0.104077  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.781\n",
      "iter 10320/30000  loss         0.104052  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.786\n",
      "iter 10321/30000  loss         0.104051  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.786\n",
      "iter 10340/30000  loss         0.104027  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.791\n",
      "iter 10341/30000  loss         0.104026  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.791\n",
      "iter 10360/30000  loss         0.104001  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.795\n",
      "iter 10361/30000  loss         0.104000  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.796\n",
      "iter 10380/30000  loss         0.103976  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.800\n",
      "iter 10381/30000  loss         0.103975  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.801\n",
      "iter 10400/30000  loss         0.103951  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.805\n",
      "iter 10401/30000  loss         0.103949  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.805\n",
      "iter 10420/30000  loss         0.103925  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.810\n",
      "iter 10421/30000  loss         0.103924  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.810\n",
      "iter 10440/30000  loss         0.103900  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.815\n",
      "iter 10441/30000  loss         0.103899  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.815\n",
      "iter 10460/30000  loss         0.103875  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.820\n",
      "iter 10461/30000  loss         0.103874  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.820\n",
      "iter 10480/30000  loss         0.103850  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.825\n",
      "iter 10481/30000  loss         0.103849  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.825\n",
      "iter 10500/30000  loss         0.103825  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.830\n",
      "iter 10501/30000  loss         0.103824  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.830\n",
      "iter 10520/30000  loss         0.103800  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.834\n",
      "iter 10521/30000  loss         0.103799  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.835\n",
      "iter 10540/30000  loss         0.103776  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.839\n",
      "iter 10541/30000  loss         0.103774  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.839\n",
      "iter 10560/30000  loss         0.103751  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.844\n",
      "iter 10561/30000  loss         0.103750  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.844\n",
      "iter 10580/30000  loss         0.103726  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.849\n",
      "iter 10581/30000  loss         0.103725  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.849\n",
      "iter 10600/30000  loss         0.103702  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.854\n",
      "iter 10601/30000  loss         0.103701  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.854\n",
      "iter 10620/30000  loss         0.103677  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.858\n",
      "iter 10621/30000  loss         0.103676  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.859\n",
      "iter 10640/30000  loss         0.103653  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.863\n",
      "iter 10641/30000  loss         0.103652  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.863\n",
      "iter 10660/30000  loss         0.103629  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.868\n",
      "iter 10661/30000  loss         0.103627  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.868\n",
      "iter 10680/30000  loss         0.103604  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.873\n",
      "iter 10681/30000  loss         0.103603  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.873\n",
      "iter 10700/30000  loss         0.103580  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.878\n",
      "iter 10701/30000  loss         0.103579  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.878\n",
      "iter 10720/30000  loss         0.103556  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.882\n",
      "iter 10721/30000  loss         0.103555  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.883\n",
      "iter 10740/30000  loss         0.103532  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.887\n",
      "iter 10741/30000  loss         0.103531  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.887\n",
      "iter 10760/30000  loss         0.103508  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.892\n",
      "iter 10761/30000  loss         0.103507  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.892\n",
      "iter 10780/30000  loss         0.103485  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.897\n",
      "iter 10781/30000  loss         0.103483  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.897\n",
      "iter 10800/30000  loss         0.103461  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.901\n",
      "iter 10801/30000  loss         0.103460  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.902\n",
      "iter 10820/30000  loss         0.103437  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.906\n",
      "iter 10821/30000  loss         0.103436  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.906\n",
      "iter 10840/30000  loss         0.103414  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.911\n",
      "iter 10841/30000  loss         0.103412  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.911\n",
      "iter 10860/30000  loss         0.103390  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.915\n",
      "iter 10861/30000  loss         0.103389  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.916\n",
      "iter 10880/30000  loss         0.103367  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.920\n",
      "iter 10881/30000  loss         0.103365  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.920\n",
      "iter 10900/30000  loss         0.103343  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.925\n",
      "iter 10901/30000  loss         0.103342  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.925\n",
      "iter 10920/30000  loss         0.103320  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.929\n",
      "iter 10921/30000  loss         0.103319  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.930\n",
      "iter 10940/30000  loss         0.103297  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.934\n",
      "iter 10941/30000  loss         0.103295  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.934\n",
      "iter 10960/30000  loss         0.103273  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.939\n",
      "iter 10961/30000  loss         0.103272  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.939\n",
      "iter 10980/30000  loss         0.103250  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.943\n",
      "iter 10981/30000  loss         0.103249  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.944\n",
      "iter 11000/30000  loss         0.103227  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.948\n",
      "iter 11001/30000  loss         0.103226  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11020/30000  loss         0.103204  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.953\n",
      "iter 11021/30000  loss         0.103203  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.953\n",
      "iter 11040/30000  loss         0.103182  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.957\n",
      "iter 11041/30000  loss         0.103180  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.958\n",
      "iter 11060/30000  loss         0.103159  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.962\n",
      "iter 11061/30000  loss         0.103158  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.962\n",
      "iter 11080/30000  loss         0.103136  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.967\n",
      "iter 11081/30000  loss         0.103135  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.967\n",
      "iter 11100/30000  loss         0.103113  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.971\n",
      "iter 11101/30000  loss         0.103112  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.971\n",
      "iter 11120/30000  loss         0.103091  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.976\n",
      "iter 11121/30000  loss         0.103090  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.976\n",
      "iter 11140/30000  loss         0.103068  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.980\n",
      "iter 11141/30000  loss         0.103067  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.981\n",
      "iter 11160/30000  loss         0.103046  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.985\n",
      "iter 11161/30000  loss         0.103045  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.985\n",
      "iter 11180/30000  loss         0.103023  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.990\n",
      "iter 11181/30000  loss         0.103022  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.990\n",
      "iter 11200/30000  loss         0.103001  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.994\n",
      "iter 11201/30000  loss         0.103000  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.994\n",
      "iter 11220/30000  loss         0.102979  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.999\n",
      "iter 11221/30000  loss         0.102978  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.999\n",
      "iter 11240/30000  loss         0.102957  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    6.003\n",
      "iter 11241/30000  loss         0.102956  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    6.004\n",
      "iter 11260/30000  loss         0.102935  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    6.008\n",
      "iter 11261/30000  loss         0.102933  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    6.008\n",
      "iter 11280/30000  loss         0.102913  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.012\n",
      "iter 11281/30000  loss         0.102911  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.013\n",
      "iter 11300/30000  loss         0.102891  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.017\n",
      "iter 11301/30000  loss         0.102889  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.017\n",
      "iter 11320/30000  loss         0.102869  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.021\n",
      "iter 11321/30000  loss         0.102868  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.022\n",
      "iter 11340/30000  loss         0.102847  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.026\n",
      "iter 11341/30000  loss         0.102846  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.026\n",
      "iter 11360/30000  loss         0.102825  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.030\n",
      "iter 11361/30000  loss         0.102824  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.031\n",
      "iter 11380/30000  loss         0.102803  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.035\n",
      "iter 11381/30000  loss         0.102802  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.035\n",
      "iter 11400/30000  loss         0.102782  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.040\n",
      "iter 11401/30000  loss         0.102781  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.040\n",
      "iter 11420/30000  loss         0.102760  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.044\n",
      "iter 11421/30000  loss         0.102759  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.044\n",
      "iter 11440/30000  loss         0.102739  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.048\n",
      "iter 11441/30000  loss         0.102738  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.049\n",
      "iter 11460/30000  loss         0.102717  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.053\n",
      "iter 11461/30000  loss         0.102716  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.053\n",
      "iter 11480/30000  loss         0.102696  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.057\n",
      "iter 11481/30000  loss         0.102695  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    6.058\n",
      "iter 11500/30000  loss         0.102675  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.062\n",
      "iter 11501/30000  loss         0.102673  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.062\n",
      "iter 11520/30000  loss         0.102653  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.066\n",
      "iter 11521/30000  loss         0.102652  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.067\n",
      "iter 11540/30000  loss         0.102632  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.071\n",
      "iter 11541/30000  loss         0.102631  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.071\n",
      "iter 11560/30000  loss         0.102611  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.075\n",
      "iter 11561/30000  loss         0.102610  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.075\n",
      "iter 11580/30000  loss         0.102590  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.080\n",
      "iter 11581/30000  loss         0.102589  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.080\n",
      "iter 11600/30000  loss         0.102569  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.084\n",
      "iter 11601/30000  loss         0.102568  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.084\n",
      "iter 11620/30000  loss         0.102548  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.088\n",
      "iter 11621/30000  loss         0.102547  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.089\n",
      "iter 11640/30000  loss         0.102527  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.093\n",
      "iter 11641/30000  loss         0.102526  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.093\n",
      "iter 11660/30000  loss         0.102506  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.097\n",
      "iter 11661/30000  loss         0.102505  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.098\n",
      "iter 11680/30000  loss         0.102486  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.102\n",
      "iter 11681/30000  loss         0.102485  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    6.102\n",
      "iter 11700/30000  loss         0.102465  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.106\n",
      "iter 11701/30000  loss         0.102464  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.106\n",
      "iter 11720/30000  loss         0.102444  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.110\n",
      "iter 11721/30000  loss         0.102443  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.111\n",
      "iter 11740/30000  loss         0.102424  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.115\n",
      "iter 11741/30000  loss         0.102423  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.115\n",
      "iter 11760/30000  loss         0.102403  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.119\n",
      "iter 11761/30000  loss         0.102402  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.119\n",
      "iter 11780/30000  loss         0.102383  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.124\n",
      "iter 11781/30000  loss         0.102382  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.124\n",
      "iter 11800/30000  loss         0.102363  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.128\n",
      "iter 11801/30000  loss         0.102362  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11820/30000  loss         0.102342  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.132\n",
      "iter 11821/30000  loss         0.102341  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.133\n",
      "iter 11840/30000  loss         0.102322  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.137\n",
      "iter 11841/30000  loss         0.102321  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.137\n",
      "iter 11860/30000  loss         0.102302  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.141\n",
      "iter 11861/30000  loss         0.102301  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.141\n",
      "iter 11880/30000  loss         0.102282  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.145\n",
      "iter 11881/30000  loss         0.102281  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.145\n",
      "iter 11900/30000  loss         0.102262  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.150\n",
      "iter 11901/30000  loss         0.102261  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    6.150\n",
      "iter 11920/30000  loss         0.102242  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.154\n",
      "iter 11921/30000  loss         0.102241  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.154\n",
      "iter 11940/30000  loss         0.102222  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.158\n",
      "iter 11941/30000  loss         0.102221  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.158\n",
      "iter 11960/30000  loss         0.102202  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.163\n",
      "iter 11961/30000  loss         0.102201  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.163\n",
      "iter 11980/30000  loss         0.102182  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.167\n",
      "iter 11981/30000  loss         0.102181  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.167\n",
      "iter 12000/30000  loss         0.102162  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.171\n",
      "iter 12001/30000  loss         0.102161  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.171\n",
      "iter 12020/30000  loss         0.102142  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.175\n",
      "iter 12021/30000  loss         0.102142  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.176\n",
      "iter 12040/30000  loss         0.102123  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.180\n",
      "iter 12041/30000  loss         0.102122  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.180\n",
      "iter 12060/30000  loss         0.102103  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.184\n",
      "iter 12061/30000  loss         0.102102  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.184\n",
      "iter 12080/30000  loss         0.102084  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.188\n",
      "iter 12081/30000  loss         0.102083  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.188\n",
      "iter 12100/30000  loss         0.102064  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.192\n",
      "iter 12101/30000  loss         0.102063  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.193\n",
      "iter 12120/30000  loss         0.102045  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.197\n",
      "iter 12121/30000  loss         0.102044  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.197\n",
      "iter 12140/30000  loss         0.102025  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.201\n",
      "iter 12141/30000  loss         0.102024  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.201\n",
      "iter 12160/30000  loss         0.102006  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.205\n",
      "iter 12161/30000  loss         0.102005  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.205\n",
      "iter 12180/30000  loss         0.101987  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.209\n",
      "iter 12181/30000  loss         0.101986  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.210\n",
      "iter 12200/30000  loss         0.101968  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.214\n",
      "iter 12201/30000  loss         0.101967  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.214\n",
      "iter 12220/30000  loss         0.101949  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.218\n",
      "iter 12221/30000  loss         0.101948  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.218\n",
      "iter 12240/30000  loss         0.101929  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.222\n",
      "iter 12241/30000  loss         0.101929  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.222\n",
      "iter 12260/30000  loss         0.101910  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.226\n",
      "iter 12261/30000  loss         0.101909  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.226\n",
      "iter 12280/30000  loss         0.101891  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.230\n",
      "iter 12281/30000  loss         0.101891  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.231\n",
      "iter 12300/30000  loss         0.101873  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.235\n",
      "iter 12301/30000  loss         0.101872  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.235\n",
      "iter 12320/30000  loss         0.101854  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.239\n",
      "iter 12321/30000  loss         0.101853  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.239\n",
      "iter 12340/30000  loss         0.101835  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.243\n",
      "iter 12341/30000  loss         0.101834  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.243\n",
      "iter 12360/30000  loss         0.101816  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.247\n",
      "iter 12361/30000  loss         0.101815  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.247\n",
      "iter 12380/30000  loss         0.101797  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.251\n",
      "iter 12381/30000  loss         0.101796  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.251\n",
      "iter 12400/30000  loss         0.101779  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.255\n",
      "iter 12401/30000  loss         0.101778  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.256\n",
      "iter 12420/30000  loss         0.101760  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.260\n",
      "iter 12421/30000  loss         0.101759  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.260\n",
      "iter 12440/30000  loss         0.101742  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.264\n",
      "iter 12441/30000  loss         0.101741  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.264\n",
      "iter 12460/30000  loss         0.101723  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.268\n",
      "iter 12461/30000  loss         0.101722  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.268\n",
      "iter 12480/30000  loss         0.101705  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.272\n",
      "iter 12481/30000  loss         0.101704  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.272\n",
      "iter 12500/30000  loss         0.101686  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.276\n",
      "iter 12501/30000  loss         0.101685  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.276\n",
      "iter 12520/30000  loss         0.101668  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.280\n",
      "iter 12521/30000  loss         0.101667  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.280\n",
      "iter 12540/30000  loss         0.101650  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.284\n",
      "iter 12541/30000  loss         0.101649  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.284\n",
      "iter 12560/30000  loss         0.101632  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.288\n",
      "iter 12561/30000  loss         0.101631  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.289\n",
      "iter 12580/30000  loss         0.101613  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.292\n",
      "iter 12581/30000  loss         0.101612  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.293\n",
      "iter 12600/30000  loss         0.101595  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.297\n",
      "iter 12601/30000  loss         0.101594  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12620/30000  loss         0.101577  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.301\n",
      "iter 12621/30000  loss         0.101576  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.301\n",
      "iter 12640/30000  loss         0.101559  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.305\n",
      "iter 12641/30000  loss         0.101558  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.305\n",
      "iter 12660/30000  loss         0.101541  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.309\n",
      "iter 12661/30000  loss         0.101540  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.309\n",
      "iter 12680/30000  loss         0.101523  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.313\n",
      "iter 12681/30000  loss         0.101522  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.313\n",
      "iter 12700/30000  loss         0.101505  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.317\n",
      "iter 12701/30000  loss         0.101505  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.317\n",
      "iter 12720/30000  loss         0.101488  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.321\n",
      "iter 12721/30000  loss         0.101487  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.321\n",
      "iter 12740/30000  loss         0.101470  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.325\n",
      "iter 12741/30000  loss         0.101469  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.325\n",
      "iter 12760/30000  loss         0.101452  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.329\n",
      "iter 12761/30000  loss         0.101451  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.329\n",
      "iter 12780/30000  loss         0.101434  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.333\n",
      "iter 12781/30000  loss         0.101434  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.333\n",
      "iter 12800/30000  loss         0.101417  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.337\n",
      "iter 12801/30000  loss         0.101416  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.337\n",
      "iter 12820/30000  loss         0.101399  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.341\n",
      "iter 12821/30000  loss         0.101398  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.341\n",
      "iter 12840/30000  loss         0.101382  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.345\n",
      "iter 12841/30000  loss         0.101381  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.345\n",
      "iter 12860/30000  loss         0.101364  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.349\n",
      "iter 12861/30000  loss         0.101363  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.349\n",
      "iter 12880/30000  loss         0.101347  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.353\n",
      "iter 12881/30000  loss         0.101346  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.353\n",
      "iter 12900/30000  loss         0.101330  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.357\n",
      "iter 12901/30000  loss         0.101329  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.357\n",
      "iter 12920/30000  loss         0.101312  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.361\n",
      "iter 12921/30000  loss         0.101311  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.361\n",
      "iter 12940/30000  loss         0.101295  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.365\n",
      "iter 12941/30000  loss         0.101294  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.365\n",
      "iter 12960/30000  loss         0.101278  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.369\n",
      "iter 12961/30000  loss         0.101277  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.369\n",
      "iter 12980/30000  loss         0.101261  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.373\n",
      "iter 12981/30000  loss         0.101260  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.373\n",
      "iter 13000/30000  loss         0.101243  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.377\n",
      "iter 13001/30000  loss         0.101243  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.377\n",
      "iter 13020/30000  loss         0.101226  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.381\n",
      "iter 13021/30000  loss         0.101226  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.381\n",
      "iter 13040/30000  loss         0.101209  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.385\n",
      "iter 13041/30000  loss         0.101209  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.385\n",
      "iter 13060/30000  loss         0.101192  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.389\n",
      "iter 13061/30000  loss         0.101192  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.389\n",
      "iter 13080/30000  loss         0.101175  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.393\n",
      "iter 13081/30000  loss         0.101175  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.393\n",
      "iter 13100/30000  loss         0.101159  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.397\n",
      "iter 13101/30000  loss         0.101158  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.397\n",
      "iter 13120/30000  loss         0.101142  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.401\n",
      "iter 13121/30000  loss         0.101141  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.401\n",
      "iter 13140/30000  loss         0.101125  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.404\n",
      "iter 13141/30000  loss         0.101124  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.405\n",
      "iter 13160/30000  loss         0.101108  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.408\n",
      "iter 13161/30000  loss         0.101107  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.409\n",
      "iter 13180/30000  loss         0.101092  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.412\n",
      "iter 13181/30000  loss         0.101091  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.413\n",
      "iter 13200/30000  loss         0.101075  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.416\n",
      "iter 13201/30000  loss         0.101074  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.416\n",
      "iter 13220/30000  loss         0.101058  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.420\n",
      "iter 13221/30000  loss         0.101057  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.420\n",
      "iter 13240/30000  loss         0.101042  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.424\n",
      "iter 13241/30000  loss         0.101041  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.424\n",
      "iter 13260/30000  loss         0.101025  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.428\n",
      "iter 13261/30000  loss         0.101024  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.428\n",
      "iter 13280/30000  loss         0.101009  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.432\n",
      "iter 13281/30000  loss         0.101008  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.432\n",
      "iter 13300/30000  loss         0.100992  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.436\n",
      "iter 13301/30000  loss         0.100991  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.436\n",
      "iter 13320/30000  loss         0.100976  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.440\n",
      "iter 13321/30000  loss         0.100975  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.440\n",
      "iter 13340/30000  loss         0.100960  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.443\n",
      "iter 13341/30000  loss         0.100959  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.444\n",
      "iter 13360/30000  loss         0.100943  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.447\n",
      "iter 13361/30000  loss         0.100942  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.447\n",
      "iter 13380/30000  loss         0.100927  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.451\n",
      "iter 13381/30000  loss         0.100926  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.451\n",
      "iter 13400/30000  loss         0.100911  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.455\n",
      "iter 13401/30000  loss         0.100910  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13420/30000  loss         0.100895  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.459\n",
      "iter 13421/30000  loss         0.100894  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.459\n",
      "iter 13440/30000  loss         0.100879  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.463\n",
      "iter 13441/30000  loss         0.100878  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.463\n",
      "iter 13460/30000  loss         0.100863  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.466\n",
      "iter 13461/30000  loss         0.100862  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.467\n",
      "iter 13480/30000  loss         0.100847  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.470\n",
      "iter 13481/30000  loss         0.100846  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.470\n",
      "iter 13500/30000  loss         0.100831  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.474\n",
      "iter 13501/30000  loss         0.100830  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.474\n",
      "iter 13520/30000  loss         0.100815  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.478\n",
      "iter 13521/30000  loss         0.100814  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.478\n",
      "iter 13540/30000  loss         0.100799  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.482\n",
      "iter 13541/30000  loss         0.100798  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.482\n",
      "iter 13560/30000  loss         0.100783  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.485\n",
      "iter 13561/30000  loss         0.100782  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.486\n",
      "iter 13580/30000  loss         0.100767  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.489\n",
      "iter 13581/30000  loss         0.100766  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.489\n",
      "iter 13600/30000  loss         0.100751  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.493\n",
      "iter 13601/30000  loss         0.100751  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.493\n",
      "iter 13620/30000  loss         0.100736  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.497\n",
      "iter 13621/30000  loss         0.100735  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.497\n",
      "iter 13640/30000  loss         0.100720  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.501\n",
      "iter 13641/30000  loss         0.100719  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.501\n",
      "iter 13660/30000  loss         0.100704  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.504\n",
      "iter 13661/30000  loss         0.100704  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.505\n",
      "iter 13680/30000  loss         0.100689  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.508\n",
      "iter 13681/30000  loss         0.100688  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.508\n",
      "iter 13700/30000  loss         0.100673  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.512\n",
      "iter 13701/30000  loss         0.100672  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.512\n",
      "iter 13720/30000  loss         0.100658  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.516\n",
      "iter 13721/30000  loss         0.100657  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.516\n",
      "iter 13740/30000  loss         0.100642  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.519\n",
      "iter 13741/30000  loss         0.100641  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.520\n",
      "iter 13760/30000  loss         0.100627  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.523\n",
      "iter 13761/30000  loss         0.100626  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.523\n",
      "iter 13780/30000  loss         0.100611  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.527\n",
      "iter 13781/30000  loss         0.100611  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.527\n",
      "iter 13800/30000  loss         0.100596  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.531\n",
      "iter 13801/30000  loss         0.100595  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.531\n",
      "iter 13820/30000  loss         0.100581  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.534\n",
      "iter 13821/30000  loss         0.100580  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.534\n",
      "iter 13840/30000  loss         0.100565  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.538\n",
      "iter 13841/30000  loss         0.100565  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.538\n",
      "iter 13860/30000  loss         0.100550  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.542\n",
      "iter 13861/30000  loss         0.100549  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.542\n",
      "iter 13880/30000  loss         0.100535  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.545\n",
      "iter 13881/30000  loss         0.100534  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.546\n",
      "iter 13900/30000  loss         0.100520  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.549\n",
      "iter 13901/30000  loss         0.100519  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.549\n",
      "iter 13920/30000  loss         0.100505  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.553\n",
      "iter 13921/30000  loss         0.100504  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.553\n",
      "iter 13940/30000  loss         0.100490  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.556\n",
      "iter 13941/30000  loss         0.100489  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.557\n",
      "iter 13960/30000  loss         0.100475  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.560\n",
      "iter 13961/30000  loss         0.100474  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.560\n",
      "iter 13980/30000  loss         0.100460  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.564\n",
      "iter 13981/30000  loss         0.100459  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.564\n",
      "iter 14000/30000  loss         0.100445  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.568\n",
      "iter 14001/30000  loss         0.100444  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.568\n",
      "iter 14020/30000  loss         0.100430  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.571\n",
      "iter 14021/30000  loss         0.100429  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.571\n",
      "iter 14040/30000  loss         0.100415  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.575\n",
      "iter 14041/30000  loss         0.100414  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.575\n",
      "iter 14060/30000  loss         0.100400  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.579\n",
      "iter 14061/30000  loss         0.100400  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.579\n",
      "iter 14080/30000  loss         0.100386  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.582\n",
      "iter 14081/30000  loss         0.100385  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.582\n",
      "iter 14100/30000  loss         0.100371  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.586\n",
      "iter 14101/30000  loss         0.100370  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.586\n",
      "iter 14120/30000  loss         0.100356  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.589\n",
      "iter 14121/30000  loss         0.100355  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.590\n",
      "iter 14140/30000  loss         0.100341  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.593\n",
      "iter 14141/30000  loss         0.100341  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.593\n",
      "iter 14160/30000  loss         0.100327  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.597\n",
      "iter 14161/30000  loss         0.100326  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.597\n",
      "iter 14180/30000  loss         0.100312  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.600\n",
      "iter 14181/30000  loss         0.100311  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.601\n",
      "iter 14200/30000  loss         0.100298  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.604\n",
      "iter 14201/30000  loss         0.100297  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14220/30000  loss         0.100283  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.608\n",
      "iter 14221/30000  loss         0.100282  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.608\n",
      "iter 14240/30000  loss         0.100269  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.611\n",
      "iter 14241/30000  loss         0.100268  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.611\n",
      "iter 14260/30000  loss         0.100254  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.615\n",
      "iter 14261/30000  loss         0.100254  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.615\n",
      "iter 14280/30000  loss         0.100240  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.618\n",
      "iter 14281/30000  loss         0.100239  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.619\n",
      "iter 14300/30000  loss         0.100226  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.622\n",
      "iter 14301/30000  loss         0.100225  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.622\n",
      "iter 14320/30000  loss         0.100211  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.626\n",
      "iter 14321/30000  loss         0.100210  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.626\n",
      "iter 14340/30000  loss         0.100197  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.629\n",
      "iter 14341/30000  loss         0.100196  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.629\n",
      "iter 14360/30000  loss         0.100183  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.633\n",
      "iter 14361/30000  loss         0.100182  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.633\n",
      "iter 14380/30000  loss         0.100168  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.636\n",
      "iter 14381/30000  loss         0.100168  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.636\n",
      "iter 14400/30000  loss         0.100154  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.640\n",
      "iter 14401/30000  loss         0.100154  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.640\n",
      "iter 14420/30000  loss         0.100140  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.643\n",
      "iter 14421/30000  loss         0.100139  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.644\n",
      "iter 14440/30000  loss         0.100126  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.647\n",
      "iter 14441/30000  loss         0.100125  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.647\n",
      "iter 14460/30000  loss         0.100112  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.651\n",
      "iter 14461/30000  loss         0.100111  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.651\n",
      "iter 14480/30000  loss         0.100098  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.654\n",
      "iter 14481/30000  loss         0.100097  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.654\n",
      "iter 14500/30000  loss         0.100084  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.658\n",
      "iter 14501/30000  loss         0.100083  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.658\n",
      "iter 14520/30000  loss         0.100070  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.661\n",
      "iter 14521/30000  loss         0.100069  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.661\n",
      "iter 14540/30000  loss         0.100056  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.665\n",
      "iter 14541/30000  loss         0.100055  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.665\n",
      "iter 14560/30000  loss         0.100042  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.668\n",
      "iter 14561/30000  loss         0.100042  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.668\n",
      "iter 14580/30000  loss         0.100028  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.672\n",
      "iter 14581/30000  loss         0.100028  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.672\n",
      "iter 14600/30000  loss         0.100015  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.675\n",
      "iter 14601/30000  loss         0.100014  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.675\n",
      "iter 14620/30000  loss         0.100001  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.679\n",
      "iter 14621/30000  loss         0.100000  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.679\n",
      "iter 14640/30000  loss         0.099987  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.682\n",
      "iter 14641/30000  loss         0.099986  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.682\n",
      "iter 14660/30000  loss         0.099973  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.686\n",
      "iter 14661/30000  loss         0.099973  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.686\n",
      "iter 14680/30000  loss         0.099960  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.689\n",
      "iter 14681/30000  loss         0.099959  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.689\n",
      "iter 14700/30000  loss         0.099946  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.693\n",
      "iter 14701/30000  loss         0.099946  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.693\n",
      "iter 14720/30000  loss         0.099933  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.696\n",
      "iter 14721/30000  loss         0.099932  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.696\n",
      "iter 14740/30000  loss         0.099919  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.700\n",
      "iter 14741/30000  loss         0.099918  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.700\n",
      "iter 14760/30000  loss         0.099906  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.703\n",
      "iter 14761/30000  loss         0.099905  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.703\n",
      "iter 14780/30000  loss         0.099892  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.707\n",
      "iter 14781/30000  loss         0.099891  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.707\n",
      "iter 14800/30000  loss         0.099879  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.710\n",
      "iter 14801/30000  loss         0.099878  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.710\n",
      "iter 14820/30000  loss         0.099865  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.714\n",
      "iter 14821/30000  loss         0.099864  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.714\n",
      "iter 14840/30000  loss         0.099852  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.717\n",
      "iter 14841/30000  loss         0.099851  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.717\n",
      "iter 14860/30000  loss         0.099838  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.720\n",
      "iter 14861/30000  loss         0.099838  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.721\n",
      "iter 14880/30000  loss         0.099825  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.724\n",
      "iter 14881/30000  loss         0.099824  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.724\n",
      "iter 14900/30000  loss         0.099812  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.727\n",
      "iter 14901/30000  loss         0.099811  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.728\n",
      "iter 14920/30000  loss         0.099799  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.731\n",
      "iter 14921/30000  loss         0.099798  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.731\n",
      "iter 14940/30000  loss         0.099785  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.734\n",
      "iter 14941/30000  loss         0.099785  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.734\n",
      "iter 14960/30000  loss         0.099772  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.738\n",
      "iter 14961/30000  loss         0.099772  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.738\n",
      "iter 14980/30000  loss         0.099759  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.741\n",
      "iter 14981/30000  loss         0.099758  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.741\n",
      "iter 15000/30000  loss         0.099746  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.744\n",
      "iter 15001/30000  loss         0.099745  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15020/30000  loss         0.099733  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.748\n",
      "iter 15021/30000  loss         0.099732  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.748\n",
      "iter 15040/30000  loss         0.099720  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.751\n",
      "iter 15041/30000  loss         0.099719  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.751\n",
      "iter 15060/30000  loss         0.099707  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.755\n",
      "iter 15061/30000  loss         0.099706  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.755\n",
      "iter 15080/30000  loss         0.099694  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.758\n",
      "iter 15081/30000  loss         0.099693  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.758\n",
      "iter 15100/30000  loss         0.099681  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.761\n",
      "iter 15101/30000  loss         0.099680  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.762\n",
      "iter 15120/30000  loss         0.099668  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.765\n",
      "iter 15121/30000  loss         0.099667  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.765\n",
      "iter 15140/30000  loss         0.099655  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.768\n",
      "iter 15141/30000  loss         0.099654  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.768\n",
      "iter 15160/30000  loss         0.099642  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.772\n",
      "iter 15161/30000  loss         0.099642  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.772\n",
      "iter 15180/30000  loss         0.099629  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.775\n",
      "iter 15181/30000  loss         0.099629  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.775\n",
      "iter 15200/30000  loss         0.099617  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.778\n",
      "iter 15201/30000  loss         0.099616  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.778\n",
      "iter 15220/30000  loss         0.099604  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.782\n",
      "iter 15221/30000  loss         0.099603  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.782\n",
      "iter 15240/30000  loss         0.099591  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.785\n",
      "iter 15241/30000  loss         0.099591  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.785\n",
      "iter 15260/30000  loss         0.099578  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.788\n",
      "iter 15261/30000  loss         0.099578  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.789\n",
      "iter 15280/30000  loss         0.099566  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.792\n",
      "iter 15281/30000  loss         0.099565  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.792\n",
      "iter 15300/30000  loss         0.099553  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.795\n",
      "iter 15301/30000  loss         0.099553  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.795\n",
      "iter 15320/30000  loss         0.099541  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.798\n",
      "iter 15321/30000  loss         0.099540  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.799\n",
      "iter 15340/30000  loss         0.099528  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.802\n",
      "iter 15341/30000  loss         0.099527  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.802\n",
      "iter 15360/30000  loss         0.099516  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.805\n",
      "iter 15361/30000  loss         0.099515  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.805\n",
      "iter 15380/30000  loss         0.099503  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.808\n",
      "iter 15381/30000  loss         0.099502  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.809\n",
      "iter 15400/30000  loss         0.099491  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.812\n",
      "iter 15401/30000  loss         0.099490  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.812\n",
      "iter 15420/30000  loss         0.099478  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.815\n",
      "iter 15421/30000  loss         0.099477  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.815\n",
      "iter 15440/30000  loss         0.099466  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.818\n",
      "iter 15441/30000  loss         0.099465  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.818\n",
      "iter 15460/30000  loss         0.099453  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.822\n",
      "iter 15461/30000  loss         0.099453  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.822\n",
      "iter 15480/30000  loss         0.099441  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.825\n",
      "iter 15481/30000  loss         0.099440  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.825\n",
      "iter 15500/30000  loss         0.099429  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.828\n",
      "iter 15501/30000  loss         0.099428  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.828\n",
      "iter 15520/30000  loss         0.099416  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.832\n",
      "iter 15521/30000  loss         0.099416  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.832\n",
      "iter 15540/30000  loss         0.099404  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.835\n",
      "iter 15541/30000  loss         0.099404  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.835\n",
      "iter 15560/30000  loss         0.099392  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.838\n",
      "iter 15561/30000  loss         0.099391  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.838\n",
      "iter 15580/30000  loss         0.099380  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.841\n",
      "iter 15581/30000  loss         0.099379  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.842\n",
      "iter 15600/30000  loss         0.099368  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.845\n",
      "iter 15601/30000  loss         0.099367  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.845\n",
      "iter 15620/30000  loss         0.099355  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.848\n",
      "iter 15621/30000  loss         0.099355  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.848\n",
      "iter 15640/30000  loss         0.099343  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.851\n",
      "iter 15641/30000  loss         0.099343  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.851\n",
      "iter 15660/30000  loss         0.099331  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.854\n",
      "iter 15661/30000  loss         0.099331  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.855\n",
      "iter 15680/30000  loss         0.099319  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.858\n",
      "iter 15681/30000  loss         0.099319  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.858\n",
      "iter 15700/30000  loss         0.099307  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.861\n",
      "iter 15701/30000  loss         0.099307  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.861\n",
      "iter 15720/30000  loss         0.099295  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.864\n",
      "iter 15721/30000  loss         0.099295  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.864\n",
      "iter 15740/30000  loss         0.099283  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.867\n",
      "iter 15741/30000  loss         0.099283  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.868\n",
      "iter 15760/30000  loss         0.099271  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.871\n",
      "iter 15761/30000  loss         0.099271  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.871\n",
      "iter 15780/30000  loss         0.099259  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.874\n",
      "iter 15781/30000  loss         0.099259  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.874\n",
      "iter 15800/30000  loss         0.099248  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.877\n",
      "iter 15801/30000  loss         0.099247  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15820/30000  loss         0.099236  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.880\n",
      "iter 15821/30000  loss         0.099235  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.880\n",
      "iter 15840/30000  loss         0.099224  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.884\n",
      "iter 15841/30000  loss         0.099223  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.884\n",
      "iter 15860/30000  loss         0.099212  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.887\n",
      "iter 15861/30000  loss         0.099212  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.887\n",
      "iter 15880/30000  loss         0.099200  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.890\n",
      "iter 15881/30000  loss         0.099200  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.890\n",
      "iter 15900/30000  loss         0.099189  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.893\n",
      "iter 15901/30000  loss         0.099188  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.893\n",
      "iter 15920/30000  loss         0.099177  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.896\n",
      "iter 15921/30000  loss         0.099176  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.896\n",
      "iter 15940/30000  loss         0.099165  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.900\n",
      "iter 15941/30000  loss         0.099165  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.900\n",
      "iter 15960/30000  loss         0.099154  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.903\n",
      "iter 15961/30000  loss         0.099153  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.903\n",
      "iter 15980/30000  loss         0.099142  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.906\n",
      "iter 15981/30000  loss         0.099142  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.906\n",
      "iter 16000/30000  loss         0.099131  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.909\n",
      "iter 16001/30000  loss         0.099130  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.909\n",
      "iter 16020/30000  loss         0.099119  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.912\n",
      "iter 16021/30000  loss         0.099118  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.912\n",
      "iter 16040/30000  loss         0.099107  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.915\n",
      "iter 16041/30000  loss         0.099107  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.916\n",
      "iter 16060/30000  loss         0.099096  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.919\n",
      "iter 16061/30000  loss         0.099095  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.919\n",
      "iter 16080/30000  loss         0.099084  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.922\n",
      "iter 16081/30000  loss         0.099084  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.922\n",
      "iter 16100/30000  loss         0.099073  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.925\n",
      "iter 16101/30000  loss         0.099072  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.925\n",
      "iter 16120/30000  loss         0.099062  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.928\n",
      "iter 16121/30000  loss         0.099061  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.928\n",
      "iter 16140/30000  loss         0.099050  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.931\n",
      "iter 16141/30000  loss         0.099050  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.931\n",
      "iter 16160/30000  loss         0.099039  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.934\n",
      "iter 16161/30000  loss         0.099038  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.935\n",
      "iter 16180/30000  loss         0.099028  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.937\n",
      "iter 16181/30000  loss         0.099027  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.938\n",
      "iter 16200/30000  loss         0.099016  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.941\n",
      "iter 16201/30000  loss         0.099016  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.941\n",
      "iter 16220/30000  loss         0.099005  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.944\n",
      "iter 16221/30000  loss         0.099004  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.944\n",
      "iter 16240/30000  loss         0.098994  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.947\n",
      "iter 16241/30000  loss         0.098993  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.947\n",
      "iter 16260/30000  loss         0.098983  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.950\n",
      "iter 16261/30000  loss         0.098982  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.950\n",
      "iter 16280/30000  loss         0.098971  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.953\n",
      "iter 16281/30000  loss         0.098971  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.953\n",
      "iter 16300/30000  loss         0.098960  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.956\n",
      "iter 16301/30000  loss         0.098960  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.956\n",
      "iter 16320/30000  loss         0.098949  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.959\n",
      "iter 16321/30000  loss         0.098948  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.960\n",
      "iter 16340/30000  loss         0.098938  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.962\n",
      "iter 16341/30000  loss         0.098937  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.963\n",
      "iter 16360/30000  loss         0.098927  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.966\n",
      "iter 16361/30000  loss         0.098926  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.966\n",
      "iter 16380/30000  loss         0.098916  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.969\n",
      "iter 16381/30000  loss         0.098915  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.969\n",
      "iter 16400/30000  loss         0.098905  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.972\n",
      "iter 16401/30000  loss         0.098904  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.972\n",
      "iter 16420/30000  loss         0.098894  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.975\n",
      "iter 16421/30000  loss         0.098893  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.975\n",
      "iter 16440/30000  loss         0.098883  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.978\n",
      "iter 16441/30000  loss         0.098882  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.978\n",
      "iter 16460/30000  loss         0.098872  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.981\n",
      "iter 16461/30000  loss         0.098871  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.981\n",
      "iter 16480/30000  loss         0.098861  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.984\n",
      "iter 16481/30000  loss         0.098860  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.984\n",
      "iter 16500/30000  loss         0.098850  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.987\n",
      "iter 16501/30000  loss         0.098849  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.987\n",
      "iter 16520/30000  loss         0.098839  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.990\n",
      "iter 16521/30000  loss         0.098839  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.990\n",
      "iter 16540/30000  loss         0.098828  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.993\n",
      "iter 16541/30000  loss         0.098828  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.993\n",
      "iter 16560/30000  loss         0.098817  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.996\n",
      "iter 16561/30000  loss         0.098817  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.996\n",
      "iter 16580/30000  loss         0.098807  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.999\n",
      "iter 16581/30000  loss         0.098806  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.000\n",
      "iter 16600/30000  loss         0.098796  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.002\n",
      "iter 16601/30000  loss         0.098795  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16620/30000  loss         0.098785  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.005\n",
      "iter 16621/30000  loss         0.098785  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.006\n",
      "iter 16640/30000  loss         0.098774  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.009\n",
      "iter 16641/30000  loss         0.098774  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.009\n",
      "iter 16660/30000  loss         0.098764  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.012\n",
      "iter 16661/30000  loss         0.098763  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.012\n",
      "iter 16680/30000  loss         0.098753  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.015\n",
      "iter 16681/30000  loss         0.098753  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.015\n",
      "iter 16700/30000  loss         0.098742  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.018\n",
      "iter 16701/30000  loss         0.098742  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.018\n",
      "iter 16720/30000  loss         0.098732  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.021\n",
      "iter 16721/30000  loss         0.098731  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.021\n",
      "iter 16740/30000  loss         0.098721  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.024\n",
      "iter 16741/30000  loss         0.098721  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.024\n",
      "iter 16760/30000  loss         0.098711  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.027\n",
      "iter 16761/30000  loss         0.098710  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.027\n",
      "iter 16780/30000  loss         0.098700  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.030\n",
      "iter 16781/30000  loss         0.098700  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.030\n",
      "iter 16800/30000  loss         0.098690  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.033\n",
      "iter 16801/30000  loss         0.098689  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.033\n",
      "iter 16820/30000  loss         0.098679  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.036\n",
      "iter 16821/30000  loss         0.098679  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.036\n",
      "iter 16840/30000  loss         0.098669  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.039\n",
      "iter 16841/30000  loss         0.098668  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.039\n",
      "iter 16860/30000  loss         0.098658  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.042\n",
      "iter 16861/30000  loss         0.098658  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.042\n",
      "iter 16880/30000  loss         0.098648  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.045\n",
      "iter 16881/30000  loss         0.098647  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.045\n",
      "iter 16900/30000  loss         0.098637  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.048\n",
      "iter 16901/30000  loss         0.098637  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.048\n",
      "iter 16920/30000  loss         0.098627  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.051\n",
      "iter 16921/30000  loss         0.098626  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.051\n",
      "iter 16940/30000  loss         0.098617  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.054\n",
      "iter 16941/30000  loss         0.098616  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    7.054\n",
      "iter 16960/30000  loss         0.098606  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.057\n",
      "iter 16961/30000  loss         0.098606  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.057\n",
      "iter 16980/30000  loss         0.098596  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.060\n",
      "iter 16981/30000  loss         0.098595  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.060\n",
      "iter 17000/30000  loss         0.098586  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.063\n",
      "iter 17001/30000  loss         0.098585  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.063\n",
      "iter 17020/30000  loss         0.098575  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.066\n",
      "iter 17021/30000  loss         0.098575  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.066\n",
      "iter 17040/30000  loss         0.098565  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.068\n",
      "iter 17041/30000  loss         0.098565  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.069\n",
      "iter 17060/30000  loss         0.098555  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.071\n",
      "iter 17061/30000  loss         0.098554  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.072\n",
      "iter 17080/30000  loss         0.098545  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.074\n",
      "iter 17081/30000  loss         0.098544  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.075\n",
      "iter 17100/30000  loss         0.098535  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.077\n",
      "iter 17101/30000  loss         0.098534  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.077\n",
      "iter 17120/30000  loss         0.098525  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.080\n",
      "iter 17121/30000  loss         0.098524  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.080\n",
      "iter 17140/30000  loss         0.098514  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.083\n",
      "iter 17141/30000  loss         0.098514  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.083\n",
      "iter 17160/30000  loss         0.098504  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.086\n",
      "iter 17161/30000  loss         0.098504  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.086\n",
      "iter 17180/30000  loss         0.098494  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.089\n",
      "iter 17181/30000  loss         0.098494  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.089\n",
      "iter 17200/30000  loss         0.098484  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.092\n",
      "iter 17201/30000  loss         0.098484  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.092\n",
      "iter 17220/30000  loss         0.098474  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.095\n",
      "iter 17221/30000  loss         0.098474  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.095\n",
      "iter 17240/30000  loss         0.098464  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.098\n",
      "iter 17241/30000  loss         0.098464  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.098\n",
      "iter 17260/30000  loss         0.098454  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.101\n",
      "iter 17261/30000  loss         0.098454  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.101\n",
      "iter 17280/30000  loss         0.098444  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.104\n",
      "iter 17281/30000  loss         0.098444  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.104\n",
      "iter 17300/30000  loss         0.098434  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.107\n",
      "iter 17301/30000  loss         0.098434  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.107\n",
      "iter 17320/30000  loss         0.098424  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.110\n",
      "iter 17321/30000  loss         0.098424  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.110\n",
      "iter 17340/30000  loss         0.098415  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.112\n",
      "iter 17341/30000  loss         0.098414  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    7.113\n",
      "iter 17360/30000  loss         0.098405  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.115\n",
      "iter 17361/30000  loss         0.098404  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.115\n",
      "iter 17380/30000  loss         0.098395  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.118\n",
      "iter 17381/30000  loss         0.098394  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.118\n",
      "iter 17400/30000  loss         0.098385  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.121\n",
      "iter 17401/30000  loss         0.098385  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17420/30000  loss         0.098375  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.124\n",
      "iter 17421/30000  loss         0.098375  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.124\n",
      "iter 17440/30000  loss         0.098366  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.127\n",
      "iter 17441/30000  loss         0.098365  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.127\n",
      "iter 17460/30000  loss         0.098356  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.130\n",
      "iter 17461/30000  loss         0.098355  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.130\n",
      "iter 17480/30000  loss         0.098346  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.133\n",
      "iter 17481/30000  loss         0.098346  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.133\n",
      "iter 17500/30000  loss         0.098336  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.135\n",
      "iter 17501/30000  loss         0.098336  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.136\n",
      "iter 17520/30000  loss         0.098327  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.138\n",
      "iter 17521/30000  loss         0.098326  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.138\n",
      "iter 17540/30000  loss         0.098317  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.141\n",
      "iter 17541/30000  loss         0.098317  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.141\n",
      "iter 17560/30000  loss         0.098307  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.144\n",
      "iter 17561/30000  loss         0.098307  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.144\n",
      "iter 17580/30000  loss         0.098298  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.147\n",
      "iter 17581/30000  loss         0.098297  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.147\n",
      "iter 17600/30000  loss         0.098288  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.150\n",
      "iter 17601/30000  loss         0.098288  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.150\n",
      "iter 17620/30000  loss         0.098279  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.153\n",
      "iter 17621/30000  loss         0.098278  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.153\n",
      "iter 17640/30000  loss         0.098269  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.155\n",
      "iter 17641/30000  loss         0.098269  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.156\n",
      "iter 17660/30000  loss         0.098260  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.158\n",
      "iter 17661/30000  loss         0.098259  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.158\n",
      "iter 17680/30000  loss         0.098250  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.161\n",
      "iter 17681/30000  loss         0.098250  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.161\n",
      "iter 17700/30000  loss         0.098241  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.164\n",
      "iter 17701/30000  loss         0.098240  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.164\n",
      "iter 17720/30000  loss         0.098231  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.167\n",
      "iter 17721/30000  loss         0.098231  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.167\n",
      "iter 17740/30000  loss         0.098222  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.170\n",
      "iter 17741/30000  loss         0.098221  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.170\n",
      "iter 17760/30000  loss         0.098212  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.172\n",
      "iter 17761/30000  loss         0.098212  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.173\n",
      "iter 17780/30000  loss         0.098203  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.175\n",
      "iter 17781/30000  loss         0.098202  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.175\n",
      "iter 17800/30000  loss         0.098194  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.178\n",
      "iter 17801/30000  loss         0.098193  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.178\n",
      "iter 17820/30000  loss         0.098184  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.181\n",
      "iter 17821/30000  loss         0.098184  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.181\n",
      "iter 17840/30000  loss         0.098175  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.184\n",
      "iter 17841/30000  loss         0.098174  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.184\n",
      "iter 17860/30000  loss         0.098166  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.186\n",
      "iter 17861/30000  loss         0.098165  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.187\n",
      "iter 17880/30000  loss         0.098156  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.189\n",
      "iter 17881/30000  loss         0.098156  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.189\n",
      "iter 17900/30000  loss         0.098147  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.192\n",
      "iter 17901/30000  loss         0.098147  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.192\n",
      "iter 17920/30000  loss         0.098138  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.195\n",
      "iter 17921/30000  loss         0.098137  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.195\n",
      "iter 17940/30000  loss         0.098129  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.198\n",
      "iter 17941/30000  loss         0.098128  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.198\n",
      "iter 17960/30000  loss         0.098119  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.200\n",
      "iter 17961/30000  loss         0.098119  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.201\n",
      "iter 17980/30000  loss         0.098110  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.203\n",
      "iter 17981/30000  loss         0.098110  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.203\n",
      "iter 18000/30000  loss         0.098101  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.206\n",
      "iter 18001/30000  loss         0.098101  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.206\n",
      "iter 18020/30000  loss         0.098092  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.209\n",
      "iter 18021/30000  loss         0.098091  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.209\n",
      "iter 18040/30000  loss         0.098083  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.211\n",
      "iter 18041/30000  loss         0.098082  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.212\n",
      "iter 18060/30000  loss         0.098074  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.214\n",
      "iter 18061/30000  loss         0.098073  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.214\n",
      "iter 18080/30000  loss         0.098065  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.217\n",
      "iter 18081/30000  loss         0.098064  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.217\n",
      "iter 18100/30000  loss         0.098055  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.220\n",
      "iter 18101/30000  loss         0.098055  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.220\n",
      "iter 18120/30000  loss         0.098046  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.223\n",
      "iter 18121/30000  loss         0.098046  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.223\n",
      "iter 18140/30000  loss         0.098037  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.225\n",
      "iter 18141/30000  loss         0.098037  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.225\n",
      "iter 18160/30000  loss         0.098028  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.228\n",
      "iter 18161/30000  loss         0.098028  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.228\n",
      "iter 18180/30000  loss         0.098019  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.231\n",
      "iter 18181/30000  loss         0.098019  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.231\n",
      "iter 18200/30000  loss         0.098010  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.233\n",
      "iter 18201/30000  loss         0.098010  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18220/30000  loss         0.098002  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.236\n",
      "iter 18221/30000  loss         0.098001  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.236\n",
      "iter 18240/30000  loss         0.097993  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.239\n",
      "iter 18241/30000  loss         0.097992  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.239\n",
      "iter 18260/30000  loss         0.097984  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.242\n",
      "iter 18261/30000  loss         0.097983  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.242\n",
      "iter 18280/30000  loss         0.097975  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.244\n",
      "iter 18281/30000  loss         0.097974  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.245\n",
      "iter 18300/30000  loss         0.097966  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.247\n",
      "iter 18301/30000  loss         0.097966  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.247\n",
      "iter 18320/30000  loss         0.097957  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.250\n",
      "iter 18321/30000  loss         0.097957  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.250\n",
      "iter 18340/30000  loss         0.097948  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.253\n",
      "iter 18341/30000  loss         0.097948  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.253\n",
      "iter 18360/30000  loss         0.097939  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.255\n",
      "iter 18361/30000  loss         0.097939  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.255\n",
      "iter 18380/30000  loss         0.097931  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.258\n",
      "iter 18381/30000  loss         0.097930  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.258\n",
      "iter 18400/30000  loss         0.097922  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.261\n",
      "iter 18401/30000  loss         0.097922  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.261\n",
      "iter 18420/30000  loss         0.097913  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.263\n",
      "iter 18421/30000  loss         0.097913  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.264\n",
      "iter 18440/30000  loss         0.097904  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.266\n",
      "iter 18441/30000  loss         0.097904  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.266\n",
      "iter 18460/30000  loss         0.097896  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.269\n",
      "iter 18461/30000  loss         0.097895  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.269\n",
      "iter 18480/30000  loss         0.097887  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.271\n",
      "iter 18481/30000  loss         0.097887  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.272\n",
      "iter 18500/30000  loss         0.097878  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.274\n",
      "iter 18501/30000  loss         0.097878  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.274\n",
      "iter 18520/30000  loss         0.097870  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.277\n",
      "iter 18521/30000  loss         0.097869  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.277\n",
      "iter 18540/30000  loss         0.097861  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.279\n",
      "iter 18541/30000  loss         0.097861  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.280\n",
      "iter 18560/30000  loss         0.097853  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.282\n",
      "iter 18561/30000  loss         0.097852  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.282\n",
      "iter 18580/30000  loss         0.097844  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.285\n",
      "iter 18581/30000  loss         0.097844  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.285\n",
      "iter 18600/30000  loss         0.097835  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.287\n",
      "iter 18601/30000  loss         0.097835  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.288\n",
      "iter 18620/30000  loss         0.097827  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.290\n",
      "iter 18621/30000  loss         0.097826  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.290\n",
      "iter 18640/30000  loss         0.097818  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.293\n",
      "iter 18641/30000  loss         0.097818  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.293\n",
      "iter 18660/30000  loss         0.097810  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.295\n",
      "iter 18661/30000  loss         0.097809  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.296\n",
      "iter 18680/30000  loss         0.097801  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.298\n",
      "iter 18681/30000  loss         0.097801  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.298\n",
      "iter 18700/30000  loss         0.097793  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.301\n",
      "iter 18701/30000  loss         0.097792  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.301\n",
      "iter 18720/30000  loss         0.097784  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.303\n",
      "iter 18721/30000  loss         0.097784  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.304\n",
      "iter 18740/30000  loss         0.097776  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.306\n",
      "iter 18741/30000  loss         0.097775  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.306\n",
      "iter 18760/30000  loss         0.097767  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.309\n",
      "iter 18761/30000  loss         0.097767  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.309\n",
      "iter 18780/30000  loss         0.097759  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.311\n",
      "iter 18781/30000  loss         0.097759  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.311\n",
      "iter 18800/30000  loss         0.097751  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.314\n",
      "iter 18801/30000  loss         0.097750  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.314\n",
      "iter 18820/30000  loss         0.097742  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.317\n",
      "iter 18821/30000  loss         0.097742  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.317\n",
      "iter 18840/30000  loss         0.097734  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.319\n",
      "iter 18841/30000  loss         0.097734  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.319\n",
      "iter 18860/30000  loss         0.097726  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.322\n",
      "iter 18861/30000  loss         0.097725  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.322\n",
      "iter 18880/30000  loss         0.097717  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.324\n",
      "iter 18881/30000  loss         0.097717  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.325\n",
      "iter 18900/30000  loss         0.097709  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.327\n",
      "iter 18901/30000  loss         0.097709  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.327\n",
      "iter 18920/30000  loss         0.097701  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.330\n",
      "iter 18921/30000  loss         0.097700  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.330\n",
      "iter 18940/30000  loss         0.097692  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.332\n",
      "iter 18941/30000  loss         0.097692  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.332\n",
      "iter 18960/30000  loss         0.097684  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.335\n",
      "iter 18961/30000  loss         0.097684  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.335\n",
      "iter 18980/30000  loss         0.097676  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.338\n",
      "iter 18981/30000  loss         0.097676  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.338\n",
      "iter 19000/30000  loss         0.097668  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.340\n",
      "iter 19001/30000  loss         0.097667  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19020/30000  loss         0.097660  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.343\n",
      "iter 19021/30000  loss         0.097659  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.343\n",
      "iter 19040/30000  loss         0.097651  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.345\n",
      "iter 19041/30000  loss         0.097651  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.345\n",
      "iter 19060/30000  loss         0.097643  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.348\n",
      "iter 19061/30000  loss         0.097643  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.348\n",
      "iter 19080/30000  loss         0.097635  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.350\n",
      "iter 19081/30000  loss         0.097635  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.351\n",
      "iter 19100/30000  loss         0.097627  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.353\n",
      "iter 19101/30000  loss         0.097627  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.353\n",
      "iter 19120/30000  loss         0.097619  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.356\n",
      "iter 19121/30000  loss         0.097619  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.356\n",
      "iter 19140/30000  loss         0.097611  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.358\n",
      "iter 19141/30000  loss         0.097610  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.358\n",
      "iter 19160/30000  loss         0.097603  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.361\n",
      "iter 19161/30000  loss         0.097602  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.361\n",
      "iter 19180/30000  loss         0.097595  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.363\n",
      "iter 19181/30000  loss         0.097594  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.363\n",
      "iter 19200/30000  loss         0.097587  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.366\n",
      "iter 19201/30000  loss         0.097586  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.366\n",
      "iter 19220/30000  loss         0.097579  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.368\n",
      "iter 19221/30000  loss         0.097578  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.369\n",
      "iter 19240/30000  loss         0.097571  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.371\n",
      "iter 19241/30000  loss         0.097570  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.371\n",
      "iter 19260/30000  loss         0.097563  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.374\n",
      "iter 19261/30000  loss         0.097562  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.374\n",
      "iter 19280/30000  loss         0.097555  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.376\n",
      "iter 19281/30000  loss         0.097554  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.376\n",
      "iter 19300/30000  loss         0.097547  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.379\n",
      "iter 19301/30000  loss         0.097546  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.379\n",
      "iter 19320/30000  loss         0.097539  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.381\n",
      "iter 19321/30000  loss         0.097539  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.381\n",
      "iter 19340/30000  loss         0.097531  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.384\n",
      "iter 19341/30000  loss         0.097531  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.384\n",
      "iter 19360/30000  loss         0.097523  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.386\n",
      "iter 19361/30000  loss         0.097523  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.386\n",
      "iter 19380/30000  loss         0.097515  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.389\n",
      "iter 19381/30000  loss         0.097515  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.389\n",
      "iter 19400/30000  loss         0.097507  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.391\n",
      "iter 19401/30000  loss         0.097507  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.392\n",
      "iter 19420/30000  loss         0.097500  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.394\n",
      "iter 19421/30000  loss         0.097499  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.394\n",
      "iter 19440/30000  loss         0.097492  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.396\n",
      "iter 19441/30000  loss         0.097491  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.397\n",
      "iter 19460/30000  loss         0.097484  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.399\n",
      "iter 19461/30000  loss         0.097484  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.399\n",
      "iter 19480/30000  loss         0.097476  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.401\n",
      "iter 19481/30000  loss         0.097476  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.402\n",
      "iter 19500/30000  loss         0.097468  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.404\n",
      "iter 19501/30000  loss         0.097468  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.404\n",
      "iter 19520/30000  loss         0.097461  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.407\n",
      "iter 19521/30000  loss         0.097460  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.407\n",
      "iter 19540/30000  loss         0.097453  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.409\n",
      "iter 19541/30000  loss         0.097452  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.409\n",
      "iter 19560/30000  loss         0.097445  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.412\n",
      "iter 19561/30000  loss         0.097445  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.412\n",
      "iter 19580/30000  loss         0.097437  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.414\n",
      "iter 19581/30000  loss         0.097437  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.414\n",
      "iter 19600/30000  loss         0.097430  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.417\n",
      "iter 19601/30000  loss         0.097429  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.417\n",
      "iter 19620/30000  loss         0.097422  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.419\n",
      "iter 19621/30000  loss         0.097422  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.419\n",
      "iter 19640/30000  loss         0.097414  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.422\n",
      "iter 19641/30000  loss         0.097414  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.422\n",
      "iter 19660/30000  loss         0.097407  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.424\n",
      "iter 19661/30000  loss         0.097406  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.424\n",
      "iter 19680/30000  loss         0.097399  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.426\n",
      "iter 19681/30000  loss         0.097399  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.427\n",
      "iter 19700/30000  loss         0.097391  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.429\n",
      "iter 19701/30000  loss         0.097391  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.429\n",
      "iter 19720/30000  loss         0.097384  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.431\n",
      "iter 19721/30000  loss         0.097383  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.432\n",
      "iter 19740/30000  loss         0.097376  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.434\n",
      "iter 19741/30000  loss         0.097376  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.434\n",
      "iter 19760/30000  loss         0.097369  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.436\n",
      "iter 19761/30000  loss         0.097368  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.437\n",
      "iter 19780/30000  loss         0.097361  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.439\n",
      "iter 19781/30000  loss         0.097361  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.439\n",
      "iter 19800/30000  loss         0.097354  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.441\n",
      "iter 19801/30000  loss         0.097353  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19820/30000  loss         0.097346  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.444\n",
      "iter 19821/30000  loss         0.097346  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.444\n",
      "iter 19840/30000  loss         0.097339  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.446\n",
      "iter 19841/30000  loss         0.097338  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.446\n",
      "iter 19860/30000  loss         0.097331  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.449\n",
      "iter 19861/30000  loss         0.097331  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.449\n",
      "iter 19880/30000  loss         0.097324  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.451\n",
      "iter 19881/30000  loss         0.097323  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.451\n",
      "iter 19900/30000  loss         0.097316  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.454\n",
      "iter 19901/30000  loss         0.097316  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.454\n",
      "iter 19920/30000  loss         0.097309  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.456\n",
      "iter 19921/30000  loss         0.097308  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.456\n",
      "iter 19940/30000  loss         0.097301  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.459\n",
      "iter 19941/30000  loss         0.097301  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.459\n",
      "iter 19960/30000  loss         0.097294  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.461\n",
      "iter 19961/30000  loss         0.097294  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.461\n",
      "iter 19980/30000  loss         0.097287  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.463\n",
      "iter 19981/30000  loss         0.097286  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.464\n",
      "iter 20000/30000  loss         0.097279  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.466\n",
      "iter 20001/30000  loss         0.097279  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.466\n",
      "iter 20020/30000  loss         0.097272  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.468\n",
      "iter 20021/30000  loss         0.097271  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.468\n",
      "iter 20040/30000  loss         0.097264  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.471\n",
      "iter 20041/30000  loss         0.097264  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.471\n",
      "iter 20060/30000  loss         0.097257  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.473\n",
      "iter 20061/30000  loss         0.097257  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.473\n",
      "iter 20080/30000  loss         0.097250  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.476\n",
      "iter 20081/30000  loss         0.097249  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.476\n",
      "iter 20100/30000  loss         0.097243  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.478\n",
      "iter 20101/30000  loss         0.097242  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.478\n",
      "iter 20120/30000  loss         0.097235  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.480\n",
      "iter 20121/30000  loss         0.097235  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.481\n",
      "iter 20140/30000  loss         0.097228  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.483\n",
      "iter 20141/30000  loss         0.097228  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.483\n",
      "iter 20160/30000  loss         0.097221  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.485\n",
      "iter 20161/30000  loss         0.097220  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.485\n",
      "iter 20180/30000  loss         0.097213  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.488\n",
      "iter 20181/30000  loss         0.097213  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.488\n",
      "iter 20200/30000  loss         0.097206  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.490\n",
      "iter 20201/30000  loss         0.097206  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.490\n",
      "iter 20220/30000  loss         0.097199  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.492\n",
      "iter 20221/30000  loss         0.097199  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.493\n",
      "iter 20240/30000  loss         0.097192  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.495\n",
      "iter 20241/30000  loss         0.097192  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.495\n",
      "iter 20260/30000  loss         0.097185  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.497\n",
      "iter 20261/30000  loss         0.097184  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.497\n",
      "iter 20280/30000  loss         0.097178  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.500\n",
      "iter 20281/30000  loss         0.097177  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.500\n",
      "iter 20300/30000  loss         0.097170  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.502\n",
      "iter 20301/30000  loss         0.097170  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.502\n",
      "iter 20320/30000  loss         0.097163  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.504\n",
      "iter 20321/30000  loss         0.097163  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.505\n",
      "iter 20340/30000  loss         0.097156  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.507\n",
      "iter 20341/30000  loss         0.097156  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.507\n",
      "iter 20360/30000  loss         0.097149  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.509\n",
      "iter 20361/30000  loss         0.097149  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.509\n",
      "iter 20380/30000  loss         0.097142  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.512\n",
      "iter 20381/30000  loss         0.097142  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.512\n",
      "iter 20400/30000  loss         0.097135  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.514\n",
      "iter 20401/30000  loss         0.097134  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.514\n",
      "iter 20420/30000  loss         0.097128  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.516\n",
      "iter 20421/30000  loss         0.097127  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.516\n",
      "iter 20440/30000  loss         0.097121  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.519\n",
      "iter 20441/30000  loss         0.097120  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.519\n",
      "iter 20460/30000  loss         0.097114  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.521\n",
      "iter 20461/30000  loss         0.097113  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.521\n",
      "iter 20480/30000  loss         0.097107  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.523\n",
      "iter 20481/30000  loss         0.097106  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.524\n",
      "iter 20500/30000  loss         0.097100  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.526\n",
      "iter 20501/30000  loss         0.097099  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.526\n",
      "iter 20520/30000  loss         0.097093  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.528\n",
      "iter 20521/30000  loss         0.097092  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.528\n",
      "iter 20540/30000  loss         0.097086  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.530\n",
      "iter 20541/30000  loss         0.097085  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.531\n",
      "iter 20560/30000  loss         0.097079  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.533\n",
      "iter 20561/30000  loss         0.097078  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.533\n",
      "iter 20580/30000  loss         0.097072  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.535\n",
      "iter 20581/30000  loss         0.097071  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.535\n",
      "iter 20600/30000  loss         0.097065  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.538\n",
      "iter 20601/30000  loss         0.097065  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20620/30000  loss         0.097058  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.540\n",
      "iter 20621/30000  loss         0.097058  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.540\n",
      "iter 20640/30000  loss         0.097051  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.542\n",
      "iter 20641/30000  loss         0.097051  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.542\n",
      "iter 20660/30000  loss         0.097044  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.545\n",
      "iter 20661/30000  loss         0.097044  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.545\n",
      "iter 20680/30000  loss         0.097037  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.547\n",
      "iter 20681/30000  loss         0.097037  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.547\n",
      "iter 20700/30000  loss         0.097030  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.549\n",
      "iter 20701/30000  loss         0.097030  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.549\n",
      "iter 20720/30000  loss         0.097024  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.552\n",
      "iter 20721/30000  loss         0.097023  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.552\n",
      "iter 20740/30000  loss         0.097017  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.554\n",
      "iter 20741/30000  loss         0.097016  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.554\n",
      "iter 20760/30000  loss         0.097010  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.556\n",
      "iter 20761/30000  loss         0.097010  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.556\n",
      "iter 20780/30000  loss         0.097003  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.559\n",
      "iter 20781/30000  loss         0.097003  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.559\n",
      "iter 20800/30000  loss         0.096996  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.561\n",
      "iter 20801/30000  loss         0.096996  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.561\n",
      "iter 20820/30000  loss         0.096990  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.563\n",
      "iter 20821/30000  loss         0.096989  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.563\n",
      "iter 20840/30000  loss         0.096983  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.565\n",
      "iter 20841/30000  loss         0.096982  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.566\n",
      "iter 20860/30000  loss         0.096976  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.568\n",
      "iter 20861/30000  loss         0.096976  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.568\n",
      "iter 20880/30000  loss         0.096969  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.570\n",
      "iter 20881/30000  loss         0.096969  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.570\n",
      "iter 20900/30000  loss         0.096963  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.572\n",
      "iter 20901/30000  loss         0.096962  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.573\n",
      "iter 20920/30000  loss         0.096956  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.575\n",
      "iter 20921/30000  loss         0.096956  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.575\n",
      "iter 20940/30000  loss         0.096949  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.577\n",
      "iter 20941/30000  loss         0.096949  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.577\n",
      "iter 20960/30000  loss         0.096943  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.579\n",
      "iter 20961/30000  loss         0.096942  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.579\n",
      "iter 20980/30000  loss         0.096936  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.582\n",
      "iter 20981/30000  loss         0.096936  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.582\n",
      "iter 21000/30000  loss         0.096929  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.584\n",
      "iter 21001/30000  loss         0.096929  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.584\n",
      "iter 21020/30000  loss         0.096923  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.586\n",
      "iter 21021/30000  loss         0.096922  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.586\n",
      "iter 21040/30000  loss         0.096916  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.588\n",
      "iter 21041/30000  loss         0.096916  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.589\n",
      "iter 21060/30000  loss         0.096909  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.591\n",
      "iter 21061/30000  loss         0.096909  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.591\n",
      "iter 21080/30000  loss         0.096903  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.593\n",
      "iter 21081/30000  loss         0.096902  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.593\n",
      "iter 21100/30000  loss         0.096896  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.595\n",
      "iter 21101/30000  loss         0.096896  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.595\n",
      "iter 21120/30000  loss         0.096890  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.598\n",
      "iter 21121/30000  loss         0.096889  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.598\n",
      "iter 21140/30000  loss         0.096883  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.600\n",
      "iter 21141/30000  loss         0.096883  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.600\n",
      "iter 21160/30000  loss         0.096876  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.602\n",
      "iter 21161/30000  loss         0.096876  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.602\n",
      "iter 21180/30000  loss         0.096870  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.604\n",
      "iter 21181/30000  loss         0.096870  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.604\n",
      "iter 21200/30000  loss         0.096863  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.607\n",
      "iter 21201/30000  loss         0.096863  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.607\n",
      "iter 21220/30000  loss         0.096857  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.609\n",
      "iter 21221/30000  loss         0.096857  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.609\n",
      "iter 21240/30000  loss         0.096850  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.611\n",
      "iter 21241/30000  loss         0.096850  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.611\n",
      "iter 21260/30000  loss         0.096844  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.613\n",
      "iter 21261/30000  loss         0.096844  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.613\n",
      "iter 21280/30000  loss         0.096837  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.616\n",
      "iter 21281/30000  loss         0.096837  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.616\n",
      "iter 21300/30000  loss         0.096831  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.618\n",
      "iter 21301/30000  loss         0.096831  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.618\n",
      "iter 21320/30000  loss         0.096825  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.620\n",
      "iter 21321/30000  loss         0.096824  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.620\n",
      "iter 21340/30000  loss         0.096818  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.622\n",
      "iter 21341/30000  loss         0.096818  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.622\n",
      "iter 21360/30000  loss         0.096812  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.625\n",
      "iter 21361/30000  loss         0.096811  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.625\n",
      "iter 21380/30000  loss         0.096805  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.627\n",
      "iter 21381/30000  loss         0.096805  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.627\n",
      "iter 21400/30000  loss         0.096799  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.629\n",
      "iter 21401/30000  loss         0.096799  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21420/30000  loss         0.096792  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.631\n",
      "iter 21421/30000  loss         0.096792  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.631\n",
      "iter 21440/30000  loss         0.096786  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.634\n",
      "iter 21441/30000  loss         0.096786  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.634\n",
      "iter 21460/30000  loss         0.096780  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.636\n",
      "iter 21461/30000  loss         0.096779  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.636\n",
      "iter 21480/30000  loss         0.096773  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.638\n",
      "iter 21481/30000  loss         0.096773  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.638\n",
      "iter 21500/30000  loss         0.096767  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.640\n",
      "iter 21501/30000  loss         0.096767  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.640\n",
      "iter 21520/30000  loss         0.096761  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.642\n",
      "iter 21521/30000  loss         0.096760  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.642\n",
      "iter 21540/30000  loss         0.096754  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.645\n",
      "iter 21541/30000  loss         0.096754  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.645\n",
      "iter 21560/30000  loss         0.096748  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.647\n",
      "iter 21561/30000  loss         0.096748  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.647\n",
      "iter 21580/30000  loss         0.096742  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.649\n",
      "iter 21581/30000  loss         0.096742  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.649\n",
      "iter 21600/30000  loss         0.096736  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.651\n",
      "iter 21601/30000  loss         0.096735  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.651\n",
      "iter 21620/30000  loss         0.096729  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.653\n",
      "iter 21621/30000  loss         0.096729  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.654\n",
      "iter 21640/30000  loss         0.096723  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.656\n",
      "iter 21641/30000  loss         0.096723  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.656\n",
      "iter 21660/30000  loss         0.096717  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.658\n",
      "iter 21661/30000  loss         0.096717  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.658\n",
      "iter 21680/30000  loss         0.096711  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.660\n",
      "iter 21681/30000  loss         0.096710  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.660\n",
      "iter 21700/30000  loss         0.096705  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.662\n",
      "iter 21701/30000  loss         0.096704  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.662\n",
      "iter 21720/30000  loss         0.096698  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.664\n",
      "iter 21721/30000  loss         0.096698  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.665\n",
      "iter 21740/30000  loss         0.096692  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.667\n",
      "iter 21741/30000  loss         0.096692  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.667\n",
      "iter 21760/30000  loss         0.096686  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.669\n",
      "iter 21761/30000  loss         0.096686  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.669\n",
      "iter 21780/30000  loss         0.096680  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.671\n",
      "iter 21781/30000  loss         0.096680  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.671\n",
      "iter 21800/30000  loss         0.096674  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.673\n",
      "iter 21801/30000  loss         0.096673  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.673\n",
      "iter 21820/30000  loss         0.096668  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.675\n",
      "iter 21821/30000  loss         0.096667  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.675\n",
      "iter 21840/30000  loss         0.096661  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.677\n",
      "iter 21841/30000  loss         0.096661  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.678\n",
      "iter 21860/30000  loss         0.096655  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.680\n",
      "iter 21861/30000  loss         0.096655  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.680\n",
      "iter 21880/30000  loss         0.096649  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.682\n",
      "iter 21881/30000  loss         0.096649  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.682\n",
      "iter 21900/30000  loss         0.096643  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.684\n",
      "iter 21901/30000  loss         0.096643  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.684\n",
      "iter 21920/30000  loss         0.096637  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.686\n",
      "iter 21921/30000  loss         0.096637  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.686\n",
      "iter 21940/30000  loss         0.096631  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.688\n",
      "iter 21941/30000  loss         0.096631  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.688\n",
      "iter 21960/30000  loss         0.096625  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.690\n",
      "iter 21961/30000  loss         0.096625  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.691\n",
      "iter 21980/30000  loss         0.096619  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.693\n",
      "iter 21981/30000  loss         0.096619  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.693\n",
      "iter 22000/30000  loss         0.096613  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.695\n",
      "iter 22001/30000  loss         0.096613  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.695\n",
      "iter 22020/30000  loss         0.096607  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.697\n",
      "iter 22021/30000  loss         0.096607  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.697\n",
      "iter 22040/30000  loss         0.096601  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.699\n",
      "iter 22041/30000  loss         0.096601  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.699\n",
      "iter 22060/30000  loss         0.096595  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.701\n",
      "iter 22061/30000  loss         0.096595  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.701\n",
      "iter 22080/30000  loss         0.096589  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.703\n",
      "iter 22081/30000  loss         0.096589  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.703\n",
      "iter 22100/30000  loss         0.096583  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.705\n",
      "iter 22101/30000  loss         0.096583  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.706\n",
      "iter 22120/30000  loss         0.096577  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.708\n",
      "iter 22121/30000  loss         0.096577  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.708\n",
      "iter 22140/30000  loss         0.096571  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.710\n",
      "iter 22141/30000  loss         0.096571  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.710\n",
      "iter 22160/30000  loss         0.096565  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.712\n",
      "iter 22161/30000  loss         0.096565  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.712\n",
      "iter 22180/30000  loss         0.096559  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.714\n",
      "iter 22181/30000  loss         0.096559  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.714\n",
      "iter 22200/30000  loss         0.096553  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.716\n",
      "iter 22201/30000  loss         0.096553  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22220/30000  loss         0.096547  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.718\n",
      "iter 22221/30000  loss         0.096547  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.718\n",
      "iter 22240/30000  loss         0.096542  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.720\n",
      "iter 22241/30000  loss         0.096541  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.720\n",
      "iter 22260/30000  loss         0.096536  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.723\n",
      "iter 22261/30000  loss         0.096535  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.723\n",
      "iter 22280/30000  loss         0.096530  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.725\n",
      "iter 22281/30000  loss         0.096530  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.725\n",
      "iter 22300/30000  loss         0.096524  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.727\n",
      "iter 22301/30000  loss         0.096524  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.727\n",
      "iter 22320/30000  loss         0.096518  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.729\n",
      "iter 22321/30000  loss         0.096518  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.729\n",
      "iter 22340/30000  loss         0.096512  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.731\n",
      "iter 22341/30000  loss         0.096512  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.731\n",
      "iter 22360/30000  loss         0.096506  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.733\n",
      "iter 22361/30000  loss         0.096506  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.733\n",
      "iter 22380/30000  loss         0.096501  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.735\n",
      "iter 22381/30000  loss         0.096500  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.735\n",
      "iter 22400/30000  loss         0.096495  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.737\n",
      "iter 22401/30000  loss         0.096495  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.737\n",
      "iter 22420/30000  loss         0.096489  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.739\n",
      "iter 22421/30000  loss         0.096489  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.739\n",
      "iter 22440/30000  loss         0.096483  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.741\n",
      "iter 22441/30000  loss         0.096483  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.742\n",
      "iter 22460/30000  loss         0.096478  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.744\n",
      "iter 22461/30000  loss         0.096477  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.744\n",
      "iter 22480/30000  loss         0.096472  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.746\n",
      "iter 22481/30000  loss         0.096472  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.746\n",
      "iter 22500/30000  loss         0.096466  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.748\n",
      "iter 22501/30000  loss         0.096466  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.748\n",
      "iter 22520/30000  loss         0.096460  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.750\n",
      "iter 22521/30000  loss         0.096460  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.750\n",
      "iter 22540/30000  loss         0.096455  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.752\n",
      "iter 22541/30000  loss         0.096454  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.752\n",
      "iter 22560/30000  loss         0.096449  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.754\n",
      "iter 22561/30000  loss         0.096449  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.754\n",
      "iter 22580/30000  loss         0.096443  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.756\n",
      "iter 22581/30000  loss         0.096443  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.756\n",
      "iter 22600/30000  loss         0.096438  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.758\n",
      "iter 22601/30000  loss         0.096437  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.758\n",
      "iter 22620/30000  loss         0.096432  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.760\n",
      "iter 22621/30000  loss         0.096432  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.760\n",
      "iter 22640/30000  loss         0.096426  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.762\n",
      "iter 22641/30000  loss         0.096426  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.762\n",
      "iter 22660/30000  loss         0.096421  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.764\n",
      "iter 22661/30000  loss         0.096420  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.764\n",
      "iter 22680/30000  loss         0.096415  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.766\n",
      "iter 22681/30000  loss         0.096415  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.767\n",
      "iter 22700/30000  loss         0.096409  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.768\n",
      "iter 22701/30000  loss         0.096409  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.769\n",
      "iter 22720/30000  loss         0.096404  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.771\n",
      "iter 22721/30000  loss         0.096403  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.771\n",
      "iter 22740/30000  loss         0.096398  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.773\n",
      "iter 22741/30000  loss         0.096398  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.773\n",
      "iter 22760/30000  loss         0.096393  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.775\n",
      "iter 22761/30000  loss         0.096392  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.775\n",
      "iter 22780/30000  loss         0.096387  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.777\n",
      "iter 22781/30000  loss         0.096387  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.777\n",
      "iter 22800/30000  loss         0.096381  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.779\n",
      "iter 22801/30000  loss         0.096381  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.779\n",
      "iter 22820/30000  loss         0.096376  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.781\n",
      "iter 22821/30000  loss         0.096376  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.781\n",
      "iter 22840/30000  loss         0.096370  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.783\n",
      "iter 22841/30000  loss         0.096370  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.783\n",
      "iter 22860/30000  loss         0.096365  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.785\n",
      "iter 22861/30000  loss         0.096364  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.785\n",
      "iter 22880/30000  loss         0.096359  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.787\n",
      "iter 22881/30000  loss         0.096359  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.787\n",
      "iter 22900/30000  loss         0.096354  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.789\n",
      "iter 22901/30000  loss         0.096353  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.789\n",
      "iter 22920/30000  loss         0.096348  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.791\n",
      "iter 22921/30000  loss         0.096348  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.791\n",
      "iter 22940/30000  loss         0.096343  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.793\n",
      "iter 22941/30000  loss         0.096342  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.793\n",
      "iter 22960/30000  loss         0.096337  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.795\n",
      "iter 22961/30000  loss         0.096337  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.795\n",
      "iter 22980/30000  loss         0.096332  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.797\n",
      "iter 22981/30000  loss         0.096331  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.797\n",
      "iter 23000/30000  loss         0.096326  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.799\n",
      "iter 23001/30000  loss         0.096326  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23020/30000  loss         0.096321  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.801\n",
      "iter 23021/30000  loss         0.096320  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.801\n",
      "iter 23040/30000  loss         0.096315  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.803\n",
      "iter 23041/30000  loss         0.096315  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.803\n",
      "iter 23060/30000  loss         0.096310  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.805\n",
      "iter 23061/30000  loss         0.096310  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.805\n",
      "iter 23080/30000  loss         0.096304  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.807\n",
      "iter 23081/30000  loss         0.096304  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.807\n",
      "iter 23100/30000  loss         0.096299  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.809\n",
      "iter 23101/30000  loss         0.096299  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.809\n",
      "iter 23120/30000  loss         0.096294  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.811\n",
      "iter 23121/30000  loss         0.096293  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.811\n",
      "iter 23140/30000  loss         0.096288  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.813\n",
      "iter 23141/30000  loss         0.096288  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.813\n",
      "iter 23160/30000  loss         0.096283  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.815\n",
      "iter 23161/30000  loss         0.096283  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.815\n",
      "iter 23180/30000  loss         0.096277  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.817\n",
      "iter 23181/30000  loss         0.096277  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.817\n",
      "iter 23200/30000  loss         0.096272  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.819\n",
      "iter 23201/30000  loss         0.096272  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.819\n",
      "iter 23220/30000  loss         0.096267  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.821\n",
      "iter 23221/30000  loss         0.096266  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.821\n",
      "iter 23240/30000  loss         0.096261  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.823\n",
      "iter 23241/30000  loss         0.096261  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.823\n",
      "iter 23260/30000  loss         0.096256  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.825\n",
      "iter 23261/30000  loss         0.096256  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.825\n",
      "iter 23280/30000  loss         0.096251  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.827\n",
      "iter 23281/30000  loss         0.096250  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.827\n",
      "iter 23300/30000  loss         0.096245  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.829\n",
      "iter 23301/30000  loss         0.096245  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.829\n",
      "iter 23320/30000  loss         0.096240  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.831\n",
      "iter 23321/30000  loss         0.096240  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.831\n",
      "iter 23340/30000  loss         0.096235  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.833\n",
      "iter 23341/30000  loss         0.096235  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.833\n",
      "iter 23360/30000  loss         0.096230  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.835\n",
      "iter 23361/30000  loss         0.096229  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.835\n",
      "iter 23380/30000  loss         0.096224  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.837\n",
      "iter 23381/30000  loss         0.096224  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.837\n",
      "iter 23400/30000  loss         0.096219  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.839\n",
      "iter 23401/30000  loss         0.096219  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.839\n",
      "iter 23420/30000  loss         0.096214  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.841\n",
      "iter 23421/30000  loss         0.096213  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.841\n",
      "iter 23440/30000  loss         0.096208  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.843\n",
      "iter 23441/30000  loss         0.096208  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.843\n",
      "iter 23460/30000  loss         0.096203  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.845\n",
      "iter 23461/30000  loss         0.096203  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.845\n",
      "iter 23480/30000  loss         0.096198  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.847\n",
      "iter 23481/30000  loss         0.096198  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.847\n",
      "iter 23500/30000  loss         0.096193  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.849\n",
      "iter 23501/30000  loss         0.096193  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.849\n",
      "iter 23520/30000  loss         0.096188  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.851\n",
      "iter 23521/30000  loss         0.096187  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.851\n",
      "iter 23540/30000  loss         0.096182  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.853\n",
      "iter 23541/30000  loss         0.096182  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.853\n",
      "iter 23560/30000  loss         0.096177  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.855\n",
      "iter 23561/30000  loss         0.096177  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.855\n",
      "iter 23580/30000  loss         0.096172  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.857\n",
      "iter 23581/30000  loss         0.096172  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.857\n",
      "iter 23600/30000  loss         0.096167  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.859\n",
      "iter 23601/30000  loss         0.096167  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.859\n",
      "iter 23620/30000  loss         0.096162  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.861\n",
      "iter 23621/30000  loss         0.096161  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.861\n",
      "iter 23640/30000  loss         0.096157  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.863\n",
      "iter 23641/30000  loss         0.096156  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.863\n",
      "iter 23660/30000  loss         0.096151  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.865\n",
      "iter 23661/30000  loss         0.096151  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.865\n",
      "iter 23680/30000  loss         0.096146  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.867\n",
      "iter 23681/30000  loss         0.096146  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.867\n",
      "iter 23700/30000  loss         0.096141  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.869\n",
      "iter 23701/30000  loss         0.096141  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.869\n",
      "iter 23720/30000  loss         0.096136  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.870\n",
      "iter 23721/30000  loss         0.096136  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.871\n",
      "iter 23740/30000  loss         0.096131  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.872\n",
      "iter 23741/30000  loss         0.096131  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.872\n",
      "iter 23760/30000  loss         0.096126  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.874\n",
      "iter 23761/30000  loss         0.096126  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.874\n",
      "iter 23780/30000  loss         0.096121  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.876\n",
      "iter 23781/30000  loss         0.096121  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.876\n",
      "iter 23800/30000  loss         0.096116  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.878\n",
      "iter 23801/30000  loss         0.096115  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23820/30000  loss         0.096111  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.880\n",
      "iter 23821/30000  loss         0.096110  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.880\n",
      "iter 23840/30000  loss         0.096106  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.882\n",
      "iter 23841/30000  loss         0.096105  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.882\n",
      "iter 23860/30000  loss         0.096101  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.884\n",
      "iter 23861/30000  loss         0.096100  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.884\n",
      "iter 23880/30000  loss         0.096096  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.886\n",
      "iter 23881/30000  loss         0.096095  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.886\n",
      "iter 23900/30000  loss         0.096091  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.888\n",
      "iter 23901/30000  loss         0.096090  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.888\n",
      "iter 23920/30000  loss         0.096086  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.890\n",
      "iter 23921/30000  loss         0.096085  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.890\n",
      "iter 23940/30000  loss         0.096080  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.892\n",
      "iter 23941/30000  loss         0.096080  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.892\n",
      "iter 23960/30000  loss         0.096076  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.894\n",
      "iter 23961/30000  loss         0.096075  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.894\n",
      "iter 23980/30000  loss         0.096071  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.895\n",
      "iter 23981/30000  loss         0.096070  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.896\n",
      "iter 24000/30000  loss         0.096066  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.897\n",
      "iter 24001/30000  loss         0.096065  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.897\n",
      "iter 24020/30000  loss         0.096061  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.899\n",
      "iter 24021/30000  loss         0.096060  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.899\n",
      "iter 24040/30000  loss         0.096056  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.901\n",
      "iter 24041/30000  loss         0.096055  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.901\n",
      "iter 24060/30000  loss         0.096051  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.903\n",
      "iter 24061/30000  loss         0.096050  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.903\n",
      "iter 24080/30000  loss         0.096046  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.905\n",
      "iter 24081/30000  loss         0.096045  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.905\n",
      "iter 24100/30000  loss         0.096041  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.907\n",
      "iter 24101/30000  loss         0.096041  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.907\n",
      "iter 24120/30000  loss         0.096036  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.909\n",
      "iter 24121/30000  loss         0.096036  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.909\n",
      "iter 24140/30000  loss         0.096031  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.911\n",
      "iter 24141/30000  loss         0.096031  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.911\n",
      "iter 24160/30000  loss         0.096026  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.913\n",
      "iter 24161/30000  loss         0.096026  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.913\n",
      "iter 24180/30000  loss         0.096021  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.914\n",
      "iter 24181/30000  loss         0.096021  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.915\n",
      "iter 24200/30000  loss         0.096016  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.916\n",
      "iter 24201/30000  loss         0.096016  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.916\n",
      "iter 24220/30000  loss         0.096011  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.918\n",
      "iter 24221/30000  loss         0.096011  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.918\n",
      "iter 24240/30000  loss         0.096007  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.920\n",
      "iter 24241/30000  loss         0.096006  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.920\n",
      "iter 24260/30000  loss         0.096002  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.922\n",
      "iter 24261/30000  loss         0.096001  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.922\n",
      "iter 24280/30000  loss         0.095997  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.924\n",
      "iter 24281/30000  loss         0.095997  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.924\n",
      "iter 24300/30000  loss         0.095992  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.926\n",
      "iter 24301/30000  loss         0.095992  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.926\n",
      "iter 24320/30000  loss         0.095987  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.928\n",
      "iter 24321/30000  loss         0.095987  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.928\n",
      "iter 24340/30000  loss         0.095982  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.930\n",
      "iter 24341/30000  loss         0.095982  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.930\n",
      "iter 24360/30000  loss         0.095977  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.931\n",
      "iter 24361/30000  loss         0.095977  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.931\n",
      "iter 24380/30000  loss         0.095973  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.933\n",
      "iter 24381/30000  loss         0.095972  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.933\n",
      "iter 24400/30000  loss         0.095968  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.935\n",
      "iter 24401/30000  loss         0.095968  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.935\n",
      "iter 24420/30000  loss         0.095963  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.937\n",
      "iter 24421/30000  loss         0.095963  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.937\n",
      "iter 24440/30000  loss         0.095958  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.939\n",
      "iter 24441/30000  loss         0.095958  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.939\n",
      "iter 24460/30000  loss         0.095954  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.941\n",
      "iter 24461/30000  loss         0.095953  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.941\n",
      "iter 24480/30000  loss         0.095949  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.943\n",
      "iter 24481/30000  loss         0.095949  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.943\n",
      "iter 24500/30000  loss         0.095944  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.944\n",
      "iter 24501/30000  loss         0.095944  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.945\n",
      "iter 24520/30000  loss         0.095939  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.946\n",
      "iter 24521/30000  loss         0.095939  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.946\n",
      "iter 24540/30000  loss         0.095935  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.948\n",
      "iter 24541/30000  loss         0.095934  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.948\n",
      "iter 24560/30000  loss         0.095930  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.950\n",
      "iter 24561/30000  loss         0.095930  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.950\n",
      "iter 24580/30000  loss         0.095925  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.952\n",
      "iter 24581/30000  loss         0.095925  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.952\n",
      "iter 24600/30000  loss         0.095920  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.954\n",
      "iter 24601/30000  loss         0.095920  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24620/30000  loss         0.095916  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.955\n",
      "iter 24621/30000  loss         0.095915  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.956\n",
      "iter 24640/30000  loss         0.095911  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.957\n",
      "iter 24641/30000  loss         0.095911  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.957\n",
      "iter 24660/30000  loss         0.095906  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.959\n",
      "iter 24661/30000  loss         0.095906  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.959\n",
      "iter 24680/30000  loss         0.095902  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.961\n",
      "iter 24681/30000  loss         0.095901  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.961\n",
      "iter 24700/30000  loss         0.095897  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.963\n",
      "iter 24701/30000  loss         0.095897  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.963\n",
      "iter 24720/30000  loss         0.095892  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.965\n",
      "iter 24721/30000  loss         0.095892  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.965\n",
      "iter 24740/30000  loss         0.095888  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.966\n",
      "iter 24741/30000  loss         0.095887  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.967\n",
      "iter 24760/30000  loss         0.095883  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.968\n",
      "iter 24761/30000  loss         0.095883  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.968\n",
      "iter 24780/30000  loss         0.095878  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.970\n",
      "iter 24781/30000  loss         0.095878  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.970\n",
      "iter 24800/30000  loss         0.095874  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.972\n",
      "iter 24801/30000  loss         0.095873  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.972\n",
      "iter 24820/30000  loss         0.095869  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.974\n",
      "iter 24821/30000  loss         0.095869  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.974\n",
      "iter 24840/30000  loss         0.095864  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.976\n",
      "iter 24841/30000  loss         0.095864  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.976\n",
      "iter 24860/30000  loss         0.095860  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.977\n",
      "iter 24861/30000  loss         0.095860  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.978\n",
      "iter 24880/30000  loss         0.095855  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.979\n",
      "iter 24881/30000  loss         0.095855  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.979\n",
      "iter 24900/30000  loss         0.095851  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.981\n",
      "iter 24901/30000  loss         0.095850  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.981\n",
      "iter 24920/30000  loss         0.095846  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.983\n",
      "iter 24921/30000  loss         0.095846  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.983\n",
      "iter 24940/30000  loss         0.095842  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.985\n",
      "iter 24941/30000  loss         0.095841  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.985\n",
      "iter 24960/30000  loss         0.095837  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.986\n",
      "iter 24961/30000  loss         0.095837  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.987\n",
      "iter 24980/30000  loss         0.095832  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.988\n",
      "iter 24981/30000  loss         0.095832  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.988\n",
      "iter 25000/30000  loss         0.095828  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.990\n",
      "iter 25001/30000  loss         0.095828  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.990\n",
      "iter 25020/30000  loss         0.095823  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.992\n",
      "iter 25021/30000  loss         0.095823  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.992\n",
      "iter 25040/30000  loss         0.095819  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.994\n",
      "iter 25041/30000  loss         0.095819  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.994\n",
      "iter 25060/30000  loss         0.095814  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.995\n",
      "iter 25061/30000  loss         0.095814  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.996\n",
      "iter 25080/30000  loss         0.095810  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.997\n",
      "iter 25081/30000  loss         0.095810  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.997\n",
      "iter 25100/30000  loss         0.095805  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.999\n",
      "iter 25101/30000  loss         0.095805  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.999\n",
      "iter 25120/30000  loss         0.095801  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.001\n",
      "iter 25121/30000  loss         0.095801  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.001\n",
      "iter 25140/30000  loss         0.095796  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.003\n",
      "iter 25141/30000  loss         0.095796  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.003\n",
      "iter 25160/30000  loss         0.095792  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.004\n",
      "iter 25161/30000  loss         0.095792  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.005\n",
      "iter 25180/30000  loss         0.095787  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.006\n",
      "iter 25181/30000  loss         0.095787  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.006\n",
      "iter 25200/30000  loss         0.095783  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.008\n",
      "iter 25201/30000  loss         0.095783  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.008\n",
      "iter 25220/30000  loss         0.095778  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.010\n",
      "iter 25221/30000  loss         0.095778  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.010\n",
      "iter 25240/30000  loss         0.095774  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.012\n",
      "iter 25241/30000  loss         0.095774  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.012\n",
      "iter 25260/30000  loss         0.095770  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.013\n",
      "iter 25261/30000  loss         0.095769  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    8.013\n",
      "iter 25280/30000  loss         0.095765  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.015\n",
      "iter 25281/30000  loss         0.095765  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.015\n",
      "iter 25300/30000  loss         0.095761  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.017\n",
      "iter 25301/30000  loss         0.095760  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.017\n",
      "iter 25320/30000  loss         0.095756  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.019\n",
      "iter 25321/30000  loss         0.095756  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.019\n",
      "iter 25340/30000  loss         0.095752  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.020\n",
      "iter 25341/30000  loss         0.095752  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.020\n",
      "iter 25360/30000  loss         0.095747  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.022\n",
      "iter 25361/30000  loss         0.095747  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.022\n",
      "iter 25380/30000  loss         0.095743  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.024\n",
      "iter 25381/30000  loss         0.095743  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.024\n",
      "iter 25400/30000  loss         0.095739  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.026\n",
      "iter 25401/30000  loss         0.095739  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25420/30000  loss         0.095734  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.027\n",
      "iter 25421/30000  loss         0.095734  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.028\n",
      "iter 25440/30000  loss         0.095730  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.029\n",
      "iter 25441/30000  loss         0.095730  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.029\n",
      "iter 25460/30000  loss         0.095726  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.031\n",
      "iter 25461/30000  loss         0.095725  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.031\n",
      "iter 25480/30000  loss         0.095721  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.033\n",
      "iter 25481/30000  loss         0.095721  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.033\n",
      "iter 25500/30000  loss         0.095717  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.034\n",
      "iter 25501/30000  loss         0.095717  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.035\n",
      "iter 25520/30000  loss         0.095713  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.036\n",
      "iter 25521/30000  loss         0.095712  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.036\n",
      "iter 25540/30000  loss         0.095708  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.038\n",
      "iter 25541/30000  loss         0.095708  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.038\n",
      "iter 25560/30000  loss         0.095704  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.040\n",
      "iter 25561/30000  loss         0.095704  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.040\n",
      "iter 25580/30000  loss         0.095700  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.041\n",
      "iter 25581/30000  loss         0.095699  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.042\n",
      "iter 25600/30000  loss         0.095695  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.043\n",
      "iter 25601/30000  loss         0.095695  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.043\n",
      "iter 25620/30000  loss         0.095691  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.045\n",
      "iter 25621/30000  loss         0.095691  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.045\n",
      "iter 25640/30000  loss         0.095687  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.047\n",
      "iter 25641/30000  loss         0.095686  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.047\n",
      "iter 25660/30000  loss         0.095682  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.048\n",
      "iter 25661/30000  loss         0.095682  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.048\n",
      "iter 25680/30000  loss         0.095678  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.050\n",
      "iter 25681/30000  loss         0.095678  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.050\n",
      "iter 25700/30000  loss         0.095674  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.052\n",
      "iter 25701/30000  loss         0.095674  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.052\n",
      "iter 25720/30000  loss         0.095670  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.054\n",
      "iter 25721/30000  loss         0.095669  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.054\n",
      "iter 25740/30000  loss         0.095665  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.055\n",
      "iter 25741/30000  loss         0.095665  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.055\n",
      "iter 25760/30000  loss         0.095661  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.057\n",
      "iter 25761/30000  loss         0.095661  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.057\n",
      "iter 25780/30000  loss         0.095657  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.059\n",
      "iter 25781/30000  loss         0.095657  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.059\n",
      "iter 25800/30000  loss         0.095653  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.060\n",
      "iter 25801/30000  loss         0.095652  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.061\n",
      "iter 25820/30000  loss         0.095648  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.062\n",
      "iter 25821/30000  loss         0.095648  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.062\n",
      "iter 25840/30000  loss         0.095644  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.064\n",
      "iter 25841/30000  loss         0.095644  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.064\n",
      "iter 25860/30000  loss         0.095640  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.066\n",
      "iter 25861/30000  loss         0.095640  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.066\n",
      "iter 25880/30000  loss         0.095636  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.067\n",
      "iter 25881/30000  loss         0.095636  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.067\n",
      "iter 25900/30000  loss         0.095632  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.069\n",
      "iter 25901/30000  loss         0.095631  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.069\n",
      "iter 25920/30000  loss         0.095627  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.071\n",
      "iter 25921/30000  loss         0.095627  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.071\n",
      "iter 25940/30000  loss         0.095623  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.072\n",
      "iter 25941/30000  loss         0.095623  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.073\n",
      "iter 25960/30000  loss         0.095619  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.074\n",
      "iter 25961/30000  loss         0.095619  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.074\n",
      "iter 25980/30000  loss         0.095615  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.076\n",
      "iter 25981/30000  loss         0.095615  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.076\n",
      "iter 26000/30000  loss         0.095611  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.078\n",
      "iter 26001/30000  loss         0.095611  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.078\n",
      "iter 26020/30000  loss         0.095607  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.079\n",
      "iter 26021/30000  loss         0.095606  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.079\n",
      "iter 26040/30000  loss         0.095602  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.081\n",
      "iter 26041/30000  loss         0.095602  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.081\n",
      "iter 26060/30000  loss         0.095598  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.083\n",
      "iter 26061/30000  loss         0.095598  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.083\n",
      "iter 26080/30000  loss         0.095594  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.084\n",
      "iter 26081/30000  loss         0.095594  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.084\n",
      "iter 26100/30000  loss         0.095590  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.086\n",
      "iter 26101/30000  loss         0.095590  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.086\n",
      "iter 26120/30000  loss         0.095586  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.088\n",
      "iter 26121/30000  loss         0.095586  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.088\n",
      "iter 26140/30000  loss         0.095582  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.089\n",
      "iter 26141/30000  loss         0.095582  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.089\n",
      "iter 26160/30000  loss         0.095578  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.091\n",
      "iter 26161/30000  loss         0.095578  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.091\n",
      "iter 26180/30000  loss         0.095574  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.093\n",
      "iter 26181/30000  loss         0.095573  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.093\n",
      "iter 26200/30000  loss         0.095570  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.094\n",
      "iter 26201/30000  loss         0.095569  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26220/30000  loss         0.095565  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.096\n",
      "iter 26221/30000  loss         0.095565  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.096\n",
      "iter 26240/30000  loss         0.095561  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.098\n",
      "iter 26241/30000  loss         0.095561  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.098\n",
      "iter 26260/30000  loss         0.095557  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.099\n",
      "iter 26261/30000  loss         0.095557  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.100\n",
      "iter 26280/30000  loss         0.095553  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.101\n",
      "iter 26281/30000  loss         0.095553  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.101\n",
      "iter 26300/30000  loss         0.095549  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.103\n",
      "iter 26301/30000  loss         0.095549  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.103\n",
      "iter 26320/30000  loss         0.095545  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.104\n",
      "iter 26321/30000  loss         0.095545  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.105\n",
      "iter 26340/30000  loss         0.095541  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.106\n",
      "iter 26341/30000  loss         0.095541  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.106\n",
      "iter 26360/30000  loss         0.095537  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.108\n",
      "iter 26361/30000  loss         0.095537  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.108\n",
      "iter 26380/30000  loss         0.095533  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.109\n",
      "iter 26381/30000  loss         0.095533  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.110\n",
      "iter 26400/30000  loss         0.095529  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.111\n",
      "iter 26401/30000  loss         0.095529  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.111\n",
      "iter 26420/30000  loss         0.095525  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.113\n",
      "iter 26421/30000  loss         0.095525  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.113\n",
      "iter 26440/30000  loss         0.095521  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.114\n",
      "iter 26441/30000  loss         0.095521  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.115\n",
      "iter 26460/30000  loss         0.095517  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.116\n",
      "iter 26461/30000  loss         0.095517  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.116\n",
      "iter 26480/30000  loss         0.095513  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.118\n",
      "iter 26481/30000  loss         0.095513  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.118\n",
      "iter 26500/30000  loss         0.095509  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.119\n",
      "iter 26501/30000  loss         0.095509  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.120\n",
      "iter 26520/30000  loss         0.095505  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.121\n",
      "iter 26521/30000  loss         0.095505  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.121\n",
      "iter 26540/30000  loss         0.095501  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.123\n",
      "iter 26541/30000  loss         0.095501  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.123\n",
      "iter 26560/30000  loss         0.095497  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.124\n",
      "iter 26561/30000  loss         0.095497  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.124\n",
      "iter 26580/30000  loss         0.095493  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.126\n",
      "iter 26581/30000  loss         0.095493  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.126\n",
      "iter 26600/30000  loss         0.095489  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.128\n",
      "iter 26601/30000  loss         0.095489  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.128\n",
      "iter 26620/30000  loss         0.095485  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.129\n",
      "iter 26621/30000  loss         0.095485  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.129\n",
      "iter 26640/30000  loss         0.095481  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.131\n",
      "iter 26641/30000  loss         0.095481  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.131\n",
      "iter 26660/30000  loss         0.095478  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.133\n",
      "iter 26661/30000  loss         0.095477  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.133\n",
      "iter 26680/30000  loss         0.095474  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.134\n",
      "iter 26681/30000  loss         0.095473  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.134\n",
      "iter 26700/30000  loss         0.095470  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.136\n",
      "iter 26701/30000  loss         0.095469  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.136\n",
      "iter 26720/30000  loss         0.095466  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.138\n",
      "iter 26721/30000  loss         0.095466  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.138\n",
      "iter 26740/30000  loss         0.095462  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.139\n",
      "iter 26741/30000  loss         0.095462  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.139\n",
      "iter 26760/30000  loss         0.095458  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.141\n",
      "iter 26761/30000  loss         0.095458  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.141\n",
      "iter 26780/30000  loss         0.095454  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.142\n",
      "iter 26781/30000  loss         0.095454  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.142\n",
      "iter 26800/30000  loss         0.095450  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.144\n",
      "iter 26801/30000  loss         0.095450  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.144\n",
      "iter 26820/30000  loss         0.095446  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.146\n",
      "iter 26821/30000  loss         0.095446  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.146\n",
      "iter 26840/30000  loss         0.095442  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.147\n",
      "iter 26841/30000  loss         0.095442  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.147\n",
      "iter 26860/30000  loss         0.095439  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.149\n",
      "iter 26861/30000  loss         0.095438  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.149\n",
      "iter 26880/30000  loss         0.095435  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.151\n",
      "iter 26881/30000  loss         0.095435  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.151\n",
      "iter 26900/30000  loss         0.095431  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.152\n",
      "iter 26901/30000  loss         0.095431  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.152\n",
      "iter 26920/30000  loss         0.095427  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.154\n",
      "iter 26921/30000  loss         0.095427  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.154\n",
      "iter 26940/30000  loss         0.095423  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.155\n",
      "iter 26941/30000  loss         0.095423  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.155\n",
      "iter 26960/30000  loss         0.095419  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.157\n",
      "iter 26961/30000  loss         0.095419  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.157\n",
      "iter 26980/30000  loss         0.095416  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.159\n",
      "iter 26981/30000  loss         0.095415  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.159\n",
      "iter 27000/30000  loss         0.095412  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.160\n",
      "iter 27001/30000  loss         0.095412  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27020/30000  loss         0.095408  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.162\n",
      "iter 27021/30000  loss         0.095408  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.162\n",
      "iter 27040/30000  loss         0.095404  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.163\n",
      "iter 27041/30000  loss         0.095404  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.163\n",
      "iter 27060/30000  loss         0.095400  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.165\n",
      "iter 27061/30000  loss         0.095400  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.165\n",
      "iter 27080/30000  loss         0.095397  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.167\n",
      "iter 27081/30000  loss         0.095396  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.167\n",
      "iter 27100/30000  loss         0.095393  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.168\n",
      "iter 27101/30000  loss         0.095393  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.168\n",
      "iter 27120/30000  loss         0.095389  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.170\n",
      "iter 27121/30000  loss         0.095389  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.170\n",
      "iter 27140/30000  loss         0.095385  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.171\n",
      "iter 27141/30000  loss         0.095385  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.171\n",
      "iter 27160/30000  loss         0.095382  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.173\n",
      "iter 27161/30000  loss         0.095381  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.173\n",
      "iter 27180/30000  loss         0.095378  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.175\n",
      "iter 27181/30000  loss         0.095378  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.175\n",
      "iter 27200/30000  loss         0.095374  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.176\n",
      "iter 27201/30000  loss         0.095374  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.176\n",
      "iter 27220/30000  loss         0.095370  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.178\n",
      "iter 27221/30000  loss         0.095370  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.178\n",
      "iter 27240/30000  loss         0.095367  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.179\n",
      "iter 27241/30000  loss         0.095366  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.179\n",
      "iter 27260/30000  loss         0.095363  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.181\n",
      "iter 27261/30000  loss         0.095363  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.181\n",
      "iter 27280/30000  loss         0.095359  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.182\n",
      "iter 27281/30000  loss         0.095359  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.183\n",
      "iter 27300/30000  loss         0.095355  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.184\n",
      "iter 27301/30000  loss         0.095355  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.184\n",
      "iter 27320/30000  loss         0.095352  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.186\n",
      "iter 27321/30000  loss         0.095351  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.186\n",
      "iter 27340/30000  loss         0.095348  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.187\n",
      "iter 27341/30000  loss         0.095348  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.187\n",
      "iter 27360/30000  loss         0.095344  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.189\n",
      "iter 27361/30000  loss         0.095344  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.189\n",
      "iter 27380/30000  loss         0.095341  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.190\n",
      "iter 27381/30000  loss         0.095340  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.190\n",
      "iter 27400/30000  loss         0.095337  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.192\n",
      "iter 27401/30000  loss         0.095337  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.192\n",
      "iter 27420/30000  loss         0.095333  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.194\n",
      "iter 27421/30000  loss         0.095333  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.194\n",
      "iter 27440/30000  loss         0.095330  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.195\n",
      "iter 27441/30000  loss         0.095329  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.195\n",
      "iter 27460/30000  loss         0.095326  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.197\n",
      "iter 27461/30000  loss         0.095326  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.197\n",
      "iter 27480/30000  loss         0.095322  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.198\n",
      "iter 27481/30000  loss         0.095322  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.198\n",
      "iter 27500/30000  loss         0.095319  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.200\n",
      "iter 27501/30000  loss         0.095318  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.200\n",
      "iter 27520/30000  loss         0.095315  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.201\n",
      "iter 27521/30000  loss         0.095315  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.201\n",
      "iter 27540/30000  loss         0.095311  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.203\n",
      "iter 27541/30000  loss         0.095311  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.203\n",
      "iter 27560/30000  loss         0.095308  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.204\n",
      "iter 27561/30000  loss         0.095307  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.205\n",
      "iter 27580/30000  loss         0.095304  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.206\n",
      "iter 27581/30000  loss         0.095304  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.206\n",
      "iter 27600/30000  loss         0.095300  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.208\n",
      "iter 27601/30000  loss         0.095300  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.208\n",
      "iter 27620/30000  loss         0.095297  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.209\n",
      "iter 27621/30000  loss         0.095297  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.209\n",
      "iter 27640/30000  loss         0.095293  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.211\n",
      "iter 27641/30000  loss         0.095293  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.211\n",
      "iter 27660/30000  loss         0.095290  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.212\n",
      "iter 27661/30000  loss         0.095289  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.212\n",
      "iter 27680/30000  loss         0.095286  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.214\n",
      "iter 27681/30000  loss         0.095286  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.214\n",
      "iter 27700/30000  loss         0.095282  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.215\n",
      "iter 27701/30000  loss         0.095282  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.215\n",
      "iter 27720/30000  loss         0.095279  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.217\n",
      "iter 27721/30000  loss         0.095279  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.217\n",
      "iter 27740/30000  loss         0.095275  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.218\n",
      "iter 27741/30000  loss         0.095275  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.218\n",
      "iter 27760/30000  loss         0.095272  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.220\n",
      "iter 27761/30000  loss         0.095271  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.220\n",
      "iter 27780/30000  loss         0.095268  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.221\n",
      "iter 27781/30000  loss         0.095268  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.222\n",
      "iter 27800/30000  loss         0.095264  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.223\n",
      "iter 27801/30000  loss         0.095264  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27820/30000  loss         0.095261  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.225\n",
      "iter 27821/30000  loss         0.095261  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.225\n",
      "iter 27840/30000  loss         0.095257  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.226\n",
      "iter 27841/30000  loss         0.095257  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.226\n",
      "iter 27860/30000  loss         0.095254  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.228\n",
      "iter 27861/30000  loss         0.095254  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.228\n",
      "iter 27880/30000  loss         0.095250  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.229\n",
      "iter 27881/30000  loss         0.095250  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.229\n",
      "iter 27900/30000  loss         0.095247  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.231\n",
      "iter 27901/30000  loss         0.095247  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.231\n",
      "iter 27920/30000  loss         0.095243  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.232\n",
      "iter 27921/30000  loss         0.095243  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.232\n",
      "iter 27940/30000  loss         0.095240  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.234\n",
      "iter 27941/30000  loss         0.095239  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.234\n",
      "iter 27960/30000  loss         0.095236  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.235\n",
      "iter 27961/30000  loss         0.095236  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.235\n",
      "iter 27980/30000  loss         0.095233  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.237\n",
      "iter 27981/30000  loss         0.095232  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.237\n",
      "iter 28000/30000  loss         0.095229  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.238\n",
      "iter 28001/30000  loss         0.095229  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.238\n",
      "iter 28020/30000  loss         0.095226  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.240\n",
      "iter 28021/30000  loss         0.095225  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.240\n",
      "iter 28040/30000  loss         0.095222  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.241\n",
      "iter 28041/30000  loss         0.095222  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.241\n",
      "iter 28060/30000  loss         0.095219  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.243\n",
      "iter 28061/30000  loss         0.095218  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.243\n",
      "iter 28080/30000  loss         0.095215  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.244\n",
      "iter 28081/30000  loss         0.095215  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.244\n",
      "iter 28100/30000  loss         0.095212  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.246\n",
      "iter 28101/30000  loss         0.095212  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.246\n",
      "iter 28120/30000  loss         0.095208  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.247\n",
      "iter 28121/30000  loss         0.095208  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.247\n",
      "iter 28140/30000  loss         0.095205  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.249\n",
      "iter 28141/30000  loss         0.095205  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.249\n",
      "iter 28160/30000  loss         0.095201  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.250\n",
      "iter 28161/30000  loss         0.095201  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.250\n",
      "iter 28180/30000  loss         0.095198  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.252\n",
      "iter 28181/30000  loss         0.095198  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.252\n",
      "iter 28200/30000  loss         0.095194  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.253\n",
      "iter 28201/30000  loss         0.095194  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.253\n",
      "iter 28220/30000  loss         0.095191  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.255\n",
      "iter 28221/30000  loss         0.095191  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.255\n",
      "iter 28240/30000  loss         0.095188  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.256\n",
      "iter 28241/30000  loss         0.095187  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.256\n",
      "iter 28260/30000  loss         0.095184  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.258\n",
      "iter 28261/30000  loss         0.095184  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.258\n",
      "iter 28280/30000  loss         0.095181  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.259\n",
      "iter 28281/30000  loss         0.095180  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.259\n",
      "iter 28300/30000  loss         0.095177  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.261\n",
      "iter 28301/30000  loss         0.095177  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.261\n",
      "iter 28320/30000  loss         0.095174  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.262\n",
      "iter 28321/30000  loss         0.095174  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.262\n",
      "iter 28340/30000  loss         0.095170  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.264\n",
      "iter 28341/30000  loss         0.095170  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.264\n",
      "iter 28360/30000  loss         0.095167  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.265\n",
      "iter 28361/30000  loss         0.095167  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.265\n",
      "iter 28380/30000  loss         0.095164  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.267\n",
      "iter 28381/30000  loss         0.095163  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.267\n",
      "iter 28400/30000  loss         0.095160  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.268\n",
      "iter 28401/30000  loss         0.095160  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.268\n",
      "iter 28420/30000  loss         0.095157  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.270\n",
      "iter 28421/30000  loss         0.095157  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.270\n",
      "iter 28440/30000  loss         0.095153  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.271\n",
      "iter 28441/30000  loss         0.095153  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.271\n",
      "iter 28460/30000  loss         0.095150  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.273\n",
      "iter 28461/30000  loss         0.095150  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.273\n",
      "iter 28480/30000  loss         0.095147  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.274\n",
      "iter 28481/30000  loss         0.095147  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.274\n",
      "iter 28500/30000  loss         0.095143  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.276\n",
      "iter 28501/30000  loss         0.095143  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.276\n",
      "iter 28520/30000  loss         0.095140  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.277\n",
      "iter 28521/30000  loss         0.095140  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.277\n",
      "iter 28540/30000  loss         0.095137  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.279\n",
      "iter 28541/30000  loss         0.095136  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.279\n",
      "iter 28560/30000  loss         0.095133  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.280\n",
      "iter 28561/30000  loss         0.095133  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.280\n",
      "iter 28580/30000  loss         0.095130  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.282\n",
      "iter 28581/30000  loss         0.095130  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.282\n",
      "iter 28600/30000  loss         0.095127  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.283\n",
      "iter 28601/30000  loss         0.095126  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28620/30000  loss         0.095123  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.285\n",
      "iter 28621/30000  loss         0.095123  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.285\n",
      "iter 28640/30000  loss         0.095120  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.286\n",
      "iter 28641/30000  loss         0.095120  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.286\n",
      "iter 28660/30000  loss         0.095117  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.288\n",
      "iter 28661/30000  loss         0.095116  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.288\n",
      "iter 28680/30000  loss         0.095113  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.289\n",
      "iter 28681/30000  loss         0.095113  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.289\n",
      "iter 28700/30000  loss         0.095110  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.290\n",
      "iter 28701/30000  loss         0.095110  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.291\n",
      "iter 28720/30000  loss         0.095107  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.292\n",
      "iter 28721/30000  loss         0.095107  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.292\n",
      "iter 28740/30000  loss         0.095103  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.293\n",
      "iter 28741/30000  loss         0.095103  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.293\n",
      "iter 28760/30000  loss         0.095100  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.295\n",
      "iter 28761/30000  loss         0.095100  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.295\n",
      "iter 28780/30000  loss         0.095097  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.296\n",
      "iter 28781/30000  loss         0.095097  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.296\n",
      "iter 28800/30000  loss         0.095094  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.298\n",
      "iter 28801/30000  loss         0.095093  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.298\n",
      "iter 28820/30000  loss         0.095090  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.299\n",
      "iter 28821/30000  loss         0.095090  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.299\n",
      "iter 28840/30000  loss         0.095087  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.301\n",
      "iter 28841/30000  loss         0.095087  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.301\n",
      "iter 28860/30000  loss         0.095084  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.302\n",
      "iter 28861/30000  loss         0.095084  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.302\n",
      "iter 28880/30000  loss         0.095081  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.304\n",
      "iter 28881/30000  loss         0.095080  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.304\n",
      "iter 28900/30000  loss         0.095077  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.305\n",
      "iter 28901/30000  loss         0.095077  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.305\n",
      "iter 28920/30000  loss         0.095074  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.306\n",
      "iter 28921/30000  loss         0.095074  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.307\n",
      "iter 28940/30000  loss         0.095071  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.308\n",
      "iter 28941/30000  loss         0.095071  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.308\n",
      "iter 28960/30000  loss         0.095068  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.309\n",
      "iter 28961/30000  loss         0.095067  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.309\n",
      "iter 28980/30000  loss         0.095064  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.311\n",
      "iter 28981/30000  loss         0.095064  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.311\n",
      "iter 29000/30000  loss         0.095061  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.312\n",
      "iter 29001/30000  loss         0.095061  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.312\n",
      "iter 29020/30000  loss         0.095058  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.314\n",
      "iter 29021/30000  loss         0.095058  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.314\n",
      "iter 29040/30000  loss         0.095055  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.315\n",
      "iter 29041/30000  loss         0.095054  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.315\n",
      "iter 29060/30000  loss         0.095051  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.317\n",
      "iter 29061/30000  loss         0.095051  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.317\n",
      "iter 29080/30000  loss         0.095048  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.318\n",
      "iter 29081/30000  loss         0.095048  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.318\n",
      "iter 29100/30000  loss         0.095045  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.319\n",
      "iter 29101/30000  loss         0.095045  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.319\n",
      "iter 29120/30000  loss         0.095042  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.321\n",
      "iter 29121/30000  loss         0.095042  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.321\n",
      "iter 29140/30000  loss         0.095039  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.322\n",
      "iter 29141/30000  loss         0.095038  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.322\n",
      "iter 29160/30000  loss         0.095035  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.324\n",
      "iter 29161/30000  loss         0.095035  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.324\n",
      "iter 29180/30000  loss         0.095032  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.325\n",
      "iter 29181/30000  loss         0.095032  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.325\n",
      "iter 29200/30000  loss         0.095029  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.327\n",
      "iter 29201/30000  loss         0.095029  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.327\n",
      "iter 29220/30000  loss         0.095026  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.328\n",
      "iter 29221/30000  loss         0.095026  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.328\n",
      "iter 29240/30000  loss         0.095023  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.329\n",
      "iter 29241/30000  loss         0.095023  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.329\n",
      "iter 29260/30000  loss         0.095020  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.331\n",
      "iter 29261/30000  loss         0.095019  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.331\n",
      "iter 29280/30000  loss         0.095016  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.332\n",
      "iter 29281/30000  loss         0.095016  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.332\n",
      "iter 29300/30000  loss         0.095013  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.334\n",
      "iter 29301/30000  loss         0.095013  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.334\n",
      "iter 29320/30000  loss         0.095010  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.335\n",
      "iter 29321/30000  loss         0.095010  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.335\n",
      "iter 29340/30000  loss         0.095007  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.336\n",
      "iter 29341/30000  loss         0.095007  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.337\n",
      "iter 29360/30000  loss         0.095004  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.338\n",
      "iter 29361/30000  loss         0.095004  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.338\n",
      "iter 29380/30000  loss         0.095001  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.339\n",
      "iter 29381/30000  loss         0.095001  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.339\n",
      "iter 29400/30000  loss         0.094998  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.341\n",
      "iter 29401/30000  loss         0.094997  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29420/30000  loss         0.094994  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.342\n",
      "iter 29421/30000  loss         0.094994  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.342\n",
      "iter 29440/30000  loss         0.094991  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.343\n",
      "iter 29441/30000  loss         0.094991  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.344\n",
      "iter 29460/30000  loss         0.094988  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.345\n",
      "iter 29461/30000  loss         0.094988  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.345\n",
      "iter 29480/30000  loss         0.094985  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.346\n",
      "iter 29481/30000  loss         0.094985  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.346\n",
      "iter 29500/30000  loss         0.094982  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.348\n",
      "iter 29501/30000  loss         0.094982  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.348\n",
      "iter 29520/30000  loss         0.094979  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.349\n",
      "iter 29521/30000  loss         0.094979  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.349\n",
      "iter 29540/30000  loss         0.094976  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.350\n",
      "iter 29541/30000  loss         0.094976  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.351\n",
      "iter 29560/30000  loss         0.094973  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.352\n",
      "iter 29561/30000  loss         0.094973  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.352\n",
      "iter 29580/30000  loss         0.094970  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.353\n",
      "iter 29581/30000  loss         0.094970  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.353\n",
      "iter 29600/30000  loss         0.094967  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.355\n",
      "iter 29601/30000  loss         0.094966  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.355\n",
      "iter 29620/30000  loss         0.094964  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.356\n",
      "iter 29621/30000  loss         0.094963  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.356\n",
      "iter 29640/30000  loss         0.094960  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.357\n",
      "iter 29641/30000  loss         0.094960  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.358\n",
      "iter 29660/30000  loss         0.094957  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.359\n",
      "iter 29661/30000  loss         0.094957  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.359\n",
      "iter 29680/30000  loss         0.094954  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.360\n",
      "iter 29681/30000  loss         0.094954  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.360\n",
      "iter 29700/30000  loss         0.094951  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.362\n",
      "iter 29701/30000  loss         0.094951  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.362\n",
      "iter 29720/30000  loss         0.094948  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.363\n",
      "iter 29721/30000  loss         0.094948  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.363\n",
      "iter 29740/30000  loss         0.094945  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.364\n",
      "iter 29741/30000  loss         0.094945  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.364\n",
      "iter 29760/30000  loss         0.094942  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.366\n",
      "iter 29761/30000  loss         0.094942  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.366\n",
      "iter 29780/30000  loss         0.094939  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.367\n",
      "iter 29781/30000  loss         0.094939  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.367\n",
      "iter 29800/30000  loss         0.094936  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.369\n",
      "iter 29801/30000  loss         0.094936  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.369\n",
      "iter 29820/30000  loss         0.094933  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.370\n",
      "iter 29821/30000  loss         0.094933  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.370\n",
      "iter 29840/30000  loss         0.094930  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.371\n",
      "iter 29841/30000  loss         0.094930  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.371\n",
      "iter 29860/30000  loss         0.094927  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.373\n",
      "iter 29861/30000  loss         0.094927  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.373\n",
      "iter 29880/30000  loss         0.094924  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.374\n",
      "iter 29881/30000  loss         0.094924  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.374\n",
      "iter 29900/30000  loss         0.094921  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.375\n",
      "iter 29901/30000  loss         0.094921  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.375\n",
      "iter 29920/30000  loss         0.094918  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.377\n",
      "iter 29921/30000  loss         0.094918  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.377\n",
      "iter 29940/30000  loss         0.094915  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.378\n",
      "iter 29941/30000  loss         0.094915  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.378\n",
      "iter 29960/30000  loss         0.094912  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.379\n",
      "iter 29961/30000  loss         0.094912  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.380\n",
      "iter 29980/30000  loss         0.094909  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.381\n",
      "iter 29981/30000  loss         0.094909  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.381\n",
      "iter 30000/30000  loss         0.094906  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.382\n",
      "Done. Did NOT converge.\n"
     ]
    }
   ],
   "source": [
    "lr_p3_bw = LogisticRegressionGradientDescent(alpha=1, step_size=0.1, init_w_recipe='zeros')\n",
    "lr_p3_bw.fit(x_trbw, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.1\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.031943  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         0.905018  avg_L1_norm_grad         0.026338  w[0]    0.000 bias    0.000\n",
      "iter    2/30000  loss         0.839647  avg_L1_norm_grad         0.018624  w[0]    0.000 bias    0.022\n",
      "iter    3/30000  loss         0.796985  avg_L1_norm_grad         0.015870  w[0]    0.000 bias    0.028\n",
      "iter    4/30000  loss         0.765567  avg_L1_norm_grad         0.012866  w[0]    0.000 bias    0.043\n",
      "iter    5/30000  loss         0.741351  avg_L1_norm_grad         0.011733  w[0]    0.000 bias    0.053\n",
      "iter    6/30000  loss         0.721433  avg_L1_norm_grad         0.010433  w[0]    0.000 bias    0.065\n",
      "iter    7/30000  loss         0.704357  avg_L1_norm_grad         0.009763  w[0]    0.000 bias    0.075\n",
      "iter    8/30000  loss         0.689274  avg_L1_norm_grad         0.009088  w[0]    0.000 bias    0.086\n",
      "iter    9/30000  loss         0.675680  avg_L1_norm_grad         0.008604  w[0]    0.000 bias    0.096\n",
      "iter   10/30000  loss         0.663249  avg_L1_norm_grad         0.008169  w[0]    0.000 bias    0.106\n",
      "iter   11/30000  loss         0.651762  avg_L1_norm_grad         0.007807  w[0]    0.000 bias    0.116\n",
      "iter   12/30000  loss         0.641063  avg_L1_norm_grad         0.007491  w[0]    0.000 bias    0.125\n",
      "iter   13/30000  loss         0.631037  avg_L1_norm_grad         0.007216  w[0]    0.000 bias    0.135\n",
      "iter   14/30000  loss         0.621599  avg_L1_norm_grad         0.006968  w[0]    0.000 bias    0.144\n",
      "iter   15/30000  loss         0.612681  avg_L1_norm_grad         0.006744  w[0]    0.000 bias    0.153\n",
      "iter   16/30000  loss         0.604231  avg_L1_norm_grad         0.006540  w[0]    0.000 bias    0.162\n",
      "iter   17/30000  loss         0.596205  avg_L1_norm_grad         0.006352  w[0]    0.000 bias    0.171\n",
      "iter   18/30000  loss         0.588566  avg_L1_norm_grad         0.006177  w[0]    0.000 bias    0.180\n",
      "iter   19/30000  loss         0.581283  avg_L1_norm_grad         0.006015  w[0]    0.000 bias    0.189\n",
      "iter   20/30000  loss         0.574331  avg_L1_norm_grad         0.005863  w[0]    0.000 bias    0.197\n",
      "iter   21/30000  loss         0.567685  avg_L1_norm_grad         0.005721  w[0]    0.000 bias    0.205\n",
      "iter   40/30000  loss         0.480553  avg_L1_norm_grad         0.003957  w[0]    0.000 bias    0.343\n",
      "iter   41/30000  loss         0.477372  avg_L1_norm_grad         0.003894  w[0]    0.000 bias    0.349\n",
      "iter   60/30000  loss         0.431385  avg_L1_norm_grad         0.002976  w[0]    0.000 bias    0.458\n",
      "iter   61/30000  loss         0.429539  avg_L1_norm_grad         0.002938  w[0]    0.000 bias    0.463\n",
      "iter   80/30000  loss         0.401312  avg_L1_norm_grad         0.002370  w[0]    0.000 bias    0.552\n",
      "iter   81/30000  loss         0.400116  avg_L1_norm_grad         0.002346  w[0]    0.000 bias    0.556\n",
      "iter  100/30000  loss         0.381164  avg_L1_norm_grad         0.001957  w[0]    0.000 bias    0.632\n",
      "iter  101/30000  loss         0.380333  avg_L1_norm_grad         0.001940  w[0]    0.000 bias    0.635\n",
      "iter  120/30000  loss         0.366844  avg_L1_norm_grad         0.001659  w[0]    0.000 bias    0.701\n",
      "iter  121/30000  loss         0.366239  avg_L1_norm_grad         0.001646  w[0]    0.000 bias    0.704\n",
      "iter  140/30000  loss         0.356241  avg_L1_norm_grad         0.001432  w[0]    0.000 bias    0.761\n",
      "iter  141/30000  loss         0.355785  avg_L1_norm_grad         0.001422  w[0]    0.000 bias    0.764\n",
      "iter  160/30000  loss         0.348151  avg_L1_norm_grad         0.001254  w[0]    0.000 bias    0.815\n",
      "iter  161/30000  loss         0.347797  avg_L1_norm_grad         0.001246  w[0]    0.000 bias    0.817\n",
      "iter  180/30000  loss         0.341833  avg_L1_norm_grad         0.001109  w[0]    0.000 bias    0.862\n",
      "iter  181/30000  loss         0.341555  avg_L1_norm_grad         0.001102  w[0]    0.000 bias    0.864\n",
      "iter  200/30000  loss         0.336810  avg_L1_norm_grad         0.000989  w[0]    0.000 bias    0.905\n",
      "iter  201/30000  loss         0.336587  avg_L1_norm_grad         0.000984  w[0]    0.000 bias    0.907\n",
      "iter  220/30000  loss         0.332757  avg_L1_norm_grad         0.000889  w[0]    0.000 bias    0.943\n",
      "iter  221/30000  loss         0.332576  avg_L1_norm_grad         0.000884  w[0]    0.000 bias    0.945\n",
      "iter  240/30000  loss         0.329447  avg_L1_norm_grad         0.000803  w[0]    0.000 bias    0.978\n",
      "iter  241/30000  loss         0.329297  avg_L1_norm_grad         0.000799  w[0]    0.000 bias    0.980\n",
      "iter  260/30000  loss         0.326714  avg_L1_norm_grad         0.000729  w[0]    0.000 bias    1.010\n",
      "iter  261/30000  loss         0.326591  avg_L1_norm_grad         0.000726  w[0]    0.000 bias    1.011\n",
      "iter  280/30000  loss         0.324440  avg_L1_norm_grad         0.000665  w[0]    0.000 bias    1.039\n",
      "iter  281/30000  loss         0.324336  avg_L1_norm_grad         0.000662  w[0]    0.000 bias    1.040\n",
      "iter  300/30000  loss         0.322531  avg_L1_norm_grad         0.000610  w[0]    0.000 bias    1.065\n",
      "iter  301/30000  loss         0.322444  avg_L1_norm_grad         0.000607  w[0]    0.000 bias    1.067\n",
      "iter  320/30000  loss         0.320918  avg_L1_norm_grad         0.000561  w[0]    0.000 bias    1.090\n",
      "iter  321/30000  loss         0.320845  avg_L1_norm_grad         0.000559  w[0]    0.000 bias    1.091\n",
      "iter  340/30000  loss         0.319548  avg_L1_norm_grad         0.000519  w[0]    0.000 bias    1.112\n",
      "iter  341/30000  loss         0.319485  avg_L1_norm_grad         0.000517  w[0]    0.000 bias    1.113\n",
      "iter  360/30000  loss         0.318377  avg_L1_norm_grad         0.000481  w[0]    0.000 bias    1.132\n",
      "iter  361/30000  loss         0.318323  avg_L1_norm_grad         0.000479  w[0]    0.000 bias    1.133\n",
      "iter  380/30000  loss         0.317372  avg_L1_norm_grad         0.000447  w[0]    0.000 bias    1.151\n",
      "iter  381/30000  loss         0.317326  avg_L1_norm_grad         0.000446  w[0]    0.000 bias    1.152\n",
      "iter  400/30000  loss         0.316505  avg_L1_norm_grad         0.000417  w[0]    0.000 bias    1.169\n",
      "iter  401/30000  loss         0.316465  avg_L1_norm_grad         0.000415  w[0]    0.000 bias    1.169\n",
      "iter  420/30000  loss         0.315754  avg_L1_norm_grad         0.000389  w[0]    0.000 bias    1.185\n",
      "iter  421/30000  loss         0.315719  avg_L1_norm_grad         0.000388  w[0]    0.000 bias    1.185\n",
      "iter  440/30000  loss         0.315102  avg_L1_norm_grad         0.000364  w[0]    0.000 bias    1.200\n",
      "iter  441/30000  loss         0.315071  avg_L1_norm_grad         0.000363  w[0]    0.000 bias    1.200\n",
      "iter  460/30000  loss         0.314532  avg_L1_norm_grad         0.000341  w[0]    0.000 bias    1.213\n",
      "iter  461/30000  loss         0.314506  avg_L1_norm_grad         0.000340  w[0]    0.000 bias    1.214\n",
      "iter  480/30000  loss         0.314034  avg_L1_norm_grad         0.000320  w[0]    0.000 bias    1.226\n",
      "iter  481/30000  loss         0.314011  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    1.227\n",
      "iter  500/30000  loss         0.313597  avg_L1_norm_grad         0.000301  w[0]    0.000 bias    1.238\n",
      "iter  501/30000  loss         0.313576  avg_L1_norm_grad         0.000300  w[0]    0.000 bias    1.238\n",
      "iter  520/30000  loss         0.313212  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    1.249\n",
      "iter  521/30000  loss         0.313194  avg_L1_norm_grad         0.000282  w[0]    0.000 bias    1.249\n",
      "iter  540/30000  loss         0.312872  avg_L1_norm_grad         0.000266  w[0]    0.000 bias    1.259\n",
      "iter  541/30000  loss         0.312856  avg_L1_norm_grad         0.000266  w[0]    0.000 bias    1.260\n",
      "iter  560/30000  loss         0.312571  avg_L1_norm_grad         0.000251  w[0]    0.000 bias    1.269\n",
      "iter  561/30000  loss         0.312557  avg_L1_norm_grad         0.000251  w[0]    0.000 bias    1.269\n",
      "iter  580/30000  loss         0.312305  avg_L1_norm_grad         0.000237  w[0]    0.000 bias    1.278\n",
      "iter  581/30000  loss         0.312292  avg_L1_norm_grad         0.000237  w[0]    0.000 bias    1.278\n",
      "iter  600/30000  loss         0.312067  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    1.286\n",
      "iter  601/30000  loss         0.312056  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    1.286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.311856  avg_L1_norm_grad         0.000212  w[0]    0.000 bias    1.293\n",
      "iter  621/30000  loss         0.311846  avg_L1_norm_grad         0.000212  w[0]    0.000 bias    1.294\n",
      "iter  640/30000  loss         0.311667  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    1.301\n",
      "iter  641/30000  loss         0.311658  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    1.301\n",
      "iter  660/30000  loss         0.311499  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    1.307\n",
      "iter  661/30000  loss         0.311491  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    1.308\n",
      "iter  680/30000  loss         0.311347  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    1.313\n",
      "iter  681/30000  loss         0.311340  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    1.314\n",
      "iter  700/30000  loss         0.311211  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    1.319\n",
      "iter  701/30000  loss         0.311205  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    1.320\n",
      "iter  720/30000  loss         0.311089  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    1.325\n",
      "iter  721/30000  loss         0.311083  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    1.325\n",
      "iter  740/30000  loss         0.310979  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    1.330\n",
      "iter  741/30000  loss         0.310974  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    1.330\n",
      "iter  760/30000  loss         0.310880  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    1.334\n",
      "iter  761/30000  loss         0.310875  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    1.335\n",
      "iter  780/30000  loss         0.310790  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    1.339\n",
      "iter  781/30000  loss         0.310786  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    1.339\n",
      "iter  800/30000  loss         0.310709  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    1.343\n",
      "iter  801/30000  loss         0.310705  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    1.343\n",
      "iter  820/30000  loss         0.310635  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    1.347\n",
      "iter  821/30000  loss         0.310631  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    1.347\n",
      "iter  840/30000  loss         0.310568  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    1.351\n",
      "iter  841/30000  loss         0.310565  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    1.351\n",
      "iter  860/30000  loss         0.310507  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    1.354\n",
      "iter  861/30000  loss         0.310504  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    1.354\n",
      "iter  880/30000  loss         0.310452  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    1.357\n",
      "iter  881/30000  loss         0.310449  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    1.357\n",
      "iter  900/30000  loss         0.310402  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    1.360\n",
      "iter  901/30000  loss         0.310399  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    1.360\n",
      "iter  920/30000  loss         0.310356  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    1.363\n",
      "iter  921/30000  loss         0.310353  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    1.363\n",
      "iter  940/30000  loss         0.310314  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    1.365\n",
      "iter  941/30000  loss         0.310312  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    1.366\n",
      "iter  960/30000  loss         0.310275  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    1.368\n",
      "iter  961/30000  loss         0.310274  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    1.368\n",
      "iter  980/30000  loss         0.310240  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    1.370\n",
      "iter  981/30000  loss         0.310239  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    1.370\n",
      "iter 1000/30000  loss         0.310208  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    1.372\n",
      "iter 1001/30000  loss         0.310207  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    1.372\n",
      "iter 1020/30000  loss         0.310179  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    1.374\n",
      "iter 1021/30000  loss         0.310177  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    1.374\n",
      "iter 1040/30000  loss         0.310152  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    1.376\n",
      "iter 1041/30000  loss         0.310150  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    1.376\n",
      "iter 1060/30000  loss         0.310127  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    1.378\n",
      "iter 1061/30000  loss         0.310126  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    1.378\n",
      "iter 1080/30000  loss         0.310104  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    1.379\n",
      "iter 1081/30000  loss         0.310103  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    1.380\n",
      "iter 1100/30000  loss         0.310083  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    1.381\n",
      "iter 1101/30000  loss         0.310082  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    1.381\n",
      "iter 1120/30000  loss         0.310064  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    1.382\n",
      "iter 1121/30000  loss         0.310063  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    1.383\n",
      "iter 1140/30000  loss         0.310046  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    1.384\n",
      "iter 1141/30000  loss         0.310045  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    1.384\n",
      "iter 1160/30000  loss         0.310029  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    1.385\n",
      "iter 1161/30000  loss         0.310029  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    1.385\n",
      "iter 1180/30000  loss         0.310014  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    1.386\n",
      "iter 1181/30000  loss         0.310014  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    1.386\n",
      "iter 1200/30000  loss         0.310000  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    1.387\n",
      "iter 1201/30000  loss         0.310000  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    1.387\n",
      "iter 1220/30000  loss         0.309987  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    1.388\n",
      "iter 1221/30000  loss         0.309987  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    1.388\n",
      "iter 1240/30000  loss         0.309975  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    1.389\n",
      "iter 1241/30000  loss         0.309975  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    1.389\n",
      "iter 1260/30000  loss         0.309964  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    1.390\n",
      "iter 1261/30000  loss         0.309964  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    1.390\n",
      "iter 1280/30000  loss         0.309954  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    1.391\n",
      "iter 1281/30000  loss         0.309954  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    1.391\n",
      "iter 1300/30000  loss         0.309945  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    1.392\n",
      "iter 1301/30000  loss         0.309944  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    1.392\n",
      "iter 1320/30000  loss         0.309936  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    1.393\n",
      "iter 1321/30000  loss         0.309935  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    1.393\n",
      "iter 1340/30000  loss         0.309928  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    1.393\n",
      "iter 1341/30000  loss         0.309927  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    1.393\n",
      "iter 1360/30000  loss         0.309920  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    1.394\n",
      "iter 1361/30000  loss         0.309920  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    1.394\n",
      "iter 1380/30000  loss         0.309913  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    1.395\n",
      "iter 1381/30000  loss         0.309913  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    1.395\n",
      "iter 1400/30000  loss         0.309907  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    1.395\n",
      "iter 1401/30000  loss         0.309906  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    1.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.309901  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    1.396\n",
      "iter 1421/30000  loss         0.309900  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    1.396\n",
      "iter 1440/30000  loss         0.309895  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    1.396\n",
      "iter 1441/30000  loss         0.309895  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    1.396\n",
      "iter 1460/30000  loss         0.309890  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    1.397\n",
      "iter 1461/30000  loss         0.309889  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    1.397\n",
      "iter 1480/30000  loss         0.309885  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    1.397\n",
      "iter 1481/30000  loss         0.309885  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    1.397\n",
      "iter 1500/30000  loss         0.309880  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    1.398\n",
      "iter 1501/30000  loss         0.309880  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    1.398\n",
      "iter 1520/30000  loss         0.309876  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    1.398\n",
      "iter 1521/30000  loss         0.309876  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    1.398\n",
      "iter 1540/30000  loss         0.309872  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    1.398\n",
      "iter 1541/30000  loss         0.309872  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    1.398\n",
      "iter 1560/30000  loss         0.309868  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.399\n",
      "iter 1561/30000  loss         0.309868  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    1.399\n",
      "iter 1580/30000  loss         0.309865  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    1.399\n",
      "iter 1581/30000  loss         0.309865  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    1.399\n",
      "iter 1600/30000  loss         0.309862  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.399\n",
      "iter 1601/30000  loss         0.309862  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    1.399\n",
      "iter 1620/30000  loss         0.309859  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    1.400\n",
      "iter 1621/30000  loss         0.309859  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    1.400\n",
      "iter 1640/30000  loss         0.309856  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.400\n",
      "iter 1641/30000  loss         0.309856  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    1.400\n",
      "iter 1660/30000  loss         0.309853  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.400\n",
      "iter 1661/30000  loss         0.309853  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    1.400\n",
      "iter 1680/30000  loss         0.309851  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.400\n",
      "iter 1681/30000  loss         0.309851  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.400\n",
      "iter 1700/30000  loss         0.309849  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.401\n",
      "iter 1701/30000  loss         0.309849  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    1.401\n",
      "iter 1720/30000  loss         0.309847  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.401\n",
      "iter 1721/30000  loss         0.309847  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    1.401\n",
      "iter 1740/30000  loss         0.309845  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.401\n",
      "iter 1741/30000  loss         0.309845  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    1.401\n",
      "iter 1760/30000  loss         0.309843  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.401\n",
      "iter 1761/30000  loss         0.309843  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.401\n",
      "iter 1780/30000  loss         0.309841  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.401\n",
      "iter 1781/30000  loss         0.309841  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    1.401\n",
      "iter 1800/30000  loss         0.309839  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.401\n",
      "iter 1801/30000  loss         0.309839  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    1.401\n",
      "iter 1820/30000  loss         0.309838  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.402\n",
      "iter 1821/30000  loss         0.309838  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.402\n",
      "iter 1840/30000  loss         0.309837  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.402\n",
      "iter 1841/30000  loss         0.309836  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    1.402\n",
      "iter 1860/30000  loss         0.309835  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.402\n",
      "iter 1861/30000  loss         0.309835  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    1.402\n",
      "iter 1880/30000  loss         0.309834  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.402\n",
      "iter 1881/30000  loss         0.309834  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.402\n",
      "iter 1900/30000  loss         0.309833  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.402\n",
      "iter 1901/30000  loss         0.309833  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    1.402\n",
      "iter 1920/30000  loss         0.309832  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.402\n",
      "iter 1921/30000  loss         0.309832  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.402\n",
      "iter 1940/30000  loss         0.309831  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.402\n",
      "iter 1941/30000  loss         0.309831  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    1.402\n",
      "iter 1960/30000  loss         0.309830  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.402\n",
      "iter 1961/30000  loss         0.309830  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.402\n",
      "iter 1980/30000  loss         0.309829  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.402\n",
      "iter 1981/30000  loss         0.309829  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    1.402\n",
      "iter 2000/30000  loss         0.309828  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.402\n",
      "iter 2001/30000  loss         0.309828  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.402\n",
      "iter 2020/30000  loss         0.309827  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.402\n",
      "iter 2021/30000  loss         0.309827  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    1.402\n",
      "iter 2040/30000  loss         0.309826  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.403\n",
      "iter 2041/30000  loss         0.309826  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.403\n",
      "iter 2060/30000  loss         0.309826  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.403\n",
      "iter 2061/30000  loss         0.309826  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    1.403\n",
      "iter 2080/30000  loss         0.309825  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.403\n",
      "iter 2081/30000  loss         0.309825  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.403\n",
      "iter 2100/30000  loss         0.309825  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.403\n",
      "iter 2101/30000  loss         0.309824  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.403\n",
      "iter 2120/30000  loss         0.309824  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.403\n",
      "iter 2121/30000  loss         0.309824  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    1.403\n",
      "iter 2140/30000  loss         0.309823  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.403\n",
      "iter 2141/30000  loss         0.309823  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.403\n",
      "iter 2160/30000  loss         0.309823  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.403\n",
      "iter 2161/30000  loss         0.309823  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    1.403\n",
      "iter 2180/30000  loss         0.309822  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.403\n",
      "iter 2181/30000  loss         0.309822  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.403\n",
      "iter 2200/30000  loss         0.309822  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.403\n",
      "iter 2201/30000  loss         0.309822  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.309822  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.403\n",
      "iter 2221/30000  loss         0.309822  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    1.403\n",
      "Done. Converged after 2223 iterations.\n"
     ]
    }
   ],
   "source": [
    "lr_p3_bw = LogisticRegressionGradientDescent(alpha=100, step_size=0.1, init_w_recipe='zeros')\n",
    "lr_p3_bw.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5925</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>149</td>\n",
       "      <td>5851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        5925    75\n",
       "1.0         149  5851"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lr_p3_bw.predict(x_trbw)\n",
    "y_actu = pd.Series(y_tr, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5820</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>383</td>\n",
       "      <td>5617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        5820   180\n",
       "1.0         383  5617"
      ]
     },
     "execution_count": 1057,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lr_p3_bw.predict(x_tr)\n",
    "y_actu = pd.Series(y_tr, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_p2 = lr_p3_bw.predict_proba(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_tr, y_hat_p[:,1])\n",
    "fpr2, tpr2, thresholds2 = sklearn.metrics.roc_curve(y_tr, y_hat_p2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Threshold')"
      ]
     },
     "execution_count": 1085,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr, '-r', label='transformed')\n",
    "plt.plot(fpr2, tpr2, '-b', label='orginal')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1a50ae7240>"
      ]
     },
     "execution_count": 1025,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(lr_p3_bw.w_G[0:784].reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1a4ddc55f8>"
      ]
     },
     "execution_count": 1048,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(lr_p3_bw.w_G[0:784].reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "FP_list = list()\n",
    "FN_list = list()\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i] == y_tr[i]  == 1:\n",
    "        TP = TP + 1\n",
    "    elif y_hat[i] == y_tr[i]  == 0:\n",
    "        TN = TN + 1\n",
    "    elif (y_hat[i] == 1) & (y_tr[i]  == 0):\n",
    "        FP = FP + 1\n",
    "        FP_list.append(i)\n",
    "    else:\n",
    "        FN = FN + 1\n",
    "        FN_list.append(i)\n",
    "        \n",
    "x_FP = [x_trbw[i] for i in FP_list]\n",
    "y_FP = [y_tr[i] for i in FP_list]\n",
    "#x_FP = x_FP[25:46]\n",
    "\n",
    "x_FN = [x_trbw[i] for i in FN_list]\n",
    "y_FN = [y_tr[i] for i in FN_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_FP, y_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a51b63c18>"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_tr[FP_list[3]].reshape(28,28), interpolation='nearest', vmin=0, vmax=1, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_FN, y_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a519dea90>"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_tr[FN_list[4]].reshape(28,28), interpolation='nearest', vmin=0, vmax=1, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trbw_29 = np.zeros((12000,29,29))\n",
    "\n",
    "for i in range(12000):\n",
    "    for j in range(28):\n",
    "        for k in range(28):\n",
    "            x_trbw_29[i][j][k] = x_trbw[i].reshape(28,28)[j][k]\n",
    "        x_trbw_29[i][j][28] = np.sum(x_trbw_29[i][j])/28\n",
    "    x_trbw_29[i][28] = np.append(np.sum(x_trbw[i].reshape(28,28),axis=0),np.sum(x_trbw[i])/28)/28\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trbw_29_2 = np.zeros((12000,841))\n",
    "for i in range(12000):\n",
    "    x_trbw_29_2[i] = x_trbw_29[i].reshape(841)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tebw_29 = np.zeros((2000,29,29))\n",
    "for i in range(2000):\n",
    "    for j in range(28):\n",
    "        for k in range(28):\n",
    "            x_tebw_29[i][j][k] = x_tebw[i].reshape(28,28)[j][k]\n",
    "        x_tebw_29[i][j][28] = np.sum(x_tebw_29[i][j])/28\n",
    "    x_tebw_29[i][28] = np.append(np.sum(x_tebw[i].reshape(28,28),axis=0),np.sum(x_trbw[i])/28)/28\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((12000, 4))\n",
    "x_trbw_29_3 = np.column_stack((x_trbw_29_2,b))\n",
    "for i in range(12000):\n",
    "    x_trbw_29_3[i][-1] = (x_trbw_29_3[i][-20]+x_trbw_29_3[i][-19]+x_trbw_29_3[i][-18]+x_trbw_29_3[i][-17]+x_trbw_29_3[i][-16]+x_trbw_29_3[i][-15])/6\n",
    "    x_trbw_29_3[i][-2] = (x_trbw_29_3[i][-5]+x_trbw_29_3[i][-6]+x_trbw_29_3[i][-7]+x_trbw_29_3[i][-8]+x_trbw_29_3[i][-9]+x_trbw_29_3[i][-10])/6\n",
    "    x_trbw_29_3[i][-3] = (x_trbw_29_3[i][318]+x_trbw_29_3[i][347]+x_trbw_29_3[i][376]+x_trbw_29_3[i][405]+x_trbw_29_3[i][434]+x_trbw_29_3[i][463])/6\n",
    "    x_trbw_29_3[i][-2] = (x_trbw_29_3[i][144]+x_trbw_29_3[i][173]+x_trbw_29_3[i][202]+x_trbw_29_3[i][231]+x_trbw_29_3[i][260]+x_trbw_29_3[i][289])/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tebw_29_2 = np.zeros((2000,841))\n",
    "for i in range(2000):\n",
    "    x_tebw_29_2[i] = x_tebw_29[i].reshape(841)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((2000, 4))\n",
    "x_tebw_29_3 = np.column_stack((x_tebw_29_2,b))\n",
    "for i in range(2000):\n",
    "    x_tebw_29_3[i][-1] = (x_tebw_29_3[i][-20]+x_tebw_29_3[i][-19]+x_tebw_29_3[i][-18]+x_tebw_29_3[i][-17]+x_tebw_29_3[i][-16]+x_tebw_29_3[i][-15])/6\n",
    "    x_tebw_29_3[i][-2] = (x_tebw_29_3[i][-5]+x_tebw_29_3[i][-6]+x_tebw_29_3[i][-7]+x_tebw_29_3[i][-8]+x_tebw_29_3[i][-9]+x_tebw_29_3[i][-10])/6\n",
    "    x_tebw_29_3[i][-3] = (x_tebw_29_3[i][318]+x_tebw_29_3[i][347]+x_tebw_29_3[i][376]+x_tebw_29_3[i][405]+x_tebw_29_3[i][434]+x_tebw_29_3[i][463])/6\n",
    "    x_tebw_29_3[i][-2] = (x_tebw_29_3[i][144]+x_tebw_29_3[i][173]+x_tebw_29_3[i][202]+x_tebw_29_3[i][231]+x_tebw_29_3[i][260]+x_tebw_29_3[i][289])/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_trbw_29_2)\n",
    "x_trbw_29_3 = scaler.transform(x_trbw_29_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_tebw_29_2)\n",
    "x_tebw_29_3 = scaler.transform(x_tebw_29_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1], dtype=int32)"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a8aa786d8>"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_trbw_29_2[4].reshape(29,29), interpolation='nearest', vmin=0, vmax=1, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227.0"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x_trbw[202])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAILCAYAAAB8Yz9AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE3FJREFUeJzt3T+I5Hf9x/HXO55YRIMWYrAwlSD4J8UWYmFjIVdYpFFBomhrGbA2YCPYBAQVDoQQhSAIlhZC1MLqtogQtbG4xigcxsSghcjnV+z5y73vZndn7jsz3/nOPB6w3LHc7X12933Lk8++Z7bGGAEA+J/H5j4AAHBYxAEA0IgDAKARBwBAIw4AgEYcAACNOAAAGnGwY1X1par6XVX9q6p+Pfd5YFNmmKWrqvdU1Y+r6q2q+mtVPTf3mQ7djbkPcAL+nuSFJB9L8rmZzwKPwgyzdM8n+WiSp5I8meSVqvrDGOOXs57qgLk5uEJVfauqfv7A675fVS+s+zbGGL8aY/wsyV+2fkC4hhlm6bYxw0m+luQ7Y4w3xhh/THIryde3eMyjIw6u9pMkN6vq/UlSVTeSfDnJS1X1g6r6xyUvv5/11PAOM8zSTZrhqvpAkg8nefW+t/lqko/v+f1YFN9WuMIY4/Wq+m2SL+aiNG8muTvGOE9ynuSbc54PrmOGWbotzPB77/365n2vezPJ+7Z91mPi5uB6LyZ59t7vn03y0oxngUdhhlm6KTP89r1fn7jvdU8k+ecWznW0xMH1fpHkU1X1iSRfSPLTJKmqH1XV25e8vDbriaEzwyzdI8/wGOONJK8nefq+t/d0EjN+hfIjm69XVbeSfDoXV1kbbWtX1buSvDsXyy9fSfL5JP8dY/xn2+eEy5hhlm7iDH83yWeSPJPkQ0leSfINj1a4nJuD9byY5JN5tOvYryb5d5IfJvnsvd/f2t7RYC1mmKWbMsPfTvLnJHeS/CbJ94TB1dwcrKGqPpLkT0meHGO8Nfd5YFNmmKUzw/vl5uAaVfVYkueSvGwgWSIzzNKZ4f3zUMYrVNXjSf6Wi6uomzMfBzZmhlk6MzwP31YAABrfVgAAGnEAADQb7RxUle9BMMXdMcYH5zyAGWYiM8zSrTXDbg7YpztzHwAmMsMs3VozLA4AgEYcAACNOAAAGnEAADTiAABoxAEA0IgDAKARBwBA46cyHrF1f6hWVe34JAAsiZsDAKARBwBAIw4AgEYcAACNhURWLi5aUgQ4XW4OAIBGHAAAjTgAABpxAAA04gAAaDxagZUue+plj2IAOH5uDgCARhwAAI04AAAacQAANBYSj8RlC4SPyuIhwOlycwAANOIAAGjEAQDQiAMAoLGQiOVDABo3BwBAIw4AgEYcAACNOAAAGguJD9j2Mw0CwNK4OQAAGnEAADTiAABoxAEA0FhIfMA+ni3Q0iPAvDb5OnyKzyLr5gAAaMQBANCIAwCgEQcAQGMhcQa7WG6ZsuS46u+e4gIOq607W4c2MxbOTtO+Fr6P/eummwMAoBEHAEAjDgCARhwAAI2FxAXyDIvsyrYXWzex7jKX+Wff1p25Q5vNKQuSbg4AgEYcAACNOAAAGnEAADTiAABoPFqBlS7bul21/brUp9flYVM+v1Md2qY3h8/M7I6bAwCgEQcAQCMOAIBGHAAAjYVENmIB6PTMuaS4L6veHwu0h+XYZu7QuTkAABpxAAA04gAAaMQBANBYSGQnC2eWubZrk8/HPj72S15SNJuHYykzc4rcHAAAjTgAABpxAAA04gAAaCwkHrgpCzvrLo1ZCtqPOX/08bEv4R37+7c0vqYsn5sDAKARBwBAIw4AgEYcAACNhcQDt+5S4bafse6yBS+LRg9bwsfkFJcU2Y8lzD+bc3MAADTiAABoxAEA0IgDAKCxkHjgtr18aAmNY7SvpbhD/f9jKZBtc3MAADTiAABoxAEA0IgDAKARBws0xnjoZZWqeujl1J2dna38+E15WSofh2VaNcOwbeIAAGjEAQDQiAMAoBEHAEAjDgCAxtMnH4ipG8ceiQDbdaj/p87Pz9c6m0cxMIWbAwCgEQcAQCMOAIBGHAAAzWIWEpewXLPuApPlw/msu8wFh+rs7Cy3b9++9s8tYc43+Vq4hPfn0Kz6+K77cXRzAAA04gAAaMQBANCIAwCgWcxC4hLsYmnSEg5wv1VLtau+9ixhifsy+1ru5nJuDgCARhwAAI04AAAacQAANBYS17BqOWbdRZgpfxdgHbtYXJ7z69Rc/7YF8He4OQAAGnEAADTiAABoxAEA0My+kLiPxZOpS4FTzjhlcRFgLr4mnTY3BwBAIw4AgEYcAACNOAAAmtkXErdtzmck9GyIABwDNwcAQCMOAIBGHAAAjTgAAJqjW0hcwo8Z9cxjABwyNwcAQCMOAIBGHAAAjTgAABpxAAA0G8XB2dlZxhhbfTl2VfXQCwAcMjcHAEAjDgCARhwAAI04AACajZ4++fz8/KGFulNYKlzFYiEAx8rNAQDQiAMAoBEHAEAjDgCAZqOFxFUs5p0Wn2+A4+fmAABoxAEA0IgDAKARBwBAs+lC4t0kd3ZxEE7CU3MfIGaYacwwS7fWDNepPv0xALCabysAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEwY5V1Zeq6ndV9a+q+vXc54FNVdV7qurHVfVWVf21qp6b+0ywCTO8uU1/ZDOb+3uSF5J8LMnnZj4LPIrnk3w0Fz/q9ckkr1TVH8YYv5z1VLC+52OGN+Lm4ApV9a2q+vkDr/t+Vb2w7tsYY/xqjPGzJH/Z+gHhGtuY4SRfS/KdMcYbY4w/JrmV5OtbPCZcygzPQxxc7SdJblbV+5Okqm4k+XKSl6rqB1X1j0tefj/rqeEdk2a4qj6Q5MNJXr3vbb6a5ON7fj84XWZ4Br6tcIUxxutV9dskX8xFad5McneMcZ7kPMk35zwfXGcLM/zee7++ed/r3kzyvm2fFVYxw/Nwc3C9F5M8e+/3zyZ5acazwKOYMsNv3/v1ifte90SSf27hXLAuM7xn4uB6v0jyqar6RJIvJPlpklTVj6rq7UteXpv1xNA98gyPMd5I8nqSp+97e08nMePskxnesxpjzH2Gg1dVt5J8OhdXWRs94qCq3pXk3blYfvlKks8n+e8Y4z/bPidcZuIMfzfJZ5I8k+RDSV5J8g2b3uyTGd4vNwfreTHJJ/No31L4apJ/J/lhks/e+/2t7R0N1jJlhr+d5M9J7iT5TZLv+aLKDMzwHrk5WENVfSTJn5I8OcZ4a+7zwKbMMEtnhvfLzcE1quqxJM8ledlAskRmmKUzw/vnoYxXqKrHk/wtF1dRN2c+DmzMDLN0Zngevq0AADS+rQAANOIAAGg22jmoKt+DYIq7Y4wPznkAM8xEZpilW2uG3RywT3fmPgBMZIZZurVmWBwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEBzY+4DAIdtjPHQ66pqb38f9mXVrCaHN6+XnfNBU87t5gAAaMQBANCIAwCgEQcAQGMhccf2sTgC27LuvK775+Y09Yz+Tx6PqbMw11LtnP/P3BwAAI04AAAacQAANOIAAGgsJD6ibS+KHNozc23y/lncOnyHtkC47QWvQ3v/mM++ZuHQZnjbX4fdHAAAjTgAABpxAAA04gAAaMQBANB4tMKBm7IRa4P7uB3b5/fY3h92b6kzs4RzuzkAABpxAAA04gAAaMQBANBYSHxEq5YC53zazn3xVMnzWMIC07HZ9tPjsr6lzvtSz72KmwMAoBEHAEAjDgCARhwAAM1iFhKnLHpYItqcj9n2HdOyElzFrO/fthdo3RwAAI04AAAacQAANOIAAGgOciFxqcsscz5rogXC7VrqDMIu+X9xOtwcAACNOAAAGnEAADTiAABoxMGOVdVDL4f4Nk/F2dlZxhjXvsCSrDPT23hhWaZ8DsUBANCIAwCgEQcAQCMOAIDmIJ8hcduObZFmqe/PISxOnp+fP3SOpX48OU1nZ2e5ffv23MfgyLk5AAAacQAANOIAAGjEAQDQbLSQaBFmc4ewhMc71p3hqZ83S47Akrk5AAAacQAANOIAAGjEAQDQiAMAoNno0QqrnnoWlmTdp0/exaMN/N/ZHY8Oge1ycwAANOIAAGjEAQDQiAMAoNloIRGO0S4WBfe15MjpWXepFqZwcwAANOIAAGjEAQDQiAMAoLGQCDvg2RDZp3XnzeLiaZnydcjNAQDQiAMAoBEHAEAjDgCAxkIiwImwKMu63BwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AAAacQAANDc2/PN3k9zZxUE4CU/NfYCYYaYxwyzdWjNcY4xdHwQAWBDfVgAAGnEAADTiAABoxAEA0IgDAKARBwBAIw4AgEYcAACNOAAAGnEAADTiAABoxAEA0IiDHauq91TVj6vqrar6a1U9N/eZYBNmmKUzw5vb9Ec2s7nnk3w0Fz8m88kkr1TVH8YYv5z1VLC+52OGWbbnY4Y34ubgClX1rar6+QOv+35VvbDBm/laku+MMd4YY/wxya0kX9/iMeFSZpilM8PzEAdX+0mSm1X1/iSpqhtJvpzkpar6QVX945KX39/78x9I8uEkr973Nl9N8vE9vx+cLjPM0pnhGfi2whXGGK9X1W+TfDEXpXkzyd0xxnmS8yTfvOZNvPfer2/e97o3k7xv22eFVcwwS2eG5+Hm4HovJnn23u+fTfLSBn/37Xu/PnHf655I8s8tnAvWZYZZOjO8Z+Lger9I8qmq+kSSLyT5aZJU1Y+q6u1LXl5LkjHGG0leT/L0fW/v6SSv7fl94LSZYZbODO9ZjTHmPsPBq6pbST6di6usz234d7+b5DNJnknyoSSvJPmGLVn2yQyzdGZ4v9wcrOfFJJ/MZldZ//PtJH9OcifJb5J8z0AyAzPM0pnhPXJzsIaq+kiSPyV5cozx1tzngU2ZYZbODO+Xm4NrVNVjSZ5L8rKBZInMMEtnhvfPQxmvUFWPJ/lbLq6ibs58HNiYGWbpzPA8fFsBAGh8WwEAaDb6tkJVuWZgirtjjA/OeQAzzERmmKVba4bdHLBPd+Y+AExkhlm6tWZYHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQHNj7gMAyzPGeOh1VTXDSeB6q+Z1E6c4224OAIBGHAAAjTgAABpxAAA0FhIPxCYLM6e4HMP2TV3SWvft7WNeLZzxP9ue68ve5qHN9bbP4+YAAGjEAQDQiAMAoBEHAEBjIXHHdrEcsw8WJJfr0GZu28tcx7Rwxn6s+lxOnaMlzPUUbg4AgEYcAACNOAAAGnEAADTiAABoPFrhEc25WTrnZrUN7sNxaNvNm1jy2dmefc3BUv+dOb/eujkAABpxAAA04gAAaMQBANBYSHzAsS9KHfv7dwx8jjhG5npzUz9mUxYa3RwAAI04AAAacQAANOIAAGhOYiHxFBZh5nxmrjmfsXFJpnyOpn4+plj1b5/C/ynWYxaOk5sDAKARBwBAIw4AgEYcAADNohcSLcJcbcpS4NSP7SktKe5r0XCb/+4h/juHZukzfKqfN7bDzQEA0IgDAKARBwBAIw4AgGZvC4mnuhyzrwWmVR/fU/2YX+Xs7Cy3b9+e+xj/z+eIbTBHbJubAwCgEQcAQCMOAIBGHAAAzeSFxGNbhNnHAuGxfczgGC3p2RBh29wcAACNOAAAGnEAADTiAABoFv0jm6fYZNlo3QXCVW/z2JYPLWlxGbMBx8PNAQDQiAMAoBEHAEAjDgCARhwAAM3sj1aY6+mKd/EogimPatiFOd/HVQ5hm/38/PyhcxzbI0rWNeccTnlkz6GdG46RmwMAoBEHAEAjDgCARhwAAM3khcSpTxl8istgp/A+L2lxa0ln5TT+/2xqyUvOHCY3BwBAIw4AgEYcAACNOAAAmp08Q6IFL06JJa3NLeFj5uvYw3xMToebAwCgEQcAQCMOAIBGHAAAzew/shmWzpIWcGzcHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA04gAAaMQBANCIAwCgEQcAQCMOAIBGHAAAjTgAABpxAAA0Nzb883eT3NnFQTgJT819gJhhpjHDLN1aM1xjjF0fBABYEN9WAAAacQAANOIAAGjEAQDQiAMAoBEHAEAjDgCARhwAAI04AACa/wNdNUhwU7GFYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_trbw[200:], y_tr[200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c2 = np.array(x_c)\n",
    "c_c2 = x_c2.reshape(12000,392)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.07142857,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 0.25      , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "       0.        , 0.        , 0.39285714, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "       0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.57142857, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.71428571,\n",
       "       0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.96428571, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.96428571, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.71428571, 0.        , 0.        , 1.        , 1.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.64285714,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.        , 0.67857143, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03571429, 0.10714286, 0.14285714,\n",
       "       0.14285714, 0.14285714, 0.14285714, 0.17857143, 0.17857143,\n",
       "       0.25      , 0.25      , 0.25      , 0.28571429, 0.32142857,\n",
       "       0.35714286, 0.35714286, 0.39285714, 0.39285714, 0.39285714,\n",
       "       0.42857143, 0.42857143, 0.39285714, 0.28571429, 0.25      ,\n",
       "       0.28571429, 0.32142857, 0.32142857, 0.25      , 0.17857143,\n",
       "       0.26658163, 0.        , 0.56547619, 0.01190476, 0.38690476])"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trbw_29_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 846 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.1\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.047714  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         1.024628  avg_L1_norm_grad         0.099551  w[0]    0.000 bias    0.000\n",
      "iter    2/30000  loss         2.233137  avg_L1_norm_grad         0.187640  w[0]    0.000 bias    0.041\n",
      "iter    3/30000  loss         3.684080  avg_L1_norm_grad         0.177505  w[0]    0.000 bias   -0.021\n",
      "iter    4/30000  loss         0.866383  avg_L1_norm_grad         0.077747  w[0]    0.000 bias    0.049\n",
      "iter    5/30000  loss         1.345333  avg_L1_norm_grad         0.112638  w[0]    0.000 bias    0.029\n",
      "iter    6/30000  loss         1.589600  avg_L1_norm_grad         0.162798  w[0]    0.000 bias    0.077\n",
      "iter    7/30000  loss         2.708757  avg_L1_norm_grad         0.154391  w[0]    0.000 bias    0.025\n",
      "iter    8/30000  loss         0.831000  avg_L1_norm_grad         0.078484  w[0]    0.000 bias    0.087\n",
      "iter    9/30000  loss         1.125078  avg_L1_norm_grad         0.090916  w[0]    0.000 bias    0.067\n",
      "iter   10/30000  loss         1.074492  avg_L1_norm_grad         0.124340  w[0]    0.000 bias    0.107\n",
      "iter   11/30000  loss         1.786711  avg_L1_norm_grad         0.124845  w[0]    0.000 bias    0.070\n",
      "iter   12/30000  loss         0.872024  avg_L1_norm_grad         0.100485  w[0]    0.000 bias    0.122\n",
      "iter   13/30000  loss         1.314712  avg_L1_norm_grad         0.104048  w[0]    0.000 bias    0.093\n",
      "iter   14/30000  loss         0.873013  avg_L1_norm_grad         0.105650  w[0]    0.000 bias    0.137\n",
      "iter   15/30000  loss         1.337292  avg_L1_norm_grad         0.106274  w[0]    0.000 bias    0.106\n",
      "iter   16/30000  loss         0.795715  avg_L1_norm_grad         0.098343  w[0]    0.000 bias    0.151\n",
      "iter   17/30000  loss         1.180126  avg_L1_norm_grad         0.099103  w[0]    0.000 bias    0.123\n",
      "iter   18/30000  loss         0.750011  avg_L1_norm_grad         0.095297  w[0]    0.000 bias    0.165\n",
      "iter   19/30000  loss         1.097398  avg_L1_norm_grad         0.095514  w[0]    0.000 bias    0.137\n",
      "iter   20/30000  loss         0.701511  avg_L1_norm_grad         0.091043  w[0]    0.000 bias    0.177\n",
      "iter   21/30000  loss         1.006248  avg_L1_norm_grad         0.090952  w[0]    0.000 bias    0.151\n",
      "iter   40/30000  loss         0.392768  avg_L1_norm_grad         0.044940  w[0]    0.000 bias    0.271\n",
      "iter   41/30000  loss         0.430844  avg_L1_norm_grad         0.043440  w[0]    0.000 bias    0.259\n",
      "iter   60/30000  loss         0.292041  avg_L1_norm_grad         0.012675  w[0]    0.000 bias    0.332\n",
      "iter   61/30000  loss         0.290899  avg_L1_norm_grad         0.011563  w[0]    0.000 bias    0.330\n",
      "iter   80/30000  loss         0.267974  avg_L1_norm_grad         0.001993  w[0]    0.000 bias    0.384\n",
      "iter   81/30000  loss         0.267256  avg_L1_norm_grad         0.001862  w[0]    0.000 bias    0.387\n",
      "iter  100/30000  loss         0.255378  avg_L1_norm_grad         0.001546  w[0]    0.000 bias    0.435\n",
      "iter  101/30000  loss         0.254826  avg_L1_norm_grad         0.001537  w[0]    0.000 bias    0.437\n",
      "iter  120/30000  loss         0.245375  avg_L1_norm_grad         0.001388  w[0]    0.000 bias    0.482\n",
      "iter  121/30000  loss         0.244926  avg_L1_norm_grad         0.001381  w[0]    0.000 bias    0.484\n",
      "iter  140/30000  loss         0.237135  avg_L1_norm_grad         0.001263  w[0]    0.000 bias    0.526\n",
      "iter  141/30000  loss         0.236760  avg_L1_norm_grad         0.001257  w[0]    0.000 bias    0.528\n",
      "iter  160/30000  loss         0.230182  avg_L1_norm_grad         0.001164  w[0]    0.000 bias    0.568\n",
      "iter  161/30000  loss         0.229862  avg_L1_norm_grad         0.001159  w[0]    0.000 bias    0.570\n",
      "iter  180/30000  loss         0.224202  avg_L1_norm_grad         0.001080  w[0]    0.000 bias    0.608\n",
      "iter  181/30000  loss         0.223924  avg_L1_norm_grad         0.001076  w[0]    0.000 bias    0.610\n",
      "iter  200/30000  loss         0.218979  avg_L1_norm_grad         0.001010  w[0]    0.000 bias    0.646\n",
      "iter  201/30000  loss         0.218735  avg_L1_norm_grad         0.001007  w[0]    0.000 bias    0.648\n",
      "iter  220/30000  loss         0.214362  avg_L1_norm_grad         0.000951  w[0]    0.000 bias    0.683\n",
      "iter  221/30000  loss         0.214144  avg_L1_norm_grad         0.000948  w[0]    0.000 bias    0.685\n",
      "iter  240/30000  loss         0.210235  avg_L1_norm_grad         0.000899  w[0]    0.000 bias    0.719\n",
      "iter  241/30000  loss         0.210040  avg_L1_norm_grad         0.000897  w[0]    0.000 bias    0.720\n",
      "iter  260/30000  loss         0.206515  avg_L1_norm_grad         0.000854  w[0]    0.000 bias    0.753\n",
      "iter  261/30000  loss         0.206339  avg_L1_norm_grad         0.000852  w[0]    0.000 bias    0.755\n",
      "iter  280/30000  loss         0.203136  avg_L1_norm_grad         0.000815  w[0]    0.000 bias    0.786\n",
      "iter  281/30000  loss         0.202975  avg_L1_norm_grad         0.000813  w[0]    0.000 bias    0.788\n",
      "iter  300/30000  loss         0.200047  avg_L1_norm_grad         0.000779  w[0]    0.000 bias    0.818\n",
      "iter  301/30000  loss         0.199899  avg_L1_norm_grad         0.000777  w[0]    0.000 bias    0.820\n",
      "iter  320/30000  loss         0.197205  avg_L1_norm_grad         0.000747  w[0]    0.000 bias    0.850\n",
      "iter  321/30000  loss         0.197069  avg_L1_norm_grad         0.000746  w[0]    0.000 bias    0.851\n",
      "iter  340/30000  loss         0.194579  avg_L1_norm_grad         0.000718  w[0]    0.000 bias    0.880\n",
      "iter  341/30000  loss         0.194453  avg_L1_norm_grad         0.000717  w[0]    0.000 bias    0.882\n",
      "iter  360/30000  loss         0.192140  avg_L1_norm_grad         0.000692  w[0]    0.000 bias    0.910\n",
      "iter  361/30000  loss         0.192023  avg_L1_norm_grad         0.000691  w[0]    0.000 bias    0.912\n",
      "iter  380/30000  loss         0.189868  avg_L1_norm_grad         0.000668  w[0]    0.000 bias    0.940\n",
      "iter  381/30000  loss         0.189758  avg_L1_norm_grad         0.000667  w[0]    0.000 bias    0.941\n",
      "iter  400/30000  loss         0.187742  avg_L1_norm_grad         0.000646  w[0]    0.000 bias    0.968\n",
      "iter  401/30000  loss         0.187639  avg_L1_norm_grad         0.000645  w[0]    0.000 bias    0.970\n",
      "iter  420/30000  loss         0.185746  avg_L1_norm_grad         0.000626  w[0]    0.000 bias    0.996\n",
      "iter  421/30000  loss         0.185650  avg_L1_norm_grad         0.000626  w[0]    0.000 bias    0.998\n",
      "iter  440/30000  loss         0.183868  avg_L1_norm_grad         0.000608  w[0]    0.000 bias    1.024\n",
      "iter  441/30000  loss         0.183777  avg_L1_norm_grad         0.000607  w[0]    0.000 bias    1.025\n",
      "iter  460/30000  loss         0.182096  avg_L1_norm_grad         0.000591  w[0]    0.000 bias    1.051\n",
      "iter  461/30000  loss         0.182010  avg_L1_norm_grad         0.000590  w[0]    0.000 bias    1.052\n",
      "iter  480/30000  loss         0.180420  avg_L1_norm_grad         0.000574  w[0]    0.000 bias    1.077\n",
      "iter  481/30000  loss         0.180338  avg_L1_norm_grad         0.000574  w[0]    0.000 bias    1.078\n",
      "iter  500/30000  loss         0.178830  avg_L1_norm_grad         0.000559  w[0]    0.000 bias    1.103\n",
      "iter  501/30000  loss         0.178753  avg_L1_norm_grad         0.000559  w[0]    0.000 bias    1.104\n",
      "iter  520/30000  loss         0.177321  avg_L1_norm_grad         0.000545  w[0]    0.000 bias    1.129\n",
      "iter  521/30000  loss         0.177247  avg_L1_norm_grad         0.000545  w[0]    0.000 bias    1.130\n",
      "iter  540/30000  loss         0.175883  avg_L1_norm_grad         0.000532  w[0]    0.000 bias    1.154\n",
      "iter  541/30000  loss         0.175813  avg_L1_norm_grad         0.000531  w[0]    0.000 bias    1.155\n",
      "iter  560/30000  loss         0.174513  avg_L1_norm_grad         0.000519  w[0]    0.000 bias    1.179\n",
      "iter  561/30000  loss         0.174446  avg_L1_norm_grad         0.000519  w[0]    0.000 bias    1.180\n",
      "iter  580/30000  loss         0.173204  avg_L1_norm_grad         0.000507  w[0]    0.000 bias    1.203\n",
      "iter  581/30000  loss         0.173140  avg_L1_norm_grad         0.000507  w[0]    0.000 bias    1.204\n",
      "iter  600/30000  loss         0.171953  avg_L1_norm_grad         0.000496  w[0]    0.000 bias    1.227\n",
      "iter  601/30000  loss         0.171891  avg_L1_norm_grad         0.000496  w[0]    0.000 bias    1.229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.170754  avg_L1_norm_grad         0.000486  w[0]    0.000 bias    1.251\n",
      "iter  621/30000  loss         0.170695  avg_L1_norm_grad         0.000485  w[0]    0.000 bias    1.252\n",
      "iter  640/30000  loss         0.169604  avg_L1_norm_grad         0.000476  w[0]    0.000 bias    1.275\n",
      "iter  641/30000  loss         0.169547  avg_L1_norm_grad         0.000475  w[0]    0.000 bias    1.276\n",
      "iter  660/30000  loss         0.168499  avg_L1_norm_grad         0.000466  w[0]    0.000 bias    1.298\n",
      "iter  661/30000  loss         0.168445  avg_L1_norm_grad         0.000466  w[0]    0.000 bias    1.299\n",
      "iter  680/30000  loss         0.167437  avg_L1_norm_grad         0.000457  w[0]    0.000 bias    1.321\n",
      "iter  681/30000  loss         0.167385  avg_L1_norm_grad         0.000456  w[0]    0.000 bias    1.322\n",
      "iter  700/30000  loss         0.166415  avg_L1_norm_grad         0.000448  w[0]    0.000 bias    1.343\n",
      "iter  701/30000  loss         0.166365  avg_L1_norm_grad         0.000448  w[0]    0.000 bias    1.344\n",
      "iter  720/30000  loss         0.165431  avg_L1_norm_grad         0.000440  w[0]    0.000 bias    1.365\n",
      "iter  721/30000  loss         0.165382  avg_L1_norm_grad         0.000439  w[0]    0.000 bias    1.367\n",
      "iter  740/30000  loss         0.164481  avg_L1_norm_grad         0.000432  w[0]    0.000 bias    1.387\n",
      "iter  741/30000  loss         0.164434  avg_L1_norm_grad         0.000432  w[0]    0.000 bias    1.389\n",
      "iter  760/30000  loss         0.163564  avg_L1_norm_grad         0.000424  w[0]    0.000 bias    1.409\n",
      "iter  761/30000  loss         0.163519  avg_L1_norm_grad         0.000424  w[0]    0.000 bias    1.410\n",
      "iter  780/30000  loss         0.162678  avg_L1_norm_grad         0.000417  w[0]    0.000 bias    1.431\n",
      "iter  781/30000  loss         0.162634  avg_L1_norm_grad         0.000417  w[0]    0.000 bias    1.432\n",
      "iter  800/30000  loss         0.161821  avg_L1_norm_grad         0.000410  w[0]    0.000 bias    1.452\n",
      "iter  801/30000  loss         0.161779  avg_L1_norm_grad         0.000410  w[0]    0.000 bias    1.453\n",
      "iter  820/30000  loss         0.160992  avg_L1_norm_grad         0.000403  w[0]    0.000 bias    1.473\n",
      "iter  821/30000  loss         0.160951  avg_L1_norm_grad         0.000403  w[0]    0.000 bias    1.474\n",
      "iter  840/30000  loss         0.160188  avg_L1_norm_grad         0.000397  w[0]    0.000 bias    1.494\n",
      "iter  841/30000  loss         0.160149  avg_L1_norm_grad         0.000396  w[0]    0.000 bias    1.495\n",
      "iter  860/30000  loss         0.159410  avg_L1_norm_grad         0.000391  w[0]    0.000 bias    1.515\n",
      "iter  861/30000  loss         0.159372  avg_L1_norm_grad         0.000390  w[0]    0.000 bias    1.516\n",
      "iter  880/30000  loss         0.158655  avg_L1_norm_grad         0.000385  w[0]    0.000 bias    1.535\n",
      "iter  881/30000  loss         0.158617  avg_L1_norm_grad         0.000384  w[0]    0.000 bias    1.536\n",
      "iter  900/30000  loss         0.157921  avg_L1_norm_grad         0.000379  w[0]    0.000 bias    1.555\n",
      "iter  901/30000  loss         0.157885  avg_L1_norm_grad         0.000378  w[0]    0.000 bias    1.556\n",
      "iter  920/30000  loss         0.157209  avg_L1_norm_grad         0.000373  w[0]    0.000 bias    1.575\n",
      "iter  921/30000  loss         0.157174  avg_L1_norm_grad         0.000373  w[0]    0.000 bias    1.576\n",
      "iter  940/30000  loss         0.156517  avg_L1_norm_grad         0.000368  w[0]    0.000 bias    1.595\n",
      "iter  941/30000  loss         0.156483  avg_L1_norm_grad         0.000368  w[0]    0.000 bias    1.596\n",
      "iter  960/30000  loss         0.155844  avg_L1_norm_grad         0.000363  w[0]    0.000 bias    1.615\n",
      "iter  961/30000  loss         0.155811  avg_L1_norm_grad         0.000362  w[0]    0.000 bias    1.616\n",
      "iter  980/30000  loss         0.155189  avg_L1_norm_grad         0.000358  w[0]    0.000 bias    1.634\n",
      "iter  981/30000  loss         0.155157  avg_L1_norm_grad         0.000357  w[0]    0.000 bias    1.635\n",
      "iter 1000/30000  loss         0.154552  avg_L1_norm_grad         0.000353  w[0]    0.000 bias    1.654\n",
      "iter 1001/30000  loss         0.154520  avg_L1_norm_grad         0.000352  w[0]    0.000 bias    1.655\n",
      "iter 1020/30000  loss         0.153931  avg_L1_norm_grad         0.000348  w[0]    0.000 bias    1.673\n",
      "iter 1021/30000  loss         0.153900  avg_L1_norm_grad         0.000348  w[0]    0.000 bias    1.674\n",
      "iter 1040/30000  loss         0.153325  avg_L1_norm_grad         0.000343  w[0]    0.000 bias    1.692\n",
      "iter 1041/30000  loss         0.153295  avg_L1_norm_grad         0.000343  w[0]    0.000 bias    1.693\n",
      "iter 1060/30000  loss         0.152735  avg_L1_norm_grad         0.000339  w[0]    0.000 bias    1.711\n",
      "iter 1061/30000  loss         0.152706  avg_L1_norm_grad         0.000339  w[0]    0.000 bias    1.712\n",
      "iter 1080/30000  loss         0.152159  avg_L1_norm_grad         0.000335  w[0]    0.000 bias    1.729\n",
      "iter 1081/30000  loss         0.152131  avg_L1_norm_grad         0.000335  w[0]    0.000 bias    1.730\n",
      "iter 1100/30000  loss         0.151598  avg_L1_norm_grad         0.000331  w[0]    0.000 bias    1.748\n",
      "iter 1101/30000  loss         0.151570  avg_L1_norm_grad         0.000330  w[0]    0.000 bias    1.749\n",
      "iter 1120/30000  loss         0.151049  avg_L1_norm_grad         0.000327  w[0]    0.000 bias    1.766\n",
      "iter 1121/30000  loss         0.151022  avg_L1_norm_grad         0.000326  w[0]    0.000 bias    1.767\n",
      "iter 1140/30000  loss         0.150513  avg_L1_norm_grad         0.000323  w[0]    0.000 bias    1.784\n",
      "iter 1141/30000  loss         0.150486  avg_L1_norm_grad         0.000322  w[0]    0.000 bias    1.785\n",
      "iter 1160/30000  loss         0.149989  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    1.802\n",
      "iter 1161/30000  loss         0.149963  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    1.803\n",
      "iter 1180/30000  loss         0.149477  avg_L1_norm_grad         0.000315  w[0]    0.000 bias    1.820\n",
      "iter 1181/30000  loss         0.149452  avg_L1_norm_grad         0.000315  w[0]    0.000 bias    1.821\n",
      "iter 1200/30000  loss         0.148977  avg_L1_norm_grad         0.000312  w[0]    0.000 bias    1.838\n",
      "iter 1201/30000  loss         0.148952  avg_L1_norm_grad         0.000311  w[0]    0.000 bias    1.839\n",
      "iter 1220/30000  loss         0.148487  avg_L1_norm_grad         0.000308  w[0]    0.000 bias    1.856\n",
      "iter 1221/30000  loss         0.148463  avg_L1_norm_grad         0.000308  w[0]    0.000 bias    1.857\n",
      "iter 1240/30000  loss         0.148007  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    1.873\n",
      "iter 1241/30000  loss         0.147984  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    1.874\n",
      "iter 1260/30000  loss         0.147538  avg_L1_norm_grad         0.000301  w[0]    0.000 bias    1.891\n",
      "iter 1261/30000  loss         0.147515  avg_L1_norm_grad         0.000301  w[0]    0.000 bias    1.892\n",
      "iter 1280/30000  loss         0.147078  avg_L1_norm_grad         0.000298  w[0]    0.000 bias    1.908\n",
      "iter 1281/30000  loss         0.147056  avg_L1_norm_grad         0.000298  w[0]    0.000 bias    1.909\n",
      "iter 1300/30000  loss         0.146628  avg_L1_norm_grad         0.000295  w[0]    0.000 bias    1.925\n",
      "iter 1301/30000  loss         0.146606  avg_L1_norm_grad         0.000295  w[0]    0.000 bias    1.926\n",
      "iter 1320/30000  loss         0.146187  avg_L1_norm_grad         0.000292  w[0]    0.000 bias    1.942\n",
      "iter 1321/30000  loss         0.146165  avg_L1_norm_grad         0.000292  w[0]    0.000 bias    1.943\n",
      "iter 1340/30000  loss         0.145754  avg_L1_norm_grad         0.000289  w[0]    0.000 bias    1.959\n",
      "iter 1341/30000  loss         0.145733  avg_L1_norm_grad         0.000289  w[0]    0.000 bias    1.960\n",
      "iter 1360/30000  loss         0.145330  avg_L1_norm_grad         0.000286  w[0]    0.000 bias    1.976\n",
      "iter 1361/30000  loss         0.145309  avg_L1_norm_grad         0.000286  w[0]    0.000 bias    1.977\n",
      "iter 1380/30000  loss         0.144914  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    1.993\n",
      "iter 1381/30000  loss         0.144893  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    1.994\n",
      "iter 1400/30000  loss         0.144505  avg_L1_norm_grad         0.000281  w[0]    0.000 bias    2.009\n",
      "iter 1401/30000  loss         0.144485  avg_L1_norm_grad         0.000281  w[0]    0.000 bias    2.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.144104  avg_L1_norm_grad         0.000278  w[0]    0.000 bias    2.026\n",
      "iter 1421/30000  loss         0.144085  avg_L1_norm_grad         0.000278  w[0]    0.000 bias    2.027\n",
      "iter 1440/30000  loss         0.143711  avg_L1_norm_grad         0.000275  w[0]    0.000 bias    2.042\n",
      "iter 1441/30000  loss         0.143692  avg_L1_norm_grad         0.000275  w[0]    0.000 bias    2.043\n",
      "iter 1460/30000  loss         0.143325  avg_L1_norm_grad         0.000273  w[0]    0.000 bias    2.058\n",
      "iter 1461/30000  loss         0.143306  avg_L1_norm_grad         0.000273  w[0]    0.000 bias    2.059\n",
      "iter 1480/30000  loss         0.142945  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    2.075\n",
      "iter 1481/30000  loss         0.142927  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    2.075\n",
      "iter 1500/30000  loss         0.142573  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    2.091\n",
      "iter 1501/30000  loss         0.142554  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    2.092\n",
      "iter 1520/30000  loss         0.142206  avg_L1_norm_grad         0.000265  w[0]    0.000 bias    2.107\n",
      "iter 1521/30000  loss         0.142188  avg_L1_norm_grad         0.000265  w[0]    0.000 bias    2.108\n",
      "iter 1540/30000  loss         0.141846  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    2.123\n",
      "iter 1541/30000  loss         0.141828  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    2.123\n",
      "iter 1560/30000  loss         0.141492  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    2.138\n",
      "iter 1561/30000  loss         0.141474  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    2.139\n",
      "iter 1580/30000  loss         0.141144  avg_L1_norm_grad         0.000259  w[0]    0.000 bias    2.154\n",
      "iter 1581/30000  loss         0.141127  avg_L1_norm_grad         0.000258  w[0]    0.000 bias    2.155\n",
      "iter 1600/30000  loss         0.140801  avg_L1_norm_grad         0.000256  w[0]    0.000 bias    2.170\n",
      "iter 1601/30000  loss         0.140784  avg_L1_norm_grad         0.000256  w[0]    0.000 bias    2.170\n",
      "iter 1620/30000  loss         0.140465  avg_L1_norm_grad         0.000254  w[0]    0.000 bias    2.185\n",
      "iter 1621/30000  loss         0.140448  avg_L1_norm_grad         0.000254  w[0]    0.000 bias    2.186\n",
      "iter 1640/30000  loss         0.140133  avg_L1_norm_grad         0.000252  w[0]    0.000 bias    2.201\n",
      "iter 1641/30000  loss         0.140117  avg_L1_norm_grad         0.000252  w[0]    0.000 bias    2.201\n",
      "iter 1660/30000  loss         0.139807  avg_L1_norm_grad         0.000250  w[0]    0.000 bias    2.216\n",
      "iter 1661/30000  loss         0.139791  avg_L1_norm_grad         0.000250  w[0]    0.000 bias    2.217\n",
      "iter 1680/30000  loss         0.139486  avg_L1_norm_grad         0.000248  w[0]    0.000 bias    2.231\n",
      "iter 1681/30000  loss         0.139470  avg_L1_norm_grad         0.000248  w[0]    0.000 bias    2.232\n",
      "iter 1700/30000  loss         0.139169  avg_L1_norm_grad         0.000246  w[0]    0.000 bias    2.246\n",
      "iter 1701/30000  loss         0.139154  avg_L1_norm_grad         0.000246  w[0]    0.000 bias    2.247\n",
      "iter 1720/30000  loss         0.138858  avg_L1_norm_grad         0.000244  w[0]    0.000 bias    2.261\n",
      "iter 1721/30000  loss         0.138843  avg_L1_norm_grad         0.000244  w[0]    0.000 bias    2.262\n",
      "iter 1740/30000  loss         0.138551  avg_L1_norm_grad         0.000242  w[0]    0.000 bias    2.276\n",
      "iter 1741/30000  loss         0.138536  avg_L1_norm_grad         0.000242  w[0]    0.000 bias    2.277\n",
      "iter 1760/30000  loss         0.138249  avg_L1_norm_grad         0.000240  w[0]    0.000 bias    2.291\n",
      "iter 1761/30000  loss         0.138234  avg_L1_norm_grad         0.000240  w[0]    0.000 bias    2.292\n",
      "iter 1780/30000  loss         0.137951  avg_L1_norm_grad         0.000238  w[0]    0.000 bias    2.306\n",
      "iter 1781/30000  loss         0.137937  avg_L1_norm_grad         0.000238  w[0]    0.000 bias    2.307\n",
      "iter 1800/30000  loss         0.137658  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    2.321\n",
      "iter 1801/30000  loss         0.137643  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    2.322\n",
      "iter 1820/30000  loss         0.137369  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    2.335\n",
      "iter 1821/30000  loss         0.137354  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    2.336\n",
      "iter 1840/30000  loss         0.137084  avg_L1_norm_grad         0.000233  w[0]    0.000 bias    2.350\n",
      "iter 1841/30000  loss         0.137070  avg_L1_norm_grad         0.000233  w[0]    0.000 bias    2.351\n",
      "iter 1860/30000  loss         0.136803  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    2.365\n",
      "iter 1861/30000  loss         0.136789  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    2.365\n",
      "iter 1880/30000  loss         0.136525  avg_L1_norm_grad         0.000229  w[0]    0.000 bias    2.379\n",
      "iter 1881/30000  loss         0.136512  avg_L1_norm_grad         0.000229  w[0]    0.000 bias    2.380\n",
      "iter 1900/30000  loss         0.136252  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.393\n",
      "iter 1901/30000  loss         0.136238  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.394\n",
      "iter 1920/30000  loss         0.135982  avg_L1_norm_grad         0.000226  w[0]    0.000 bias    2.408\n",
      "iter 1921/30000  loss         0.135969  avg_L1_norm_grad         0.000226  w[0]    0.000 bias    2.408\n",
      "iter 1940/30000  loss         0.135716  avg_L1_norm_grad         0.000225  w[0]    0.000 bias    2.422\n",
      "iter 1941/30000  loss         0.135703  avg_L1_norm_grad         0.000225  w[0]    0.000 bias    2.423\n",
      "iter 1960/30000  loss         0.135454  avg_L1_norm_grad         0.000223  w[0]    0.000 bias    2.436\n",
      "iter 1961/30000  loss         0.135441  avg_L1_norm_grad         0.000223  w[0]    0.000 bias    2.437\n",
      "iter 1980/30000  loss         0.135195  avg_L1_norm_grad         0.000221  w[0]    0.000 bias    2.450\n",
      "iter 1981/30000  loss         0.135182  avg_L1_norm_grad         0.000221  w[0]    0.000 bias    2.451\n",
      "iter 2000/30000  loss         0.134939  avg_L1_norm_grad         0.000220  w[0]    0.000 bias    2.464\n",
      "iter 2001/30000  loss         0.134926  avg_L1_norm_grad         0.000220  w[0]    0.000 bias    2.465\n",
      "iter 2020/30000  loss         0.134686  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.478\n",
      "iter 2021/30000  loss         0.134674  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.479\n",
      "iter 2040/30000  loss         0.134437  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    2.492\n",
      "iter 2041/30000  loss         0.134425  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    2.493\n",
      "iter 2060/30000  loss         0.134191  avg_L1_norm_grad         0.000216  w[0]    0.000 bias    2.506\n",
      "iter 2061/30000  loss         0.134179  avg_L1_norm_grad         0.000215  w[0]    0.000 bias    2.506\n",
      "iter 2080/30000  loss         0.133948  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    2.520\n",
      "iter 2081/30000  loss         0.133936  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    2.520\n",
      "iter 2100/30000  loss         0.133708  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.533\n",
      "iter 2101/30000  loss         0.133696  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.534\n",
      "iter 2120/30000  loss         0.133471  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.547\n",
      "iter 2121/30000  loss         0.133459  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.547\n",
      "iter 2140/30000  loss         0.133237  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    2.560\n",
      "iter 2141/30000  loss         0.133225  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    2.561\n",
      "iter 2160/30000  loss         0.133006  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    2.574\n",
      "iter 2161/30000  loss         0.132994  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    2.575\n",
      "iter 2180/30000  loss         0.132777  avg_L1_norm_grad         0.000207  w[0]    0.000 bias    2.587\n",
      "iter 2181/30000  loss         0.132766  avg_L1_norm_grad         0.000207  w[0]    0.000 bias    2.588\n",
      "iter 2200/30000  loss         0.132551  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    2.601\n",
      "iter 2201/30000  loss         0.132540  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    2.601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.132328  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    2.614\n",
      "iter 2221/30000  loss         0.132317  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    2.615\n",
      "iter 2240/30000  loss         0.132107  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    2.627\n",
      "iter 2241/30000  loss         0.132096  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    2.628\n",
      "iter 2260/30000  loss         0.131889  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    2.640\n",
      "iter 2261/30000  loss         0.131878  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    2.641\n",
      "iter 2280/30000  loss         0.131673  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    2.654\n",
      "iter 2281/30000  loss         0.131663  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    2.654\n",
      "iter 2300/30000  loss         0.131460  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.667\n",
      "iter 2301/30000  loss         0.131449  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.667\n",
      "iter 2320/30000  loss         0.131249  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    2.680\n",
      "iter 2321/30000  loss         0.131239  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    2.680\n",
      "iter 2340/30000  loss         0.131041  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    2.693\n",
      "iter 2341/30000  loss         0.131030  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    2.693\n",
      "iter 2360/30000  loss         0.130834  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    2.705\n",
      "iter 2361/30000  loss         0.130824  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    2.706\n",
      "iter 2380/30000  loss         0.130630  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    2.718\n",
      "iter 2381/30000  loss         0.130620  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    2.719\n",
      "iter 2400/30000  loss         0.130428  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.731\n",
      "iter 2401/30000  loss         0.130418  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.732\n",
      "iter 2420/30000  loss         0.130229  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.744\n",
      "iter 2421/30000  loss         0.130219  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.745\n",
      "iter 2440/30000  loss         0.130031  avg_L1_norm_grad         0.000192  w[0]    0.000 bias    2.757\n",
      "iter 2441/30000  loss         0.130021  avg_L1_norm_grad         0.000192  w[0]    0.000 bias    2.757\n",
      "iter 2460/30000  loss         0.129836  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.769\n",
      "iter 2461/30000  loss         0.129826  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.770\n",
      "iter 2480/30000  loss         0.129642  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.782\n",
      "iter 2481/30000  loss         0.129632  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.782\n",
      "iter 2500/30000  loss         0.129451  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.794\n",
      "iter 2501/30000  loss         0.129441  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.795\n",
      "iter 2520/30000  loss         0.129261  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.807\n",
      "iter 2521/30000  loss         0.129252  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.807\n",
      "iter 2540/30000  loss         0.129074  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.819\n",
      "iter 2541/30000  loss         0.129064  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.820\n",
      "iter 2560/30000  loss         0.128888  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.832\n",
      "iter 2561/30000  loss         0.128879  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.832\n",
      "iter 2580/30000  loss         0.128704  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.844\n",
      "iter 2581/30000  loss         0.128695  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.845\n",
      "iter 2600/30000  loss         0.128522  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.856\n",
      "iter 2601/30000  loss         0.128513  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.857\n",
      "iter 2620/30000  loss         0.128342  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.869\n",
      "iter 2621/30000  loss         0.128333  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.869\n",
      "iter 2640/30000  loss         0.128163  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.881\n",
      "iter 2641/30000  loss         0.128154  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.881\n",
      "iter 2660/30000  loss         0.127987  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.893\n",
      "iter 2661/30000  loss         0.127978  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.893\n",
      "iter 2680/30000  loss         0.127812  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    2.905\n",
      "iter 2681/30000  loss         0.127803  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    2.906\n",
      "iter 2700/30000  loss         0.127638  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    2.917\n",
      "iter 2701/30000  loss         0.127630  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    2.918\n",
      "iter 2720/30000  loss         0.127466  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.929\n",
      "iter 2721/30000  loss         0.127458  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.930\n",
      "iter 2740/30000  loss         0.127296  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.941\n",
      "iter 2741/30000  loss         0.127288  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.942\n",
      "iter 2760/30000  loss         0.127128  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    2.953\n",
      "iter 2761/30000  loss         0.127119  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    2.953\n",
      "iter 2780/30000  loss         0.126961  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    2.965\n",
      "iter 2781/30000  loss         0.126953  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    2.965\n",
      "iter 2800/30000  loss         0.126795  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    2.977\n",
      "iter 2801/30000  loss         0.126787  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    2.977\n",
      "iter 2820/30000  loss         0.126631  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    2.988\n",
      "iter 2821/30000  loss         0.126623  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    2.989\n",
      "iter 2840/30000  loss         0.126469  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.000\n",
      "iter 2841/30000  loss         0.126461  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.001\n",
      "iter 2860/30000  loss         0.126308  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.012\n",
      "iter 2861/30000  loss         0.126300  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.012\n",
      "iter 2880/30000  loss         0.126149  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    3.023\n",
      "iter 2881/30000  loss         0.126141  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    3.024\n",
      "iter 2900/30000  loss         0.125990  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.035\n",
      "iter 2901/30000  loss         0.125983  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.036\n",
      "iter 2920/30000  loss         0.125834  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.047\n",
      "iter 2921/30000  loss         0.125826  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.047\n",
      "iter 2940/30000  loss         0.125678  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.058\n",
      "iter 2941/30000  loss         0.125671  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.059\n",
      "iter 2960/30000  loss         0.125524  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    3.070\n",
      "iter 2961/30000  loss         0.125517  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    3.070\n",
      "iter 2980/30000  loss         0.125372  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.081\n",
      "iter 2981/30000  loss         0.125364  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.082\n",
      "iter 3000/30000  loss         0.125220  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.092\n",
      "iter 3001/30000  loss         0.125213  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.125070  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    3.104\n",
      "iter 3021/30000  loss         0.125063  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    3.104\n",
      "iter 3040/30000  loss         0.124922  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.115\n",
      "iter 3041/30000  loss         0.124914  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.116\n",
      "iter 3060/30000  loss         0.124774  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.126\n",
      "iter 3061/30000  loss         0.124767  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.127\n",
      "iter 3080/30000  loss         0.124628  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.138\n",
      "iter 3081/30000  loss         0.124621  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.138\n",
      "iter 3100/30000  loss         0.124483  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    3.149\n",
      "iter 3101/30000  loss         0.124476  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    3.149\n",
      "iter 3120/30000  loss         0.124339  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.160\n",
      "iter 3121/30000  loss         0.124332  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.161\n",
      "iter 3140/30000  loss         0.124196  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.171\n",
      "iter 3141/30000  loss         0.124189  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.172\n",
      "iter 3160/30000  loss         0.124055  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.182\n",
      "iter 3161/30000  loss         0.124048  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.183\n",
      "iter 3180/30000  loss         0.123914  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.193\n",
      "iter 3181/30000  loss         0.123907  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.194\n",
      "iter 3200/30000  loss         0.123775  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.204\n",
      "iter 3201/30000  loss         0.123768  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.205\n",
      "iter 3220/30000  loss         0.123637  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.215\n",
      "iter 3221/30000  loss         0.123630  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.216\n",
      "iter 3240/30000  loss         0.123500  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.226\n",
      "iter 3241/30000  loss         0.123493  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.227\n",
      "iter 3260/30000  loss         0.123364  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.237\n",
      "iter 3261/30000  loss         0.123357  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.238\n",
      "iter 3280/30000  loss         0.123229  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.248\n",
      "iter 3281/30000  loss         0.123222  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.248\n",
      "iter 3300/30000  loss         0.123095  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.259\n",
      "iter 3301/30000  loss         0.123089  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.259\n",
      "iter 3320/30000  loss         0.122963  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.270\n",
      "iter 3321/30000  loss         0.122956  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.270\n",
      "iter 3340/30000  loss         0.122831  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.280\n",
      "iter 3341/30000  loss         0.122824  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.281\n",
      "iter 3360/30000  loss         0.122700  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.291\n",
      "iter 3361/30000  loss         0.122694  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.292\n",
      "iter 3380/30000  loss         0.122570  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.302\n",
      "iter 3381/30000  loss         0.122564  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.302\n",
      "iter 3400/30000  loss         0.122442  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.312\n",
      "iter 3401/30000  loss         0.122435  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.313\n",
      "iter 3420/30000  loss         0.122314  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.323\n",
      "iter 3421/30000  loss         0.122308  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.323\n",
      "iter 3440/30000  loss         0.122187  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.334\n",
      "iter 3441/30000  loss         0.122181  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.334\n",
      "iter 3460/30000  loss         0.122061  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.344\n",
      "iter 3461/30000  loss         0.122055  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.345\n",
      "iter 3480/30000  loss         0.121936  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.355\n",
      "iter 3481/30000  loss         0.121930  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.355\n",
      "iter 3500/30000  loss         0.121812  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.365\n",
      "iter 3501/30000  loss         0.121806  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.366\n",
      "iter 3520/30000  loss         0.121689  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.376\n",
      "iter 3521/30000  loss         0.121683  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.376\n",
      "iter 3540/30000  loss         0.121567  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.386\n",
      "iter 3541/30000  loss         0.121561  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.386\n",
      "iter 3560/30000  loss         0.121446  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.396\n",
      "iter 3561/30000  loss         0.121440  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.397\n",
      "iter 3580/30000  loss         0.121326  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.407\n",
      "iter 3581/30000  loss         0.121320  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.407\n",
      "iter 3600/30000  loss         0.121206  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.417\n",
      "iter 3601/30000  loss         0.121200  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.417\n",
      "iter 3620/30000  loss         0.121087  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.427\n",
      "iter 3621/30000  loss         0.121081  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.428\n",
      "iter 3640/30000  loss         0.120970  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.438\n",
      "iter 3641/30000  loss         0.120964  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.438\n",
      "iter 3660/30000  loss         0.120853  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.448\n",
      "iter 3661/30000  loss         0.120847  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.448\n",
      "iter 3680/30000  loss         0.120736  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.458\n",
      "iter 3681/30000  loss         0.120731  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.458\n",
      "iter 3700/30000  loss         0.120621  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.468\n",
      "iter 3701/30000  loss         0.120615  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.469\n",
      "iter 3720/30000  loss         0.120507  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.478\n",
      "iter 3721/30000  loss         0.120501  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.479\n",
      "iter 3740/30000  loss         0.120393  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.488\n",
      "iter 3741/30000  loss         0.120387  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.489\n",
      "iter 3760/30000  loss         0.120280  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.498\n",
      "iter 3761/30000  loss         0.120274  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.499\n",
      "iter 3780/30000  loss         0.120168  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.508\n",
      "iter 3781/30000  loss         0.120162  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.509\n",
      "iter 3800/30000  loss         0.120056  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.518\n",
      "iter 3801/30000  loss         0.120051  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.119946  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.528\n",
      "iter 3821/30000  loss         0.119940  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.529\n",
      "iter 3840/30000  loss         0.119836  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.538\n",
      "iter 3841/30000  loss         0.119830  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.539\n",
      "iter 3860/30000  loss         0.119727  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.548\n",
      "iter 3861/30000  loss         0.119721  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.549\n",
      "iter 3880/30000  loss         0.119618  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.558\n",
      "iter 3881/30000  loss         0.119613  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.559\n",
      "iter 3900/30000  loss         0.119511  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.568\n",
      "iter 3901/30000  loss         0.119505  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.568\n",
      "iter 3920/30000  loss         0.119404  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.578\n",
      "iter 3921/30000  loss         0.119398  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.578\n",
      "iter 3940/30000  loss         0.119297  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.588\n",
      "iter 3941/30000  loss         0.119292  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.588\n",
      "iter 3960/30000  loss         0.119192  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.597\n",
      "iter 3961/30000  loss         0.119187  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.598\n",
      "iter 3980/30000  loss         0.119087  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.607\n",
      "iter 3981/30000  loss         0.119082  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.608\n",
      "iter 4000/30000  loss         0.118983  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.617\n",
      "iter 4001/30000  loss         0.118978  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.617\n",
      "iter 4020/30000  loss         0.118879  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.627\n",
      "iter 4021/30000  loss         0.118874  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.627\n",
      "iter 4040/30000  loss         0.118777  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.636\n",
      "iter 4041/30000  loss         0.118772  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.637\n",
      "iter 4060/30000  loss         0.118675  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.646\n",
      "iter 4061/30000  loss         0.118669  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.646\n",
      "iter 4080/30000  loss         0.118573  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.655\n",
      "iter 4081/30000  loss         0.118568  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.656\n",
      "iter 4100/30000  loss         0.118472  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.665\n",
      "iter 4101/30000  loss         0.118467  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.665\n",
      "iter 4120/30000  loss         0.118372  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.675\n",
      "iter 4121/30000  loss         0.118367  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.675\n",
      "iter 4140/30000  loss         0.118272  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.684\n",
      "iter 4141/30000  loss         0.118268  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.685\n",
      "iter 4160/30000  loss         0.118174  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.694\n",
      "iter 4161/30000  loss         0.118169  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.694\n",
      "iter 4180/30000  loss         0.118075  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.703\n",
      "iter 4181/30000  loss         0.118070  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.703\n",
      "iter 4200/30000  loss         0.117978  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.712\n",
      "iter 4201/30000  loss         0.117973  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.713\n",
      "iter 4220/30000  loss         0.117881  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.722\n",
      "iter 4221/30000  loss         0.117876  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.722\n",
      "iter 4240/30000  loss         0.117784  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.731\n",
      "iter 4241/30000  loss         0.117779  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.732\n",
      "iter 4260/30000  loss         0.117688  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.741\n",
      "iter 4261/30000  loss         0.117683  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.741\n",
      "iter 4280/30000  loss         0.117593  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.750\n",
      "iter 4281/30000  loss         0.117588  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.750\n",
      "iter 4300/30000  loss         0.117498  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.759\n",
      "iter 4301/30000  loss         0.117494  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.760\n",
      "iter 4320/30000  loss         0.117404  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.769\n",
      "iter 4321/30000  loss         0.117399  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.769\n",
      "iter 4340/30000  loss         0.117311  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.778\n",
      "iter 4341/30000  loss         0.117306  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.778\n",
      "iter 4360/30000  loss         0.117218  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.787\n",
      "iter 4361/30000  loss         0.117213  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.788\n",
      "iter 4380/30000  loss         0.117125  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.796\n",
      "iter 4381/30000  loss         0.117121  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.797\n",
      "iter 4400/30000  loss         0.117033  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.805\n",
      "iter 4401/30000  loss         0.117029  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.806\n",
      "iter 4420/30000  loss         0.116942  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.815\n",
      "iter 4421/30000  loss         0.116938  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.815\n",
      "iter 4440/30000  loss         0.116851  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.824\n",
      "iter 4441/30000  loss         0.116847  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.824\n",
      "iter 4460/30000  loss         0.116761  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.833\n",
      "iter 4461/30000  loss         0.116757  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.833\n",
      "iter 4480/30000  loss         0.116671  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.842\n",
      "iter 4481/30000  loss         0.116667  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.842\n",
      "iter 4500/30000  loss         0.116582  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.851\n",
      "iter 4501/30000  loss         0.116578  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.851\n",
      "iter 4520/30000  loss         0.116494  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.860\n",
      "iter 4521/30000  loss         0.116489  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.860\n",
      "iter 4540/30000  loss         0.116406  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.869\n",
      "iter 4541/30000  loss         0.116401  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.869\n",
      "iter 4560/30000  loss         0.116318  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.878\n",
      "iter 4561/30000  loss         0.116314  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.878\n",
      "iter 4580/30000  loss         0.116231  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.887\n",
      "iter 4581/30000  loss         0.116227  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.887\n",
      "iter 4600/30000  loss         0.116144  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.896\n",
      "iter 4601/30000  loss         0.116140  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4620/30000  loss         0.116058  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.905\n",
      "iter 4621/30000  loss         0.116054  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.905\n",
      "iter 4640/30000  loss         0.115973  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.914\n",
      "iter 4641/30000  loss         0.115968  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.914\n",
      "iter 4660/30000  loss         0.115888  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.923\n",
      "iter 4661/30000  loss         0.115883  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.923\n",
      "iter 4680/30000  loss         0.115803  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.931\n",
      "iter 4681/30000  loss         0.115799  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.932\n",
      "iter 4700/30000  loss         0.115719  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.940\n",
      "iter 4701/30000  loss         0.115715  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.941\n",
      "iter 4720/30000  loss         0.115635  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.949\n",
      "iter 4721/30000  loss         0.115631  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.949\n",
      "iter 4740/30000  loss         0.115552  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.958\n",
      "iter 4741/30000  loss         0.115548  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.958\n",
      "iter 4760/30000  loss         0.115469  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.967\n",
      "iter 4761/30000  loss         0.115465  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.967\n",
      "iter 4780/30000  loss         0.115387  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.975\n",
      "iter 4781/30000  loss         0.115383  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.976\n",
      "iter 4800/30000  loss         0.115305  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.984\n",
      "iter 4801/30000  loss         0.115301  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.984\n",
      "iter 4820/30000  loss         0.115224  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.993\n",
      "iter 4821/30000  loss         0.115220  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.993\n",
      "iter 4840/30000  loss         0.115143  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.001\n",
      "iter 4841/30000  loss         0.115139  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.002\n",
      "iter 4860/30000  loss         0.115063  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.010\n",
      "iter 4861/30000  loss         0.115059  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    4.010\n",
      "iter 4880/30000  loss         0.114983  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.019\n",
      "iter 4881/30000  loss         0.114979  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.019\n",
      "iter 4900/30000  loss         0.114903  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.027\n",
      "iter 4901/30000  loss         0.114899  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.028\n",
      "iter 4920/30000  loss         0.114824  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.036\n",
      "iter 4921/30000  loss         0.114820  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    4.036\n",
      "iter 4940/30000  loss         0.114746  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.044\n",
      "iter 4941/30000  loss         0.114742  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.045\n",
      "iter 4960/30000  loss         0.114667  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.053\n",
      "iter 4961/30000  loss         0.114664  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.053\n",
      "iter 4980/30000  loss         0.114590  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.061\n",
      "iter 4981/30000  loss         0.114586  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    4.062\n",
      "iter 5000/30000  loss         0.114512  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.070\n",
      "iter 5001/30000  loss         0.114508  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.070\n",
      "iter 5020/30000  loss         0.114435  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.078\n",
      "iter 5021/30000  loss         0.114431  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.079\n",
      "iter 5040/30000  loss         0.114359  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.087\n",
      "iter 5041/30000  loss         0.114355  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    4.087\n",
      "iter 5060/30000  loss         0.114283  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.095\n",
      "iter 5061/30000  loss         0.114279  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.096\n",
      "iter 5080/30000  loss         0.114207  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.104\n",
      "iter 5081/30000  loss         0.114203  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.104\n",
      "iter 5100/30000  loss         0.114132  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.112\n",
      "iter 5101/30000  loss         0.114128  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    4.112\n",
      "iter 5120/30000  loss         0.114057  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.120\n",
      "iter 5121/30000  loss         0.114053  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.121\n",
      "iter 5140/30000  loss         0.113982  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.129\n",
      "iter 5141/30000  loss         0.113978  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.129\n",
      "iter 5160/30000  loss         0.113908  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.137\n",
      "iter 5161/30000  loss         0.113904  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    4.137\n",
      "iter 5180/30000  loss         0.113834  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.145\n",
      "iter 5181/30000  loss         0.113830  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.146\n",
      "iter 5200/30000  loss         0.113761  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.154\n",
      "iter 5201/30000  loss         0.113757  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.154\n",
      "iter 5220/30000  loss         0.113688  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.162\n",
      "iter 5221/30000  loss         0.113684  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    4.162\n",
      "iter 5240/30000  loss         0.113615  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.170\n",
      "iter 5241/30000  loss         0.113612  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.171\n",
      "iter 5260/30000  loss         0.113543  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.178\n",
      "iter 5261/30000  loss         0.113539  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.179\n",
      "iter 5280/30000  loss         0.113471  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.187\n",
      "iter 5281/30000  loss         0.113468  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    4.187\n",
      "iter 5300/30000  loss         0.113400  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.195\n",
      "iter 5301/30000  loss         0.113396  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.195\n",
      "iter 5320/30000  loss         0.113329  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.203\n",
      "iter 5321/30000  loss         0.113325  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.203\n",
      "iter 5340/30000  loss         0.113258  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.211\n",
      "iter 5341/30000  loss         0.113254  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.211\n",
      "iter 5360/30000  loss         0.113187  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.219\n",
      "iter 5361/30000  loss         0.113184  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    4.220\n",
      "iter 5380/30000  loss         0.113117  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.227\n",
      "iter 5381/30000  loss         0.113114  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.228\n",
      "iter 5400/30000  loss         0.113048  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.235\n",
      "iter 5401/30000  loss         0.113044  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5420/30000  loss         0.112978  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.243\n",
      "iter 5421/30000  loss         0.112975  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    4.244\n",
      "iter 5440/30000  loss         0.112909  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.251\n",
      "iter 5441/30000  loss         0.112906  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.252\n",
      "iter 5460/30000  loss         0.112841  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.260\n",
      "iter 5461/30000  loss         0.112837  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.260\n",
      "iter 5480/30000  loss         0.112772  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.268\n",
      "iter 5481/30000  loss         0.112769  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    4.268\n",
      "iter 5500/30000  loss         0.112705  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.276\n",
      "iter 5501/30000  loss         0.112701  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.276\n",
      "iter 5520/30000  loss         0.112637  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.283\n",
      "iter 5521/30000  loss         0.112634  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.284\n",
      "iter 5540/30000  loss         0.112570  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.291\n",
      "iter 5541/30000  loss         0.112566  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.292\n",
      "iter 5560/30000  loss         0.112503  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.299\n",
      "iter 5561/30000  loss         0.112499  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.300\n",
      "iter 5580/30000  loss         0.112436  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.307\n",
      "iter 5581/30000  loss         0.112433  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.308\n",
      "iter 5600/30000  loss         0.112370  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.315\n",
      "iter 5601/30000  loss         0.112367  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.316\n",
      "iter 5620/30000  loss         0.112304  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.323\n",
      "iter 5621/30000  loss         0.112301  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.323\n",
      "iter 5640/30000  loss         0.112238  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.331\n",
      "iter 5641/30000  loss         0.112235  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.331\n",
      "iter 5660/30000  loss         0.112173  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.339\n",
      "iter 5661/30000  loss         0.112170  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.339\n",
      "iter 5680/30000  loss         0.112108  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.347\n",
      "iter 5681/30000  loss         0.112105  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.347\n",
      "iter 5700/30000  loss         0.112043  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.354\n",
      "iter 5701/30000  loss         0.112040  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.355\n",
      "iter 5720/30000  loss         0.111979  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.362\n",
      "iter 5721/30000  loss         0.111976  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.363\n",
      "iter 5740/30000  loss         0.111915  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.370\n",
      "iter 5741/30000  loss         0.111912  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.370\n",
      "iter 5760/30000  loss         0.111851  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.378\n",
      "iter 5761/30000  loss         0.111848  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.378\n",
      "iter 5780/30000  loss         0.111788  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.385\n",
      "iter 5781/30000  loss         0.111785  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.386\n",
      "iter 5800/30000  loss         0.111725  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.393\n",
      "iter 5801/30000  loss         0.111722  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.394\n",
      "iter 5820/30000  loss         0.111662  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.401\n",
      "iter 5821/30000  loss         0.111659  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.401\n",
      "iter 5840/30000  loss         0.111600  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.409\n",
      "iter 5841/30000  loss         0.111596  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.409\n",
      "iter 5860/30000  loss         0.111537  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.416\n",
      "iter 5861/30000  loss         0.111534  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.417\n",
      "iter 5880/30000  loss         0.111475  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.424\n",
      "iter 5881/30000  loss         0.111472  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.424\n",
      "iter 5900/30000  loss         0.111414  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.431\n",
      "iter 5901/30000  loss         0.111411  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.432\n",
      "iter 5920/30000  loss         0.111352  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.439\n",
      "iter 5921/30000  loss         0.111349  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.439\n",
      "iter 5940/30000  loss         0.111291  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.447\n",
      "iter 5941/30000  loss         0.111288  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.447\n",
      "iter 5960/30000  loss         0.111231  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.454\n",
      "iter 5961/30000  loss         0.111228  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.455\n",
      "iter 5980/30000  loss         0.111170  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.462\n",
      "iter 5981/30000  loss         0.111167  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.462\n",
      "iter 6000/30000  loss         0.111110  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.469\n",
      "iter 6001/30000  loss         0.111107  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.470\n",
      "iter 6020/30000  loss         0.111050  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.477\n",
      "iter 6021/30000  loss         0.111047  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.477\n",
      "iter 6040/30000  loss         0.110991  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.484\n",
      "iter 6041/30000  loss         0.110988  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.485\n",
      "iter 6060/30000  loss         0.110931  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.492\n",
      "iter 6061/30000  loss         0.110928  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.492\n",
      "iter 6080/30000  loss         0.110872  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.499\n",
      "iter 6081/30000  loss         0.110869  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.500\n",
      "iter 6100/30000  loss         0.110813  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.507\n",
      "iter 6101/30000  loss         0.110810  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.507\n",
      "iter 6120/30000  loss         0.110755  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.514\n",
      "iter 6121/30000  loss         0.110752  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.515\n",
      "iter 6140/30000  loss         0.110697  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.522\n",
      "iter 6141/30000  loss         0.110694  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.522\n",
      "iter 6160/30000  loss         0.110639  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.529\n",
      "iter 6161/30000  loss         0.110636  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.529\n",
      "iter 6180/30000  loss         0.110581  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.536\n",
      "iter 6181/30000  loss         0.110578  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.537\n",
      "iter 6200/30000  loss         0.110523  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.544\n",
      "iter 6201/30000  loss         0.110521  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6220/30000  loss         0.110466  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.551\n",
      "iter 6221/30000  loss         0.110463  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.552\n",
      "iter 6240/30000  loss         0.110409  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.559\n",
      "iter 6241/30000  loss         0.110407  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.559\n",
      "iter 6260/30000  loss         0.110353  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.566\n",
      "iter 6261/30000  loss         0.110350  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.566\n",
      "iter 6280/30000  loss         0.110296  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.573\n",
      "iter 6281/30000  loss         0.110293  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.574\n",
      "iter 6300/30000  loss         0.110240  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.580\n",
      "iter 6301/30000  loss         0.110237  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.581\n",
      "iter 6320/30000  loss         0.110184  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.588\n",
      "iter 6321/30000  loss         0.110181  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.588\n",
      "iter 6340/30000  loss         0.110129  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.595\n",
      "iter 6341/30000  loss         0.110126  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.595\n",
      "iter 6360/30000  loss         0.110073  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.602\n",
      "iter 6361/30000  loss         0.110070  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.603\n",
      "iter 6380/30000  loss         0.110018  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.609\n",
      "iter 6381/30000  loss         0.110015  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.610\n",
      "iter 6400/30000  loss         0.109963  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.617\n",
      "iter 6401/30000  loss         0.109960  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.617\n",
      "iter 6420/30000  loss         0.109908  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.624\n",
      "iter 6421/30000  loss         0.109906  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.624\n",
      "iter 6440/30000  loss         0.109854  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.631\n",
      "iter 6441/30000  loss         0.109851  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.631\n",
      "iter 6460/30000  loss         0.109800  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.638\n",
      "iter 6461/30000  loss         0.109797  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.639\n",
      "iter 6480/30000  loss         0.109746  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.645\n",
      "iter 6481/30000  loss         0.109743  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.646\n",
      "iter 6500/30000  loss         0.109692  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.653\n",
      "iter 6501/30000  loss         0.109690  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.653\n",
      "iter 6520/30000  loss         0.109639  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.660\n",
      "iter 6521/30000  loss         0.109636  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.660\n",
      "iter 6540/30000  loss         0.109586  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.667\n",
      "iter 6541/30000  loss         0.109583  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.667\n",
      "iter 6560/30000  loss         0.109533  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.674\n",
      "iter 6561/30000  loss         0.109530  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.674\n",
      "iter 6580/30000  loss         0.109480  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.681\n",
      "iter 6581/30000  loss         0.109477  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.681\n",
      "iter 6600/30000  loss         0.109427  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.688\n",
      "iter 6601/30000  loss         0.109425  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.688\n",
      "iter 6620/30000  loss         0.109375  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.695\n",
      "iter 6621/30000  loss         0.109372  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.695\n",
      "iter 6640/30000  loss         0.109323  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.702\n",
      "iter 6641/30000  loss         0.109320  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.702\n",
      "iter 6660/30000  loss         0.109271  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.709\n",
      "iter 6661/30000  loss         0.109268  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.709\n",
      "iter 6680/30000  loss         0.109219  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.716\n",
      "iter 6681/30000  loss         0.109217  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.716\n",
      "iter 6700/30000  loss         0.109168  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.723\n",
      "iter 6701/30000  loss         0.109165  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.723\n",
      "iter 6720/30000  loss         0.109117  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.730\n",
      "iter 6721/30000  loss         0.109114  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.730\n",
      "iter 6740/30000  loss         0.109066  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.737\n",
      "iter 6741/30000  loss         0.109063  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.737\n",
      "iter 6760/30000  loss         0.109015  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.744\n",
      "iter 6761/30000  loss         0.109013  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.744\n",
      "iter 6780/30000  loss         0.108965  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.751\n",
      "iter 6781/30000  loss         0.108962  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.751\n",
      "iter 6800/30000  loss         0.108914  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.758\n",
      "iter 6801/30000  loss         0.108912  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.758\n",
      "iter 6820/30000  loss         0.108864  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.765\n",
      "iter 6821/30000  loss         0.108862  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.765\n",
      "iter 6840/30000  loss         0.108814  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.772\n",
      "iter 6841/30000  loss         0.108812  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.772\n",
      "iter 6860/30000  loss         0.108765  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.779\n",
      "iter 6861/30000  loss         0.108762  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.779\n",
      "iter 6880/30000  loss         0.108715  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.785\n",
      "iter 6881/30000  loss         0.108713  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.786\n",
      "iter 6900/30000  loss         0.108666  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.792\n",
      "iter 6901/30000  loss         0.108664  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.793\n",
      "iter 6920/30000  loss         0.108617  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.799\n",
      "iter 6921/30000  loss         0.108615  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.799\n",
      "iter 6940/30000  loss         0.108568  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.806\n",
      "iter 6941/30000  loss         0.108566  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.806\n",
      "iter 6960/30000  loss         0.108520  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.813\n",
      "iter 6961/30000  loss         0.108517  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.813\n",
      "iter 6980/30000  loss         0.108471  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.819\n",
      "iter 6981/30000  loss         0.108469  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.820\n",
      "iter 7000/30000  loss         0.108423  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.826\n",
      "iter 7001/30000  loss         0.108421  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7020/30000  loss         0.108375  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.833\n",
      "iter 7021/30000  loss         0.108373  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.833\n",
      "iter 7040/30000  loss         0.108327  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.840\n",
      "iter 7041/30000  loss         0.108325  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.840\n",
      "iter 7060/30000  loss         0.108280  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.846\n",
      "iter 7061/30000  loss         0.108277  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.847\n",
      "iter 7080/30000  loss         0.108232  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.853\n",
      "iter 7081/30000  loss         0.108230  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.854\n",
      "iter 7100/30000  loss         0.108185  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.860\n",
      "iter 7101/30000  loss         0.108183  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.860\n",
      "iter 7120/30000  loss         0.108138  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.867\n",
      "iter 7121/30000  loss         0.108136  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.867\n",
      "iter 7140/30000  loss         0.108091  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.873\n",
      "iter 7141/30000  loss         0.108089  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.874\n",
      "iter 7160/30000  loss         0.108044  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.880\n",
      "iter 7161/30000  loss         0.108042  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.880\n",
      "iter 7180/30000  loss         0.107998  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.887\n",
      "iter 7181/30000  loss         0.107996  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.887\n",
      "iter 7200/30000  loss         0.107952  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.893\n",
      "iter 7201/30000  loss         0.107950  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.894\n",
      "iter 7220/30000  loss         0.107906  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.900\n",
      "iter 7221/30000  loss         0.107903  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.900\n",
      "iter 7240/30000  loss         0.107860  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.906\n",
      "iter 7241/30000  loss         0.107858  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.907\n",
      "iter 7260/30000  loss         0.107814  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.913\n",
      "iter 7261/30000  loss         0.107812  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.913\n",
      "iter 7280/30000  loss         0.107769  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.920\n",
      "iter 7281/30000  loss         0.107766  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.920\n",
      "iter 7300/30000  loss         0.107723  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.926\n",
      "iter 7301/30000  loss         0.107721  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.927\n",
      "iter 7320/30000  loss         0.107678  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.933\n",
      "iter 7321/30000  loss         0.107676  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.933\n",
      "iter 7340/30000  loss         0.107633  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.939\n",
      "iter 7341/30000  loss         0.107631  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.940\n",
      "iter 7360/30000  loss         0.107589  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.946\n",
      "iter 7361/30000  loss         0.107586  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.946\n",
      "iter 7380/30000  loss         0.107544  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.952\n",
      "iter 7381/30000  loss         0.107542  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.953\n",
      "iter 7400/30000  loss         0.107500  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.959\n",
      "iter 7401/30000  loss         0.107497  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.959\n",
      "iter 7420/30000  loss         0.107455  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.965\n",
      "iter 7421/30000  loss         0.107453  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.966\n",
      "iter 7440/30000  loss         0.107411  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.972\n",
      "iter 7441/30000  loss         0.107409  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.972\n",
      "iter 7460/30000  loss         0.107368  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.978\n",
      "iter 7461/30000  loss         0.107365  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.979\n",
      "iter 7480/30000  loss         0.107324  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.985\n",
      "iter 7481/30000  loss         0.107322  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.985\n",
      "iter 7500/30000  loss         0.107280  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.991\n",
      "iter 7501/30000  loss         0.107278  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.992\n",
      "iter 7520/30000  loss         0.107237  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.998\n",
      "iter 7521/30000  loss         0.107235  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.998\n",
      "iter 7540/30000  loss         0.107194  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.004\n",
      "iter 7541/30000  loss         0.107192  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.004\n",
      "iter 7560/30000  loss         0.107151  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.010\n",
      "iter 7561/30000  loss         0.107149  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    5.011\n",
      "iter 7580/30000  loss         0.107108  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.017\n",
      "iter 7581/30000  loss         0.107106  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.017\n",
      "iter 7600/30000  loss         0.107065  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.023\n",
      "iter 7601/30000  loss         0.107063  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.024\n",
      "iter 7620/30000  loss         0.107023  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.030\n",
      "iter 7621/30000  loss         0.107021  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.030\n",
      "iter 7640/30000  loss         0.106981  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.036\n",
      "iter 7641/30000  loss         0.106979  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.036\n",
      "iter 7660/30000  loss         0.106939  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.042\n",
      "iter 7661/30000  loss         0.106936  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.043\n",
      "iter 7680/30000  loss         0.106897  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.049\n",
      "iter 7681/30000  loss         0.106894  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    5.049\n",
      "iter 7700/30000  loss         0.106855  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.055\n",
      "iter 7701/30000  loss         0.106853  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.055\n",
      "iter 7720/30000  loss         0.106813  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.061\n",
      "iter 7721/30000  loss         0.106811  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.062\n",
      "iter 7740/30000  loss         0.106772  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.068\n",
      "iter 7741/30000  loss         0.106770  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.068\n",
      "iter 7760/30000  loss         0.106730  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.074\n",
      "iter 7761/30000  loss         0.106728  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.074\n",
      "iter 7780/30000  loss         0.106689  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.080\n",
      "iter 7781/30000  loss         0.106687  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.080\n",
      "iter 7800/30000  loss         0.106648  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.086\n",
      "iter 7801/30000  loss         0.106646  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    5.087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7820/30000  loss         0.106607  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.093\n",
      "iter 7821/30000  loss         0.106605  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.093\n",
      "iter 7840/30000  loss         0.106567  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.099\n",
      "iter 7841/30000  loss         0.106565  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.099\n",
      "iter 7860/30000  loss         0.106526  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.105\n",
      "iter 7861/30000  loss         0.106524  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.105\n",
      "iter 7880/30000  loss         0.106486  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.111\n",
      "iter 7881/30000  loss         0.106484  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.112\n",
      "iter 7900/30000  loss         0.106446  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.117\n",
      "iter 7901/30000  loss         0.106444  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.118\n",
      "iter 7920/30000  loss         0.106406  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.124\n",
      "iter 7921/30000  loss         0.106404  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    5.124\n",
      "iter 7940/30000  loss         0.106366  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.130\n",
      "iter 7941/30000  loss         0.106364  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.130\n",
      "iter 7960/30000  loss         0.106326  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.136\n",
      "iter 7961/30000  loss         0.106324  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.136\n",
      "iter 7980/30000  loss         0.106286  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.142\n",
      "iter 7981/30000  loss         0.106284  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.142\n",
      "iter 8000/30000  loss         0.106247  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.148\n",
      "iter 8001/30000  loss         0.106245  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.149\n",
      "iter 8020/30000  loss         0.106208  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.154\n",
      "iter 8021/30000  loss         0.106206  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.155\n",
      "iter 8040/30000  loss         0.106168  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.160\n",
      "iter 8041/30000  loss         0.106167  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.161\n",
      "iter 8060/30000  loss         0.106129  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.167\n",
      "iter 8061/30000  loss         0.106128  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    5.167\n",
      "iter 8080/30000  loss         0.106091  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.173\n",
      "iter 8081/30000  loss         0.106089  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.173\n",
      "iter 8100/30000  loss         0.106052  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.179\n",
      "iter 8101/30000  loss         0.106050  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.179\n",
      "iter 8120/30000  loss         0.106013  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.185\n",
      "iter 8121/30000  loss         0.106012  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.185\n",
      "iter 8140/30000  loss         0.105975  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.191\n",
      "iter 8141/30000  loss         0.105973  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.191\n",
      "iter 8160/30000  loss         0.105937  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.197\n",
      "iter 8161/30000  loss         0.105935  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.197\n",
      "iter 8180/30000  loss         0.105899  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.203\n",
      "iter 8181/30000  loss         0.105897  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    5.203\n",
      "iter 8200/30000  loss         0.105861  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.209\n",
      "iter 8201/30000  loss         0.105859  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.209\n",
      "iter 8220/30000  loss         0.105823  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.215\n",
      "iter 8221/30000  loss         0.105821  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.215\n",
      "iter 8240/30000  loss         0.105785  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.221\n",
      "iter 8241/30000  loss         0.105783  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.221\n",
      "iter 8260/30000  loss         0.105748  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.227\n",
      "iter 8261/30000  loss         0.105746  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.227\n",
      "iter 8280/30000  loss         0.105710  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.233\n",
      "iter 8281/30000  loss         0.105708  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.233\n",
      "iter 8300/30000  loss         0.105673  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.239\n",
      "iter 8301/30000  loss         0.105671  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.239\n",
      "iter 8320/30000  loss         0.105636  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.245\n",
      "iter 8321/30000  loss         0.105634  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    5.245\n",
      "iter 8340/30000  loss         0.105599  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.251\n",
      "iter 8341/30000  loss         0.105597  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.251\n",
      "iter 8360/30000  loss         0.105562  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.257\n",
      "iter 8361/30000  loss         0.105560  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.257\n",
      "iter 8380/30000  loss         0.105525  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.263\n",
      "iter 8381/30000  loss         0.105524  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.263\n",
      "iter 8400/30000  loss         0.105489  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.269\n",
      "iter 8401/30000  loss         0.105487  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.269\n",
      "iter 8420/30000  loss         0.105453  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.274\n",
      "iter 8421/30000  loss         0.105451  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.275\n",
      "iter 8440/30000  loss         0.105416  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.280\n",
      "iter 8441/30000  loss         0.105414  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.281\n",
      "iter 8460/30000  loss         0.105380  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.286\n",
      "iter 8461/30000  loss         0.105378  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    5.287\n",
      "iter 8480/30000  loss         0.105344  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.292\n",
      "iter 8481/30000  loss         0.105342  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.292\n",
      "iter 8500/30000  loss         0.105308  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.298\n",
      "iter 8501/30000  loss         0.105306  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.298\n",
      "iter 8520/30000  loss         0.105272  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.304\n",
      "iter 8521/30000  loss         0.105271  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.304\n",
      "iter 8540/30000  loss         0.105237  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.310\n",
      "iter 8541/30000  loss         0.105235  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.310\n",
      "iter 8560/30000  loss         0.105201  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.315\n",
      "iter 8561/30000  loss         0.105200  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.316\n",
      "iter 8580/30000  loss         0.105166  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.321\n",
      "iter 8581/30000  loss         0.105164  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.322\n",
      "iter 8600/30000  loss         0.105131  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.327\n",
      "iter 8601/30000  loss         0.105129  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    5.327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8620/30000  loss         0.105096  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.333\n",
      "iter 8621/30000  loss         0.105094  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.333\n",
      "iter 8640/30000  loss         0.105061  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.339\n",
      "iter 8641/30000  loss         0.105059  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.339\n",
      "iter 8660/30000  loss         0.105026  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.344\n",
      "iter 8661/30000  loss         0.105024  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.345\n",
      "iter 8680/30000  loss         0.104991  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.350\n",
      "iter 8681/30000  loss         0.104989  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.350\n",
      "iter 8700/30000  loss         0.104957  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.356\n",
      "iter 8701/30000  loss         0.104955  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.356\n",
      "iter 8720/30000  loss         0.104922  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.362\n",
      "iter 8721/30000  loss         0.104920  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.362\n",
      "iter 8740/30000  loss         0.104888  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.367\n",
      "iter 8741/30000  loss         0.104886  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    5.368\n",
      "iter 8760/30000  loss         0.104854  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.373\n",
      "iter 8761/30000  loss         0.104852  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.373\n",
      "iter 8780/30000  loss         0.104819  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.379\n",
      "iter 8781/30000  loss         0.104818  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.379\n",
      "iter 8800/30000  loss         0.104785  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.384\n",
      "iter 8801/30000  loss         0.104784  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.385\n",
      "iter 8820/30000  loss         0.104752  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.390\n",
      "iter 8821/30000  loss         0.104750  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.390\n",
      "iter 8840/30000  loss         0.104718  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.396\n",
      "iter 8841/30000  loss         0.104716  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.396\n",
      "iter 8860/30000  loss         0.104684  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.401\n",
      "iter 8861/30000  loss         0.104683  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.402\n",
      "iter 8880/30000  loss         0.104651  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.407\n",
      "iter 8881/30000  loss         0.104649  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    5.407\n",
      "iter 8900/30000  loss         0.104617  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.413\n",
      "iter 8901/30000  loss         0.104616  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.413\n",
      "iter 8920/30000  loss         0.104584  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.418\n",
      "iter 8921/30000  loss         0.104583  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.419\n",
      "iter 8940/30000  loss         0.104551  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.424\n",
      "iter 8941/30000  loss         0.104549  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.424\n",
      "iter 8960/30000  loss         0.104518  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.430\n",
      "iter 8961/30000  loss         0.104516  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.430\n",
      "iter 8980/30000  loss         0.104485  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.435\n",
      "iter 8981/30000  loss         0.104484  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.436\n",
      "iter 9000/30000  loss         0.104452  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.441\n",
      "iter 9001/30000  loss         0.104451  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.441\n",
      "iter 9020/30000  loss         0.104420  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.446\n",
      "iter 9021/30000  loss         0.104418  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.447\n",
      "iter 9040/30000  loss         0.104387  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.452\n",
      "iter 9041/30000  loss         0.104386  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    5.452\n",
      "iter 9060/30000  loss         0.104355  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.458\n",
      "iter 9061/30000  loss         0.104353  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.458\n",
      "iter 9080/30000  loss         0.104323  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.463\n",
      "iter 9081/30000  loss         0.104321  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.463\n",
      "iter 9100/30000  loss         0.104290  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.469\n",
      "iter 9101/30000  loss         0.104289  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.469\n",
      "iter 9120/30000  loss         0.104258  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.474\n",
      "iter 9121/30000  loss         0.104257  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.474\n",
      "iter 9140/30000  loss         0.104226  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.480\n",
      "iter 9141/30000  loss         0.104225  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.480\n",
      "iter 9160/30000  loss         0.104195  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.485\n",
      "iter 9161/30000  loss         0.104193  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.486\n",
      "iter 9180/30000  loss         0.104163  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.491\n",
      "iter 9181/30000  loss         0.104161  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.491\n",
      "iter 9200/30000  loss         0.104131  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.496\n",
      "iter 9201/30000  loss         0.104130  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    5.496\n",
      "iter 9220/30000  loss         0.104100  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.502\n",
      "iter 9221/30000  loss         0.104098  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.502\n",
      "iter 9240/30000  loss         0.104068  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.507\n",
      "iter 9241/30000  loss         0.104067  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.507\n",
      "iter 9260/30000  loss         0.104037  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.513\n",
      "iter 9261/30000  loss         0.104035  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.513\n",
      "iter 9280/30000  loss         0.104006  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.518\n",
      "iter 9281/30000  loss         0.104004  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.518\n",
      "iter 9300/30000  loss         0.103975  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.524\n",
      "iter 9301/30000  loss         0.103973  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.524\n",
      "iter 9320/30000  loss         0.103944  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.529\n",
      "iter 9321/30000  loss         0.103942  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.529\n",
      "iter 9340/30000  loss         0.103913  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.534\n",
      "iter 9341/30000  loss         0.103911  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.535\n",
      "iter 9360/30000  loss         0.103882  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.540\n",
      "iter 9361/30000  loss         0.103881  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    5.540\n",
      "iter 9380/30000  loss         0.103851  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.545\n",
      "iter 9381/30000  loss         0.103850  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.545\n",
      "iter 9400/30000  loss         0.103821  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.551\n",
      "iter 9401/30000  loss         0.103819  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9420/30000  loss         0.103791  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.556\n",
      "iter 9421/30000  loss         0.103789  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.556\n",
      "iter 9440/30000  loss         0.103760  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.561\n",
      "iter 9441/30000  loss         0.103759  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.562\n",
      "iter 9460/30000  loss         0.103730  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.567\n",
      "iter 9461/30000  loss         0.103728  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.567\n",
      "iter 9480/30000  loss         0.103700  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.572\n",
      "iter 9481/30000  loss         0.103698  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.572\n",
      "iter 9500/30000  loss         0.103670  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.577\n",
      "iter 9501/30000  loss         0.103668  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.578\n",
      "iter 9520/30000  loss         0.103640  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.583\n",
      "iter 9521/30000  loss         0.103638  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    5.583\n",
      "iter 9540/30000  loss         0.103610  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.588\n",
      "iter 9541/30000  loss         0.103609  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.588\n",
      "iter 9560/30000  loss         0.103580  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.593\n",
      "iter 9561/30000  loss         0.103579  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.594\n",
      "iter 9580/30000  loss         0.103551  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.599\n",
      "iter 9581/30000  loss         0.103549  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.599\n",
      "iter 9600/30000  loss         0.103521  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.604\n",
      "iter 9601/30000  loss         0.103520  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.604\n",
      "iter 9620/30000  loss         0.103492  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.609\n",
      "iter 9621/30000  loss         0.103490  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.610\n",
      "iter 9640/30000  loss         0.103463  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.615\n",
      "iter 9641/30000  loss         0.103461  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.615\n",
      "iter 9660/30000  loss         0.103433  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.620\n",
      "iter 9661/30000  loss         0.103432  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.620\n",
      "iter 9680/30000  loss         0.103404  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.625\n",
      "iter 9681/30000  loss         0.103403  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.625\n",
      "iter 9700/30000  loss         0.103375  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.630\n",
      "iter 9701/30000  loss         0.103374  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    5.631\n",
      "iter 9720/30000  loss         0.103346  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.636\n",
      "iter 9721/30000  loss         0.103345  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.636\n",
      "iter 9740/30000  loss         0.103318  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.641\n",
      "iter 9741/30000  loss         0.103316  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.641\n",
      "iter 9760/30000  loss         0.103289  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.646\n",
      "iter 9761/30000  loss         0.103287  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.646\n",
      "iter 9780/30000  loss         0.103260  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.651\n",
      "iter 9781/30000  loss         0.103259  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.652\n",
      "iter 9800/30000  loss         0.103232  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.657\n",
      "iter 9801/30000  loss         0.103230  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.657\n",
      "iter 9820/30000  loss         0.103203  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.662\n",
      "iter 9821/30000  loss         0.103202  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.662\n",
      "iter 9840/30000  loss         0.103175  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.667\n",
      "iter 9841/30000  loss         0.103174  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.667\n",
      "iter 9860/30000  loss         0.103147  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.672\n",
      "iter 9861/30000  loss         0.103145  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.672\n",
      "iter 9880/30000  loss         0.103119  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.677\n",
      "iter 9881/30000  loss         0.103117  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    5.678\n",
      "iter 9900/30000  loss         0.103091  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.682\n",
      "iter 9901/30000  loss         0.103089  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.683\n",
      "iter 9920/30000  loss         0.103063  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.688\n",
      "iter 9921/30000  loss         0.103061  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.688\n",
      "iter 9940/30000  loss         0.103035  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.693\n",
      "iter 9941/30000  loss         0.103033  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.693\n",
      "iter 9960/30000  loss         0.103007  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.698\n",
      "iter 9961/30000  loss         0.103006  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.698\n",
      "iter 9980/30000  loss         0.102979  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.703\n",
      "iter 9981/30000  loss         0.102978  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.703\n",
      "iter 10000/30000  loss         0.102952  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.708\n",
      "iter 10001/30000  loss         0.102950  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.708\n",
      "iter 10020/30000  loss         0.102924  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.713\n",
      "iter 10021/30000  loss         0.102923  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.714\n",
      "iter 10040/30000  loss         0.102897  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.718\n",
      "iter 10041/30000  loss         0.102896  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.719\n",
      "iter 10060/30000  loss         0.102870  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.723\n",
      "iter 10061/30000  loss         0.102868  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    5.724\n",
      "iter 10080/30000  loss         0.102843  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.729\n",
      "iter 10081/30000  loss         0.102841  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.729\n",
      "iter 10100/30000  loss         0.102815  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.734\n",
      "iter 10101/30000  loss         0.102814  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.734\n",
      "iter 10120/30000  loss         0.102788  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.739\n",
      "iter 10121/30000  loss         0.102787  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.739\n",
      "iter 10140/30000  loss         0.102761  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.744\n",
      "iter 10141/30000  loss         0.102760  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.744\n",
      "iter 10160/30000  loss         0.102735  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.749\n",
      "iter 10161/30000  loss         0.102733  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.749\n",
      "iter 10180/30000  loss         0.102708  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.754\n",
      "iter 10181/30000  loss         0.102706  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.754\n",
      "iter 10200/30000  loss         0.102681  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.759\n",
      "iter 10201/30000  loss         0.102680  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10220/30000  loss         0.102655  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.764\n",
      "iter 10221/30000  loss         0.102653  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.764\n",
      "iter 10240/30000  loss         0.102628  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.769\n",
      "iter 10241/30000  loss         0.102627  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    5.769\n",
      "iter 10260/30000  loss         0.102602  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.774\n",
      "iter 10261/30000  loss         0.102600  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.774\n",
      "iter 10280/30000  loss         0.102575  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.779\n",
      "iter 10281/30000  loss         0.102574  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.779\n",
      "iter 10300/30000  loss         0.102549  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.784\n",
      "iter 10301/30000  loss         0.102548  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.784\n",
      "iter 10320/30000  loss         0.102523  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.789\n",
      "iter 10321/30000  loss         0.102522  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.789\n",
      "iter 10340/30000  loss         0.102497  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.794\n",
      "iter 10341/30000  loss         0.102496  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.794\n",
      "iter 10360/30000  loss         0.102471  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.799\n",
      "iter 10361/30000  loss         0.102470  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.799\n",
      "iter 10380/30000  loss         0.102445  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.804\n",
      "iter 10381/30000  loss         0.102444  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.804\n",
      "iter 10400/30000  loss         0.102419  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.809\n",
      "iter 10401/30000  loss         0.102418  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.809\n",
      "iter 10420/30000  loss         0.102393  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.814\n",
      "iter 10421/30000  loss         0.102392  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.814\n",
      "iter 10440/30000  loss         0.102368  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.819\n",
      "iter 10441/30000  loss         0.102367  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    5.819\n",
      "iter 10460/30000  loss         0.102342  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.824\n",
      "iter 10461/30000  loss         0.102341  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.824\n",
      "iter 10480/30000  loss         0.102317  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.829\n",
      "iter 10481/30000  loss         0.102316  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.829\n",
      "iter 10500/30000  loss         0.102291  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.833\n",
      "iter 10501/30000  loss         0.102290  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.834\n",
      "iter 10520/30000  loss         0.102266  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.838\n",
      "iter 10521/30000  loss         0.102265  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.839\n",
      "iter 10540/30000  loss         0.102241  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.843\n",
      "iter 10541/30000  loss         0.102240  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.843\n",
      "iter 10560/30000  loss         0.102216  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.848\n",
      "iter 10561/30000  loss         0.102214  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.848\n",
      "iter 10580/30000  loss         0.102191  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.853\n",
      "iter 10581/30000  loss         0.102189  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.853\n",
      "iter 10600/30000  loss         0.102166  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.858\n",
      "iter 10601/30000  loss         0.102164  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.858\n",
      "iter 10620/30000  loss         0.102141  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.863\n",
      "iter 10621/30000  loss         0.102139  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.863\n",
      "iter 10640/30000  loss         0.102116  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.868\n",
      "iter 10641/30000  loss         0.102115  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    5.868\n",
      "iter 10660/30000  loss         0.102091  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.872\n",
      "iter 10661/30000  loss         0.102090  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.873\n",
      "iter 10680/30000  loss         0.102066  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.877\n",
      "iter 10681/30000  loss         0.102065  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.878\n",
      "iter 10700/30000  loss         0.102042  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.882\n",
      "iter 10701/30000  loss         0.102041  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.882\n",
      "iter 10720/30000  loss         0.102017  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.887\n",
      "iter 10721/30000  loss         0.102016  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.887\n",
      "iter 10740/30000  loss         0.101993  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.892\n",
      "iter 10741/30000  loss         0.101992  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.892\n",
      "iter 10760/30000  loss         0.101969  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.897\n",
      "iter 10761/30000  loss         0.101967  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.897\n",
      "iter 10780/30000  loss         0.101944  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.901\n",
      "iter 10781/30000  loss         0.101943  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.902\n",
      "iter 10800/30000  loss         0.101920  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.906\n",
      "iter 10801/30000  loss         0.101919  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.906\n",
      "iter 10820/30000  loss         0.101896  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.911\n",
      "iter 10821/30000  loss         0.101895  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.911\n",
      "iter 10840/30000  loss         0.101872  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.916\n",
      "iter 10841/30000  loss         0.101871  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    5.916\n",
      "iter 10860/30000  loss         0.101848  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.920\n",
      "iter 10861/30000  loss         0.101847  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.921\n",
      "iter 10880/30000  loss         0.101824  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.925\n",
      "iter 10881/30000  loss         0.101823  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.925\n",
      "iter 10900/30000  loss         0.101800  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.930\n",
      "iter 10901/30000  loss         0.101799  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.930\n",
      "iter 10920/30000  loss         0.101776  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.935\n",
      "iter 10921/30000  loss         0.101775  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.935\n",
      "iter 10940/30000  loss         0.101753  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.939\n",
      "iter 10941/30000  loss         0.101752  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.940\n",
      "iter 10960/30000  loss         0.101729  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.944\n",
      "iter 10961/30000  loss         0.101728  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.944\n",
      "iter 10980/30000  loss         0.101706  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.949\n",
      "iter 10981/30000  loss         0.101705  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.949\n",
      "iter 11000/30000  loss         0.101682  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.954\n",
      "iter 11001/30000  loss         0.101681  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11020/30000  loss         0.101659  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.958\n",
      "iter 11021/30000  loss         0.101658  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.959\n",
      "iter 11040/30000  loss         0.101636  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.963\n",
      "iter 11041/30000  loss         0.101634  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.963\n",
      "iter 11060/30000  loss         0.101612  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.968\n",
      "iter 11061/30000  loss         0.101611  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    5.968\n",
      "iter 11080/30000  loss         0.101589  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.972\n",
      "iter 11081/30000  loss         0.101588  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.973\n",
      "iter 11100/30000  loss         0.101566  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.977\n",
      "iter 11101/30000  loss         0.101565  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.977\n",
      "iter 11120/30000  loss         0.101543  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.982\n",
      "iter 11121/30000  loss         0.101542  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.982\n",
      "iter 11140/30000  loss         0.101520  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.986\n",
      "iter 11141/30000  loss         0.101519  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.987\n",
      "iter 11160/30000  loss         0.101497  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.991\n",
      "iter 11161/30000  loss         0.101496  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.991\n",
      "iter 11180/30000  loss         0.101474  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.996\n",
      "iter 11181/30000  loss         0.101473  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    5.996\n",
      "iter 11200/30000  loss         0.101452  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.000\n",
      "iter 11201/30000  loss         0.101451  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.000\n",
      "iter 11220/30000  loss         0.101429  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.005\n",
      "iter 11221/30000  loss         0.101428  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.005\n",
      "iter 11240/30000  loss         0.101406  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.009\n",
      "iter 11241/30000  loss         0.101405  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.010\n",
      "iter 11260/30000  loss         0.101384  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.014\n",
      "iter 11261/30000  loss         0.101383  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.014\n",
      "iter 11280/30000  loss         0.101362  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    6.019\n",
      "iter 11281/30000  loss         0.101360  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.019\n",
      "iter 11300/30000  loss         0.101339  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.023\n",
      "iter 11301/30000  loss         0.101338  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.024\n",
      "iter 11320/30000  loss         0.101317  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.028\n",
      "iter 11321/30000  loss         0.101316  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.028\n",
      "iter 11340/30000  loss         0.101295  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.032\n",
      "iter 11341/30000  loss         0.101293  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.033\n",
      "iter 11360/30000  loss         0.101272  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.037\n",
      "iter 11361/30000  loss         0.101271  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.037\n",
      "iter 11380/30000  loss         0.101250  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.042\n",
      "iter 11381/30000  loss         0.101249  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.042\n",
      "iter 11400/30000  loss         0.101228  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.046\n",
      "iter 11401/30000  loss         0.101227  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.046\n",
      "iter 11420/30000  loss         0.101206  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.051\n",
      "iter 11421/30000  loss         0.101205  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.051\n",
      "iter 11440/30000  loss         0.101184  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.055\n",
      "iter 11441/30000  loss         0.101183  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.056\n",
      "iter 11460/30000  loss         0.101163  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.060\n",
      "iter 11461/30000  loss         0.101161  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.060\n",
      "iter 11480/30000  loss         0.101141  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.064\n",
      "iter 11481/30000  loss         0.101140  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.065\n",
      "iter 11500/30000  loss         0.101119  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.069\n",
      "iter 11501/30000  loss         0.101118  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    6.069\n",
      "iter 11520/30000  loss         0.101097  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.073\n",
      "iter 11521/30000  loss         0.101096  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.074\n",
      "iter 11540/30000  loss         0.101076  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.078\n",
      "iter 11541/30000  loss         0.101075  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.078\n",
      "iter 11560/30000  loss         0.101054  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.082\n",
      "iter 11561/30000  loss         0.101053  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.083\n",
      "iter 11580/30000  loss         0.101033  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.087\n",
      "iter 11581/30000  loss         0.101032  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.087\n",
      "iter 11600/30000  loss         0.101012  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.091\n",
      "iter 11601/30000  loss         0.101010  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.092\n",
      "iter 11620/30000  loss         0.100990  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.096\n",
      "iter 11621/30000  loss         0.100989  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.096\n",
      "iter 11640/30000  loss         0.100969  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.100\n",
      "iter 11641/30000  loss         0.100968  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.101\n",
      "iter 11660/30000  loss         0.100948  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.105\n",
      "iter 11661/30000  loss         0.100947  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.105\n",
      "iter 11680/30000  loss         0.100927  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.109\n",
      "iter 11681/30000  loss         0.100926  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.109\n",
      "iter 11700/30000  loss         0.100906  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.114\n",
      "iter 11701/30000  loss         0.100905  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.114\n",
      "iter 11720/30000  loss         0.100885  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.118\n",
      "iter 11721/30000  loss         0.100884  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.118\n",
      "iter 11740/30000  loss         0.100864  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.123\n",
      "iter 11741/30000  loss         0.100863  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    6.123\n",
      "iter 11760/30000  loss         0.100843  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.127\n",
      "iter 11761/30000  loss         0.100842  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.127\n",
      "iter 11780/30000  loss         0.100822  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.131\n",
      "iter 11781/30000  loss         0.100821  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.132\n",
      "iter 11800/30000  loss         0.100801  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.136\n",
      "iter 11801/30000  loss         0.100800  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11820/30000  loss         0.100781  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.140\n",
      "iter 11821/30000  loss         0.100780  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.140\n",
      "iter 11840/30000  loss         0.100760  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.145\n",
      "iter 11841/30000  loss         0.100759  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.145\n",
      "iter 11860/30000  loss         0.100739  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.149\n",
      "iter 11861/30000  loss         0.100738  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.149\n",
      "iter 11880/30000  loss         0.100719  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.153\n",
      "iter 11881/30000  loss         0.100718  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.154\n",
      "iter 11900/30000  loss         0.100698  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.158\n",
      "iter 11901/30000  loss         0.100697  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.158\n",
      "iter 11920/30000  loss         0.100678  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.162\n",
      "iter 11921/30000  loss         0.100677  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.162\n",
      "iter 11940/30000  loss         0.100658  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.167\n",
      "iter 11941/30000  loss         0.100657  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.167\n",
      "iter 11960/30000  loss         0.100638  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.171\n",
      "iter 11961/30000  loss         0.100637  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.171\n",
      "iter 11980/30000  loss         0.100617  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.175\n",
      "iter 11981/30000  loss         0.100616  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    6.176\n",
      "iter 12000/30000  loss         0.100597  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.180\n",
      "iter 12001/30000  loss         0.100596  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.180\n",
      "iter 12020/30000  loss         0.100577  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.184\n",
      "iter 12021/30000  loss         0.100576  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.184\n",
      "iter 12040/30000  loss         0.100557  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.188\n",
      "iter 12041/30000  loss         0.100556  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.189\n",
      "iter 12060/30000  loss         0.100537  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.193\n",
      "iter 12061/30000  loss         0.100536  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.193\n",
      "iter 12080/30000  loss         0.100517  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.197\n",
      "iter 12081/30000  loss         0.100516  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.197\n",
      "iter 12100/30000  loss         0.100497  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.201\n",
      "iter 12101/30000  loss         0.100496  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.202\n",
      "iter 12120/30000  loss         0.100478  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.206\n",
      "iter 12121/30000  loss         0.100477  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.206\n",
      "iter 12140/30000  loss         0.100458  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.210\n",
      "iter 12141/30000  loss         0.100457  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.210\n",
      "iter 12160/30000  loss         0.100438  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.214\n",
      "iter 12161/30000  loss         0.100437  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.214\n",
      "iter 12180/30000  loss         0.100419  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.218\n",
      "iter 12181/30000  loss         0.100418  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.219\n",
      "iter 12200/30000  loss         0.100399  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.223\n",
      "iter 12201/30000  loss         0.100398  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.223\n",
      "iter 12220/30000  loss         0.100380  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.227\n",
      "iter 12221/30000  loss         0.100379  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    6.227\n",
      "iter 12240/30000  loss         0.100360  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.231\n",
      "iter 12241/30000  loss         0.100359  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.232\n",
      "iter 12260/30000  loss         0.100341  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.236\n",
      "iter 12261/30000  loss         0.100340  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.236\n",
      "iter 12280/30000  loss         0.100322  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.240\n",
      "iter 12281/30000  loss         0.100321  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.240\n",
      "iter 12300/30000  loss         0.100302  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.244\n",
      "iter 12301/30000  loss         0.100301  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.244\n",
      "iter 12320/30000  loss         0.100283  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.248\n",
      "iter 12321/30000  loss         0.100282  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.249\n",
      "iter 12340/30000  loss         0.100264  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.253\n",
      "iter 12341/30000  loss         0.100263  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.253\n",
      "iter 12360/30000  loss         0.100245  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.257\n",
      "iter 12361/30000  loss         0.100244  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.257\n",
      "iter 12380/30000  loss         0.100226  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.261\n",
      "iter 12381/30000  loss         0.100225  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.261\n",
      "iter 12400/30000  loss         0.100207  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.265\n",
      "iter 12401/30000  loss         0.100206  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.265\n",
      "iter 12420/30000  loss         0.100188  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.269\n",
      "iter 12421/30000  loss         0.100187  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.270\n",
      "iter 12440/30000  loss         0.100169  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.274\n",
      "iter 12441/30000  loss         0.100168  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.274\n",
      "iter 12460/30000  loss         0.100150  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.278\n",
      "iter 12461/30000  loss         0.100149  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.278\n",
      "iter 12480/30000  loss         0.100131  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.282\n",
      "iter 12481/30000  loss         0.100130  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    6.282\n",
      "iter 12500/30000  loss         0.100113  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.286\n",
      "iter 12501/30000  loss         0.100112  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.286\n",
      "iter 12520/30000  loss         0.100094  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.290\n",
      "iter 12521/30000  loss         0.100093  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.291\n",
      "iter 12540/30000  loss         0.100075  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.295\n",
      "iter 12541/30000  loss         0.100074  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.295\n",
      "iter 12560/30000  loss         0.100057  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.299\n",
      "iter 12561/30000  loss         0.100056  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.299\n",
      "iter 12580/30000  loss         0.100038  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.303\n",
      "iter 12581/30000  loss         0.100037  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.303\n",
      "iter 12600/30000  loss         0.100020  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.307\n",
      "iter 12601/30000  loss         0.100019  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12620/30000  loss         0.100002  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.311\n",
      "iter 12621/30000  loss         0.100001  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.311\n",
      "iter 12640/30000  loss         0.099983  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.315\n",
      "iter 12641/30000  loss         0.099982  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.315\n",
      "iter 12660/30000  loss         0.099965  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.319\n",
      "iter 12661/30000  loss         0.099964  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.320\n",
      "iter 12680/30000  loss         0.099947  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.324\n",
      "iter 12681/30000  loss         0.099946  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.324\n",
      "iter 12700/30000  loss         0.099928  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.328\n",
      "iter 12701/30000  loss         0.099928  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.328\n",
      "iter 12720/30000  loss         0.099910  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.332\n",
      "iter 12721/30000  loss         0.099909  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.332\n",
      "iter 12740/30000  loss         0.099892  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.336\n",
      "iter 12741/30000  loss         0.099891  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.336\n",
      "iter 12760/30000  loss         0.099874  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.340\n",
      "iter 12761/30000  loss         0.099873  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    6.340\n",
      "iter 12780/30000  loss         0.099856  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.344\n",
      "iter 12781/30000  loss         0.099855  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.344\n",
      "iter 12800/30000  loss         0.099838  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.348\n",
      "iter 12801/30000  loss         0.099837  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.348\n",
      "iter 12820/30000  loss         0.099820  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.352\n",
      "iter 12821/30000  loss         0.099820  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.352\n",
      "iter 12840/30000  loss         0.099803  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.356\n",
      "iter 12841/30000  loss         0.099802  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.356\n",
      "iter 12860/30000  loss         0.099785  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.360\n",
      "iter 12861/30000  loss         0.099784  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.361\n",
      "iter 12880/30000  loss         0.099767  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.364\n",
      "iter 12881/30000  loss         0.099766  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.365\n",
      "iter 12900/30000  loss         0.099749  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.368\n",
      "iter 12901/30000  loss         0.099749  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.369\n",
      "iter 12920/30000  loss         0.099732  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.373\n",
      "iter 12921/30000  loss         0.099731  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.373\n",
      "iter 12940/30000  loss         0.099714  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.377\n",
      "iter 12941/30000  loss         0.099713  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.377\n",
      "iter 12960/30000  loss         0.099697  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.381\n",
      "iter 12961/30000  loss         0.099696  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.381\n",
      "iter 12980/30000  loss         0.099679  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.385\n",
      "iter 12981/30000  loss         0.099678  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.385\n",
      "iter 13000/30000  loss         0.099662  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.389\n",
      "iter 13001/30000  loss         0.099661  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.389\n",
      "iter 13020/30000  loss         0.099644  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.393\n",
      "iter 13021/30000  loss         0.099644  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.393\n",
      "iter 13040/30000  loss         0.099627  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    6.397\n",
      "iter 13041/30000  loss         0.099626  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.397\n",
      "iter 13060/30000  loss         0.099610  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.401\n",
      "iter 13061/30000  loss         0.099609  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.401\n",
      "iter 13080/30000  loss         0.099593  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.405\n",
      "iter 13081/30000  loss         0.099592  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.405\n",
      "iter 13100/30000  loss         0.099575  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.409\n",
      "iter 13101/30000  loss         0.099575  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.409\n",
      "iter 13120/30000  loss         0.099558  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.413\n",
      "iter 13121/30000  loss         0.099557  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.413\n",
      "iter 13140/30000  loss         0.099541  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.417\n",
      "iter 13141/30000  loss         0.099540  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.417\n",
      "iter 13160/30000  loss         0.099524  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.421\n",
      "iter 13161/30000  loss         0.099523  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.421\n",
      "iter 13180/30000  loss         0.099507  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.425\n",
      "iter 13181/30000  loss         0.099506  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.425\n",
      "iter 13200/30000  loss         0.099490  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.429\n",
      "iter 13201/30000  loss         0.099489  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.429\n",
      "iter 13220/30000  loss         0.099473  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.433\n",
      "iter 13221/30000  loss         0.099472  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.433\n",
      "iter 13240/30000  loss         0.099456  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.436\n",
      "iter 13241/30000  loss         0.099456  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.437\n",
      "iter 13260/30000  loss         0.099440  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.440\n",
      "iter 13261/30000  loss         0.099439  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.441\n",
      "iter 13280/30000  loss         0.099423  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.444\n",
      "iter 13281/30000  loss         0.099422  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.445\n",
      "iter 13300/30000  loss         0.099406  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.448\n",
      "iter 13301/30000  loss         0.099405  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.449\n",
      "iter 13320/30000  loss         0.099389  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.452\n",
      "iter 13321/30000  loss         0.099389  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    6.452\n",
      "iter 13340/30000  loss         0.099373  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.456\n",
      "iter 13341/30000  loss         0.099372  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.456\n",
      "iter 13360/30000  loss         0.099356  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.460\n",
      "iter 13361/30000  loss         0.099355  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.460\n",
      "iter 13380/30000  loss         0.099340  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.464\n",
      "iter 13381/30000  loss         0.099339  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.464\n",
      "iter 13400/30000  loss         0.099323  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.468\n",
      "iter 13401/30000  loss         0.099322  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13420/30000  loss         0.099307  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.472\n",
      "iter 13421/30000  loss         0.099306  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.472\n",
      "iter 13440/30000  loss         0.099290  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.476\n",
      "iter 13441/30000  loss         0.099290  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.476\n",
      "iter 13460/30000  loss         0.099274  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.480\n",
      "iter 13461/30000  loss         0.099273  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.480\n",
      "iter 13480/30000  loss         0.099258  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.483\n",
      "iter 13481/30000  loss         0.099257  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.484\n",
      "iter 13500/30000  loss         0.099242  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.487\n",
      "iter 13501/30000  loss         0.099241  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.488\n",
      "iter 13520/30000  loss         0.099225  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.491\n",
      "iter 13521/30000  loss         0.099225  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.491\n",
      "iter 13540/30000  loss         0.099209  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.495\n",
      "iter 13541/30000  loss         0.099208  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.495\n",
      "iter 13560/30000  loss         0.099193  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.499\n",
      "iter 13561/30000  loss         0.099192  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.499\n",
      "iter 13580/30000  loss         0.099177  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.503\n",
      "iter 13581/30000  loss         0.099176  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.503\n",
      "iter 13600/30000  loss         0.099161  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.507\n",
      "iter 13601/30000  loss         0.099160  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.507\n",
      "iter 13620/30000  loss         0.099145  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.510\n",
      "iter 13621/30000  loss         0.099144  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    6.511\n",
      "iter 13640/30000  loss         0.099129  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.514\n",
      "iter 13641/30000  loss         0.099128  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.515\n",
      "iter 13660/30000  loss         0.099113  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.518\n",
      "iter 13661/30000  loss         0.099112  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.518\n",
      "iter 13680/30000  loss         0.099097  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.522\n",
      "iter 13681/30000  loss         0.099096  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.522\n",
      "iter 13700/30000  loss         0.099081  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.526\n",
      "iter 13701/30000  loss         0.099081  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.526\n",
      "iter 13720/30000  loss         0.099066  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.530\n",
      "iter 13721/30000  loss         0.099065  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.530\n",
      "iter 13740/30000  loss         0.099050  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.533\n",
      "iter 13741/30000  loss         0.099049  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.534\n",
      "iter 13760/30000  loss         0.099034  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.537\n",
      "iter 13761/30000  loss         0.099033  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.537\n",
      "iter 13780/30000  loss         0.099018  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.541\n",
      "iter 13781/30000  loss         0.099018  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.541\n",
      "iter 13800/30000  loss         0.099003  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.545\n",
      "iter 13801/30000  loss         0.099002  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.545\n",
      "iter 13820/30000  loss         0.098987  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.549\n",
      "iter 13821/30000  loss         0.098987  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.549\n",
      "iter 13840/30000  loss         0.098972  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.552\n",
      "iter 13841/30000  loss         0.098971  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.553\n",
      "iter 13860/30000  loss         0.098956  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.556\n",
      "iter 13861/30000  loss         0.098956  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.556\n",
      "iter 13880/30000  loss         0.098941  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.560\n",
      "iter 13881/30000  loss         0.098940  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.560\n",
      "iter 13900/30000  loss         0.098925  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.564\n",
      "iter 13901/30000  loss         0.098925  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.564\n",
      "iter 13920/30000  loss         0.098910  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.567\n",
      "iter 13921/30000  loss         0.098909  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    6.568\n",
      "iter 13940/30000  loss         0.098895  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.571\n",
      "iter 13941/30000  loss         0.098894  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.571\n",
      "iter 13960/30000  loss         0.098880  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.575\n",
      "iter 13961/30000  loss         0.098879  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.575\n",
      "iter 13980/30000  loss         0.098864  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.579\n",
      "iter 13981/30000  loss         0.098864  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.579\n",
      "iter 14000/30000  loss         0.098849  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.582\n",
      "iter 14001/30000  loss         0.098848  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.583\n",
      "iter 14020/30000  loss         0.098834  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.586\n",
      "iter 14021/30000  loss         0.098833  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.586\n",
      "iter 14040/30000  loss         0.098819  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.590\n",
      "iter 14041/30000  loss         0.098818  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.590\n",
      "iter 14060/30000  loss         0.098804  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.594\n",
      "iter 14061/30000  loss         0.098803  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.594\n",
      "iter 14080/30000  loss         0.098789  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.597\n",
      "iter 14081/30000  loss         0.098788  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.598\n",
      "iter 14100/30000  loss         0.098774  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.601\n",
      "iter 14101/30000  loss         0.098773  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.601\n",
      "iter 14120/30000  loss         0.098759  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.605\n",
      "iter 14121/30000  loss         0.098758  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.605\n",
      "iter 14140/30000  loss         0.098744  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.608\n",
      "iter 14141/30000  loss         0.098743  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.609\n",
      "iter 14160/30000  loss         0.098729  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.612\n",
      "iter 14161/30000  loss         0.098728  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.612\n",
      "iter 14180/30000  loss         0.098714  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.616\n",
      "iter 14181/30000  loss         0.098713  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.616\n",
      "iter 14200/30000  loss         0.098699  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.620\n",
      "iter 14201/30000  loss         0.098699  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14220/30000  loss         0.098685  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.623\n",
      "iter 14221/30000  loss         0.098684  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.623\n",
      "iter 14240/30000  loss         0.098670  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.627\n",
      "iter 14241/30000  loss         0.098669  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    6.627\n",
      "iter 14260/30000  loss         0.098655  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.631\n",
      "iter 14261/30000  loss         0.098654  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.631\n",
      "iter 14280/30000  loss         0.098641  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.634\n",
      "iter 14281/30000  loss         0.098640  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.634\n",
      "iter 14300/30000  loss         0.098626  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.638\n",
      "iter 14301/30000  loss         0.098625  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.638\n",
      "iter 14320/30000  loss         0.098611  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.642\n",
      "iter 14321/30000  loss         0.098611  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.642\n",
      "iter 14340/30000  loss         0.098597  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.645\n",
      "iter 14341/30000  loss         0.098596  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.645\n",
      "iter 14360/30000  loss         0.098582  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.649\n",
      "iter 14361/30000  loss         0.098582  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.649\n",
      "iter 14380/30000  loss         0.098568  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.652\n",
      "iter 14381/30000  loss         0.098567  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.653\n",
      "iter 14400/30000  loss         0.098553  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.656\n",
      "iter 14401/30000  loss         0.098553  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.656\n",
      "iter 14420/30000  loss         0.098539  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.660\n",
      "iter 14421/30000  loss         0.098538  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.660\n",
      "iter 14440/30000  loss         0.098525  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.663\n",
      "iter 14441/30000  loss         0.098524  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.663\n",
      "iter 14460/30000  loss         0.098510  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.667\n",
      "iter 14461/30000  loss         0.098510  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.667\n",
      "iter 14480/30000  loss         0.098496  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.671\n",
      "iter 14481/30000  loss         0.098496  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.671\n",
      "iter 14500/30000  loss         0.098482  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.674\n",
      "iter 14501/30000  loss         0.098481  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.674\n",
      "iter 14520/30000  loss         0.098468  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.678\n",
      "iter 14521/30000  loss         0.098467  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.678\n",
      "iter 14540/30000  loss         0.098454  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.681\n",
      "iter 14541/30000  loss         0.098453  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.682\n",
      "iter 14560/30000  loss         0.098440  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.685\n",
      "iter 14561/30000  loss         0.098439  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    6.685\n",
      "iter 14580/30000  loss         0.098425  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.688\n",
      "iter 14581/30000  loss         0.098425  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.689\n",
      "iter 14600/30000  loss         0.098411  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.692\n",
      "iter 14601/30000  loss         0.098411  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.692\n",
      "iter 14620/30000  loss         0.098397  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.696\n",
      "iter 14621/30000  loss         0.098397  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.696\n",
      "iter 14640/30000  loss         0.098383  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.699\n",
      "iter 14641/30000  loss         0.098383  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.699\n",
      "iter 14660/30000  loss         0.098370  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.703\n",
      "iter 14661/30000  loss         0.098369  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.703\n",
      "iter 14680/30000  loss         0.098356  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.706\n",
      "iter 14681/30000  loss         0.098355  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.707\n",
      "iter 14700/30000  loss         0.098342  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.710\n",
      "iter 14701/30000  loss         0.098341  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.710\n",
      "iter 14720/30000  loss         0.098328  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.713\n",
      "iter 14721/30000  loss         0.098327  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.714\n",
      "iter 14740/30000  loss         0.098314  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.717\n",
      "iter 14741/30000  loss         0.098313  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.717\n",
      "iter 14760/30000  loss         0.098300  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.721\n",
      "iter 14761/30000  loss         0.098300  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.721\n",
      "iter 14780/30000  loss         0.098287  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.724\n",
      "iter 14781/30000  loss         0.098286  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.724\n",
      "iter 14800/30000  loss         0.098273  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.728\n",
      "iter 14801/30000  loss         0.098272  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.728\n",
      "iter 14820/30000  loss         0.098259  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.731\n",
      "iter 14821/30000  loss         0.098259  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.731\n",
      "iter 14840/30000  loss         0.098246  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.735\n",
      "iter 14841/30000  loss         0.098245  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.735\n",
      "iter 14860/30000  loss         0.098232  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.738\n",
      "iter 14861/30000  loss         0.098231  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.738\n",
      "iter 14880/30000  loss         0.098219  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.742\n",
      "iter 14881/30000  loss         0.098218  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.742\n",
      "iter 14900/30000  loss         0.098205  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.745\n",
      "iter 14901/30000  loss         0.098204  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    6.745\n",
      "iter 14920/30000  loss         0.098192  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.749\n",
      "iter 14921/30000  loss         0.098191  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.749\n",
      "iter 14940/30000  loss         0.098178  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.752\n",
      "iter 14941/30000  loss         0.098178  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.752\n",
      "iter 14960/30000  loss         0.098165  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.756\n",
      "iter 14961/30000  loss         0.098164  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.756\n",
      "iter 14980/30000  loss         0.098151  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.759\n",
      "iter 14981/30000  loss         0.098151  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.759\n",
      "iter 15000/30000  loss         0.098138  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.763\n",
      "iter 15001/30000  loss         0.098137  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15020/30000  loss         0.098125  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.766\n",
      "iter 15021/30000  loss         0.098124  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.766\n",
      "iter 15040/30000  loss         0.098112  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.770\n",
      "iter 15041/30000  loss         0.098111  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.770\n",
      "iter 15060/30000  loss         0.098098  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.773\n",
      "iter 15061/30000  loss         0.098098  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.773\n",
      "iter 15080/30000  loss         0.098085  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.776\n",
      "iter 15081/30000  loss         0.098084  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.777\n",
      "iter 15100/30000  loss         0.098072  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.780\n",
      "iter 15101/30000  loss         0.098071  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.780\n",
      "iter 15120/30000  loss         0.098059  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.783\n",
      "iter 15121/30000  loss         0.098058  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.784\n",
      "iter 15140/30000  loss         0.098046  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.787\n",
      "iter 15141/30000  loss         0.098045  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.787\n",
      "iter 15160/30000  loss         0.098033  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.790\n",
      "iter 15161/30000  loss         0.098032  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.790\n",
      "iter 15180/30000  loss         0.098020  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.794\n",
      "iter 15181/30000  loss         0.098019  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.794\n",
      "iter 15200/30000  loss         0.098007  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.797\n",
      "iter 15201/30000  loss         0.098006  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.797\n",
      "iter 15220/30000  loss         0.097994  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.801\n",
      "iter 15221/30000  loss         0.097993  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.801\n",
      "iter 15240/30000  loss         0.097981  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.804\n",
      "iter 15241/30000  loss         0.097980  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.804\n",
      "iter 15260/30000  loss         0.097968  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.807\n",
      "iter 15261/30000  loss         0.097967  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    6.808\n",
      "iter 15280/30000  loss         0.097955  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.811\n",
      "iter 15281/30000  loss         0.097954  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.811\n",
      "iter 15300/30000  loss         0.097942  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.814\n",
      "iter 15301/30000  loss         0.097941  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.814\n",
      "iter 15320/30000  loss         0.097929  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.818\n",
      "iter 15321/30000  loss         0.097929  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.818\n",
      "iter 15340/30000  loss         0.097917  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.821\n",
      "iter 15341/30000  loss         0.097916  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.821\n",
      "iter 15360/30000  loss         0.097904  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.824\n",
      "iter 15361/30000  loss         0.097903  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.825\n",
      "iter 15380/30000  loss         0.097891  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.828\n",
      "iter 15381/30000  loss         0.097890  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.828\n",
      "iter 15400/30000  loss         0.097878  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.831\n",
      "iter 15401/30000  loss         0.097878  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.831\n",
      "iter 15420/30000  loss         0.097866  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.835\n",
      "iter 15421/30000  loss         0.097865  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.835\n",
      "iter 15440/30000  loss         0.097853  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.838\n",
      "iter 15441/30000  loss         0.097852  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.838\n",
      "iter 15460/30000  loss         0.097841  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.841\n",
      "iter 15461/30000  loss         0.097840  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.841\n",
      "iter 15480/30000  loss         0.097828  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.845\n",
      "iter 15481/30000  loss         0.097827  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.845\n",
      "iter 15500/30000  loss         0.097815  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.848\n",
      "iter 15501/30000  loss         0.097815  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.848\n",
      "iter 15520/30000  loss         0.097803  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.851\n",
      "iter 15521/30000  loss         0.097802  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.851\n",
      "iter 15540/30000  loss         0.097791  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.855\n",
      "iter 15541/30000  loss         0.097790  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.855\n",
      "iter 15560/30000  loss         0.097778  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.858\n",
      "iter 15561/30000  loss         0.097777  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.858\n",
      "iter 15580/30000  loss         0.097766  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.861\n",
      "iter 15581/30000  loss         0.097765  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.862\n",
      "iter 15600/30000  loss         0.097753  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.865\n",
      "iter 15601/30000  loss         0.097753  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.865\n",
      "iter 15620/30000  loss         0.097741  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.868\n",
      "iter 15621/30000  loss         0.097740  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    6.868\n",
      "iter 15640/30000  loss         0.097729  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.871\n",
      "iter 15641/30000  loss         0.097728  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.872\n",
      "iter 15660/30000  loss         0.097716  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.875\n",
      "iter 15661/30000  loss         0.097716  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.875\n",
      "iter 15680/30000  loss         0.097704  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.878\n",
      "iter 15681/30000  loss         0.097704  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.878\n",
      "iter 15700/30000  loss         0.097692  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.881\n",
      "iter 15701/30000  loss         0.097691  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.881\n",
      "iter 15720/30000  loss         0.097680  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.885\n",
      "iter 15721/30000  loss         0.097679  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.885\n",
      "iter 15740/30000  loss         0.097668  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.888\n",
      "iter 15741/30000  loss         0.097667  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.888\n",
      "iter 15760/30000  loss         0.097656  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.891\n",
      "iter 15761/30000  loss         0.097655  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.891\n",
      "iter 15780/30000  loss         0.097643  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.894\n",
      "iter 15781/30000  loss         0.097643  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.895\n",
      "iter 15800/30000  loss         0.097631  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.898\n",
      "iter 15801/30000  loss         0.097631  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15820/30000  loss         0.097619  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.901\n",
      "iter 15821/30000  loss         0.097619  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.901\n",
      "iter 15840/30000  loss         0.097607  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.904\n",
      "iter 15841/30000  loss         0.097607  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.905\n",
      "iter 15860/30000  loss         0.097595  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.908\n",
      "iter 15861/30000  loss         0.097595  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.908\n",
      "iter 15880/30000  loss         0.097583  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.911\n",
      "iter 15881/30000  loss         0.097583  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.911\n",
      "iter 15900/30000  loss         0.097571  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.914\n",
      "iter 15901/30000  loss         0.097571  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.914\n",
      "iter 15920/30000  loss         0.097560  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.917\n",
      "iter 15921/30000  loss         0.097559  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.918\n",
      "iter 15940/30000  loss         0.097548  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.921\n",
      "iter 15941/30000  loss         0.097547  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.921\n",
      "iter 15960/30000  loss         0.097536  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.924\n",
      "iter 15961/30000  loss         0.097535  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.924\n",
      "iter 15980/30000  loss         0.097524  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.927\n",
      "iter 15981/30000  loss         0.097524  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.927\n",
      "iter 16000/30000  loss         0.097512  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.930\n",
      "iter 16001/30000  loss         0.097512  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    6.931\n",
      "iter 16020/30000  loss         0.097501  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.934\n",
      "iter 16021/30000  loss         0.097500  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.934\n",
      "iter 16040/30000  loss         0.097489  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.937\n",
      "iter 16041/30000  loss         0.097488  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.937\n",
      "iter 16060/30000  loss         0.097477  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.940\n",
      "iter 16061/30000  loss         0.097477  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.940\n",
      "iter 16080/30000  loss         0.097466  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.943\n",
      "iter 16081/30000  loss         0.097465  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.944\n",
      "iter 16100/30000  loss         0.097454  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.947\n",
      "iter 16101/30000  loss         0.097453  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.947\n",
      "iter 16120/30000  loss         0.097442  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.950\n",
      "iter 16121/30000  loss         0.097442  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.950\n",
      "iter 16140/30000  loss         0.097431  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.953\n",
      "iter 16141/30000  loss         0.097430  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.953\n",
      "iter 16160/30000  loss         0.097419  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.956\n",
      "iter 16161/30000  loss         0.097419  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.956\n",
      "iter 16180/30000  loss         0.097408  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.959\n",
      "iter 16181/30000  loss         0.097407  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.960\n",
      "iter 16200/30000  loss         0.097396  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.963\n",
      "iter 16201/30000  loss         0.097396  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.963\n",
      "iter 16220/30000  loss         0.097385  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.966\n",
      "iter 16221/30000  loss         0.097384  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.966\n",
      "iter 16240/30000  loss         0.097373  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.969\n",
      "iter 16241/30000  loss         0.097373  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.969\n",
      "iter 16260/30000  loss         0.097362  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.972\n",
      "iter 16261/30000  loss         0.097361  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.972\n",
      "iter 16280/30000  loss         0.097351  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.975\n",
      "iter 16281/30000  loss         0.097350  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.976\n",
      "iter 16300/30000  loss         0.097339  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.979\n",
      "iter 16301/30000  loss         0.097339  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.979\n",
      "iter 16320/30000  loss         0.097328  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.982\n",
      "iter 16321/30000  loss         0.097327  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.982\n",
      "iter 16340/30000  loss         0.097317  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.985\n",
      "iter 16341/30000  loss         0.097316  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.985\n",
      "iter 16360/30000  loss         0.097305  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.988\n",
      "iter 16361/30000  loss         0.097305  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.988\n",
      "iter 16380/30000  loss         0.097294  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.991\n",
      "iter 16381/30000  loss         0.097294  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    6.991\n",
      "iter 16400/30000  loss         0.097283  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    6.994\n",
      "iter 16401/30000  loss         0.097282  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    6.995\n",
      "iter 16420/30000  loss         0.097272  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    6.998\n",
      "iter 16421/30000  loss         0.097271  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    6.998\n",
      "iter 16440/30000  loss         0.097261  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.001\n",
      "iter 16441/30000  loss         0.097260  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.001\n",
      "iter 16460/30000  loss         0.097249  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.004\n",
      "iter 16461/30000  loss         0.097249  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.004\n",
      "iter 16480/30000  loss         0.097238  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.007\n",
      "iter 16481/30000  loss         0.097238  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.007\n",
      "iter 16500/30000  loss         0.097227  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.010\n",
      "iter 16501/30000  loss         0.097227  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.010\n",
      "iter 16520/30000  loss         0.097216  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.013\n",
      "iter 16521/30000  loss         0.097216  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.013\n",
      "iter 16540/30000  loss         0.097205  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.016\n",
      "iter 16541/30000  loss         0.097205  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.017\n",
      "iter 16560/30000  loss         0.097194  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.020\n",
      "iter 16561/30000  loss         0.097194  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.020\n",
      "iter 16580/30000  loss         0.097183  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.023\n",
      "iter 16581/30000  loss         0.097183  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.023\n",
      "iter 16600/30000  loss         0.097172  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.026\n",
      "iter 16601/30000  loss         0.097172  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16620/30000  loss         0.097161  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.029\n",
      "iter 16621/30000  loss         0.097161  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.029\n",
      "iter 16640/30000  loss         0.097150  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.032\n",
      "iter 16641/30000  loss         0.097150  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.032\n",
      "iter 16660/30000  loss         0.097140  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.035\n",
      "iter 16661/30000  loss         0.097139  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.035\n",
      "iter 16680/30000  loss         0.097129  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.038\n",
      "iter 16681/30000  loss         0.097128  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.038\n",
      "iter 16700/30000  loss         0.097118  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.041\n",
      "iter 16701/30000  loss         0.097117  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.041\n",
      "iter 16720/30000  loss         0.097107  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.044\n",
      "iter 16721/30000  loss         0.097107  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.044\n",
      "iter 16740/30000  loss         0.097096  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.047\n",
      "iter 16741/30000  loss         0.097096  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.048\n",
      "iter 16760/30000  loss         0.097086  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.051\n",
      "iter 16761/30000  loss         0.097085  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.051\n",
      "iter 16780/30000  loss         0.097075  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.054\n",
      "iter 16781/30000  loss         0.097074  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.054\n",
      "iter 16800/30000  loss         0.097064  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.057\n",
      "iter 16801/30000  loss         0.097064  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    7.057\n",
      "iter 16820/30000  loss         0.097053  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.060\n",
      "iter 16821/30000  loss         0.097053  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.060\n",
      "iter 16840/30000  loss         0.097043  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.063\n",
      "iter 16841/30000  loss         0.097042  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.063\n",
      "iter 16860/30000  loss         0.097032  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.066\n",
      "iter 16861/30000  loss         0.097032  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.066\n",
      "iter 16880/30000  loss         0.097022  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.069\n",
      "iter 16881/30000  loss         0.097021  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.069\n",
      "iter 16900/30000  loss         0.097011  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.072\n",
      "iter 16901/30000  loss         0.097011  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.072\n",
      "iter 16920/30000  loss         0.097001  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.075\n",
      "iter 16921/30000  loss         0.097000  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.075\n",
      "iter 16940/30000  loss         0.096990  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.078\n",
      "iter 16941/30000  loss         0.096989  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.078\n",
      "iter 16960/30000  loss         0.096979  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.081\n",
      "iter 16961/30000  loss         0.096979  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.081\n",
      "iter 16980/30000  loss         0.096969  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.084\n",
      "iter 16981/30000  loss         0.096969  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.084\n",
      "iter 17000/30000  loss         0.096959  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.087\n",
      "iter 17001/30000  loss         0.096958  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.087\n",
      "iter 17020/30000  loss         0.096948  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.090\n",
      "iter 17021/30000  loss         0.096948  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.090\n",
      "iter 17040/30000  loss         0.096938  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.093\n",
      "iter 17041/30000  loss         0.096937  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.093\n",
      "iter 17060/30000  loss         0.096927  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.096\n",
      "iter 17061/30000  loss         0.096927  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.096\n",
      "iter 17080/30000  loss         0.096917  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.099\n",
      "iter 17081/30000  loss         0.096917  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.099\n",
      "iter 17100/30000  loss         0.096907  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.102\n",
      "iter 17101/30000  loss         0.096906  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.102\n",
      "iter 17120/30000  loss         0.096896  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.105\n",
      "iter 17121/30000  loss         0.096896  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.105\n",
      "iter 17140/30000  loss         0.096886  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.108\n",
      "iter 17141/30000  loss         0.096886  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.108\n",
      "iter 17160/30000  loss         0.096876  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.111\n",
      "iter 17161/30000  loss         0.096875  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.111\n",
      "iter 17180/30000  loss         0.096866  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.114\n",
      "iter 17181/30000  loss         0.096865  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.114\n",
      "iter 17200/30000  loss         0.096855  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.117\n",
      "iter 17201/30000  loss         0.096855  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.117\n",
      "iter 17220/30000  loss         0.096845  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.120\n",
      "iter 17221/30000  loss         0.096845  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    7.120\n",
      "iter 17240/30000  loss         0.096835  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.123\n",
      "iter 17241/30000  loss         0.096835  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.123\n",
      "iter 17260/30000  loss         0.096825  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.126\n",
      "iter 17261/30000  loss         0.096825  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.126\n",
      "iter 17280/30000  loss         0.096815  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.129\n",
      "iter 17281/30000  loss         0.096814  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.129\n",
      "iter 17300/30000  loss         0.096805  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.132\n",
      "iter 17301/30000  loss         0.096804  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.132\n",
      "iter 17320/30000  loss         0.096795  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.135\n",
      "iter 17321/30000  loss         0.096794  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.135\n",
      "iter 17340/30000  loss         0.096785  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.138\n",
      "iter 17341/30000  loss         0.096784  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.138\n",
      "iter 17360/30000  loss         0.096775  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.141\n",
      "iter 17361/30000  loss         0.096774  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.141\n",
      "iter 17380/30000  loss         0.096765  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.144\n",
      "iter 17381/30000  loss         0.096764  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.144\n",
      "iter 17400/30000  loss         0.096755  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.147\n",
      "iter 17401/30000  loss         0.096754  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17420/30000  loss         0.096745  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.150\n",
      "iter 17421/30000  loss         0.096744  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.150\n",
      "iter 17440/30000  loss         0.096735  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.153\n",
      "iter 17441/30000  loss         0.096734  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.153\n",
      "iter 17460/30000  loss         0.096725  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.156\n",
      "iter 17461/30000  loss         0.096725  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.156\n",
      "iter 17480/30000  loss         0.096715  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.159\n",
      "iter 17481/30000  loss         0.096715  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.159\n",
      "iter 17500/30000  loss         0.096705  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.162\n",
      "iter 17501/30000  loss         0.096705  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.162\n",
      "iter 17520/30000  loss         0.096695  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.165\n",
      "iter 17521/30000  loss         0.096695  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.165\n",
      "iter 17540/30000  loss         0.096686  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.168\n",
      "iter 17541/30000  loss         0.096685  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.168\n",
      "iter 17560/30000  loss         0.096676  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.170\n",
      "iter 17561/30000  loss         0.096675  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.171\n",
      "iter 17580/30000  loss         0.096666  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.173\n",
      "iter 17581/30000  loss         0.096666  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.174\n",
      "iter 17600/30000  loss         0.096656  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.176\n",
      "iter 17601/30000  loss         0.096656  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.176\n",
      "iter 17620/30000  loss         0.096647  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.179\n",
      "iter 17621/30000  loss         0.096646  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.179\n",
      "iter 17640/30000  loss         0.096637  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.182\n",
      "iter 17641/30000  loss         0.096636  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    7.182\n",
      "iter 17660/30000  loss         0.096627  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.185\n",
      "iter 17661/30000  loss         0.096627  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.185\n",
      "iter 17680/30000  loss         0.096618  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.188\n",
      "iter 17681/30000  loss         0.096617  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.188\n",
      "iter 17700/30000  loss         0.096608  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.191\n",
      "iter 17701/30000  loss         0.096607  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.191\n",
      "iter 17720/30000  loss         0.096598  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.194\n",
      "iter 17721/30000  loss         0.096598  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.194\n",
      "iter 17740/30000  loss         0.096589  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.197\n",
      "iter 17741/30000  loss         0.096588  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.197\n",
      "iter 17760/30000  loss         0.096579  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.199\n",
      "iter 17761/30000  loss         0.096579  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.200\n",
      "iter 17780/30000  loss         0.096570  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.202\n",
      "iter 17781/30000  loss         0.096569  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.202\n",
      "iter 17800/30000  loss         0.096560  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.205\n",
      "iter 17801/30000  loss         0.096560  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.205\n",
      "iter 17820/30000  loss         0.096551  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.208\n",
      "iter 17821/30000  loss         0.096550  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.208\n",
      "iter 17840/30000  loss         0.096541  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.211\n",
      "iter 17841/30000  loss         0.096541  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.211\n",
      "iter 17860/30000  loss         0.096532  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.214\n",
      "iter 17861/30000  loss         0.096531  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.214\n",
      "iter 17880/30000  loss         0.096522  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.217\n",
      "iter 17881/30000  loss         0.096522  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.217\n",
      "iter 17900/30000  loss         0.096513  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.220\n",
      "iter 17901/30000  loss         0.096512  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.220\n",
      "iter 17920/30000  loss         0.096503  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.222\n",
      "iter 17921/30000  loss         0.096503  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.223\n",
      "iter 17940/30000  loss         0.096494  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.225\n",
      "iter 17941/30000  loss         0.096494  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.225\n",
      "iter 17960/30000  loss         0.096485  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.228\n",
      "iter 17961/30000  loss         0.096484  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.228\n",
      "iter 17980/30000  loss         0.096475  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.231\n",
      "iter 17981/30000  loss         0.096475  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.231\n",
      "iter 18000/30000  loss         0.096466  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.234\n",
      "iter 18001/30000  loss         0.096466  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.234\n",
      "iter 18020/30000  loss         0.096457  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.237\n",
      "iter 18021/30000  loss         0.096456  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.237\n",
      "iter 18040/30000  loss         0.096448  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.239\n",
      "iter 18041/30000  loss         0.096447  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.240\n",
      "iter 18060/30000  loss         0.096438  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.242\n",
      "iter 18061/30000  loss         0.096438  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.242\n",
      "iter 18080/30000  loss         0.096429  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.245\n",
      "iter 18081/30000  loss         0.096429  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.245\n",
      "iter 18100/30000  loss         0.096420  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.248\n",
      "iter 18101/30000  loss         0.096419  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    7.248\n",
      "iter 18120/30000  loss         0.096411  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.251\n",
      "iter 18121/30000  loss         0.096410  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.251\n",
      "iter 18140/30000  loss         0.096402  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.254\n",
      "iter 18141/30000  loss         0.096401  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.254\n",
      "iter 18160/30000  loss         0.096392  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.256\n",
      "iter 18161/30000  loss         0.096392  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.256\n",
      "iter 18180/30000  loss         0.096383  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.259\n",
      "iter 18181/30000  loss         0.096383  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.259\n",
      "iter 18200/30000  loss         0.096374  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.262\n",
      "iter 18201/30000  loss         0.096374  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18220/30000  loss         0.096365  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.265\n",
      "iter 18221/30000  loss         0.096365  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.265\n",
      "iter 18240/30000  loss         0.096356  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.268\n",
      "iter 18241/30000  loss         0.096356  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.268\n",
      "iter 18260/30000  loss         0.096347  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.270\n",
      "iter 18261/30000  loss         0.096347  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.270\n",
      "iter 18280/30000  loss         0.096338  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.273\n",
      "iter 18281/30000  loss         0.096338  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.273\n",
      "iter 18300/30000  loss         0.096329  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.276\n",
      "iter 18301/30000  loss         0.096329  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.276\n",
      "iter 18320/30000  loss         0.096320  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.279\n",
      "iter 18321/30000  loss         0.096320  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.279\n",
      "iter 18340/30000  loss         0.096311  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.281\n",
      "iter 18341/30000  loss         0.096311  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.282\n",
      "iter 18360/30000  loss         0.096302  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.284\n",
      "iter 18361/30000  loss         0.096302  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.284\n",
      "iter 18380/30000  loss         0.096293  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.287\n",
      "iter 18381/30000  loss         0.096293  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.287\n",
      "iter 18400/30000  loss         0.096284  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.290\n",
      "iter 18401/30000  loss         0.096284  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.290\n",
      "iter 18420/30000  loss         0.096275  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.293\n",
      "iter 18421/30000  loss         0.096275  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.293\n",
      "iter 18440/30000  loss         0.096267  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.295\n",
      "iter 18441/30000  loss         0.096266  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.295\n",
      "iter 18460/30000  loss         0.096258  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.298\n",
      "iter 18461/30000  loss         0.096257  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.298\n",
      "iter 18480/30000  loss         0.096249  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.301\n",
      "iter 18481/30000  loss         0.096248  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.301\n",
      "iter 18500/30000  loss         0.096240  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.304\n",
      "iter 18501/30000  loss         0.096240  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.304\n",
      "iter 18520/30000  loss         0.096231  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.306\n",
      "iter 18521/30000  loss         0.096231  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.306\n",
      "iter 18540/30000  loss         0.096223  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.309\n",
      "iter 18541/30000  loss         0.096222  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.309\n",
      "iter 18560/30000  loss         0.096214  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.312\n",
      "iter 18561/30000  loss         0.096213  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.312\n",
      "iter 18580/30000  loss         0.096205  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.315\n",
      "iter 18581/30000  loss         0.096205  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    7.315\n",
      "iter 18600/30000  loss         0.096196  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.317\n",
      "iter 18601/30000  loss         0.096196  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.317\n",
      "iter 18620/30000  loss         0.096188  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.320\n",
      "iter 18621/30000  loss         0.096187  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.320\n",
      "iter 18640/30000  loss         0.096179  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.323\n",
      "iter 18641/30000  loss         0.096179  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.323\n",
      "iter 18660/30000  loss         0.096170  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.325\n",
      "iter 18661/30000  loss         0.096170  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.326\n",
      "iter 18680/30000  loss         0.096162  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.328\n",
      "iter 18681/30000  loss         0.096161  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.328\n",
      "iter 18700/30000  loss         0.096153  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.331\n",
      "iter 18701/30000  loss         0.096153  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.331\n",
      "iter 18720/30000  loss         0.096145  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.334\n",
      "iter 18721/30000  loss         0.096144  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.334\n",
      "iter 18740/30000  loss         0.096136  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.336\n",
      "iter 18741/30000  loss         0.096136  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.336\n",
      "iter 18760/30000  loss         0.096127  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.339\n",
      "iter 18761/30000  loss         0.096127  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.339\n",
      "iter 18780/30000  loss         0.096119  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.342\n",
      "iter 18781/30000  loss         0.096118  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.342\n",
      "iter 18800/30000  loss         0.096110  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.344\n",
      "iter 18801/30000  loss         0.096110  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.345\n",
      "iter 18820/30000  loss         0.096102  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.347\n",
      "iter 18821/30000  loss         0.096101  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.347\n",
      "iter 18840/30000  loss         0.096093  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.350\n",
      "iter 18841/30000  loss         0.096093  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.350\n",
      "iter 18860/30000  loss         0.096085  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.352\n",
      "iter 18861/30000  loss         0.096084  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.353\n",
      "iter 18880/30000  loss         0.096076  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.355\n",
      "iter 18881/30000  loss         0.096076  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.355\n",
      "iter 18900/30000  loss         0.096068  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.358\n",
      "iter 18901/30000  loss         0.096068  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.358\n",
      "iter 18920/30000  loss         0.096060  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.360\n",
      "iter 18921/30000  loss         0.096059  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.361\n",
      "iter 18940/30000  loss         0.096051  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.363\n",
      "iter 18941/30000  loss         0.096051  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.363\n",
      "iter 18960/30000  loss         0.096043  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.366\n",
      "iter 18961/30000  loss         0.096042  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.366\n",
      "iter 18980/30000  loss         0.096035  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.369\n",
      "iter 18981/30000  loss         0.096034  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.369\n",
      "iter 19000/30000  loss         0.096026  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.371\n",
      "iter 19001/30000  loss         0.096026  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19020/30000  loss         0.096018  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.374\n",
      "iter 19021/30000  loss         0.096017  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.374\n",
      "iter 19040/30000  loss         0.096010  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.376\n",
      "iter 19041/30000  loss         0.096009  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.377\n",
      "iter 19060/30000  loss         0.096001  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.379\n",
      "iter 19061/30000  loss         0.096001  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    7.379\n",
      "iter 19080/30000  loss         0.095993  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.382\n",
      "iter 19081/30000  loss         0.095993  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.382\n",
      "iter 19100/30000  loss         0.095985  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.384\n",
      "iter 19101/30000  loss         0.095984  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.385\n",
      "iter 19120/30000  loss         0.095977  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.387\n",
      "iter 19121/30000  loss         0.095976  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.387\n",
      "iter 19140/30000  loss         0.095968  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.390\n",
      "iter 19141/30000  loss         0.095968  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.390\n",
      "iter 19160/30000  loss         0.095960  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.392\n",
      "iter 19161/30000  loss         0.095960  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.392\n",
      "iter 19180/30000  loss         0.095952  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.395\n",
      "iter 19181/30000  loss         0.095952  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.395\n",
      "iter 19200/30000  loss         0.095944  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.398\n",
      "iter 19201/30000  loss         0.095943  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.398\n",
      "iter 19220/30000  loss         0.095936  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.400\n",
      "iter 19221/30000  loss         0.095935  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.400\n",
      "iter 19240/30000  loss         0.095928  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.403\n",
      "iter 19241/30000  loss         0.095927  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.403\n",
      "iter 19260/30000  loss         0.095920  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.405\n",
      "iter 19261/30000  loss         0.095919  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.406\n",
      "iter 19280/30000  loss         0.095911  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.408\n",
      "iter 19281/30000  loss         0.095911  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.408\n",
      "iter 19300/30000  loss         0.095903  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.411\n",
      "iter 19301/30000  loss         0.095903  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.411\n",
      "iter 19320/30000  loss         0.095895  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.413\n",
      "iter 19321/30000  loss         0.095895  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.413\n",
      "iter 19340/30000  loss         0.095887  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.416\n",
      "iter 19341/30000  loss         0.095887  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.416\n",
      "iter 19360/30000  loss         0.095879  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.419\n",
      "iter 19361/30000  loss         0.095879  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.419\n",
      "iter 19380/30000  loss         0.095871  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.421\n",
      "iter 19381/30000  loss         0.095871  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.421\n",
      "iter 19400/30000  loss         0.095863  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.424\n",
      "iter 19401/30000  loss         0.095863  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.424\n",
      "iter 19420/30000  loss         0.095855  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.426\n",
      "iter 19421/30000  loss         0.095855  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.426\n",
      "iter 19440/30000  loss         0.095847  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.429\n",
      "iter 19441/30000  loss         0.095847  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.429\n",
      "iter 19460/30000  loss         0.095839  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.431\n",
      "iter 19461/30000  loss         0.095839  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.432\n",
      "iter 19480/30000  loss         0.095832  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.434\n",
      "iter 19481/30000  loss         0.095831  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.434\n",
      "iter 19500/30000  loss         0.095824  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.437\n",
      "iter 19501/30000  loss         0.095823  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.437\n",
      "iter 19520/30000  loss         0.095816  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.439\n",
      "iter 19521/30000  loss         0.095815  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.439\n",
      "iter 19540/30000  loss         0.095808  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.442\n",
      "iter 19541/30000  loss         0.095808  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.442\n",
      "iter 19560/30000  loss         0.095800  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.444\n",
      "iter 19561/30000  loss         0.095800  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.445\n",
      "iter 19580/30000  loss         0.095792  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.447\n",
      "iter 19581/30000  loss         0.095792  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    7.447\n",
      "iter 19600/30000  loss         0.095784  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.450\n",
      "iter 19601/30000  loss         0.095784  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.450\n",
      "iter 19620/30000  loss         0.095777  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.452\n",
      "iter 19621/30000  loss         0.095776  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.452\n",
      "iter 19640/30000  loss         0.095769  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.455\n",
      "iter 19641/30000  loss         0.095768  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.455\n",
      "iter 19660/30000  loss         0.095761  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.457\n",
      "iter 19661/30000  loss         0.095761  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.457\n",
      "iter 19680/30000  loss         0.095753  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.460\n",
      "iter 19681/30000  loss         0.095753  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.460\n",
      "iter 19700/30000  loss         0.095746  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.462\n",
      "iter 19701/30000  loss         0.095745  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.462\n",
      "iter 19720/30000  loss         0.095738  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.465\n",
      "iter 19721/30000  loss         0.095738  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.465\n",
      "iter 19740/30000  loss         0.095730  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.467\n",
      "iter 19741/30000  loss         0.095730  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.467\n",
      "iter 19760/30000  loss         0.095723  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.470\n",
      "iter 19761/30000  loss         0.095722  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.470\n",
      "iter 19780/30000  loss         0.095715  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.472\n",
      "iter 19781/30000  loss         0.095715  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.473\n",
      "iter 19800/30000  loss         0.095707  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.475\n",
      "iter 19801/30000  loss         0.095707  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19820/30000  loss         0.095700  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.477\n",
      "iter 19821/30000  loss         0.095699  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.478\n",
      "iter 19840/30000  loss         0.095692  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.480\n",
      "iter 19841/30000  loss         0.095692  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.480\n",
      "iter 19860/30000  loss         0.095684  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.483\n",
      "iter 19861/30000  loss         0.095684  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.483\n",
      "iter 19880/30000  loss         0.095677  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.485\n",
      "iter 19881/30000  loss         0.095676  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.485\n",
      "iter 19900/30000  loss         0.095669  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.488\n",
      "iter 19901/30000  loss         0.095669  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.488\n",
      "iter 19920/30000  loss         0.095662  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.490\n",
      "iter 19921/30000  loss         0.095661  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.490\n",
      "iter 19940/30000  loss         0.095654  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.493\n",
      "iter 19941/30000  loss         0.095654  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.493\n",
      "iter 19960/30000  loss         0.095647  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.495\n",
      "iter 19961/30000  loss         0.095646  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.495\n",
      "iter 19980/30000  loss         0.095639  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.498\n",
      "iter 19981/30000  loss         0.095639  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.498\n",
      "iter 20000/30000  loss         0.095632  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.500\n",
      "iter 20001/30000  loss         0.095631  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.500\n",
      "iter 20020/30000  loss         0.095624  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.503\n",
      "iter 20021/30000  loss         0.095624  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.503\n",
      "iter 20040/30000  loss         0.095617  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.505\n",
      "iter 20041/30000  loss         0.095616  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.505\n",
      "iter 20060/30000  loss         0.095609  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.508\n",
      "iter 20061/30000  loss         0.095609  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.508\n",
      "iter 20080/30000  loss         0.095602  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.510\n",
      "iter 20081/30000  loss         0.095602  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.510\n",
      "iter 20100/30000  loss         0.095594  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.513\n",
      "iter 20101/30000  loss         0.095594  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.513\n",
      "iter 20120/30000  loss         0.095587  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.515\n",
      "iter 20121/30000  loss         0.095587  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    7.515\n",
      "iter 20140/30000  loss         0.095580  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.518\n",
      "iter 20141/30000  loss         0.095579  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.518\n",
      "iter 20160/30000  loss         0.095572  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.520\n",
      "iter 20161/30000  loss         0.095572  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.520\n",
      "iter 20180/30000  loss         0.095565  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.522\n",
      "iter 20181/30000  loss         0.095565  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.523\n",
      "iter 20200/30000  loss         0.095558  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.525\n",
      "iter 20201/30000  loss         0.095557  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.525\n",
      "iter 20220/30000  loss         0.095550  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.527\n",
      "iter 20221/30000  loss         0.095550  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.528\n",
      "iter 20240/30000  loss         0.095543  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.530\n",
      "iter 20241/30000  loss         0.095543  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.530\n",
      "iter 20260/30000  loss         0.095536  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.532\n",
      "iter 20261/30000  loss         0.095535  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.532\n",
      "iter 20280/30000  loss         0.095528  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.535\n",
      "iter 20281/30000  loss         0.095528  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.535\n",
      "iter 20300/30000  loss         0.095521  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.537\n",
      "iter 20301/30000  loss         0.095521  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.537\n",
      "iter 20320/30000  loss         0.095514  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.540\n",
      "iter 20321/30000  loss         0.095514  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.540\n",
      "iter 20340/30000  loss         0.095507  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.542\n",
      "iter 20341/30000  loss         0.095506  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.542\n",
      "iter 20360/30000  loss         0.095500  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.545\n",
      "iter 20361/30000  loss         0.095499  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.545\n",
      "iter 20380/30000  loss         0.095492  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.547\n",
      "iter 20381/30000  loss         0.095492  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.547\n",
      "iter 20400/30000  loss         0.095485  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.549\n",
      "iter 20401/30000  loss         0.095485  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.550\n",
      "iter 20420/30000  loss         0.095478  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.552\n",
      "iter 20421/30000  loss         0.095478  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.552\n",
      "iter 20440/30000  loss         0.095471  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.554\n",
      "iter 20441/30000  loss         0.095471  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.554\n",
      "iter 20460/30000  loss         0.095464  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.557\n",
      "iter 20461/30000  loss         0.095463  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.557\n",
      "iter 20480/30000  loss         0.095457  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.559\n",
      "iter 20481/30000  loss         0.095456  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.559\n",
      "iter 20500/30000  loss         0.095450  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.562\n",
      "iter 20501/30000  loss         0.095449  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.562\n",
      "iter 20520/30000  loss         0.095442  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.564\n",
      "iter 20521/30000  loss         0.095442  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.564\n",
      "iter 20540/30000  loss         0.095435  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.566\n",
      "iter 20541/30000  loss         0.095435  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.567\n",
      "iter 20560/30000  loss         0.095428  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.569\n",
      "iter 20561/30000  loss         0.095428  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.569\n",
      "iter 20580/30000  loss         0.095421  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.571\n",
      "iter 20581/30000  loss         0.095421  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.571\n",
      "iter 20600/30000  loss         0.095414  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.574\n",
      "iter 20601/30000  loss         0.095414  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20620/30000  loss         0.095407  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.576\n",
      "iter 20621/30000  loss         0.095407  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.576\n",
      "iter 20640/30000  loss         0.095400  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.579\n",
      "iter 20641/30000  loss         0.095400  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.579\n",
      "iter 20660/30000  loss         0.095393  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.581\n",
      "iter 20661/30000  loss         0.095393  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.581\n",
      "iter 20680/30000  loss         0.095386  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.583\n",
      "iter 20681/30000  loss         0.095386  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.583\n",
      "iter 20700/30000  loss         0.095379  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.586\n",
      "iter 20701/30000  loss         0.095379  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    7.586\n",
      "iter 20720/30000  loss         0.095372  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.588\n",
      "iter 20721/30000  loss         0.095372  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.588\n",
      "iter 20740/30000  loss         0.095365  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.591\n",
      "iter 20741/30000  loss         0.095365  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.591\n",
      "iter 20760/30000  loss         0.095358  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.593\n",
      "iter 20761/30000  loss         0.095358  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.593\n",
      "iter 20780/30000  loss         0.095352  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.595\n",
      "iter 20781/30000  loss         0.095351  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.595\n",
      "iter 20800/30000  loss         0.095345  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.598\n",
      "iter 20801/30000  loss         0.095344  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.598\n",
      "iter 20820/30000  loss         0.095338  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.600\n",
      "iter 20821/30000  loss         0.095337  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.600\n",
      "iter 20840/30000  loss         0.095331  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.602\n",
      "iter 20841/30000  loss         0.095331  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.603\n",
      "iter 20860/30000  loss         0.095324  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.605\n",
      "iter 20861/30000  loss         0.095324  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.605\n",
      "iter 20880/30000  loss         0.095317  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.607\n",
      "iter 20881/30000  loss         0.095317  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.607\n",
      "iter 20900/30000  loss         0.095310  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.610\n",
      "iter 20901/30000  loss         0.095310  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.610\n",
      "iter 20920/30000  loss         0.095304  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.612\n",
      "iter 20921/30000  loss         0.095303  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.612\n",
      "iter 20940/30000  loss         0.095297  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.614\n",
      "iter 20941/30000  loss         0.095296  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.614\n",
      "iter 20960/30000  loss         0.095290  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.617\n",
      "iter 20961/30000  loss         0.095290  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.617\n",
      "iter 20980/30000  loss         0.095283  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.619\n",
      "iter 20981/30000  loss         0.095283  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.619\n",
      "iter 21000/30000  loss         0.095277  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.621\n",
      "iter 21001/30000  loss         0.095276  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.621\n",
      "iter 21020/30000  loss         0.095270  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.624\n",
      "iter 21021/30000  loss         0.095269  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.624\n",
      "iter 21040/30000  loss         0.095263  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.626\n",
      "iter 21041/30000  loss         0.095263  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.626\n",
      "iter 21060/30000  loss         0.095256  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.628\n",
      "iter 21061/30000  loss         0.095256  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.628\n",
      "iter 21080/30000  loss         0.095250  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.631\n",
      "iter 21081/30000  loss         0.095249  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.631\n",
      "iter 21100/30000  loss         0.095243  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.633\n",
      "iter 21101/30000  loss         0.095243  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.633\n",
      "iter 21120/30000  loss         0.095236  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.635\n",
      "iter 21121/30000  loss         0.095236  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.636\n",
      "iter 21140/30000  loss         0.095230  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.638\n",
      "iter 21141/30000  loss         0.095229  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.638\n",
      "iter 21160/30000  loss         0.095223  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.640\n",
      "iter 21161/30000  loss         0.095223  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.640\n",
      "iter 21180/30000  loss         0.095216  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.642\n",
      "iter 21181/30000  loss         0.095216  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.642\n",
      "iter 21200/30000  loss         0.095210  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.645\n",
      "iter 21201/30000  loss         0.095209  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.645\n",
      "iter 21220/30000  loss         0.095203  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.647\n",
      "iter 21221/30000  loss         0.095203  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.647\n",
      "iter 21240/30000  loss         0.095197  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.649\n",
      "iter 21241/30000  loss         0.095196  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.649\n",
      "iter 21260/30000  loss         0.095190  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.652\n",
      "iter 21261/30000  loss         0.095190  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.652\n",
      "iter 21280/30000  loss         0.095183  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.654\n",
      "iter 21281/30000  loss         0.095183  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    7.654\n",
      "iter 21300/30000  loss         0.095177  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.656\n",
      "iter 21301/30000  loss         0.095177  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.656\n",
      "iter 21320/30000  loss         0.095170  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.659\n",
      "iter 21321/30000  loss         0.095170  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.659\n",
      "iter 21340/30000  loss         0.095164  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.661\n",
      "iter 21341/30000  loss         0.095163  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.661\n",
      "iter 21360/30000  loss         0.095157  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.663\n",
      "iter 21361/30000  loss         0.095157  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.663\n",
      "iter 21380/30000  loss         0.095151  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.665\n",
      "iter 21381/30000  loss         0.095150  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.666\n",
      "iter 21400/30000  loss         0.095144  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.668\n",
      "iter 21401/30000  loss         0.095144  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21420/30000  loss         0.095138  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.670\n",
      "iter 21421/30000  loss         0.095138  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.670\n",
      "iter 21440/30000  loss         0.095131  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.672\n",
      "iter 21441/30000  loss         0.095131  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.672\n",
      "iter 21460/30000  loss         0.095125  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.675\n",
      "iter 21461/30000  loss         0.095125  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.675\n",
      "iter 21480/30000  loss         0.095119  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.677\n",
      "iter 21481/30000  loss         0.095118  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.677\n",
      "iter 21500/30000  loss         0.095112  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.679\n",
      "iter 21501/30000  loss         0.095112  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.679\n",
      "iter 21520/30000  loss         0.095106  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.682\n",
      "iter 21521/30000  loss         0.095105  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.682\n",
      "iter 21540/30000  loss         0.095099  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.684\n",
      "iter 21541/30000  loss         0.095099  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.684\n",
      "iter 21560/30000  loss         0.095093  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.686\n",
      "iter 21561/30000  loss         0.095093  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.686\n",
      "iter 21580/30000  loss         0.095087  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.688\n",
      "iter 21581/30000  loss         0.095086  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.688\n",
      "iter 21600/30000  loss         0.095080  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.691\n",
      "iter 21601/30000  loss         0.095080  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.691\n",
      "iter 21620/30000  loss         0.095074  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.693\n",
      "iter 21621/30000  loss         0.095074  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.693\n",
      "iter 21640/30000  loss         0.095068  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.695\n",
      "iter 21641/30000  loss         0.095067  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.695\n",
      "iter 21660/30000  loss         0.095061  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.697\n",
      "iter 21661/30000  loss         0.095061  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.698\n",
      "iter 21680/30000  loss         0.095055  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.700\n",
      "iter 21681/30000  loss         0.095055  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.700\n",
      "iter 21700/30000  loss         0.095049  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.702\n",
      "iter 21701/30000  loss         0.095048  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.702\n",
      "iter 21720/30000  loss         0.095042  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.704\n",
      "iter 21721/30000  loss         0.095042  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.704\n",
      "iter 21740/30000  loss         0.095036  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.706\n",
      "iter 21741/30000  loss         0.095036  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.707\n",
      "iter 21760/30000  loss         0.095030  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.709\n",
      "iter 21761/30000  loss         0.095029  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.709\n",
      "iter 21780/30000  loss         0.095024  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.711\n",
      "iter 21781/30000  loss         0.095023  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.711\n",
      "iter 21800/30000  loss         0.095017  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.713\n",
      "iter 21801/30000  loss         0.095017  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.713\n",
      "iter 21820/30000  loss         0.095011  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.715\n",
      "iter 21821/30000  loss         0.095011  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.715\n",
      "iter 21840/30000  loss         0.095005  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.718\n",
      "iter 21841/30000  loss         0.095005  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.718\n",
      "iter 21860/30000  loss         0.094999  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.720\n",
      "iter 21861/30000  loss         0.094998  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.720\n",
      "iter 21880/30000  loss         0.094993  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.722\n",
      "iter 21881/30000  loss         0.094992  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.722\n",
      "iter 21900/30000  loss         0.094986  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.724\n",
      "iter 21901/30000  loss         0.094986  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.724\n",
      "iter 21920/30000  loss         0.094980  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    7.727\n",
      "iter 21921/30000  loss         0.094980  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.727\n",
      "iter 21940/30000  loss         0.094974  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.729\n",
      "iter 21941/30000  loss         0.094974  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.729\n",
      "iter 21960/30000  loss         0.094968  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.731\n",
      "iter 21961/30000  loss         0.094968  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.731\n",
      "iter 21980/30000  loss         0.094962  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.733\n",
      "iter 21981/30000  loss         0.094962  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.733\n",
      "iter 22000/30000  loss         0.094956  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.735\n",
      "iter 22001/30000  loss         0.094955  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.736\n",
      "iter 22020/30000  loss         0.094950  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.738\n",
      "iter 22021/30000  loss         0.094949  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.738\n",
      "iter 22040/30000  loss         0.094944  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.740\n",
      "iter 22041/30000  loss         0.094943  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.740\n",
      "iter 22060/30000  loss         0.094937  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.742\n",
      "iter 22061/30000  loss         0.094937  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.742\n",
      "iter 22080/30000  loss         0.094931  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.744\n",
      "iter 22081/30000  loss         0.094931  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.744\n",
      "iter 22100/30000  loss         0.094925  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.746\n",
      "iter 22101/30000  loss         0.094925  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.747\n",
      "iter 22120/30000  loss         0.094919  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.749\n",
      "iter 22121/30000  loss         0.094919  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.749\n",
      "iter 22140/30000  loss         0.094913  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.751\n",
      "iter 22141/30000  loss         0.094913  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.751\n",
      "iter 22160/30000  loss         0.094907  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.753\n",
      "iter 22161/30000  loss         0.094907  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.753\n",
      "iter 22180/30000  loss         0.094901  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.755\n",
      "iter 22181/30000  loss         0.094901  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.755\n",
      "iter 22200/30000  loss         0.094895  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.757\n",
      "iter 22201/30000  loss         0.094895  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22220/30000  loss         0.094889  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.760\n",
      "iter 22221/30000  loss         0.094889  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.760\n",
      "iter 22240/30000  loss         0.094883  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.762\n",
      "iter 22241/30000  loss         0.094883  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.762\n",
      "iter 22260/30000  loss         0.094877  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.764\n",
      "iter 22261/30000  loss         0.094877  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.764\n",
      "iter 22280/30000  loss         0.094871  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.766\n",
      "iter 22281/30000  loss         0.094871  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.766\n",
      "iter 22300/30000  loss         0.094866  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.768\n",
      "iter 22301/30000  loss         0.094865  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.768\n",
      "iter 22320/30000  loss         0.094860  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.771\n",
      "iter 22321/30000  loss         0.094859  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.771\n",
      "iter 22340/30000  loss         0.094854  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.773\n",
      "iter 22341/30000  loss         0.094853  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.773\n",
      "iter 22360/30000  loss         0.094848  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.775\n",
      "iter 22361/30000  loss         0.094848  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.775\n",
      "iter 22380/30000  loss         0.094842  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.777\n",
      "iter 22381/30000  loss         0.094842  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.777\n",
      "iter 22400/30000  loss         0.094836  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.779\n",
      "iter 22401/30000  loss         0.094836  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.779\n",
      "iter 22420/30000  loss         0.094830  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.781\n",
      "iter 22421/30000  loss         0.094830  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.781\n",
      "iter 22440/30000  loss         0.094824  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.783\n",
      "iter 22441/30000  loss         0.094824  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.784\n",
      "iter 22460/30000  loss         0.094818  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.786\n",
      "iter 22461/30000  loss         0.094818  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.786\n",
      "iter 22480/30000  loss         0.094813  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.788\n",
      "iter 22481/30000  loss         0.094812  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.788\n",
      "iter 22500/30000  loss         0.094807  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.790\n",
      "iter 22501/30000  loss         0.094807  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.790\n",
      "iter 22520/30000  loss         0.094801  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.792\n",
      "iter 22521/30000  loss         0.094801  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.792\n",
      "iter 22540/30000  loss         0.094795  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.794\n",
      "iter 22541/30000  loss         0.094795  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.794\n",
      "iter 22560/30000  loss         0.094789  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.796\n",
      "iter 22561/30000  loss         0.094789  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    7.797\n",
      "iter 22580/30000  loss         0.094784  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.799\n",
      "iter 22581/30000  loss         0.094783  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.799\n",
      "iter 22600/30000  loss         0.094778  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.801\n",
      "iter 22601/30000  loss         0.094778  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.801\n",
      "iter 22620/30000  loss         0.094772  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.803\n",
      "iter 22621/30000  loss         0.094772  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.803\n",
      "iter 22640/30000  loss         0.094766  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.805\n",
      "iter 22641/30000  loss         0.094766  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.805\n",
      "iter 22660/30000  loss         0.094761  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.807\n",
      "iter 22661/30000  loss         0.094760  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.807\n",
      "iter 22680/30000  loss         0.094755  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.809\n",
      "iter 22681/30000  loss         0.094755  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.809\n",
      "iter 22700/30000  loss         0.094749  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.811\n",
      "iter 22701/30000  loss         0.094749  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.811\n",
      "iter 22720/30000  loss         0.094744  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.813\n",
      "iter 22721/30000  loss         0.094743  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.814\n",
      "iter 22740/30000  loss         0.094738  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.816\n",
      "iter 22741/30000  loss         0.094738  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.816\n",
      "iter 22760/30000  loss         0.094732  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.818\n",
      "iter 22761/30000  loss         0.094732  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.818\n",
      "iter 22780/30000  loss         0.094727  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.820\n",
      "iter 22781/30000  loss         0.094726  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.820\n",
      "iter 22800/30000  loss         0.094721  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.822\n",
      "iter 22801/30000  loss         0.094721  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.822\n",
      "iter 22820/30000  loss         0.094715  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.824\n",
      "iter 22821/30000  loss         0.094715  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.824\n",
      "iter 22840/30000  loss         0.094710  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.826\n",
      "iter 22841/30000  loss         0.094709  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.826\n",
      "iter 22860/30000  loss         0.094704  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.828\n",
      "iter 22861/30000  loss         0.094704  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.828\n",
      "iter 22880/30000  loss         0.094698  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.830\n",
      "iter 22881/30000  loss         0.094698  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.830\n",
      "iter 22900/30000  loss         0.094693  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.832\n",
      "iter 22901/30000  loss         0.094693  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.833\n",
      "iter 22920/30000  loss         0.094687  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.835\n",
      "iter 22921/30000  loss         0.094687  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.835\n",
      "iter 22940/30000  loss         0.094682  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.837\n",
      "iter 22941/30000  loss         0.094681  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.837\n",
      "iter 22960/30000  loss         0.094676  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.839\n",
      "iter 22961/30000  loss         0.094676  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.839\n",
      "iter 22980/30000  loss         0.094671  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.841\n",
      "iter 22981/30000  loss         0.094670  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.841\n",
      "iter 23000/30000  loss         0.094665  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.843\n",
      "iter 23001/30000  loss         0.094665  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23020/30000  loss         0.094660  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.845\n",
      "iter 23021/30000  loss         0.094659  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.845\n",
      "iter 23040/30000  loss         0.094654  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.847\n",
      "iter 23041/30000  loss         0.094654  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.847\n",
      "iter 23060/30000  loss         0.094648  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.849\n",
      "iter 23061/30000  loss         0.094648  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.849\n",
      "iter 23080/30000  loss         0.094643  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.851\n",
      "iter 23081/30000  loss         0.094643  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.851\n",
      "iter 23100/30000  loss         0.094637  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.853\n",
      "iter 23101/30000  loss         0.094637  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.853\n",
      "iter 23120/30000  loss         0.094632  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.855\n",
      "iter 23121/30000  loss         0.094632  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.856\n",
      "iter 23140/30000  loss         0.094627  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.857\n",
      "iter 23141/30000  loss         0.094626  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.858\n",
      "iter 23160/30000  loss         0.094621  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.860\n",
      "iter 23161/30000  loss         0.094621  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.860\n",
      "iter 23180/30000  loss         0.094616  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.862\n",
      "iter 23181/30000  loss         0.094615  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.862\n",
      "iter 23200/30000  loss         0.094610  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.864\n",
      "iter 23201/30000  loss         0.094610  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.864\n",
      "iter 23220/30000  loss         0.094605  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.866\n",
      "iter 23221/30000  loss         0.094605  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.866\n",
      "iter 23240/30000  loss         0.094599  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.868\n",
      "iter 23241/30000  loss         0.094599  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    7.868\n",
      "iter 23260/30000  loss         0.094594  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.870\n",
      "iter 23261/30000  loss         0.094594  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.870\n",
      "iter 23280/30000  loss         0.094589  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.872\n",
      "iter 23281/30000  loss         0.094588  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.872\n",
      "iter 23300/30000  loss         0.094583  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.874\n",
      "iter 23301/30000  loss         0.094583  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.874\n",
      "iter 23320/30000  loss         0.094578  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.876\n",
      "iter 23321/30000  loss         0.094578  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.876\n",
      "iter 23340/30000  loss         0.094572  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.878\n",
      "iter 23341/30000  loss         0.094572  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.878\n",
      "iter 23360/30000  loss         0.094567  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.880\n",
      "iter 23361/30000  loss         0.094567  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.880\n",
      "iter 23380/30000  loss         0.094562  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.882\n",
      "iter 23381/30000  loss         0.094561  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.882\n",
      "iter 23400/30000  loss         0.094556  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.884\n",
      "iter 23401/30000  loss         0.094556  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.884\n",
      "iter 23420/30000  loss         0.094551  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.886\n",
      "iter 23421/30000  loss         0.094551  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.886\n",
      "iter 23440/30000  loss         0.094546  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.888\n",
      "iter 23441/30000  loss         0.094545  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.888\n",
      "iter 23460/30000  loss         0.094540  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.890\n",
      "iter 23461/30000  loss         0.094540  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.890\n",
      "iter 23480/30000  loss         0.094535  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.892\n",
      "iter 23481/30000  loss         0.094535  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.892\n",
      "iter 23500/30000  loss         0.094530  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.894\n",
      "iter 23501/30000  loss         0.094530  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.894\n",
      "iter 23520/30000  loss         0.094525  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.896\n",
      "iter 23521/30000  loss         0.094524  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.896\n",
      "iter 23540/30000  loss         0.094519  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.898\n",
      "iter 23541/30000  loss         0.094519  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.899\n",
      "iter 23560/30000  loss         0.094514  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.900\n",
      "iter 23561/30000  loss         0.094514  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.901\n",
      "iter 23580/30000  loss         0.094509  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.902\n",
      "iter 23581/30000  loss         0.094509  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.903\n",
      "iter 23600/30000  loss         0.094504  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.904\n",
      "iter 23601/30000  loss         0.094503  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.905\n",
      "iter 23620/30000  loss         0.094498  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.906\n",
      "iter 23621/30000  loss         0.094498  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.907\n",
      "iter 23640/30000  loss         0.094493  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.909\n",
      "iter 23641/30000  loss         0.094493  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.909\n",
      "iter 23660/30000  loss         0.094488  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.911\n",
      "iter 23661/30000  loss         0.094488  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.911\n",
      "iter 23680/30000  loss         0.094483  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.913\n",
      "iter 23681/30000  loss         0.094482  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.913\n",
      "iter 23700/30000  loss         0.094478  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.915\n",
      "iter 23701/30000  loss         0.094477  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.915\n",
      "iter 23720/30000  loss         0.094472  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.917\n",
      "iter 23721/30000  loss         0.094472  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.917\n",
      "iter 23740/30000  loss         0.094467  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.919\n",
      "iter 23741/30000  loss         0.094467  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.919\n",
      "iter 23760/30000  loss         0.094462  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.921\n",
      "iter 23761/30000  loss         0.094462  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.921\n",
      "iter 23780/30000  loss         0.094457  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.923\n",
      "iter 23781/30000  loss         0.094457  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.923\n",
      "iter 23800/30000  loss         0.094452  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.925\n",
      "iter 23801/30000  loss         0.094452  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23820/30000  loss         0.094447  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.927\n",
      "iter 23821/30000  loss         0.094446  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.927\n",
      "iter 23840/30000  loss         0.094442  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.929\n",
      "iter 23841/30000  loss         0.094441  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.929\n",
      "iter 23860/30000  loss         0.094436  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.930\n",
      "iter 23861/30000  loss         0.094436  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.931\n",
      "iter 23880/30000  loss         0.094431  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.932\n",
      "iter 23881/30000  loss         0.094431  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.933\n",
      "iter 23900/30000  loss         0.094426  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.934\n",
      "iter 23901/30000  loss         0.094426  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.935\n",
      "iter 23920/30000  loss         0.094421  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.936\n",
      "iter 23921/30000  loss         0.094421  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.937\n",
      "iter 23940/30000  loss         0.094416  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.938\n",
      "iter 23941/30000  loss         0.094416  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.939\n",
      "iter 23960/30000  loss         0.094411  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.940\n",
      "iter 23961/30000  loss         0.094411  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    7.940\n",
      "iter 23980/30000  loss         0.094406  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.942\n",
      "iter 23981/30000  loss         0.094406  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.942\n",
      "iter 24000/30000  loss         0.094401  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.944\n",
      "iter 24001/30000  loss         0.094401  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.944\n",
      "iter 24020/30000  loss         0.094396  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.946\n",
      "iter 24021/30000  loss         0.094396  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.946\n",
      "iter 24040/30000  loss         0.094391  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.948\n",
      "iter 24041/30000  loss         0.094391  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.948\n",
      "iter 24060/30000  loss         0.094386  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.950\n",
      "iter 24061/30000  loss         0.094386  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.950\n",
      "iter 24080/30000  loss         0.094381  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.952\n",
      "iter 24081/30000  loss         0.094381  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.952\n",
      "iter 24100/30000  loss         0.094376  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.954\n",
      "iter 24101/30000  loss         0.094376  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.954\n",
      "iter 24120/30000  loss         0.094371  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.956\n",
      "iter 24121/30000  loss         0.094371  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.956\n",
      "iter 24140/30000  loss         0.094366  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.958\n",
      "iter 24141/30000  loss         0.094366  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.958\n",
      "iter 24160/30000  loss         0.094361  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.960\n",
      "iter 24161/30000  loss         0.094361  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.960\n",
      "iter 24180/30000  loss         0.094356  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.962\n",
      "iter 24181/30000  loss         0.094356  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.962\n",
      "iter 24200/30000  loss         0.094351  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.964\n",
      "iter 24201/30000  loss         0.094351  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.964\n",
      "iter 24220/30000  loss         0.094346  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.966\n",
      "iter 24221/30000  loss         0.094346  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.966\n",
      "iter 24240/30000  loss         0.094341  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.968\n",
      "iter 24241/30000  loss         0.094341  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.968\n",
      "iter 24260/30000  loss         0.094336  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.970\n",
      "iter 24261/30000  loss         0.094336  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.970\n",
      "iter 24280/30000  loss         0.094331  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.972\n",
      "iter 24281/30000  loss         0.094331  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.972\n",
      "iter 24300/30000  loss         0.094326  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.974\n",
      "iter 24301/30000  loss         0.094326  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.974\n",
      "iter 24320/30000  loss         0.094322  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.976\n",
      "iter 24321/30000  loss         0.094321  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.976\n",
      "iter 24340/30000  loss         0.094317  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.978\n",
      "iter 24341/30000  loss         0.094316  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.978\n",
      "iter 24360/30000  loss         0.094312  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.979\n",
      "iter 24361/30000  loss         0.094311  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.980\n",
      "iter 24380/30000  loss         0.094307  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.981\n",
      "iter 24381/30000  loss         0.094307  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.981\n",
      "iter 24400/30000  loss         0.094302  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.983\n",
      "iter 24401/30000  loss         0.094302  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.983\n",
      "iter 24420/30000  loss         0.094297  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.985\n",
      "iter 24421/30000  loss         0.094297  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.985\n",
      "iter 24440/30000  loss         0.094292  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.987\n",
      "iter 24441/30000  loss         0.094292  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.987\n",
      "iter 24460/30000  loss         0.094287  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.989\n",
      "iter 24461/30000  loss         0.094287  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.989\n",
      "iter 24480/30000  loss         0.094283  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.991\n",
      "iter 24481/30000  loss         0.094282  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.991\n",
      "iter 24500/30000  loss         0.094278  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.993\n",
      "iter 24501/30000  loss         0.094278  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.993\n",
      "iter 24520/30000  loss         0.094273  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.995\n",
      "iter 24521/30000  loss         0.094273  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.995\n",
      "iter 24540/30000  loss         0.094268  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.997\n",
      "iter 24541/30000  loss         0.094268  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.997\n",
      "iter 24560/30000  loss         0.094263  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.999\n",
      "iter 24561/30000  loss         0.094263  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    7.999\n",
      "iter 24580/30000  loss         0.094259  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.001\n",
      "iter 24581/30000  loss         0.094258  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.001\n",
      "iter 24600/30000  loss         0.094254  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.002\n",
      "iter 24601/30000  loss         0.094254  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24620/30000  loss         0.094249  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.004\n",
      "iter 24621/30000  loss         0.094249  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.004\n",
      "iter 24640/30000  loss         0.094244  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.006\n",
      "iter 24641/30000  loss         0.094244  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.006\n",
      "iter 24660/30000  loss         0.094240  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.008\n",
      "iter 24661/30000  loss         0.094239  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.008\n",
      "iter 24680/30000  loss         0.094235  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.010\n",
      "iter 24681/30000  loss         0.094235  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.010\n",
      "iter 24700/30000  loss         0.094230  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.012\n",
      "iter 24701/30000  loss         0.094230  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.012\n",
      "iter 24720/30000  loss         0.094225  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.014\n",
      "iter 24721/30000  loss         0.094225  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    8.014\n",
      "iter 24740/30000  loss         0.094221  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.016\n",
      "iter 24741/30000  loss         0.094221  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.016\n",
      "iter 24760/30000  loss         0.094216  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.018\n",
      "iter 24761/30000  loss         0.094216  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.018\n",
      "iter 24780/30000  loss         0.094211  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.020\n",
      "iter 24781/30000  loss         0.094211  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.020\n",
      "iter 24800/30000  loss         0.094207  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.021\n",
      "iter 24801/30000  loss         0.094206  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.021\n",
      "iter 24820/30000  loss         0.094202  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.023\n",
      "iter 24821/30000  loss         0.094202  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.023\n",
      "iter 24840/30000  loss         0.094197  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.025\n",
      "iter 24841/30000  loss         0.094197  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.025\n",
      "iter 24860/30000  loss         0.094193  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.027\n",
      "iter 24861/30000  loss         0.094192  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.027\n",
      "iter 24880/30000  loss         0.094188  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.029\n",
      "iter 24881/30000  loss         0.094188  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.029\n",
      "iter 24900/30000  loss         0.094183  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.031\n",
      "iter 24901/30000  loss         0.094183  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.031\n",
      "iter 24920/30000  loss         0.094179  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.033\n",
      "iter 24921/30000  loss         0.094179  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.033\n",
      "iter 24940/30000  loss         0.094174  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.035\n",
      "iter 24941/30000  loss         0.094174  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.035\n",
      "iter 24960/30000  loss         0.094169  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.036\n",
      "iter 24961/30000  loss         0.094169  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.036\n",
      "iter 24980/30000  loss         0.094165  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.038\n",
      "iter 24981/30000  loss         0.094165  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.038\n",
      "iter 25000/30000  loss         0.094160  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.040\n",
      "iter 25001/30000  loss         0.094160  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.040\n",
      "iter 25020/30000  loss         0.094156  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.042\n",
      "iter 25021/30000  loss         0.094155  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.042\n",
      "iter 25040/30000  loss         0.094151  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.044\n",
      "iter 25041/30000  loss         0.094151  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.044\n",
      "iter 25060/30000  loss         0.094147  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.046\n",
      "iter 25061/30000  loss         0.094146  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.046\n",
      "iter 25080/30000  loss         0.094142  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.048\n",
      "iter 25081/30000  loss         0.094142  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.048\n",
      "iter 25100/30000  loss         0.094137  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.049\n",
      "iter 25101/30000  loss         0.094137  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.049\n",
      "iter 25120/30000  loss         0.094133  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.051\n",
      "iter 25121/30000  loss         0.094133  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.051\n",
      "iter 25140/30000  loss         0.094128  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.053\n",
      "iter 25141/30000  loss         0.094128  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.053\n",
      "iter 25160/30000  loss         0.094124  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.055\n",
      "iter 25161/30000  loss         0.094124  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.055\n",
      "iter 25180/30000  loss         0.094119  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.057\n",
      "iter 25181/30000  loss         0.094119  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.057\n",
      "iter 25200/30000  loss         0.094115  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.059\n",
      "iter 25201/30000  loss         0.094115  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.059\n",
      "iter 25220/30000  loss         0.094110  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.060\n",
      "iter 25221/30000  loss         0.094110  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.061\n",
      "iter 25240/30000  loss         0.094106  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.062\n",
      "iter 25241/30000  loss         0.094105  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.062\n",
      "iter 25260/30000  loss         0.094101  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.064\n",
      "iter 25261/30000  loss         0.094101  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.064\n",
      "iter 25280/30000  loss         0.094097  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.066\n",
      "iter 25281/30000  loss         0.094097  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.066\n",
      "iter 25300/30000  loss         0.094092  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.068\n",
      "iter 25301/30000  loss         0.094092  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.068\n",
      "iter 25320/30000  loss         0.094088  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.070\n",
      "iter 25321/30000  loss         0.094088  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.070\n",
      "iter 25340/30000  loss         0.094083  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.071\n",
      "iter 25341/30000  loss         0.094083  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.072\n",
      "iter 25360/30000  loss         0.094079  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.073\n",
      "iter 25361/30000  loss         0.094079  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.073\n",
      "iter 25380/30000  loss         0.094074  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.075\n",
      "iter 25381/30000  loss         0.094074  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.075\n",
      "iter 25400/30000  loss         0.094070  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.077\n",
      "iter 25401/30000  loss         0.094070  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25420/30000  loss         0.094066  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.079\n",
      "iter 25421/30000  loss         0.094065  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.079\n",
      "iter 25440/30000  loss         0.094061  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.081\n",
      "iter 25441/30000  loss         0.094061  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.081\n",
      "iter 25460/30000  loss         0.094057  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.082\n",
      "iter 25461/30000  loss         0.094057  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.082\n",
      "iter 25480/30000  loss         0.094052  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.084\n",
      "iter 25481/30000  loss         0.094052  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.084\n",
      "iter 25500/30000  loss         0.094048  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.086\n",
      "iter 25501/30000  loss         0.094048  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.086\n",
      "iter 25520/30000  loss         0.094044  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.088\n",
      "iter 25521/30000  loss         0.094043  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    8.088\n",
      "iter 25540/30000  loss         0.094039  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.090\n",
      "iter 25541/30000  loss         0.094039  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.090\n",
      "iter 25560/30000  loss         0.094035  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.091\n",
      "iter 25561/30000  loss         0.094035  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.092\n",
      "iter 25580/30000  loss         0.094030  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.093\n",
      "iter 25581/30000  loss         0.094030  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.093\n",
      "iter 25600/30000  loss         0.094026  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.095\n",
      "iter 25601/30000  loss         0.094026  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.095\n",
      "iter 25620/30000  loss         0.094022  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.097\n",
      "iter 25621/30000  loss         0.094021  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.097\n",
      "iter 25640/30000  loss         0.094017  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.099\n",
      "iter 25641/30000  loss         0.094017  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.099\n",
      "iter 25660/30000  loss         0.094013  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.100\n",
      "iter 25661/30000  loss         0.094013  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.101\n",
      "iter 25680/30000  loss         0.094009  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.102\n",
      "iter 25681/30000  loss         0.094008  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.102\n",
      "iter 25700/30000  loss         0.094004  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.104\n",
      "iter 25701/30000  loss         0.094004  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.104\n",
      "iter 25720/30000  loss         0.094000  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.106\n",
      "iter 25721/30000  loss         0.094000  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.106\n",
      "iter 25740/30000  loss         0.093996  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.108\n",
      "iter 25741/30000  loss         0.093996  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.108\n",
      "iter 25760/30000  loss         0.093991  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.109\n",
      "iter 25761/30000  loss         0.093991  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.109\n",
      "iter 25780/30000  loss         0.093987  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.111\n",
      "iter 25781/30000  loss         0.093987  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.111\n",
      "iter 25800/30000  loss         0.093983  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.113\n",
      "iter 25801/30000  loss         0.093983  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.113\n",
      "iter 25820/30000  loss         0.093979  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.115\n",
      "iter 25821/30000  loss         0.093978  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.115\n",
      "iter 25840/30000  loss         0.093974  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.117\n",
      "iter 25841/30000  loss         0.093974  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.117\n",
      "iter 25860/30000  loss         0.093970  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.118\n",
      "iter 25861/30000  loss         0.093970  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.118\n",
      "iter 25880/30000  loss         0.093966  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.120\n",
      "iter 25881/30000  loss         0.093966  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.120\n",
      "iter 25900/30000  loss         0.093962  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.122\n",
      "iter 25901/30000  loss         0.093961  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.122\n",
      "iter 25920/30000  loss         0.093957  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.124\n",
      "iter 25921/30000  loss         0.093957  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.124\n",
      "iter 25940/30000  loss         0.093953  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.125\n",
      "iter 25941/30000  loss         0.093953  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.125\n",
      "iter 25960/30000  loss         0.093949  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.127\n",
      "iter 25961/30000  loss         0.093949  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.127\n",
      "iter 25980/30000  loss         0.093945  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.129\n",
      "iter 25981/30000  loss         0.093944  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.129\n",
      "iter 26000/30000  loss         0.093940  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.131\n",
      "iter 26001/30000  loss         0.093940  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.131\n",
      "iter 26020/30000  loss         0.093936  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.132\n",
      "iter 26021/30000  loss         0.093936  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.133\n",
      "iter 26040/30000  loss         0.093932  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.134\n",
      "iter 26041/30000  loss         0.093932  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.134\n",
      "iter 26060/30000  loss         0.093928  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.136\n",
      "iter 26061/30000  loss         0.093928  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.136\n",
      "iter 26080/30000  loss         0.093924  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.138\n",
      "iter 26081/30000  loss         0.093924  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.138\n",
      "iter 26100/30000  loss         0.093920  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.139\n",
      "iter 26101/30000  loss         0.093919  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.140\n",
      "iter 26120/30000  loss         0.093915  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.141\n",
      "iter 26121/30000  loss         0.093915  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.141\n",
      "iter 26140/30000  loss         0.093911  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.143\n",
      "iter 26141/30000  loss         0.093911  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.143\n",
      "iter 26160/30000  loss         0.093907  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.145\n",
      "iter 26161/30000  loss         0.093907  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.145\n",
      "iter 26180/30000  loss         0.093903  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.146\n",
      "iter 26181/30000  loss         0.093903  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.147\n",
      "iter 26200/30000  loss         0.093899  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.148\n",
      "iter 26201/30000  loss         0.093899  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26220/30000  loss         0.093895  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.150\n",
      "iter 26221/30000  loss         0.093894  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.150\n",
      "iter 26240/30000  loss         0.093891  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.152\n",
      "iter 26241/30000  loss         0.093890  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.152\n",
      "iter 26260/30000  loss         0.093886  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.153\n",
      "iter 26261/30000  loss         0.093886  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.153\n",
      "iter 26280/30000  loss         0.093882  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.155\n",
      "iter 26281/30000  loss         0.093882  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.155\n",
      "iter 26300/30000  loss         0.093878  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.157\n",
      "iter 26301/30000  loss         0.093878  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.157\n",
      "iter 26320/30000  loss         0.093874  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.159\n",
      "iter 26321/30000  loss         0.093874  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.159\n",
      "iter 26340/30000  loss         0.093870  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.160\n",
      "iter 26341/30000  loss         0.093870  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.160\n",
      "iter 26360/30000  loss         0.093866  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.162\n",
      "iter 26361/30000  loss         0.093866  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    8.162\n",
      "iter 26380/30000  loss         0.093862  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.164\n",
      "iter 26381/30000  loss         0.093862  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.164\n",
      "iter 26400/30000  loss         0.093858  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.165\n",
      "iter 26401/30000  loss         0.093858  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.166\n",
      "iter 26420/30000  loss         0.093854  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.167\n",
      "iter 26421/30000  loss         0.093854  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.167\n",
      "iter 26440/30000  loss         0.093850  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.169\n",
      "iter 26441/30000  loss         0.093850  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.169\n",
      "iter 26460/30000  loss         0.093846  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.171\n",
      "iter 26461/30000  loss         0.093846  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.171\n",
      "iter 26480/30000  loss         0.093842  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.172\n",
      "iter 26481/30000  loss         0.093841  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.172\n",
      "iter 26500/30000  loss         0.093838  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.174\n",
      "iter 26501/30000  loss         0.093837  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.174\n",
      "iter 26520/30000  loss         0.093834  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.176\n",
      "iter 26521/30000  loss         0.093833  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.176\n",
      "iter 26540/30000  loss         0.093830  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.178\n",
      "iter 26541/30000  loss         0.093829  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.178\n",
      "iter 26560/30000  loss         0.093826  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.179\n",
      "iter 26561/30000  loss         0.093825  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.179\n",
      "iter 26580/30000  loss         0.093822  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.181\n",
      "iter 26581/30000  loss         0.093821  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.181\n",
      "iter 26600/30000  loss         0.093818  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.183\n",
      "iter 26601/30000  loss         0.093817  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.183\n",
      "iter 26620/30000  loss         0.093814  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.184\n",
      "iter 26621/30000  loss         0.093813  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.184\n",
      "iter 26640/30000  loss         0.093810  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.186\n",
      "iter 26641/30000  loss         0.093809  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.186\n",
      "iter 26660/30000  loss         0.093806  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.188\n",
      "iter 26661/30000  loss         0.093806  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.188\n",
      "iter 26680/30000  loss         0.093802  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.189\n",
      "iter 26681/30000  loss         0.093802  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.190\n",
      "iter 26700/30000  loss         0.093798  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.191\n",
      "iter 26701/30000  loss         0.093798  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.191\n",
      "iter 26720/30000  loss         0.093794  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.193\n",
      "iter 26721/30000  loss         0.093794  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.193\n",
      "iter 26740/30000  loss         0.093790  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.195\n",
      "iter 26741/30000  loss         0.093790  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.195\n",
      "iter 26760/30000  loss         0.093786  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.196\n",
      "iter 26761/30000  loss         0.093786  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.196\n",
      "iter 26780/30000  loss         0.093782  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.198\n",
      "iter 26781/30000  loss         0.093782  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.198\n",
      "iter 26800/30000  loss         0.093778  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.200\n",
      "iter 26801/30000  loss         0.093778  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.200\n",
      "iter 26820/30000  loss         0.093774  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.201\n",
      "iter 26821/30000  loss         0.093774  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.201\n",
      "iter 26840/30000  loss         0.093770  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.203\n",
      "iter 26841/30000  loss         0.093770  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.203\n",
      "iter 26860/30000  loss         0.093766  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.205\n",
      "iter 26861/30000  loss         0.093766  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.205\n",
      "iter 26880/30000  loss         0.093762  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.206\n",
      "iter 26881/30000  loss         0.093762  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.206\n",
      "iter 26900/30000  loss         0.093759  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.208\n",
      "iter 26901/30000  loss         0.093758  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.208\n",
      "iter 26920/30000  loss         0.093755  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.210\n",
      "iter 26921/30000  loss         0.093755  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.210\n",
      "iter 26940/30000  loss         0.093751  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.211\n",
      "iter 26941/30000  loss         0.093751  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.211\n",
      "iter 26960/30000  loss         0.093747  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.213\n",
      "iter 26961/30000  loss         0.093747  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.213\n",
      "iter 26980/30000  loss         0.093743  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.215\n",
      "iter 26981/30000  loss         0.093743  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.215\n",
      "iter 27000/30000  loss         0.093739  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.216\n",
      "iter 27001/30000  loss         0.093739  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27020/30000  loss         0.093735  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.218\n",
      "iter 27021/30000  loss         0.093735  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.218\n",
      "iter 27040/30000  loss         0.093732  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.220\n",
      "iter 27041/30000  loss         0.093731  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.220\n",
      "iter 27060/30000  loss         0.093728  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.221\n",
      "iter 27061/30000  loss         0.093728  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.221\n",
      "iter 27080/30000  loss         0.093724  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.223\n",
      "iter 27081/30000  loss         0.093724  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.223\n",
      "iter 27100/30000  loss         0.093720  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.225\n",
      "iter 27101/30000  loss         0.093720  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.225\n",
      "iter 27120/30000  loss         0.093716  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.226\n",
      "iter 27121/30000  loss         0.093716  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.226\n",
      "iter 27140/30000  loss         0.093712  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.228\n",
      "iter 27141/30000  loss         0.093712  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.228\n",
      "iter 27160/30000  loss         0.093709  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.230\n",
      "iter 27161/30000  loss         0.093708  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.230\n",
      "iter 27180/30000  loss         0.093705  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.231\n",
      "iter 27181/30000  loss         0.093705  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.231\n",
      "iter 27200/30000  loss         0.093701  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.233\n",
      "iter 27201/30000  loss         0.093701  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.233\n",
      "iter 27220/30000  loss         0.093697  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.235\n",
      "iter 27221/30000  loss         0.093697  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.235\n",
      "iter 27240/30000  loss         0.093693  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.236\n",
      "iter 27241/30000  loss         0.093693  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    8.236\n",
      "iter 27260/30000  loss         0.093690  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.238\n",
      "iter 27261/30000  loss         0.093690  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.238\n",
      "iter 27280/30000  loss         0.093686  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.239\n",
      "iter 27281/30000  loss         0.093686  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.240\n",
      "iter 27300/30000  loss         0.093682  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.241\n",
      "iter 27301/30000  loss         0.093682  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.241\n",
      "iter 27320/30000  loss         0.093678  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.243\n",
      "iter 27321/30000  loss         0.093678  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.243\n",
      "iter 27340/30000  loss         0.093675  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.244\n",
      "iter 27341/30000  loss         0.093675  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.244\n",
      "iter 27360/30000  loss         0.093671  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.246\n",
      "iter 27361/30000  loss         0.093671  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.246\n",
      "iter 27380/30000  loss         0.093667  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.248\n",
      "iter 27381/30000  loss         0.093667  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.248\n",
      "iter 27400/30000  loss         0.093663  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.249\n",
      "iter 27401/30000  loss         0.093663  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.249\n",
      "iter 27420/30000  loss         0.093660  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.251\n",
      "iter 27421/30000  loss         0.093660  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.251\n",
      "iter 27440/30000  loss         0.093656  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.253\n",
      "iter 27441/30000  loss         0.093656  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.253\n",
      "iter 27460/30000  loss         0.093652  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.254\n",
      "iter 27461/30000  loss         0.093652  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.254\n",
      "iter 27480/30000  loss         0.093649  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.256\n",
      "iter 27481/30000  loss         0.093648  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.256\n",
      "iter 27500/30000  loss         0.093645  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.257\n",
      "iter 27501/30000  loss         0.093645  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.258\n",
      "iter 27520/30000  loss         0.093641  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.259\n",
      "iter 27521/30000  loss         0.093641  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.259\n",
      "iter 27540/30000  loss         0.093638  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.261\n",
      "iter 27541/30000  loss         0.093637  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.261\n",
      "iter 27560/30000  loss         0.093634  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.262\n",
      "iter 27561/30000  loss         0.093634  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.262\n",
      "iter 27580/30000  loss         0.093630  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.264\n",
      "iter 27581/30000  loss         0.093630  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.264\n",
      "iter 27600/30000  loss         0.093627  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.266\n",
      "iter 27601/30000  loss         0.093626  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.266\n",
      "iter 27620/30000  loss         0.093623  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.267\n",
      "iter 27621/30000  loss         0.093623  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.267\n",
      "iter 27640/30000  loss         0.093619  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.269\n",
      "iter 27641/30000  loss         0.093619  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.269\n",
      "iter 27660/30000  loss         0.093616  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.270\n",
      "iter 27661/30000  loss         0.093615  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.270\n",
      "iter 27680/30000  loss         0.093612  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.272\n",
      "iter 27681/30000  loss         0.093612  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.272\n",
      "iter 27700/30000  loss         0.093608  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.274\n",
      "iter 27701/30000  loss         0.093608  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.274\n",
      "iter 27720/30000  loss         0.093605  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.275\n",
      "iter 27721/30000  loss         0.093605  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.275\n",
      "iter 27740/30000  loss         0.093601  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.277\n",
      "iter 27741/30000  loss         0.093601  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.277\n",
      "iter 27760/30000  loss         0.093598  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.278\n",
      "iter 27761/30000  loss         0.093597  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.278\n",
      "iter 27780/30000  loss         0.093594  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.280\n",
      "iter 27781/30000  loss         0.093594  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.280\n",
      "iter 27800/30000  loss         0.093590  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.282\n",
      "iter 27801/30000  loss         0.093590  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27820/30000  loss         0.093587  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.283\n",
      "iter 27821/30000  loss         0.093587  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.283\n",
      "iter 27840/30000  loss         0.093583  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.285\n",
      "iter 27841/30000  loss         0.093583  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.285\n",
      "iter 27860/30000  loss         0.093580  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.286\n",
      "iter 27861/30000  loss         0.093579  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.286\n",
      "iter 27880/30000  loss         0.093576  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.288\n",
      "iter 27881/30000  loss         0.093576  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.288\n",
      "iter 27900/30000  loss         0.093572  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.290\n",
      "iter 27901/30000  loss         0.093572  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.290\n",
      "iter 27920/30000  loss         0.093569  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.291\n",
      "iter 27921/30000  loss         0.093569  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.291\n",
      "iter 27940/30000  loss         0.093565  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.293\n",
      "iter 27941/30000  loss         0.093565  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.293\n",
      "iter 27960/30000  loss         0.093562  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.294\n",
      "iter 27961/30000  loss         0.093562  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.294\n",
      "iter 27980/30000  loss         0.093558  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.296\n",
      "iter 27981/30000  loss         0.093558  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.296\n",
      "iter 28000/30000  loss         0.093555  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.297\n",
      "iter 28001/30000  loss         0.093554  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.298\n",
      "iter 28020/30000  loss         0.093551  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.299\n",
      "iter 28021/30000  loss         0.093551  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.299\n",
      "iter 28040/30000  loss         0.093548  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.301\n",
      "iter 28041/30000  loss         0.093547  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.301\n",
      "iter 28060/30000  loss         0.093544  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.302\n",
      "iter 28061/30000  loss         0.093544  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.302\n",
      "iter 28080/30000  loss         0.093541  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.304\n",
      "iter 28081/30000  loss         0.093540  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.304\n",
      "iter 28100/30000  loss         0.093537  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.305\n",
      "iter 28101/30000  loss         0.093537  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.305\n",
      "iter 28120/30000  loss         0.093533  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.307\n",
      "iter 28121/30000  loss         0.093533  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.307\n",
      "iter 28140/30000  loss         0.093530  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.308\n",
      "iter 28141/30000  loss         0.093530  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.309\n",
      "iter 28160/30000  loss         0.093526  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.310\n",
      "iter 28161/30000  loss         0.093526  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.310\n",
      "iter 28180/30000  loss         0.093523  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.312\n",
      "iter 28181/30000  loss         0.093523  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    8.312\n",
      "iter 28200/30000  loss         0.093520  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.313\n",
      "iter 28201/30000  loss         0.093519  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.313\n",
      "iter 28220/30000  loss         0.093516  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.315\n",
      "iter 28221/30000  loss         0.093516  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.315\n",
      "iter 28240/30000  loss         0.093513  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.316\n",
      "iter 28241/30000  loss         0.093512  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.316\n",
      "iter 28260/30000  loss         0.093509  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.318\n",
      "iter 28261/30000  loss         0.093509  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.318\n",
      "iter 28280/30000  loss         0.093506  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.319\n",
      "iter 28281/30000  loss         0.093505  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.319\n",
      "iter 28300/30000  loss         0.093502  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.321\n",
      "iter 28301/30000  loss         0.093502  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.321\n",
      "iter 28320/30000  loss         0.093499  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.323\n",
      "iter 28321/30000  loss         0.093499  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.323\n",
      "iter 28340/30000  loss         0.093495  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.324\n",
      "iter 28341/30000  loss         0.093495  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.324\n",
      "iter 28360/30000  loss         0.093492  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.326\n",
      "iter 28361/30000  loss         0.093492  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.326\n",
      "iter 28380/30000  loss         0.093488  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.327\n",
      "iter 28381/30000  loss         0.093488  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.327\n",
      "iter 28400/30000  loss         0.093485  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.329\n",
      "iter 28401/30000  loss         0.093485  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.329\n",
      "iter 28420/30000  loss         0.093482  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.330\n",
      "iter 28421/30000  loss         0.093481  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.330\n",
      "iter 28440/30000  loss         0.093478  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.332\n",
      "iter 28441/30000  loss         0.093478  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.332\n",
      "iter 28460/30000  loss         0.093475  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.333\n",
      "iter 28461/30000  loss         0.093475  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.333\n",
      "iter 28480/30000  loss         0.093471  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.335\n",
      "iter 28481/30000  loss         0.093471  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.335\n",
      "iter 28500/30000  loss         0.093468  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.336\n",
      "iter 28501/30000  loss         0.093468  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.336\n",
      "iter 28520/30000  loss         0.093465  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.338\n",
      "iter 28521/30000  loss         0.093464  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.338\n",
      "iter 28540/30000  loss         0.093461  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.339\n",
      "iter 28541/30000  loss         0.093461  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.340\n",
      "iter 28560/30000  loss         0.093458  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.341\n",
      "iter 28561/30000  loss         0.093458  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.341\n",
      "iter 28580/30000  loss         0.093454  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.343\n",
      "iter 28581/30000  loss         0.093454  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.343\n",
      "iter 28600/30000  loss         0.093451  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.344\n",
      "iter 28601/30000  loss         0.093451  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28620/30000  loss         0.093448  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.346\n",
      "iter 28621/30000  loss         0.093448  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.346\n",
      "iter 28640/30000  loss         0.093444  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.347\n",
      "iter 28641/30000  loss         0.093444  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.347\n",
      "iter 28660/30000  loss         0.093441  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.349\n",
      "iter 28661/30000  loss         0.093441  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.349\n",
      "iter 28680/30000  loss         0.093438  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.350\n",
      "iter 28681/30000  loss         0.093437  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.350\n",
      "iter 28700/30000  loss         0.093434  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.352\n",
      "iter 28701/30000  loss         0.093434  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.352\n",
      "iter 28720/30000  loss         0.093431  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.353\n",
      "iter 28721/30000  loss         0.093431  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.353\n",
      "iter 28740/30000  loss         0.093428  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.355\n",
      "iter 28741/30000  loss         0.093427  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.355\n",
      "iter 28760/30000  loss         0.093424  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.356\n",
      "iter 28761/30000  loss         0.093424  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.356\n",
      "iter 28780/30000  loss         0.093421  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.358\n",
      "iter 28781/30000  loss         0.093421  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.358\n",
      "iter 28800/30000  loss         0.093418  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.359\n",
      "iter 28801/30000  loss         0.093418  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.359\n",
      "iter 28820/30000  loss         0.093414  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.361\n",
      "iter 28821/30000  loss         0.093414  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.361\n",
      "iter 28840/30000  loss         0.093411  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.362\n",
      "iter 28841/30000  loss         0.093411  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.362\n",
      "iter 28860/30000  loss         0.093408  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.364\n",
      "iter 28861/30000  loss         0.093408  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.364\n",
      "iter 28880/30000  loss         0.093404  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.365\n",
      "iter 28881/30000  loss         0.093404  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.365\n",
      "iter 28900/30000  loss         0.093401  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.367\n",
      "iter 28901/30000  loss         0.093401  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.367\n",
      "iter 28920/30000  loss         0.093398  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.368\n",
      "iter 28921/30000  loss         0.093398  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.368\n",
      "iter 28940/30000  loss         0.093395  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.370\n",
      "iter 28941/30000  loss         0.093394  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.370\n",
      "iter 28960/30000  loss         0.093391  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.371\n",
      "iter 28961/30000  loss         0.093391  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.371\n",
      "iter 28980/30000  loss         0.093388  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.373\n",
      "iter 28981/30000  loss         0.093388  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.373\n",
      "iter 29000/30000  loss         0.093385  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.374\n",
      "iter 29001/30000  loss         0.093385  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.374\n",
      "iter 29020/30000  loss         0.093382  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.376\n",
      "iter 29021/30000  loss         0.093381  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.376\n",
      "iter 29040/30000  loss         0.093378  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.377\n",
      "iter 29041/30000  loss         0.093378  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.377\n",
      "iter 29060/30000  loss         0.093375  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.379\n",
      "iter 29061/30000  loss         0.093375  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.379\n",
      "iter 29080/30000  loss         0.093372  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.380\n",
      "iter 29081/30000  loss         0.093372  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.380\n",
      "iter 29100/30000  loss         0.093369  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.382\n",
      "iter 29101/30000  loss         0.093368  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.382\n",
      "iter 29120/30000  loss         0.093365  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.383\n",
      "iter 29121/30000  loss         0.093365  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.383\n",
      "iter 29140/30000  loss         0.093362  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.385\n",
      "iter 29141/30000  loss         0.093362  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.385\n",
      "iter 29160/30000  loss         0.093359  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.386\n",
      "iter 29161/30000  loss         0.093359  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.386\n",
      "iter 29180/30000  loss         0.093356  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    8.388\n",
      "iter 29181/30000  loss         0.093356  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.388\n",
      "iter 29200/30000  loss         0.093353  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.389\n",
      "iter 29201/30000  loss         0.093352  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.389\n",
      "iter 29220/30000  loss         0.093349  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.391\n",
      "iter 29221/30000  loss         0.093349  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.391\n",
      "iter 29240/30000  loss         0.093346  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.392\n",
      "iter 29241/30000  loss         0.093346  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.392\n",
      "iter 29260/30000  loss         0.093343  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.394\n",
      "iter 29261/30000  loss         0.093343  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.394\n",
      "iter 29280/30000  loss         0.093340  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.395\n",
      "iter 29281/30000  loss         0.093340  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.395\n",
      "iter 29300/30000  loss         0.093337  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.397\n",
      "iter 29301/30000  loss         0.093336  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.397\n",
      "iter 29320/30000  loss         0.093333  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.398\n",
      "iter 29321/30000  loss         0.093333  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.398\n",
      "iter 29340/30000  loss         0.093330  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.400\n",
      "iter 29341/30000  loss         0.093330  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.400\n",
      "iter 29360/30000  loss         0.093327  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.401\n",
      "iter 29361/30000  loss         0.093327  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.401\n",
      "iter 29380/30000  loss         0.093324  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.403\n",
      "iter 29381/30000  loss         0.093324  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.403\n",
      "iter 29400/30000  loss         0.093321  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.404\n",
      "iter 29401/30000  loss         0.093321  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29420/30000  loss         0.093318  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.405\n",
      "iter 29421/30000  loss         0.093317  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.406\n",
      "iter 29440/30000  loss         0.093314  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.407\n",
      "iter 29441/30000  loss         0.093314  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.407\n",
      "iter 29460/30000  loss         0.093311  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.408\n",
      "iter 29461/30000  loss         0.093311  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.408\n",
      "iter 29480/30000  loss         0.093308  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.410\n",
      "iter 29481/30000  loss         0.093308  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.410\n",
      "iter 29500/30000  loss         0.093305  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.411\n",
      "iter 29501/30000  loss         0.093305  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.411\n",
      "iter 29520/30000  loss         0.093302  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.413\n",
      "iter 29521/30000  loss         0.093302  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.413\n",
      "iter 29540/30000  loss         0.093299  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.414\n",
      "iter 29541/30000  loss         0.093299  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.414\n",
      "iter 29560/30000  loss         0.093296  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.416\n",
      "iter 29561/30000  loss         0.093296  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.416\n",
      "iter 29580/30000  loss         0.093293  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.417\n",
      "iter 29581/30000  loss         0.093292  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.417\n",
      "iter 29600/30000  loss         0.093290  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.419\n",
      "iter 29601/30000  loss         0.093289  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.419\n",
      "iter 29620/30000  loss         0.093286  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.420\n",
      "iter 29621/30000  loss         0.093286  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.420\n",
      "iter 29640/30000  loss         0.093283  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.421\n",
      "iter 29641/30000  loss         0.093283  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.422\n",
      "iter 29660/30000  loss         0.093280  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.423\n",
      "iter 29661/30000  loss         0.093280  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.423\n",
      "iter 29680/30000  loss         0.093277  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.424\n",
      "iter 29681/30000  loss         0.093277  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.424\n",
      "iter 29700/30000  loss         0.093274  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.426\n",
      "iter 29701/30000  loss         0.093274  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.426\n",
      "iter 29720/30000  loss         0.093271  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.427\n",
      "iter 29721/30000  loss         0.093271  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.427\n",
      "iter 29740/30000  loss         0.093268  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.429\n",
      "iter 29741/30000  loss         0.093268  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.429\n",
      "iter 29760/30000  loss         0.093265  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.430\n",
      "iter 29761/30000  loss         0.093265  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.430\n",
      "iter 29780/30000  loss         0.093262  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.432\n",
      "iter 29781/30000  loss         0.093262  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.432\n",
      "iter 29800/30000  loss         0.093259  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.433\n",
      "iter 29801/30000  loss         0.093259  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.433\n",
      "iter 29820/30000  loss         0.093256  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.434\n",
      "iter 29821/30000  loss         0.093256  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.434\n",
      "iter 29840/30000  loss         0.093253  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.436\n",
      "iter 29841/30000  loss         0.093253  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.436\n",
      "iter 29860/30000  loss         0.093250  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.437\n",
      "iter 29861/30000  loss         0.093249  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.437\n",
      "iter 29880/30000  loss         0.093247  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.439\n",
      "iter 29881/30000  loss         0.093246  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.439\n",
      "iter 29900/30000  loss         0.093244  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.440\n",
      "iter 29901/30000  loss         0.093243  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.440\n",
      "iter 29920/30000  loss         0.093241  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.442\n",
      "iter 29921/30000  loss         0.093240  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.442\n",
      "iter 29940/30000  loss         0.093238  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.443\n",
      "iter 29941/30000  loss         0.093237  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.443\n",
      "iter 29960/30000  loss         0.093234  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.444\n",
      "iter 29961/30000  loss         0.093234  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.444\n",
      "iter 29980/30000  loss         0.093231  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.446\n",
      "iter 29981/30000  loss         0.093231  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.446\n",
      "iter 30000/30000  loss         0.093228  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    8.447\n",
      "Done. Did NOT converge.\n"
     ]
    }
   ],
   "source": [
    "lr_p3 = LogisticRegressionGradientDescent(alpha=1, step_size=0.1, init_w_recipe='zeros')\n",
    "lr_p3.fit(x_trbw_29_3, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 845)"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tebw_29_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5937</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>149</td>\n",
       "      <td>5851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        5937    63\n",
       "1.0         149  5851"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trbw_29_4 = x_trbw_29_3\n",
    "y_hat = lr_p3.predict(x_trbw_29_4)\n",
    "y_actu = pd.Series(y_tr, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "FP_list = list()\n",
    "FN_list = list()\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i] == y_tr[i]  == 1:\n",
    "        TP = TP + 1\n",
    "    elif y_hat[i] == y_tr[i]  == 0:\n",
    "        TN = TN + 1\n",
    "    elif (y_hat[i] == 1) & (y_tr[i]  == 0):\n",
    "        FP = FP + 1\n",
    "        FP_list.append(i)\n",
    "    else:\n",
    "        FN = FN + 1\n",
    "        FN_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_FP = [x_trbw[i] for i in FP_list]\n",
    "y_FP = [y_tr[i] for i in FP_list]\n",
    "x_FP = x_FP[25:46]\n",
    "\n",
    "x_FN = [x_trbw[i] for i in FN_list]\n",
    "y_FN = [y_tr[i] for i in FN_list]\n",
    "\n",
    "x_FN = x_FN[23:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAILCAYAAAB8Yz9AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEhZJREFUeJzt3T+IbGf9x/HPEy5YqEELMViYyso/KbYQW6tbWNiITRBtLQPWXrARbARBhIAQomAjWNoFre8WEaI2FreKwoWYmP78invz837i/plzZ+acOTuvFyx7WXYnz+x+Z/LmmWdmxjRNAQD4yAtrLwAAOC3iAAAo4gAAKOIAACjiAAAo4gAAKOIAACji4MjGGJ8YY/x6jPHBGOOfY4zX1l4TzGGG2TozPN+9tRdwBh4k+VKSl5O8lOStMcZfp2n646qrgt09iBlm2x7EDM9i5+AGY4wfjTF+/7Gv/WKM8fMZF/O9JD+Zpum9aZr+luT1JN8/4DLhWmaYrTPD6xAHN/tNkvtjjM8kyRjjXpLvJnlzjPHLMca/r/n4y9Pv/2ySLyR5+5nLfDvJlxe+HpwvM8zWmeEVeFjhBtM0vTvG+HOS7+RJad5P8niapsskl0l+eMtFfOrp5/ef+dr7ST596LXCVcwwW2eG12Hn4HZvJHn16b9fTfLmjJ/98OnnF5/52otJ/nOAdcGuzDBbZ4YXJg5u94ckXxtjfCXJt5L8NknGGL8aY3x4zcc7STJN03tJ3k3yyjOX90qSdxa+Dpw3M8zWmeGFDW/ZfLsxxutJvp4nW1nfnPmzP03yjSTfTvL5JG8l+YFTsizJDLN1ZnhZdg5280aSr2beVtZHfpzkH0keJflTkp8ZSFZghtk6M7wgOwc7GGN8Mcnfk7w0TdMHa68H5jLDbJ0ZXpadg1uMMV5I8lqS3xlItsgMs3VmeHmeyniDMcYnk/wrT7ai7q+8HJjNDLN1ZngdHlYAAIqHFQCAIg4AgDLrzMEYw2MQ7OPxNE2fW3MBZpg9mWG2bqcZtnPAkh6tvQDYkxlm63aaYXEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAAJR7ay8A4DrTND33z44xDrgSOC92DgCAIg4AgCIOAIAiDgCA4kAi8P/2OQB4auZcF4cXodk5AACKOAAAijgAAIo4AACKA4lHtuuhKAeiWNpdOnx4letuU3f9erObq+ZgK/fDS6zdzgEAUMQBAFDEAQBQxAEAUBxIfE4ONcFpu+42upVDZxzXXZuDQx9StHMAABRxAAAUcQAAFHEAABRxAAAUz1b4GM9CmM9LRJ8+cw3Pb4mXKz6126idAwCgiAMAoIgDAKCIAwCgnO2BxFM7/DFnPUsc7Du13w+787eDw9r1Pnefw9lXfW3N27KdAwCgiAMAoIgDAKCIAwCg3LkDiedwGOscriO3MwfPZ4lXu+M8HXqOrru8JW77dg4AgCIOAIAiDgCAIg4AgLKZA4mndvjq1F7NirvNbAEfWeIArZ0DAKCIAwCgiAMAoIgDAKDsfSDxXA9Knev15vmZGdimfd6KeavsHAAARRwAAEUcAABFHAAAZVYcXFxcZJqm+oAtuWqGl/rgdPl7cZMxxk4fazr0DNs5AACKOAAAijgAAIo4AADKrFdIvLy8XP3QBQCsbQuHVq9a467/D7dzAAAUcQAAFHEAABRxAAAUcQAAlFnPVri4uMjDhw8PugDPfljfUqdu/a3hOI5xG3Z7PW92DgCAIg4AgCIOAIAiDgCAsvrLJ5/aS1Ce4yGcc7rOV83wqc0gp2Gfl55d2lXr2neu3S62b595tXMAABRxAAAUcQAAFHEAAJRZBxKP4VQP+HA+jnGYi+3b+n3T1tfPuuwcAABFHAAARRwAAEUcAABl9QOJcIr2Pcy1hQON+1zHLVw/4PnZOQAAijgAAIo4AACKOAAAigOJcAR3/dXp7vr1g3Nn5wAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo92Z+/+Mkj46xEM7Cy2svIGaY/Zhhtm6nGR7TNB17IQDAhnhYAQAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACjiAAAo4gAAKOIAACji4MjGGJ8YY/x6jPHBGOOfY4zX1l4TzGGG2TozPN/ct2xmvgdJvpQnb5P5UpK3xhh/nabpj6uuCnb3IGaYbXsQMzyLnYMbjDF+NMb4/ce+9osxxs9nXMz3kvxkmqb3pmn6W5LXk3z/gMuEa5lhts4Mr0Mc3Ow3Se6PMT6TJGOMe0m+m+TNMcYvxxj/vubjL0+//7NJvpDk7Wcu8+0kX174enC+zDBbZ4ZX4GGFG0zT9O4Y489JvpMnpXk/yeNpmi6TXCb54S0X8amnn99/5mvvJ/n0odcKVzHDbJ0ZXoedg9u9keTVp/9+NcmbM372w6efX3zmay8m+c8B1gW7MsNsnRlemDi43R+SfG2M8ZUk30ry2yQZY/xqjPHhNR/vJMk0Te8leTfJK89c3itJ3ln4OnDezDBbZ4YXNqZpWnsNJ2+M8XqSr+fJVtY3Z/7sT5N8I8m3k3w+yVtJfuCULEsyw2ydGV6WnYPdvJHkq5m3lfWRHyf5R5JHSf6U5GcGkhWYYbbODC/IzsEOxhhfTPL3JC9N0/TB2uuBucwwW2eGl2Xn4BZjjBeSvJbkdwaSLTLDbJ0ZXp6nMt5gjPHJJP/Kk62o+ysvB2Yzw2ydGV6HhxUAgOJhBQCgiAMAoMw6czDG8BgE+3g8TdPn1lyAGWZPZpit22mG7RywpEdrLwD2ZIbZup1mWBwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAABFHAAARRwAAEUcAADl3toL4Ilpmnb+3jHGEVcCsF1z7ks/zn3rf9k5AACKOAAAijgAAIo4AACKA4kr2OfAzNp2XbuDPadvqTk8tVkww9u0xLw6GP5fdg4AgCIOAIAiDgCAIg4AgCIOAIDi2QrPacvPOOD83KV5vUvX5dz5W95s39/PPs+osHMAABRxAAAUcQAAFHEAABQHEndwaodmrlrPoV/K89SuM/9rK3+jrazz45a4nd0FW/377uuuX287BwBAEQcAQBEHAEARBwBA2fSBxH3el33N97I/xn/71A7HnNNhrlP73cN1zCq7snMAABRxAAAUcQAAFHEAAJTVDyQucUDmrh0+5LD8jdiSi4uLPHz4cO1lsAH7HAy3cwAAFHEAABRxAAAUcQAAlL0PJDrMBQB3i50DAKCIAwCgiAMAoIgDAKCs/gqJd4nDmdu071tJ+7uzpMvLy/+ZWTPIodk5AACKOAAAijgAAIo4AADK3gcSHebi3O17G4B9mUEOzc4BAFDEAQBQxAEAUMQBAFDEAQBQjvLyyVc9A+G607RO2UKbc/sBOAY7BwBAEQcAQBEHAEARBwBAOcqBxGMcnnJIi3NhroG12TkAAIo4AACKOAAAijgAAMpRDiQeg0NaALAMOwcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQLk38/sfJ3l0jIVwFl5eewExw+zHDLN1O83wmKbp2AsBADbEwwoAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQcAQBEHAEARBwBAEQdHNsb4xBjj12OMD8YY/xxjvLb2mmAOM8zWmeH55r5lM/M9SPKlPHmbzJeSvDXG+Os0TX9cdVWwuwcxw2zbg5jhWewc3GCM8aMxxu8/9rVfjDF+PuNivpfkJ9M0vTdN09+SvJ7k+wdcJlzLDLN1Zngd4uBmv0lyf4zxmSQZY9xL8t0kb44xfjnG+Pc1H395+v2fTfKFJG8/c5lvJ/nywteD82WG2TozvAIPK9xgmqZ3xxh/TvKdPCnN+0keT9N0meQyyQ9vuYhPPf38/jNfez/Jpw+9VriKGWbrzPA67Bzc7o0krz7996tJ3pzxsx8+/fziM197Mcl/DrAu2JUZZuvM8MLEwe3+kORrY4yvJPlWkt8myRjjV2OMD6/5eCdJpml6L8m7SV555vJeSfLOwteB82aG2TozvLAxTdPaazh5Y4zXk3w9T7ayvjnzZ3+a5BtJvp3k80neSvIDp2RZkhlm68zwsuwc7OaNJF/NvK2sj/w4yT+SPErypyQ/M5CswAyzdWZ4QXYOdjDG+GKSvyd5aZqmD9ZeD8xlhtk6M7wsOwe3GGO8kOS1JL8zkGyRGWbrzPDyPJXxBmOMTyb5V55sRd1feTkwmxlm68zwOjysAAAUDysAAGXWwwpjDNsM7OPxNE2fW3MBZpg9mWG2bqcZtnPAkh6tvQDYkxlm63aaYXEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAABRxAAAUcQAAFHEAAJR7ay8AOD/TND33z44xFvlv7/vfgS2zcwAAFHEAABRxAAAUcQAAFAcSgU3Z5zAjsBs7BwBAEQcAQBEHAEARBwBAcSBxBXMOVHmVNrbOAULYHjsHAEARBwBAEQcAQBEHAEBxIPGAjnHw6qrLXPOQore75ToOHnIKjjGHW7g/O/T/K+wcAABFHAAARRwAAEUcAABFHAAAxbMVPmYLJ67XfAbDFk7tcnxL3U6OMW9buI2fu1P7G53as8aucuj12DkAAIo4AACKOAAAijgAAMpZHEg8tcMtx7DPdZxzkMXLJ5+fNW8/53DbvYvW/Ltddd+z1Evb72rf+8cl7oftHAAARRwAAEUcAABFHAAAZdMHErd6WOkYBwD3sdRhHYcUT8tWbz+7um7e7vr1PqZTe2XMq9az633PORy03ed+2M4BAFDEAQBQxAEAUMQBAFAWO5B43QGMUzsosoS7fv04Pec4c+d4nW+zlVcu3PUg3a6XuYVDiqfGzgEAUMQBAFDEAQBQxAEAUGbFwcXFRaZpeq6P68z5XjhXz3u7c5t6fqf6ezzG/fASrlrPGOPKj11/ftfLPMZ67jo7BwBAEQcAQBEHAEARBwBA2fRbNnMYXins8Pz+4HZLvXLuPq+keIz1LGWfw5R2DgCAIg4AgCIOAIAiDgCAMutA4uXl5SZfLerU3qZ0C7a67jVs4WASd8dW74f3dejrPOfydn3L57vEzgEAUMQBAFDEAQBQxAEAUMQBAFDO4uWT7/qp0iUd+iVM74KtvrQqsJu7et91EzsHAEARBwBAEQcAQBEHAEA5iwOJHM7WD+ZcXFzk4cOHB73Mq34nh/49nePLt17HYU84PjsHAEARBwBAEQcAQBEHAEBxIJGzcnl5+T8H+fY94HboA3K7HjR0MA84FjsHAEARBwBAEQcAQBEHAEBxIJGzt9QrDXq7a2Ar7BwAAEUcAABFHAAARRwAAMWBRFiIQ4XAVtg5AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAMq9md//OMmjYyyEs/Dy2guIGWY/Zpit22mGxzRNx14IALAhHlYAAIo4AACKOAAAijgAAIo4AACKOAAAijgAAIo4AACKOAAAyv8BJw4md8qmhsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_FP, y_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a889af2e8>"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_tr[214].reshape(28,28), interpolation='nearest', vmin=0, vmax=1, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[214,\n",
       " 290,\n",
       " 321,\n",
       " 536,\n",
       " 579,\n",
       " 774,\n",
       " 882,\n",
       " 919,\n",
       " 926,\n",
       " 1060,\n",
       " 1141,\n",
       " 1222,\n",
       " 1235,\n",
       " 1254,\n",
       " 1267,\n",
       " 1277,\n",
       " 1338,\n",
       " 1363,\n",
       " 1380,\n",
       " 1432,\n",
       " 1469,\n",
       " 1494,\n",
       " 1542,\n",
       " 1610,\n",
       " 1689,\n",
       " 1831,\n",
       " 1877,\n",
       " 1914,\n",
       " 1920,\n",
       " 1921,\n",
       " 2023,\n",
       " 2071,\n",
       " 2169,\n",
       " 2204,\n",
       " 2299,\n",
       " 2315,\n",
       " 2537,\n",
       " 2736,\n",
       " 2823,\n",
       " 2849,\n",
       " 2906,\n",
       " 2920,\n",
       " 2951,\n",
       " 2995,\n",
       " 3042,\n",
       " 3113,\n",
       " 3212,\n",
       " 3228,\n",
       " 3285,\n",
       " 3486,\n",
       " 3524,\n",
       " 3529,\n",
       " 3534,\n",
       " 3579,\n",
       " 3588,\n",
       " 3593,\n",
       " 3678,\n",
       " 3709,\n",
       " 3750,\n",
       " 3770,\n",
       " 3790,\n",
       " 3864,\n",
       " 3926,\n",
       " 3939,\n",
       " 3996,\n",
       " 4041,\n",
       " 4232,\n",
       " 4245,\n",
       " 4258,\n",
       " 4274,\n",
       " 4275,\n",
       " 4362,\n",
       " 4618,\n",
       " 4754,\n",
       " 4783,\n",
       " 4887,\n",
       " 4968,\n",
       " 5057,\n",
       " 5071,\n",
       " 5157,\n",
       " 5160,\n",
       " 5175,\n",
       " 5183,\n",
       " 5197,\n",
       " 5223,\n",
       " 5252,\n",
       " 5321,\n",
       " 5331,\n",
       " 5352,\n",
       " 5390,\n",
       " 5440,\n",
       " 5480,\n",
       " 5546,\n",
       " 5553,\n",
       " 5708,\n",
       " 5713,\n",
       " 5748,\n",
       " 5791,\n",
       " 5871,\n",
       " 6032,\n",
       " 6070,\n",
       " 6112,\n",
       " 6230,\n",
       " 6342,\n",
       " 6393,\n",
       " 6395,\n",
       " 6648,\n",
       " 6687,\n",
       " 6826,\n",
       " 7118,\n",
       " 7191,\n",
       " 7250,\n",
       " 7264,\n",
       " 7282,\n",
       " 7309,\n",
       " 7401,\n",
       " 7454,\n",
       " 7469,\n",
       " 7581,\n",
       " 7617,\n",
       " 7696,\n",
       " 7740,\n",
       " 7779,\n",
       " 7808,\n",
       " 7845,\n",
       " 7967,\n",
       " 8039,\n",
       " 8054,\n",
       " 8116,\n",
       " 8146,\n",
       " 8149,\n",
       " 8194,\n",
       " 8275,\n",
       " 8290,\n",
       " 8305,\n",
       " 8359,\n",
       " 8437,\n",
       " 8438,\n",
       " 8471,\n",
       " 8516,\n",
       " 8548,\n",
       " 8562,\n",
       " 8599,\n",
       " 8689,\n",
       " 8763,\n",
       " 8911,\n",
       " 9001,\n",
       " 9019,\n",
       " 9077,\n",
       " 9095,\n",
       " 9096,\n",
       " 9104,\n",
       " 9120,\n",
       " 9153,\n",
       " 9221,\n",
       " 9296,\n",
       " 9405,\n",
       " 9576,\n",
       " 9647,\n",
       " 9743,\n",
       " 9774,\n",
       " 9835,\n",
       " 9909,\n",
       " 9948,\n",
       " 10059,\n",
       " 10178,\n",
       " 10379,\n",
       " 10429,\n",
       " 10535,\n",
       " 10610,\n",
       " 10699,\n",
       " 10747,\n",
       " 10842,\n",
       " 10918,\n",
       " 10928,\n",
       " 10933,\n",
       " 10934,\n",
       " 10997,\n",
       " 11054,\n",
       " 11081,\n",
       " 11306,\n",
       " 11344,\n",
       " 11380,\n",
       " 11475,\n",
       " 11477,\n",
       " 11669,\n",
       " 11835,\n",
       " 11979]"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_FN, y_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190.0"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x_FN[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "yproba1_test_N = lr_p3.predict_proba(x_tebw_29_3)[:, 1]\n",
    "\n",
    "np.savetxt('yproba3_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>3714</td>\n",
       "      <td>2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1433</td>\n",
       "      <td>4567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        3714  2286\n",
       "1.0        1433  4567"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trbw_2 = x_trbw\n",
    "\n",
    "y_hat = lr.predict(x_trbw_2)\n",
    "y_actu = pd.Series(y_tr, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trbw160 = np.zeros(784)\n",
    "y_trbw160 = np.array([])\n",
    "for i in range(12000):\n",
    "    if np.sum(x_trbw[i]) > 200:\n",
    "        x_trbw160 = np.row_stack((x_trbw160,x_trbw[i]))\n",
    "        y_trbw160 = np.append(y_trbw160, y_tr[i])\n",
    "    \n",
    "x_trbw160 = np.delete(x_trbw160, 0, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x_trbw160, y_trbw160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tebw160 = np.zeros((2000, 784))\n",
    "k = 0\n",
    "for i in range(2000):\n",
    "    if np.sum(x_tebw[i]) > 150:\n",
    "        x_tebw160[i] = x_tebw[i]\n",
    "        k = k + 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1701"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trbw160_2 = np.zeros((12000, 784))\n",
    "k = 0\n",
    "for i in range(12000):\n",
    "    if np.sum(x_trbw[i]) > 150:\n",
    "        x_trbw160_2[i] = x_trbw[i]\n",
    "        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trbw160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([])\n",
    "c = np.array([1,2,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.append(b,c[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.])"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10211"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_trbw160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10211"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_trbw160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1a4f5ce2b0>"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(lr_p3.w_G[0:784].reshape(28,28), interpolation='nearest', vmin=-0.5, vmax=0.5, cmap='RdYlBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.1\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.071016  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         1.221002  avg_L1_norm_grad         0.133613  w[0]    0.000 bias   -0.011\n",
      "iter    2/30000  loss         2.467284  avg_L1_norm_grad         0.220407  w[0]    0.000 bias    0.035\n",
      "iter    3/30000  loss         3.682488  avg_L1_norm_grad         0.173556  w[0]    0.000 bias   -0.040\n",
      "iter    4/30000  loss         0.673632  avg_L1_norm_grad         0.026777  w[0]    0.000 bias    0.019\n",
      "iter    5/30000  loss         0.637516  avg_L1_norm_grad         0.035729  w[0]    0.000 bias    0.030\n",
      "iter    6/30000  loss         0.656031  avg_L1_norm_grad         0.045353  w[0]    0.000 bias    0.023\n",
      "iter    7/30000  loss         0.723100  avg_L1_norm_grad         0.082291  w[0]    0.000 bias    0.041\n",
      "iter    8/30000  loss         1.107061  avg_L1_norm_grad         0.101107  w[0]    0.000 bias    0.017\n",
      "iter    9/30000  loss         0.939422  avg_L1_norm_grad         0.122064  w[0]    0.000 bias    0.053\n",
      "iter   10/30000  loss         1.583681  avg_L1_norm_grad         0.121444  w[0]    0.000 bias    0.015\n",
      "iter   11/30000  loss         0.681735  avg_L1_norm_grad         0.075502  w[0]    0.000 bias    0.057\n",
      "iter   12/30000  loss         0.877450  avg_L1_norm_grad         0.078520  w[0]    0.000 bias    0.036\n",
      "iter   13/30000  loss         0.698196  avg_L1_norm_grad         0.087045  w[0]    0.000 bias    0.065\n",
      "iter   14/30000  loss         0.983034  avg_L1_norm_grad         0.088689  w[0]    0.000 bias    0.040\n",
      "iter   15/30000  loss         0.662802  avg_L1_norm_grad         0.084545  w[0]    0.000 bias    0.072\n",
      "iter   16/30000  loss         0.922326  avg_L1_norm_grad         0.085463  w[0]    0.000 bias    0.047\n",
      "iter   17/30000  loss         0.626471  avg_L1_norm_grad         0.081457  w[0]    0.000 bias    0.078\n",
      "iter   18/30000  loss         0.861010  avg_L1_norm_grad         0.082129  w[0]    0.000 bias    0.054\n",
      "iter   19/30000  loss         0.595262  avg_L1_norm_grad         0.078915  w[0]    0.000 bias    0.084\n",
      "iter   20/30000  loss         0.810302  avg_L1_norm_grad         0.079386  w[0]    0.000 bias    0.061\n",
      "iter   21/30000  loss         0.566734  avg_L1_norm_grad         0.076319  w[0]    0.000 bias    0.090\n",
      "iter   40/30000  loss         0.428144  avg_L1_norm_grad         0.046637  w[0]    0.000 bias    0.120\n",
      "iter   41/30000  loss         0.367557  avg_L1_norm_grad         0.044263  w[0]    0.000 bias    0.137\n",
      "iter   60/30000  loss         0.282658  avg_L1_norm_grad         0.015614  w[0]    0.000 bias    0.164\n",
      "iter   61/30000  loss         0.277903  avg_L1_norm_grad         0.014365  w[0]    0.000 bias    0.171\n",
      "iter   80/30000  loss         0.254490  avg_L1_norm_grad         0.002326  w[0]    0.000 bias    0.199\n",
      "iter   81/30000  loss         0.253808  avg_L1_norm_grad         0.002124  w[0]    0.000 bias    0.201\n",
      "iter  100/30000  loss         0.242752  avg_L1_norm_grad         0.001681  w[0]    0.000 bias    0.229\n",
      "iter  101/30000  loss         0.242240  avg_L1_norm_grad         0.001671  w[0]    0.000 bias    0.231\n",
      "iter  120/30000  loss         0.233490  avg_L1_norm_grad         0.001509  w[0]    0.000 bias    0.258\n",
      "iter  121/30000  loss         0.233075  avg_L1_norm_grad         0.001502  w[0]    0.000 bias    0.259\n",
      "iter  140/30000  loss         0.225875  avg_L1_norm_grad         0.001373  w[0]    0.000 bias    0.285\n",
      "iter  141/30000  loss         0.225528  avg_L1_norm_grad         0.001367  w[0]    0.000 bias    0.286\n",
      "iter  160/30000  loss         0.219450  avg_L1_norm_grad         0.001264  w[0]    0.000 bias    0.310\n",
      "iter  161/30000  loss         0.219154  avg_L1_norm_grad         0.001259  w[0]    0.000 bias    0.312\n",
      "iter  180/30000  loss         0.213921  avg_L1_norm_grad         0.001173  w[0]    0.000 bias    0.335\n",
      "iter  181/30000  loss         0.213664  avg_L1_norm_grad         0.001169  w[0]    0.000 bias    0.336\n",
      "iter  200/30000  loss         0.209088  avg_L1_norm_grad         0.001096  w[0]    0.000 bias    0.359\n",
      "iter  201/30000  loss         0.208862  avg_L1_norm_grad         0.001093  w[0]    0.000 bias    0.360\n",
      "iter  220/30000  loss         0.204810  avg_L1_norm_grad         0.001032  w[0]    0.000 bias    0.382\n",
      "iter  221/30000  loss         0.204608  avg_L1_norm_grad         0.001029  w[0]    0.000 bias    0.383\n",
      "iter  240/30000  loss         0.200983  avg_L1_norm_grad         0.000976  w[0]    0.000 bias    0.404\n",
      "iter  241/30000  loss         0.200802  avg_L1_norm_grad         0.000973  w[0]    0.000 bias    0.405\n",
      "iter  260/30000  loss         0.197529  avg_L1_norm_grad         0.000928  w[0]    0.000 bias    0.426\n",
      "iter  261/30000  loss         0.197365  avg_L1_norm_grad         0.000925  w[0]    0.000 bias    0.427\n",
      "iter  280/30000  loss         0.194389  avg_L1_norm_grad         0.000885  w[0]    0.000 bias    0.447\n",
      "iter  281/30000  loss         0.194239  avg_L1_norm_grad         0.000883  w[0]    0.000 bias    0.448\n",
      "iter  300/30000  loss         0.191515  avg_L1_norm_grad         0.000847  w[0]    0.000 bias    0.468\n",
      "iter  301/30000  loss         0.191378  avg_L1_norm_grad         0.000845  w[0]    0.000 bias    0.469\n",
      "iter  320/30000  loss         0.188871  avg_L1_norm_grad         0.000814  w[0]    0.000 bias    0.488\n",
      "iter  321/30000  loss         0.188744  avg_L1_norm_grad         0.000812  w[0]    0.000 bias    0.489\n",
      "iter  340/30000  loss         0.186426  avg_L1_norm_grad         0.000783  w[0]    0.000 bias    0.508\n",
      "iter  341/30000  loss         0.186309  avg_L1_norm_grad         0.000782  w[0]    0.000 bias    0.509\n",
      "iter  360/30000  loss         0.184156  avg_L1_norm_grad         0.000755  w[0]    0.000 bias    0.527\n",
      "iter  361/30000  loss         0.184046  avg_L1_norm_grad         0.000754  w[0]    0.000 bias    0.528\n",
      "iter  380/30000  loss         0.182039  avg_L1_norm_grad         0.000730  w[0]    0.000 bias    0.546\n",
      "iter  381/30000  loss         0.181937  avg_L1_norm_grad         0.000729  w[0]    0.000 bias    0.547\n",
      "iter  400/30000  loss         0.180058  avg_L1_norm_grad         0.000707  w[0]    0.000 bias    0.565\n",
      "iter  401/30000  loss         0.179963  avg_L1_norm_grad         0.000705  w[0]    0.000 bias    0.566\n",
      "iter  420/30000  loss         0.178200  avg_L1_norm_grad         0.000685  w[0]    0.000 bias    0.583\n",
      "iter  421/30000  loss         0.178110  avg_L1_norm_grad         0.000684  w[0]    0.000 bias    0.584\n",
      "iter  440/30000  loss         0.176451  avg_L1_norm_grad         0.000665  w[0]    0.000 bias    0.601\n",
      "iter  441/30000  loss         0.176366  avg_L1_norm_grad         0.000664  w[0]    0.000 bias    0.602\n",
      "iter  460/30000  loss         0.174801  avg_L1_norm_grad         0.000647  w[0]    0.000 bias    0.618\n",
      "iter  461/30000  loss         0.174721  avg_L1_norm_grad         0.000646  w[0]    0.000 bias    0.619\n",
      "iter  480/30000  loss         0.173240  avg_L1_norm_grad         0.000629  w[0]    0.000 bias    0.636\n",
      "iter  481/30000  loss         0.173164  avg_L1_norm_grad         0.000629  w[0]    0.000 bias    0.637\n",
      "iter  500/30000  loss         0.171760  avg_L1_norm_grad         0.000613  w[0]    0.000 bias    0.653\n",
      "iter  501/30000  loss         0.171688  avg_L1_norm_grad         0.000612  w[0]    0.000 bias    0.654\n",
      "iter  520/30000  loss         0.170355  avg_L1_norm_grad         0.000598  w[0]    0.000 bias    0.670\n",
      "iter  521/30000  loss         0.170286  avg_L1_norm_grad         0.000597  w[0]    0.000 bias    0.671\n",
      "iter  540/30000  loss         0.169017  avg_L1_norm_grad         0.000584  w[0]    0.000 bias    0.686\n",
      "iter  541/30000  loss         0.168952  avg_L1_norm_grad         0.000583  w[0]    0.000 bias    0.687\n",
      "iter  560/30000  loss         0.167742  avg_L1_norm_grad         0.000571  w[0]    0.000 bias    0.703\n",
      "iter  561/30000  loss         0.167680  avg_L1_norm_grad         0.000570  w[0]    0.000 bias    0.703\n",
      "iter  580/30000  loss         0.166524  avg_L1_norm_grad         0.000558  w[0]    0.000 bias    0.719\n",
      "iter  581/30000  loss         0.166465  avg_L1_norm_grad         0.000557  w[0]    0.000 bias    0.720\n",
      "iter  600/30000  loss         0.165360  avg_L1_norm_grad         0.000546  w[0]    0.000 bias    0.735\n",
      "iter  601/30000  loss         0.165303  avg_L1_norm_grad         0.000546  w[0]    0.000 bias    0.736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.164245  avg_L1_norm_grad         0.000535  w[0]    0.000 bias    0.750\n",
      "iter  621/30000  loss         0.164190  avg_L1_norm_grad         0.000534  w[0]    0.000 bias    0.751\n",
      "iter  640/30000  loss         0.163175  avg_L1_norm_grad         0.000524  w[0]    0.000 bias    0.766\n",
      "iter  641/30000  loss         0.163123  avg_L1_norm_grad         0.000524  w[0]    0.000 bias    0.767\n",
      "iter  660/30000  loss         0.162148  avg_L1_norm_grad         0.000514  w[0]    0.000 bias    0.781\n",
      "iter  661/30000  loss         0.162098  avg_L1_norm_grad         0.000514  w[0]    0.000 bias    0.782\n",
      "iter  680/30000  loss         0.161161  avg_L1_norm_grad         0.000505  w[0]    0.000 bias    0.797\n",
      "iter  681/30000  loss         0.161112  avg_L1_norm_grad         0.000504  w[0]    0.000 bias    0.797\n",
      "iter  700/30000  loss         0.160211  avg_L1_norm_grad         0.000496  w[0]    0.000 bias    0.812\n",
      "iter  701/30000  loss         0.160164  avg_L1_norm_grad         0.000495  w[0]    0.000 bias    0.812\n",
      "iter  720/30000  loss         0.159295  avg_L1_norm_grad         0.000487  w[0]    0.000 bias    0.826\n",
      "iter  721/30000  loss         0.159250  avg_L1_norm_grad         0.000486  w[0]    0.000 bias    0.827\n",
      "iter  740/30000  loss         0.158412  avg_L1_norm_grad         0.000479  w[0]    0.000 bias    0.841\n",
      "iter  741/30000  loss         0.158369  avg_L1_norm_grad         0.000478  w[0]    0.000 bias    0.842\n",
      "iter  760/30000  loss         0.157560  avg_L1_norm_grad         0.000471  w[0]    0.000 bias    0.856\n",
      "iter  761/30000  loss         0.157518  avg_L1_norm_grad         0.000470  w[0]    0.000 bias    0.856\n",
      "iter  780/30000  loss         0.156736  avg_L1_norm_grad         0.000463  w[0]    0.000 bias    0.870\n",
      "iter  781/30000  loss         0.156695  avg_L1_norm_grad         0.000463  w[0]    0.000 bias    0.871\n",
      "iter  800/30000  loss         0.155939  avg_L1_norm_grad         0.000456  w[0]    0.000 bias    0.884\n",
      "iter  801/30000  loss         0.155900  avg_L1_norm_grad         0.000455  w[0]    0.000 bias    0.885\n",
      "iter  820/30000  loss         0.155168  avg_L1_norm_grad         0.000449  w[0]    0.000 bias    0.899\n",
      "iter  821/30000  loss         0.155130  avg_L1_norm_grad         0.000448  w[0]    0.000 bias    0.899\n",
      "iter  840/30000  loss         0.154421  avg_L1_norm_grad         0.000442  w[0]    0.000 bias    0.912\n",
      "iter  841/30000  loss         0.154384  avg_L1_norm_grad         0.000442  w[0]    0.000 bias    0.913\n",
      "iter  860/30000  loss         0.153697  avg_L1_norm_grad         0.000436  w[0]    0.000 bias    0.926\n",
      "iter  861/30000  loss         0.153661  avg_L1_norm_grad         0.000435  w[0]    0.000 bias    0.927\n",
      "iter  880/30000  loss         0.152994  avg_L1_norm_grad         0.000429  w[0]    0.000 bias    0.940\n",
      "iter  881/30000  loss         0.152960  avg_L1_norm_grad         0.000429  w[0]    0.000 bias    0.941\n",
      "iter  900/30000  loss         0.152313  avg_L1_norm_grad         0.000423  w[0]    0.000 bias    0.954\n",
      "iter  901/30000  loss         0.152279  avg_L1_norm_grad         0.000423  w[0]    0.000 bias    0.954\n",
      "iter  920/30000  loss         0.151650  avg_L1_norm_grad         0.000418  w[0]    0.000 bias    0.967\n",
      "iter  921/30000  loss         0.151618  avg_L1_norm_grad         0.000417  w[0]    0.000 bias    0.968\n",
      "iter  940/30000  loss         0.151006  avg_L1_norm_grad         0.000412  w[0]    0.000 bias    0.981\n",
      "iter  941/30000  loss         0.150975  avg_L1_norm_grad         0.000412  w[0]    0.000 bias    0.981\n",
      "iter  960/30000  loss         0.150380  avg_L1_norm_grad         0.000407  w[0]    0.000 bias    0.994\n",
      "iter  961/30000  loss         0.150349  avg_L1_norm_grad         0.000406  w[0]    0.000 bias    0.994\n",
      "iter  980/30000  loss         0.149771  avg_L1_norm_grad         0.000401  w[0]    0.000 bias    1.007\n",
      "iter  981/30000  loss         0.149741  avg_L1_norm_grad         0.000401  w[0]    0.000 bias    1.008\n",
      "iter 1000/30000  loss         0.149177  avg_L1_norm_grad         0.000396  w[0]    0.000 bias    1.020\n",
      "iter 1001/30000  loss         0.149148  avg_L1_norm_grad         0.000396  w[0]    0.000 bias    1.021\n",
      "iter 1020/30000  loss         0.148599  avg_L1_norm_grad         0.000391  w[0]    0.000 bias    1.033\n",
      "iter 1021/30000  loss         0.148571  avg_L1_norm_grad         0.000391  w[0]    0.000 bias    1.034\n",
      "iter 1040/30000  loss         0.148036  avg_L1_norm_grad         0.000386  w[0]    0.000 bias    1.046\n",
      "iter 1041/30000  loss         0.148008  avg_L1_norm_grad         0.000386  w[0]    0.000 bias    1.046\n",
      "iter 1060/30000  loss         0.147486  avg_L1_norm_grad         0.000382  w[0]    0.000 bias    1.059\n",
      "iter 1061/30000  loss         0.147459  avg_L1_norm_grad         0.000382  w[0]    0.000 bias    1.059\n",
      "iter 1080/30000  loss         0.146950  avg_L1_norm_grad         0.000377  w[0]    0.000 bias    1.071\n",
      "iter 1081/30000  loss         0.146924  avg_L1_norm_grad         0.000377  w[0]    0.000 bias    1.072\n",
      "iter 1100/30000  loss         0.146427  avg_L1_norm_grad         0.000373  w[0]    0.000 bias    1.084\n",
      "iter 1101/30000  loss         0.146401  avg_L1_norm_grad         0.000373  w[0]    0.000 bias    1.084\n",
      "iter 1120/30000  loss         0.145916  avg_L1_norm_grad         0.000369  w[0]    0.000 bias    1.096\n",
      "iter 1121/30000  loss         0.145890  avg_L1_norm_grad         0.000368  w[0]    0.000 bias    1.097\n",
      "iter 1140/30000  loss         0.145416  avg_L1_norm_grad         0.000364  w[0]    0.000 bias    1.109\n",
      "iter 1141/30000  loss         0.145392  avg_L1_norm_grad         0.000364  w[0]    0.000 bias    1.109\n",
      "iter 1160/30000  loss         0.144929  avg_L1_norm_grad         0.000360  w[0]    0.000 bias    1.121\n",
      "iter 1161/30000  loss         0.144904  avg_L1_norm_grad         0.000360  w[0]    0.000 bias    1.122\n",
      "iter 1180/30000  loss         0.144451  avg_L1_norm_grad         0.000356  w[0]    0.000 bias    1.133\n",
      "iter 1181/30000  loss         0.144428  avg_L1_norm_grad         0.000356  w[0]    0.000 bias    1.134\n",
      "iter 1200/30000  loss         0.143985  avg_L1_norm_grad         0.000353  w[0]    0.000 bias    1.145\n",
      "iter 1201/30000  loss         0.143962  avg_L1_norm_grad         0.000352  w[0]    0.000 bias    1.146\n",
      "iter 1220/30000  loss         0.143528  avg_L1_norm_grad         0.000349  w[0]    0.000 bias    1.157\n",
      "iter 1221/30000  loss         0.143506  avg_L1_norm_grad         0.000349  w[0]    0.000 bias    1.158\n",
      "iter 1240/30000  loss         0.143081  avg_L1_norm_grad         0.000345  w[0]    0.000 bias    1.169\n",
      "iter 1241/30000  loss         0.143059  avg_L1_norm_grad         0.000345  w[0]    0.000 bias    1.170\n",
      "iter 1260/30000  loss         0.142644  avg_L1_norm_grad         0.000342  w[0]    0.000 bias    1.181\n",
      "iter 1261/30000  loss         0.142622  avg_L1_norm_grad         0.000342  w[0]    0.000 bias    1.182\n",
      "iter 1280/30000  loss         0.142215  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    1.193\n",
      "iter 1281/30000  loss         0.142194  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    1.194\n",
      "iter 1300/30000  loss         0.141795  avg_L1_norm_grad         0.000335  w[0]    0.000 bias    1.205\n",
      "iter 1301/30000  loss         0.141774  avg_L1_norm_grad         0.000335  w[0]    0.000 bias    1.205\n",
      "iter 1320/30000  loss         0.141384  avg_L1_norm_grad         0.000332  w[0]    0.000 bias    1.216\n",
      "iter 1321/30000  loss         0.141363  avg_L1_norm_grad         0.000331  w[0]    0.000 bias    1.217\n",
      "iter 1340/30000  loss         0.140980  avg_L1_norm_grad         0.000328  w[0]    0.000 bias    1.228\n",
      "iter 1341/30000  loss         0.140960  avg_L1_norm_grad         0.000328  w[0]    0.000 bias    1.229\n",
      "iter 1360/30000  loss         0.140584  avg_L1_norm_grad         0.000325  w[0]    0.000 bias    1.239\n",
      "iter 1361/30000  loss         0.140565  avg_L1_norm_grad         0.000325  w[0]    0.000 bias    1.240\n",
      "iter 1380/30000  loss         0.140196  avg_L1_norm_grad         0.000322  w[0]    0.000 bias    1.251\n",
      "iter 1381/30000  loss         0.140177  avg_L1_norm_grad         0.000322  w[0]    0.000 bias    1.252\n",
      "iter 1400/30000  loss         0.139815  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    1.262\n",
      "iter 1401/30000  loss         0.139796  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    1.263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.139441  avg_L1_norm_grad         0.000316  w[0]    0.000 bias    1.274\n",
      "iter 1421/30000  loss         0.139423  avg_L1_norm_grad         0.000316  w[0]    0.000 bias    1.274\n",
      "iter 1440/30000  loss         0.139074  avg_L1_norm_grad         0.000313  w[0]    0.000 bias    1.285\n",
      "iter 1441/30000  loss         0.139056  avg_L1_norm_grad         0.000313  w[0]    0.000 bias    1.285\n",
      "iter 1460/30000  loss         0.138714  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    1.296\n",
      "iter 1461/30000  loss         0.138696  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    1.297\n",
      "iter 1480/30000  loss         0.138360  avg_L1_norm_grad         0.000308  w[0]    0.000 bias    1.307\n",
      "iter 1481/30000  loss         0.138342  avg_L1_norm_grad         0.000307  w[0]    0.000 bias    1.308\n",
      "iter 1500/30000  loss         0.138012  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    1.318\n",
      "iter 1501/30000  loss         0.137995  avg_L1_norm_grad         0.000305  w[0]    0.000 bias    1.319\n",
      "iter 1520/30000  loss         0.137670  avg_L1_norm_grad         0.000302  w[0]    0.000 bias    1.329\n",
      "iter 1521/30000  loss         0.137653  avg_L1_norm_grad         0.000302  w[0]    0.000 bias    1.330\n",
      "iter 1540/30000  loss         0.137334  avg_L1_norm_grad         0.000300  w[0]    0.000 bias    1.340\n",
      "iter 1541/30000  loss         0.137317  avg_L1_norm_grad         0.000300  w[0]    0.000 bias    1.341\n",
      "iter 1560/30000  loss         0.137003  avg_L1_norm_grad         0.000297  w[0]    0.000 bias    1.351\n",
      "iter 1561/30000  loss         0.136987  avg_L1_norm_grad         0.000297  w[0]    0.000 bias    1.352\n",
      "iter 1580/30000  loss         0.136678  avg_L1_norm_grad         0.000295  w[0]    0.000 bias    1.362\n",
      "iter 1581/30000  loss         0.136662  avg_L1_norm_grad         0.000295  w[0]    0.000 bias    1.363\n",
      "iter 1600/30000  loss         0.136359  avg_L1_norm_grad         0.000292  w[0]    0.000 bias    1.373\n",
      "iter 1601/30000  loss         0.136343  avg_L1_norm_grad         0.000292  w[0]    0.000 bias    1.373\n",
      "iter 1620/30000  loss         0.136044  avg_L1_norm_grad         0.000290  w[0]    0.000 bias    1.383\n",
      "iter 1621/30000  loss         0.136029  avg_L1_norm_grad         0.000290  w[0]    0.000 bias    1.384\n",
      "iter 1640/30000  loss         0.135735  avg_L1_norm_grad         0.000288  w[0]    0.000 bias    1.394\n",
      "iter 1641/30000  loss         0.135720  avg_L1_norm_grad         0.000287  w[0]    0.000 bias    1.395\n",
      "iter 1660/30000  loss         0.135430  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    1.405\n",
      "iter 1661/30000  loss         0.135415  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    1.405\n",
      "iter 1680/30000  loss         0.135131  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    1.415\n",
      "iter 1681/30000  loss         0.135116  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    1.416\n",
      "iter 1700/30000  loss         0.134835  avg_L1_norm_grad         0.000281  w[0]    0.000 bias    1.426\n",
      "iter 1701/30000  loss         0.134821  avg_L1_norm_grad         0.000281  w[0]    0.000 bias    1.426\n",
      "iter 1720/30000  loss         0.134545  avg_L1_norm_grad         0.000279  w[0]    0.000 bias    1.436\n",
      "iter 1721/30000  loss         0.134530  avg_L1_norm_grad         0.000279  w[0]    0.000 bias    1.437\n",
      "iter 1740/30000  loss         0.134258  avg_L1_norm_grad         0.000277  w[0]    0.000 bias    1.447\n",
      "iter 1741/30000  loss         0.134244  avg_L1_norm_grad         0.000276  w[0]    0.000 bias    1.447\n",
      "iter 1760/30000  loss         0.133976  avg_L1_norm_grad         0.000274  w[0]    0.000 bias    1.457\n",
      "iter 1761/30000  loss         0.133962  avg_L1_norm_grad         0.000274  w[0]    0.000 bias    1.458\n",
      "iter 1780/30000  loss         0.133698  avg_L1_norm_grad         0.000272  w[0]    0.000 bias    1.467\n",
      "iter 1781/30000  loss         0.133685  avg_L1_norm_grad         0.000272  w[0]    0.000 bias    1.468\n",
      "iter 1800/30000  loss         0.133425  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    1.478\n",
      "iter 1801/30000  loss         0.133411  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    1.478\n",
      "iter 1820/30000  loss         0.133155  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    1.488\n",
      "iter 1821/30000  loss         0.133141  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    1.488\n",
      "iter 1840/30000  loss         0.132889  avg_L1_norm_grad         0.000266  w[0]    0.000 bias    1.498\n",
      "iter 1841/30000  loss         0.132875  avg_L1_norm_grad         0.000266  w[0]    0.000 bias    1.498\n",
      "iter 1860/30000  loss         0.132626  avg_L1_norm_grad         0.000265  w[0]    0.000 bias    1.508\n",
      "iter 1861/30000  loss         0.132613  avg_L1_norm_grad         0.000264  w[0]    0.000 bias    1.509\n",
      "iter 1880/30000  loss         0.132367  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    1.518\n",
      "iter 1881/30000  loss         0.132355  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    1.519\n",
      "iter 1900/30000  loss         0.132112  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    1.528\n",
      "iter 1901/30000  loss         0.132100  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    1.529\n",
      "iter 1920/30000  loss         0.131861  avg_L1_norm_grad         0.000259  w[0]    0.000 bias    1.538\n",
      "iter 1921/30000  loss         0.131848  avg_L1_norm_grad         0.000259  w[0]    0.000 bias    1.539\n",
      "iter 1940/30000  loss         0.131612  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    1.548\n",
      "iter 1941/30000  loss         0.131600  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    1.549\n",
      "iter 1960/30000  loss         0.131367  avg_L1_norm_grad         0.000256  w[0]    0.000 bias    1.558\n",
      "iter 1961/30000  loss         0.131355  avg_L1_norm_grad         0.000255  w[0]    0.000 bias    1.558\n",
      "iter 1980/30000  loss         0.131126  avg_L1_norm_grad         0.000254  w[0]    0.000 bias    1.568\n",
      "iter 1981/30000  loss         0.131114  avg_L1_norm_grad         0.000254  w[0]    0.000 bias    1.568\n",
      "iter 2000/30000  loss         0.130887  avg_L1_norm_grad         0.000252  w[0]    0.000 bias    1.578\n",
      "iter 2001/30000  loss         0.130875  avg_L1_norm_grad         0.000252  w[0]    0.000 bias    1.578\n",
      "iter 2020/30000  loss         0.130652  avg_L1_norm_grad         0.000250  w[0]    0.000 bias    1.587\n",
      "iter 2021/30000  loss         0.130640  avg_L1_norm_grad         0.000250  w[0]    0.000 bias    1.588\n",
      "iter 2040/30000  loss         0.130419  avg_L1_norm_grad         0.000249  w[0]    0.000 bias    1.597\n",
      "iter 2041/30000  loss         0.130408  avg_L1_norm_grad         0.000249  w[0]    0.000 bias    1.598\n",
      "iter 2060/30000  loss         0.130190  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    1.607\n",
      "iter 2061/30000  loss         0.130178  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    1.607\n",
      "iter 2080/30000  loss         0.129963  avg_L1_norm_grad         0.000246  w[0]    0.000 bias    1.616\n",
      "iter 2081/30000  loss         0.129952  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    1.617\n",
      "iter 2100/30000  loss         0.129739  avg_L1_norm_grad         0.000244  w[0]    0.000 bias    1.626\n",
      "iter 2101/30000  loss         0.129728  avg_L1_norm_grad         0.000244  w[0]    0.000 bias    1.627\n",
      "iter 2120/30000  loss         0.129518  avg_L1_norm_grad         0.000242  w[0]    0.000 bias    1.636\n",
      "iter 2121/30000  loss         0.129507  avg_L1_norm_grad         0.000242  w[0]    0.000 bias    1.636\n",
      "iter 2140/30000  loss         0.129300  avg_L1_norm_grad         0.000241  w[0]    0.000 bias    1.645\n",
      "iter 2141/30000  loss         0.129289  avg_L1_norm_grad         0.000241  w[0]    0.000 bias    1.646\n",
      "iter 2160/30000  loss         0.129084  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    1.655\n",
      "iter 2161/30000  loss         0.129073  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    1.655\n",
      "iter 2180/30000  loss         0.128871  avg_L1_norm_grad         0.000238  w[0]    0.000 bias    1.664\n",
      "iter 2181/30000  loss         0.128860  avg_L1_norm_grad         0.000238  w[0]    0.000 bias    1.665\n",
      "iter 2200/30000  loss         0.128660  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    1.674\n",
      "iter 2201/30000  loss         0.128650  avg_L1_norm_grad         0.000236  w[0]    0.000 bias    1.674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.128452  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    1.683\n",
      "iter 2221/30000  loss         0.128442  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    1.683\n",
      "iter 2240/30000  loss         0.128246  avg_L1_norm_grad         0.000234  w[0]    0.000 bias    1.692\n",
      "iter 2241/30000  loss         0.128236  avg_L1_norm_grad         0.000233  w[0]    0.000 bias    1.693\n",
      "iter 2260/30000  loss         0.128043  avg_L1_norm_grad         0.000232  w[0]    0.000 bias    1.702\n",
      "iter 2261/30000  loss         0.128033  avg_L1_norm_grad         0.000232  w[0]    0.000 bias    1.702\n",
      "iter 2280/30000  loss         0.127842  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    1.711\n",
      "iter 2281/30000  loss         0.127832  avg_L1_norm_grad         0.000231  w[0]    0.000 bias    1.711\n",
      "iter 2300/30000  loss         0.127643  avg_L1_norm_grad         0.000229  w[0]    0.000 bias    1.720\n",
      "iter 2301/30000  loss         0.127633  avg_L1_norm_grad         0.000229  w[0]    0.000 bias    1.721\n",
      "iter 2320/30000  loss         0.127447  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    1.729\n",
      "iter 2321/30000  loss         0.127437  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    1.730\n",
      "iter 2340/30000  loss         0.127252  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    1.739\n",
      "iter 2341/30000  loss         0.127243  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    1.739\n",
      "iter 2360/30000  loss         0.127060  avg_L1_norm_grad         0.000225  w[0]    0.000 bias    1.748\n",
      "iter 2361/30000  loss         0.127051  avg_L1_norm_grad         0.000225  w[0]    0.000 bias    1.748\n",
      "iter 2380/30000  loss         0.126870  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    1.757\n",
      "iter 2381/30000  loss         0.126861  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    1.757\n",
      "iter 2400/30000  loss         0.126682  avg_L1_norm_grad         0.000223  w[0]    0.000 bias    1.766\n",
      "iter 2401/30000  loss         0.126673  avg_L1_norm_grad         0.000223  w[0]    0.000 bias    1.766\n",
      "iter 2420/30000  loss         0.126496  avg_L1_norm_grad         0.000222  w[0]    0.000 bias    1.775\n",
      "iter 2421/30000  loss         0.126487  avg_L1_norm_grad         0.000221  w[0]    0.000 bias    1.775\n",
      "iter 2440/30000  loss         0.126312  avg_L1_norm_grad         0.000220  w[0]    0.000 bias    1.784\n",
      "iter 2441/30000  loss         0.126303  avg_L1_norm_grad         0.000220  w[0]    0.000 bias    1.784\n",
      "iter 2460/30000  loss         0.126130  avg_L1_norm_grad         0.000219  w[0]    0.000 bias    1.793\n",
      "iter 2461/30000  loss         0.126121  avg_L1_norm_grad         0.000219  w[0]    0.000 bias    1.793\n",
      "iter 2480/30000  loss         0.125950  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    1.802\n",
      "iter 2481/30000  loss         0.125941  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    1.802\n",
      "iter 2500/30000  loss         0.125772  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    1.811\n",
      "iter 2501/30000  loss         0.125763  avg_L1_norm_grad         0.000217  w[0]    0.000 bias    1.811\n",
      "iter 2520/30000  loss         0.125595  avg_L1_norm_grad         0.000216  w[0]    0.000 bias    1.820\n",
      "iter 2521/30000  loss         0.125586  avg_L1_norm_grad         0.000215  w[0]    0.000 bias    1.820\n",
      "iter 2540/30000  loss         0.125421  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    1.829\n",
      "iter 2541/30000  loss         0.125412  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    1.829\n",
      "iter 2560/30000  loss         0.125248  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    1.837\n",
      "iter 2561/30000  loss         0.125239  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    1.838\n",
      "iter 2580/30000  loss         0.125077  avg_L1_norm_grad         0.000212  w[0]    0.000 bias    1.846\n",
      "iter 2581/30000  loss         0.125068  avg_L1_norm_grad         0.000212  w[0]    0.000 bias    1.847\n",
      "iter 2600/30000  loss         0.124907  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    1.855\n",
      "iter 2601/30000  loss         0.124899  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    1.855\n",
      "iter 2620/30000  loss         0.124740  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    1.864\n",
      "iter 2621/30000  loss         0.124731  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    1.864\n",
      "iter 2640/30000  loss         0.124574  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    1.872\n",
      "iter 2641/30000  loss         0.124565  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    1.873\n",
      "iter 2660/30000  loss         0.124409  avg_L1_norm_grad         0.000208  w[0]    0.000 bias    1.881\n",
      "iter 2661/30000  loss         0.124401  avg_L1_norm_grad         0.000208  w[0]    0.000 bias    1.882\n",
      "iter 2680/30000  loss         0.124246  avg_L1_norm_grad         0.000207  w[0]    0.000 bias    1.890\n",
      "iter 2681/30000  loss         0.124238  avg_L1_norm_grad         0.000207  w[0]    0.000 bias    1.890\n",
      "iter 2700/30000  loss         0.124085  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    1.898\n",
      "iter 2701/30000  loss         0.124077  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    1.899\n",
      "iter 2720/30000  loss         0.123925  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    1.907\n",
      "iter 2721/30000  loss         0.123917  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    1.907\n",
      "iter 2740/30000  loss         0.123767  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    1.916\n",
      "iter 2741/30000  loss         0.123759  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    1.916\n",
      "iter 2760/30000  loss         0.123610  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    1.924\n",
      "iter 2761/30000  loss         0.123603  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    1.925\n",
      "iter 2780/30000  loss         0.123455  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    1.933\n",
      "iter 2781/30000  loss         0.123447  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    1.933\n",
      "iter 2800/30000  loss         0.123301  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    1.941\n",
      "iter 2801/30000  loss         0.123294  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    1.942\n",
      "iter 2820/30000  loss         0.123149  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    1.950\n",
      "iter 2821/30000  loss         0.123141  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    1.950\n",
      "iter 2840/30000  loss         0.122998  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    1.958\n",
      "iter 2841/30000  loss         0.122991  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    1.958\n",
      "iter 2860/30000  loss         0.122848  avg_L1_norm_grad         0.000198  w[0]    0.000 bias    1.966\n",
      "iter 2861/30000  loss         0.122841  avg_L1_norm_grad         0.000198  w[0]    0.000 bias    1.967\n",
      "iter 2880/30000  loss         0.122700  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    1.975\n",
      "iter 2881/30000  loss         0.122693  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    1.975\n",
      "iter 2900/30000  loss         0.122553  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    1.983\n",
      "iter 2901/30000  loss         0.122546  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    1.984\n",
      "iter 2920/30000  loss         0.122408  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    1.992\n",
      "iter 2921/30000  loss         0.122400  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    1.992\n",
      "iter 2940/30000  loss         0.122263  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.000\n",
      "iter 2941/30000  loss         0.122256  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.000\n",
      "iter 2960/30000  loss         0.122120  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.008\n",
      "iter 2961/30000  loss         0.122113  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.009\n",
      "iter 2980/30000  loss         0.121978  avg_L1_norm_grad         0.000192  w[0]    0.000 bias    2.016\n",
      "iter 2981/30000  loss         0.121971  avg_L1_norm_grad         0.000192  w[0]    0.000 bias    2.017\n",
      "iter 3000/30000  loss         0.121838  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.025\n",
      "iter 3001/30000  loss         0.121831  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.121699  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.033\n",
      "iter 3021/30000  loss         0.121692  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.033\n",
      "iter 3040/30000  loss         0.121560  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.041\n",
      "iter 3041/30000  loss         0.121553  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.041\n",
      "iter 3060/30000  loss         0.121423  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.049\n",
      "iter 3061/30000  loss         0.121417  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.050\n",
      "iter 3080/30000  loss         0.121288  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.057\n",
      "iter 3081/30000  loss         0.121281  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.058\n",
      "iter 3100/30000  loss         0.121153  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.065\n",
      "iter 3101/30000  loss         0.121146  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.066\n",
      "iter 3120/30000  loss         0.121019  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.074\n",
      "iter 3121/30000  loss         0.121013  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.074\n",
      "iter 3140/30000  loss         0.120887  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.082\n",
      "iter 3141/30000  loss         0.120880  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.082\n",
      "iter 3160/30000  loss         0.120755  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.090\n",
      "iter 3161/30000  loss         0.120749  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.090\n",
      "iter 3180/30000  loss         0.120625  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.098\n",
      "iter 3181/30000  loss         0.120619  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.098\n",
      "iter 3200/30000  loss         0.120496  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.106\n",
      "iter 3201/30000  loss         0.120489  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.106\n",
      "iter 3220/30000  loss         0.120368  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.114\n",
      "iter 3221/30000  loss         0.120361  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.114\n",
      "iter 3240/30000  loss         0.120241  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.122\n",
      "iter 3241/30000  loss         0.120234  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.122\n",
      "iter 3260/30000  loss         0.120114  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.130\n",
      "iter 3261/30000  loss         0.120108  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.130\n",
      "iter 3280/30000  loss         0.119989  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    2.138\n",
      "iter 3281/30000  loss         0.119983  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    2.138\n",
      "iter 3300/30000  loss         0.119865  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    2.145\n",
      "iter 3301/30000  loss         0.119859  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    2.146\n",
      "iter 3320/30000  loss         0.119742  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.153\n",
      "iter 3321/30000  loss         0.119736  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.154\n",
      "iter 3340/30000  loss         0.119620  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.161\n",
      "iter 3341/30000  loss         0.119614  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    2.162\n",
      "iter 3360/30000  loss         0.119498  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.169\n",
      "iter 3361/30000  loss         0.119492  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.169\n",
      "iter 3380/30000  loss         0.119378  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    2.177\n",
      "iter 3381/30000  loss         0.119372  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    2.177\n",
      "iter 3400/30000  loss         0.119259  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    2.185\n",
      "iter 3401/30000  loss         0.119253  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    2.185\n",
      "iter 3420/30000  loss         0.119140  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    2.192\n",
      "iter 3421/30000  loss         0.119134  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    2.193\n",
      "iter 3440/30000  loss         0.119023  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    2.200\n",
      "iter 3441/30000  loss         0.119017  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    2.201\n",
      "iter 3460/30000  loss         0.118906  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    2.208\n",
      "iter 3461/30000  loss         0.118900  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    2.208\n",
      "iter 3480/30000  loss         0.118790  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    2.216\n",
      "iter 3481/30000  loss         0.118784  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    2.216\n",
      "iter 3500/30000  loss         0.118675  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    2.223\n",
      "iter 3501/30000  loss         0.118669  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    2.224\n",
      "iter 3520/30000  loss         0.118561  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    2.231\n",
      "iter 3521/30000  loss         0.118555  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    2.231\n",
      "iter 3540/30000  loss         0.118448  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    2.239\n",
      "iter 3541/30000  loss         0.118442  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    2.239\n",
      "iter 3560/30000  loss         0.118335  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    2.246\n",
      "iter 3561/30000  loss         0.118330  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    2.247\n",
      "iter 3580/30000  loss         0.118224  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.254\n",
      "iter 3581/30000  loss         0.118218  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.254\n",
      "iter 3600/30000  loss         0.118113  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.261\n",
      "iter 3601/30000  loss         0.118107  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.262\n",
      "iter 3620/30000  loss         0.118003  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    2.269\n",
      "iter 3621/30000  loss         0.117997  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    2.269\n",
      "iter 3640/30000  loss         0.117894  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    2.277\n",
      "iter 3641/30000  loss         0.117888  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    2.277\n",
      "iter 3660/30000  loss         0.117785  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    2.284\n",
      "iter 3661/30000  loss         0.117780  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    2.284\n",
      "iter 3680/30000  loss         0.117678  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.292\n",
      "iter 3681/30000  loss         0.117672  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.292\n",
      "iter 3700/30000  loss         0.117571  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.299\n",
      "iter 3701/30000  loss         0.117565  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.299\n",
      "iter 3720/30000  loss         0.117465  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    2.307\n",
      "iter 3721/30000  loss         0.117459  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    2.307\n",
      "iter 3740/30000  loss         0.117359  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    2.314\n",
      "iter 3741/30000  loss         0.117354  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    2.314\n",
      "iter 3760/30000  loss         0.117255  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    2.321\n",
      "iter 3761/30000  loss         0.117249  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    2.322\n",
      "iter 3780/30000  loss         0.117151  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    2.329\n",
      "iter 3781/30000  loss         0.117145  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    2.329\n",
      "iter 3800/30000  loss         0.117047  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    2.336\n",
      "iter 3801/30000  loss         0.117042  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    2.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.116945  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    2.344\n",
      "iter 3821/30000  loss         0.116940  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    2.344\n",
      "iter 3840/30000  loss         0.116843  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    2.351\n",
      "iter 3841/30000  loss         0.116838  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    2.351\n",
      "iter 3860/30000  loss         0.116742  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    2.358\n",
      "iter 3861/30000  loss         0.116737  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    2.359\n",
      "iter 3880/30000  loss         0.116641  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.366\n",
      "iter 3881/30000  loss         0.116636  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.366\n",
      "iter 3900/30000  loss         0.116542  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.373\n",
      "iter 3901/30000  loss         0.116537  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    2.373\n",
      "iter 3920/30000  loss         0.116443  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    2.380\n",
      "iter 3921/30000  loss         0.116438  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    2.381\n",
      "iter 3940/30000  loss         0.116344  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    2.388\n",
      "iter 3941/30000  loss         0.116339  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    2.388\n",
      "iter 3960/30000  loss         0.116246  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    2.395\n",
      "iter 3961/30000  loss         0.116242  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    2.395\n",
      "iter 3980/30000  loss         0.116149  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    2.402\n",
      "iter 3981/30000  loss         0.116144  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    2.403\n",
      "iter 4000/30000  loss         0.116053  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    2.409\n",
      "iter 4001/30000  loss         0.116048  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    2.410\n",
      "iter 4020/30000  loss         0.115957  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.417\n",
      "iter 4021/30000  loss         0.115952  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.417\n",
      "iter 4040/30000  loss         0.115862  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.424\n",
      "iter 4041/30000  loss         0.115857  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.424\n",
      "iter 4060/30000  loss         0.115767  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    2.431\n",
      "iter 4061/30000  loss         0.115762  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    2.431\n",
      "iter 4080/30000  loss         0.115673  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    2.438\n",
      "iter 4081/30000  loss         0.115668  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    2.439\n",
      "iter 4100/30000  loss         0.115580  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    2.445\n",
      "iter 4101/30000  loss         0.115575  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    2.446\n",
      "iter 4120/30000  loss         0.115487  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    2.452\n",
      "iter 4121/30000  loss         0.115482  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    2.453\n",
      "iter 4140/30000  loss         0.115395  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    2.460\n",
      "iter 4141/30000  loss         0.115390  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    2.460\n",
      "iter 4160/30000  loss         0.115303  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.467\n",
      "iter 4161/30000  loss         0.115298  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.467\n",
      "iter 4180/30000  loss         0.115212  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.474\n",
      "iter 4181/30000  loss         0.115207  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.474\n",
      "iter 4200/30000  loss         0.115122  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    2.481\n",
      "iter 4201/30000  loss         0.115117  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    2.481\n",
      "iter 4220/30000  loss         0.115032  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    2.488\n",
      "iter 4221/30000  loss         0.115027  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    2.488\n",
      "iter 4240/30000  loss         0.114942  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    2.495\n",
      "iter 4241/30000  loss         0.114938  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    2.495\n",
      "iter 4260/30000  loss         0.114853  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    2.502\n",
      "iter 4261/30000  loss         0.114849  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    2.502\n",
      "iter 4280/30000  loss         0.114765  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    2.509\n",
      "iter 4281/30000  loss         0.114761  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    2.509\n",
      "iter 4300/30000  loss         0.114678  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    2.516\n",
      "iter 4301/30000  loss         0.114673  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    2.516\n",
      "iter 4320/30000  loss         0.114590  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.523\n",
      "iter 4321/30000  loss         0.114586  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.523\n",
      "iter 4340/30000  loss         0.114504  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.530\n",
      "iter 4341/30000  loss         0.114499  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.530\n",
      "iter 4360/30000  loss         0.114418  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    2.537\n",
      "iter 4361/30000  loss         0.114413  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    2.537\n",
      "iter 4380/30000  loss         0.114332  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    2.544\n",
      "iter 4381/30000  loss         0.114328  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    2.544\n",
      "iter 4400/30000  loss         0.114247  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.551\n",
      "iter 4401/30000  loss         0.114243  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.551\n",
      "iter 4420/30000  loss         0.114163  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.558\n",
      "iter 4421/30000  loss         0.114158  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.558\n",
      "iter 4440/30000  loss         0.114078  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.564\n",
      "iter 4441/30000  loss         0.114074  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    2.565\n",
      "iter 4460/30000  loss         0.113995  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    2.571\n",
      "iter 4461/30000  loss         0.113991  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    2.572\n",
      "iter 4480/30000  loss         0.113912  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    2.578\n",
      "iter 4481/30000  loss         0.113908  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    2.578\n",
      "iter 4500/30000  loss         0.113829  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.585\n",
      "iter 4501/30000  loss         0.113825  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.585\n",
      "iter 4520/30000  loss         0.113747  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.592\n",
      "iter 4521/30000  loss         0.113743  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.592\n",
      "iter 4540/30000  loss         0.113666  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.599\n",
      "iter 4541/30000  loss         0.113662  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.599\n",
      "iter 4560/30000  loss         0.113585  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.605\n",
      "iter 4561/30000  loss         0.113581  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.606\n",
      "iter 4580/30000  loss         0.113504  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    2.612\n",
      "iter 4581/30000  loss         0.113500  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    2.613\n",
      "iter 4600/30000  loss         0.113424  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    2.619\n",
      "iter 4601/30000  loss         0.113420  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    2.619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4620/30000  loss         0.113344  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.626\n",
      "iter 4621/30000  loss         0.113340  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.626\n",
      "iter 4640/30000  loss         0.113265  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.632\n",
      "iter 4641/30000  loss         0.113261  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.633\n",
      "iter 4660/30000  loss         0.113186  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.639\n",
      "iter 4661/30000  loss         0.113182  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.639\n",
      "iter 4680/30000  loss         0.113108  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    2.646\n",
      "iter 4681/30000  loss         0.113104  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    2.646\n",
      "iter 4700/30000  loss         0.113030  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    2.653\n",
      "iter 4701/30000  loss         0.113026  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    2.653\n",
      "iter 4720/30000  loss         0.112952  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    2.659\n",
      "iter 4721/30000  loss         0.112948  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    2.660\n",
      "iter 4740/30000  loss         0.112875  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    2.666\n",
      "iter 4741/30000  loss         0.112872  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    2.666\n",
      "iter 4760/30000  loss         0.112799  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.673\n",
      "iter 4761/30000  loss         0.112795  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.673\n",
      "iter 4780/30000  loss         0.112723  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.679\n",
      "iter 4781/30000  loss         0.112719  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.680\n",
      "iter 4800/30000  loss         0.112647  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.686\n",
      "iter 4801/30000  loss         0.112643  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.686\n",
      "iter 4820/30000  loss         0.112572  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    2.692\n",
      "iter 4821/30000  loss         0.112568  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    2.693\n",
      "iter 4840/30000  loss         0.112497  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    2.699\n",
      "iter 4841/30000  loss         0.112493  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    2.699\n",
      "iter 4860/30000  loss         0.112422  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.706\n",
      "iter 4861/30000  loss         0.112419  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.706\n",
      "iter 4880/30000  loss         0.112348  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.712\n",
      "iter 4881/30000  loss         0.112344  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.713\n",
      "iter 4900/30000  loss         0.112275  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.719\n",
      "iter 4901/30000  loss         0.112271  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.719\n",
      "iter 4920/30000  loss         0.112201  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    2.725\n",
      "iter 4921/30000  loss         0.112198  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    2.726\n",
      "iter 4940/30000  loss         0.112128  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    2.732\n",
      "iter 4941/30000  loss         0.112125  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    2.732\n",
      "iter 4960/30000  loss         0.112056  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.738\n",
      "iter 4961/30000  loss         0.112052  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.739\n",
      "iter 4980/30000  loss         0.111984  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.745\n",
      "iter 4981/30000  loss         0.111980  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.745\n",
      "iter 5000/30000  loss         0.111912  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.751\n",
      "iter 5001/30000  loss         0.111909  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    2.752\n",
      "iter 5020/30000  loss         0.111841  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.758\n",
      "iter 5021/30000  loss         0.111837  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.758\n",
      "iter 5040/30000  loss         0.111770  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.764\n",
      "iter 5041/30000  loss         0.111767  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.765\n",
      "iter 5060/30000  loss         0.111700  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.771\n",
      "iter 5061/30000  loss         0.111696  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.771\n",
      "iter 5080/30000  loss         0.111629  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.777\n",
      "iter 5081/30000  loss         0.111626  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.778\n",
      "iter 5100/30000  loss         0.111560  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.784\n",
      "iter 5101/30000  loss         0.111556  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.784\n",
      "iter 5120/30000  loss         0.111490  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.790\n",
      "iter 5121/30000  loss         0.111487  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.790\n",
      "iter 5140/30000  loss         0.111421  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.796\n",
      "iter 5141/30000  loss         0.111418  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.797\n",
      "iter 5160/30000  loss         0.111353  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.803\n",
      "iter 5161/30000  loss         0.111349  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    2.803\n",
      "iter 5180/30000  loss         0.111284  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.809\n",
      "iter 5181/30000  loss         0.111281  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.810\n",
      "iter 5200/30000  loss         0.111216  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.816\n",
      "iter 5201/30000  loss         0.111213  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.816\n",
      "iter 5220/30000  loss         0.111149  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.822\n",
      "iter 5221/30000  loss         0.111145  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.822\n",
      "iter 5240/30000  loss         0.111081  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.828\n",
      "iter 5241/30000  loss         0.111078  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.829\n",
      "iter 5260/30000  loss         0.111015  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.835\n",
      "iter 5261/30000  loss         0.111011  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.835\n",
      "iter 5280/30000  loss         0.110948  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.841\n",
      "iter 5281/30000  loss         0.110945  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.841\n",
      "iter 5300/30000  loss         0.110882  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.847\n",
      "iter 5301/30000  loss         0.110878  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.848\n",
      "iter 5320/30000  loss         0.110816  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.854\n",
      "iter 5321/30000  loss         0.110813  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.854\n",
      "iter 5340/30000  loss         0.110750  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.860\n",
      "iter 5341/30000  loss         0.110747  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.860\n",
      "iter 5360/30000  loss         0.110685  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.866\n",
      "iter 5361/30000  loss         0.110682  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.866\n",
      "iter 5380/30000  loss         0.110620  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.872\n",
      "iter 5381/30000  loss         0.110617  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    2.873\n",
      "iter 5400/30000  loss         0.110556  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.879\n",
      "iter 5401/30000  loss         0.110552  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5420/30000  loss         0.110491  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.885\n",
      "iter 5421/30000  loss         0.110488  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.885\n",
      "iter 5440/30000  loss         0.110428  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.891\n",
      "iter 5441/30000  loss         0.110424  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.891\n",
      "iter 5460/30000  loss         0.110364  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.897\n",
      "iter 5461/30000  loss         0.110361  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.898\n",
      "iter 5480/30000  loss         0.110301  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.903\n",
      "iter 5481/30000  loss         0.110298  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.904\n",
      "iter 5500/30000  loss         0.110238  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.910\n",
      "iter 5501/30000  loss         0.110235  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.910\n",
      "iter 5520/30000  loss         0.110175  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.916\n",
      "iter 5521/30000  loss         0.110172  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.916\n",
      "iter 5540/30000  loss         0.110113  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.922\n",
      "iter 5541/30000  loss         0.110110  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.922\n",
      "iter 5560/30000  loss         0.110051  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.928\n",
      "iter 5561/30000  loss         0.110048  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.928\n",
      "iter 5580/30000  loss         0.109989  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.934\n",
      "iter 5581/30000  loss         0.109986  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.935\n",
      "iter 5600/30000  loss         0.109928  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.940\n",
      "iter 5601/30000  loss         0.109925  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.941\n",
      "iter 5620/30000  loss         0.109867  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.947\n",
      "iter 5621/30000  loss         0.109864  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.947\n",
      "iter 5640/30000  loss         0.109806  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.953\n",
      "iter 5641/30000  loss         0.109803  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.953\n",
      "iter 5660/30000  loss         0.109745  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.959\n",
      "iter 5661/30000  loss         0.109742  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.959\n",
      "iter 5680/30000  loss         0.109685  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.965\n",
      "iter 5681/30000  loss         0.109682  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.965\n",
      "iter 5700/30000  loss         0.109625  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.971\n",
      "iter 5701/30000  loss         0.109622  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.971\n",
      "iter 5720/30000  loss         0.109566  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.977\n",
      "iter 5721/30000  loss         0.109563  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.977\n",
      "iter 5740/30000  loss         0.109506  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.983\n",
      "iter 5741/30000  loss         0.109503  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    2.983\n",
      "iter 5760/30000  loss         0.109447  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.989\n",
      "iter 5761/30000  loss         0.109444  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.989\n",
      "iter 5780/30000  loss         0.109388  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.995\n",
      "iter 5781/30000  loss         0.109386  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.995\n",
      "iter 5800/30000  loss         0.109330  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.001\n",
      "iter 5801/30000  loss         0.109327  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.001\n",
      "iter 5820/30000  loss         0.109272  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.007\n",
      "iter 5821/30000  loss         0.109269  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.007\n",
      "iter 5840/30000  loss         0.109214  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.013\n",
      "iter 5841/30000  loss         0.109211  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.013\n",
      "iter 5860/30000  loss         0.109156  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.019\n",
      "iter 5861/30000  loss         0.109153  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.019\n",
      "iter 5880/30000  loss         0.109099  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.025\n",
      "iter 5881/30000  loss         0.109096  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.025\n",
      "iter 5900/30000  loss         0.109042  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.031\n",
      "iter 5901/30000  loss         0.109039  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.031\n",
      "iter 5920/30000  loss         0.108985  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.037\n",
      "iter 5921/30000  loss         0.108982  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.037\n",
      "iter 5940/30000  loss         0.108928  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.043\n",
      "iter 5941/30000  loss         0.108925  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.043\n",
      "iter 5960/30000  loss         0.108872  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.049\n",
      "iter 5961/30000  loss         0.108869  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.049\n",
      "iter 5980/30000  loss         0.108816  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.055\n",
      "iter 5981/30000  loss         0.108813  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.055\n",
      "iter 6000/30000  loss         0.108760  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.061\n",
      "iter 6001/30000  loss         0.108757  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.061\n",
      "iter 6020/30000  loss         0.108705  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.067\n",
      "iter 6021/30000  loss         0.108702  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.067\n",
      "iter 6040/30000  loss         0.108649  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.072\n",
      "iter 6041/30000  loss         0.108647  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.073\n",
      "iter 6060/30000  loss         0.108594  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.078\n",
      "iter 6061/30000  loss         0.108592  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.079\n",
      "iter 6080/30000  loss         0.108540  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.084\n",
      "iter 6081/30000  loss         0.108537  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.084\n",
      "iter 6100/30000  loss         0.108485  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.090\n",
      "iter 6101/30000  loss         0.108482  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.090\n",
      "iter 6120/30000  loss         0.108431  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.096\n",
      "iter 6121/30000  loss         0.108428  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.096\n",
      "iter 6140/30000  loss         0.108377  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.102\n",
      "iter 6141/30000  loss         0.108374  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.102\n",
      "iter 6160/30000  loss         0.108323  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.108\n",
      "iter 6161/30000  loss         0.108320  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.108\n",
      "iter 6180/30000  loss         0.108270  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.113\n",
      "iter 6181/30000  loss         0.108267  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.114\n",
      "iter 6200/30000  loss         0.108216  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.119\n",
      "iter 6201/30000  loss         0.108214  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6220/30000  loss         0.108163  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.125\n",
      "iter 6221/30000  loss         0.108161  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.125\n",
      "iter 6240/30000  loss         0.108110  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.131\n",
      "iter 6241/30000  loss         0.108108  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.131\n",
      "iter 6260/30000  loss         0.108058  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.137\n",
      "iter 6261/30000  loss         0.108055  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.137\n",
      "iter 6280/30000  loss         0.108006  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.142\n",
      "iter 6281/30000  loss         0.108003  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.143\n",
      "iter 6300/30000  loss         0.107953  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.148\n",
      "iter 6301/30000  loss         0.107951  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.148\n",
      "iter 6320/30000  loss         0.107902  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.154\n",
      "iter 6321/30000  loss         0.107899  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.154\n",
      "iter 6340/30000  loss         0.107850  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.160\n",
      "iter 6341/30000  loss         0.107847  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.160\n",
      "iter 6360/30000  loss         0.107799  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.165\n",
      "iter 6361/30000  loss         0.107796  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.166\n",
      "iter 6380/30000  loss         0.107747  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.171\n",
      "iter 6381/30000  loss         0.107745  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.171\n",
      "iter 6400/30000  loss         0.107696  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.177\n",
      "iter 6401/30000  loss         0.107694  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.177\n",
      "iter 6420/30000  loss         0.107646  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.182\n",
      "iter 6421/30000  loss         0.107643  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.183\n",
      "iter 6440/30000  loss         0.107595  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.188\n",
      "iter 6441/30000  loss         0.107593  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.188\n",
      "iter 6460/30000  loss         0.107545  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.194\n",
      "iter 6461/30000  loss         0.107543  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.194\n",
      "iter 6480/30000  loss         0.107495  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.199\n",
      "iter 6481/30000  loss         0.107492  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.200\n",
      "iter 6500/30000  loss         0.107445  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.205\n",
      "iter 6501/30000  loss         0.107443  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.205\n",
      "iter 6520/30000  loss         0.107396  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.211\n",
      "iter 6521/30000  loss         0.107393  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.211\n",
      "iter 6540/30000  loss         0.107346  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.216\n",
      "iter 6541/30000  loss         0.107344  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.217\n",
      "iter 6560/30000  loss         0.107297  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.222\n",
      "iter 6561/30000  loss         0.107295  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.222\n",
      "iter 6580/30000  loss         0.107248  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.228\n",
      "iter 6581/30000  loss         0.107246  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.228\n",
      "iter 6600/30000  loss         0.107199  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.233\n",
      "iter 6601/30000  loss         0.107197  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.233\n",
      "iter 6620/30000  loss         0.107151  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.239\n",
      "iter 6621/30000  loss         0.107148  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.239\n",
      "iter 6640/30000  loss         0.107102  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.244\n",
      "iter 6641/30000  loss         0.107100  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.245\n",
      "iter 6660/30000  loss         0.107054  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.250\n",
      "iter 6661/30000  loss         0.107052  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.250\n",
      "iter 6680/30000  loss         0.107006  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.256\n",
      "iter 6681/30000  loss         0.107004  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    3.256\n",
      "iter 6700/30000  loss         0.106959  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.261\n",
      "iter 6701/30000  loss         0.106956  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.261\n",
      "iter 6720/30000  loss         0.106911  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.267\n",
      "iter 6721/30000  loss         0.106909  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.267\n",
      "iter 6740/30000  loss         0.106864  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.272\n",
      "iter 6741/30000  loss         0.106861  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.272\n",
      "iter 6760/30000  loss         0.106817  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.278\n",
      "iter 6761/30000  loss         0.106814  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    3.278\n",
      "iter 6780/30000  loss         0.106770  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.283\n",
      "iter 6781/30000  loss         0.106767  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.284\n",
      "iter 6800/30000  loss         0.106723  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.289\n",
      "iter 6801/30000  loss         0.106721  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.289\n",
      "iter 6820/30000  loss         0.106677  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.294\n",
      "iter 6821/30000  loss         0.106674  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.295\n",
      "iter 6840/30000  loss         0.106630  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.300\n",
      "iter 6841/30000  loss         0.106628  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    3.300\n",
      "iter 6860/30000  loss         0.106584  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.305\n",
      "iter 6861/30000  loss         0.106582  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.306\n",
      "iter 6880/30000  loss         0.106538  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.311\n",
      "iter 6881/30000  loss         0.106536  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.311\n",
      "iter 6900/30000  loss         0.106493  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.316\n",
      "iter 6901/30000  loss         0.106490  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.316\n",
      "iter 6920/30000  loss         0.106447  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.322\n",
      "iter 6921/30000  loss         0.106445  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    3.322\n",
      "iter 6940/30000  loss         0.106402  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.327\n",
      "iter 6941/30000  loss         0.106399  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.327\n",
      "iter 6960/30000  loss         0.106356  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.333\n",
      "iter 6961/30000  loss         0.106354  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.333\n",
      "iter 6980/30000  loss         0.106312  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.338\n",
      "iter 6981/30000  loss         0.106309  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.338\n",
      "iter 7000/30000  loss         0.106267  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.343\n",
      "iter 7001/30000  loss         0.106264  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7020/30000  loss         0.106222  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.349\n",
      "iter 7021/30000  loss         0.106220  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    3.349\n",
      "iter 7040/30000  loss         0.106178  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.354\n",
      "iter 7041/30000  loss         0.106175  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.354\n",
      "iter 7060/30000  loss         0.106133  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.360\n",
      "iter 7061/30000  loss         0.106131  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.360\n",
      "iter 7080/30000  loss         0.106089  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.365\n",
      "iter 7081/30000  loss         0.106087  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.365\n",
      "iter 7100/30000  loss         0.106046  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.370\n",
      "iter 7101/30000  loss         0.106043  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    3.371\n",
      "iter 7120/30000  loss         0.106002  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.376\n",
      "iter 7121/30000  loss         0.106000  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.376\n",
      "iter 7140/30000  loss         0.105958  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.381\n",
      "iter 7141/30000  loss         0.105956  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.381\n",
      "iter 7160/30000  loss         0.105915  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.386\n",
      "iter 7161/30000  loss         0.105913  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.387\n",
      "iter 7180/30000  loss         0.105872  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.392\n",
      "iter 7181/30000  loss         0.105870  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.392\n",
      "iter 7200/30000  loss         0.105829  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.397\n",
      "iter 7201/30000  loss         0.105827  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    3.397\n",
      "iter 7220/30000  loss         0.105786  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.402\n",
      "iter 7221/30000  loss         0.105784  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.403\n",
      "iter 7240/30000  loss         0.105743  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.408\n",
      "iter 7241/30000  loss         0.105741  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.408\n",
      "iter 7260/30000  loss         0.105701  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.413\n",
      "iter 7261/30000  loss         0.105699  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.413\n",
      "iter 7280/30000  loss         0.105659  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.418\n",
      "iter 7281/30000  loss         0.105657  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    3.419\n",
      "iter 7300/30000  loss         0.105617  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.424\n",
      "iter 7301/30000  loss         0.105614  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.424\n",
      "iter 7320/30000  loss         0.105575  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.429\n",
      "iter 7321/30000  loss         0.105573  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.429\n",
      "iter 7340/30000  loss         0.105533  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.434\n",
      "iter 7341/30000  loss         0.105531  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.435\n",
      "iter 7360/30000  loss         0.105491  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.440\n",
      "iter 7361/30000  loss         0.105489  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.440\n",
      "iter 7380/30000  loss         0.105450  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.445\n",
      "iter 7381/30000  loss         0.105448  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    3.445\n",
      "iter 7400/30000  loss         0.105408  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.450\n",
      "iter 7401/30000  loss         0.105406  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.450\n",
      "iter 7420/30000  loss         0.105367  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.455\n",
      "iter 7421/30000  loss         0.105365  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.456\n",
      "iter 7440/30000  loss         0.105326  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.461\n",
      "iter 7441/30000  loss         0.105324  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.461\n",
      "iter 7460/30000  loss         0.105286  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.466\n",
      "iter 7461/30000  loss         0.105284  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    3.466\n",
      "iter 7480/30000  loss         0.105245  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.471\n",
      "iter 7481/30000  loss         0.105243  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.471\n",
      "iter 7500/30000  loss         0.105204  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.476\n",
      "iter 7501/30000  loss         0.105202  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.476\n",
      "iter 7520/30000  loss         0.105164  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.481\n",
      "iter 7521/30000  loss         0.105162  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.482\n",
      "iter 7540/30000  loss         0.105124  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.487\n",
      "iter 7541/30000  loss         0.105122  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.487\n",
      "iter 7560/30000  loss         0.105084  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.492\n",
      "iter 7561/30000  loss         0.105082  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    3.492\n",
      "iter 7580/30000  loss         0.105044  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.497\n",
      "iter 7581/30000  loss         0.105042  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.497\n",
      "iter 7600/30000  loss         0.105004  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.502\n",
      "iter 7601/30000  loss         0.105003  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.502\n",
      "iter 7620/30000  loss         0.104965  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.507\n",
      "iter 7621/30000  loss         0.104963  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.508\n",
      "iter 7640/30000  loss         0.104926  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.512\n",
      "iter 7641/30000  loss         0.104924  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.513\n",
      "iter 7660/30000  loss         0.104886  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.518\n",
      "iter 7661/30000  loss         0.104884  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    3.518\n",
      "iter 7680/30000  loss         0.104847  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.523\n",
      "iter 7681/30000  loss         0.104845  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.523\n",
      "iter 7700/30000  loss         0.104808  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.528\n",
      "iter 7701/30000  loss         0.104806  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.528\n",
      "iter 7720/30000  loss         0.104770  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.533\n",
      "iter 7721/30000  loss         0.104768  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.533\n",
      "iter 7740/30000  loss         0.104731  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.538\n",
      "iter 7741/30000  loss         0.104729  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.538\n",
      "iter 7760/30000  loss         0.104692  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.543\n",
      "iter 7761/30000  loss         0.104691  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    3.543\n",
      "iter 7780/30000  loss         0.104654  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.548\n",
      "iter 7781/30000  loss         0.104652  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.549\n",
      "iter 7800/30000  loss         0.104616  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.553\n",
      "iter 7801/30000  loss         0.104614  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7820/30000  loss         0.104578  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.558\n",
      "iter 7821/30000  loss         0.104576  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.559\n",
      "iter 7840/30000  loss         0.104540  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.563\n",
      "iter 7841/30000  loss         0.104538  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.564\n",
      "iter 7860/30000  loss         0.104502  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.569\n",
      "iter 7861/30000  loss         0.104500  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    3.569\n",
      "iter 7880/30000  loss         0.104465  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.574\n",
      "iter 7881/30000  loss         0.104463  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.574\n",
      "iter 7900/30000  loss         0.104427  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.579\n",
      "iter 7901/30000  loss         0.104425  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.579\n",
      "iter 7920/30000  loss         0.104390  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.584\n",
      "iter 7921/30000  loss         0.104388  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.584\n",
      "iter 7940/30000  loss         0.104353  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.589\n",
      "iter 7941/30000  loss         0.104351  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.589\n",
      "iter 7960/30000  loss         0.104316  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.594\n",
      "iter 7961/30000  loss         0.104314  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.594\n",
      "iter 7980/30000  loss         0.104279  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.599\n",
      "iter 7981/30000  loss         0.104277  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    3.599\n",
      "iter 8000/30000  loss         0.104242  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.604\n",
      "iter 8001/30000  loss         0.104240  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.604\n",
      "iter 8020/30000  loss         0.104205  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.609\n",
      "iter 8021/30000  loss         0.104204  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.609\n",
      "iter 8040/30000  loss         0.104169  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.614\n",
      "iter 8041/30000  loss         0.104167  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.614\n",
      "iter 8060/30000  loss         0.104133  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.619\n",
      "iter 8061/30000  loss         0.104131  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.619\n",
      "iter 8080/30000  loss         0.104096  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.624\n",
      "iter 8081/30000  loss         0.104095  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    3.624\n",
      "iter 8100/30000  loss         0.104060  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.629\n",
      "iter 8101/30000  loss         0.104059  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.629\n",
      "iter 8120/30000  loss         0.104024  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.634\n",
      "iter 8121/30000  loss         0.104023  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.634\n",
      "iter 8140/30000  loss         0.103989  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.639\n",
      "iter 8141/30000  loss         0.103987  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.639\n",
      "iter 8160/30000  loss         0.103953  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.644\n",
      "iter 8161/30000  loss         0.103951  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.644\n",
      "iter 8180/30000  loss         0.103917  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.649\n",
      "iter 8181/30000  loss         0.103916  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    3.649\n",
      "iter 8200/30000  loss         0.103882  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.653\n",
      "iter 8201/30000  loss         0.103880  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.654\n",
      "iter 8220/30000  loss         0.103847  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.658\n",
      "iter 8221/30000  loss         0.103845  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.659\n",
      "iter 8240/30000  loss         0.103812  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.663\n",
      "iter 8241/30000  loss         0.103810  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.664\n",
      "iter 8260/30000  loss         0.103777  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.668\n",
      "iter 8261/30000  loss         0.103775  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.668\n",
      "iter 8280/30000  loss         0.103742  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.673\n",
      "iter 8281/30000  loss         0.103740  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.673\n",
      "iter 8300/30000  loss         0.103707  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.678\n",
      "iter 8301/30000  loss         0.103705  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    3.678\n",
      "iter 8320/30000  loss         0.103672  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.683\n",
      "iter 8321/30000  loss         0.103671  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.683\n",
      "iter 8340/30000  loss         0.103638  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.688\n",
      "iter 8341/30000  loss         0.103636  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.688\n",
      "iter 8360/30000  loss         0.103603  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.693\n",
      "iter 8361/30000  loss         0.103602  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.693\n",
      "iter 8380/30000  loss         0.103569  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.698\n",
      "iter 8381/30000  loss         0.103567  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.698\n",
      "iter 8400/30000  loss         0.103535  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.702\n",
      "iter 8401/30000  loss         0.103533  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.703\n",
      "iter 8420/30000  loss         0.103501  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.707\n",
      "iter 8421/30000  loss         0.103499  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    3.707\n",
      "iter 8440/30000  loss         0.103467  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.712\n",
      "iter 8441/30000  loss         0.103465  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.712\n",
      "iter 8460/30000  loss         0.103433  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.717\n",
      "iter 8461/30000  loss         0.103432  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.717\n",
      "iter 8480/30000  loss         0.103400  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.722\n",
      "iter 8481/30000  loss         0.103398  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.722\n",
      "iter 8500/30000  loss         0.103366  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.727\n",
      "iter 8501/30000  loss         0.103365  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.727\n",
      "iter 8520/30000  loss         0.103333  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.731\n",
      "iter 8521/30000  loss         0.103331  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    3.732\n",
      "iter 8540/30000  loss         0.103300  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.736\n",
      "iter 8541/30000  loss         0.103298  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.736\n",
      "iter 8560/30000  loss         0.103266  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.741\n",
      "iter 8561/30000  loss         0.103265  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.741\n",
      "iter 8580/30000  loss         0.103233  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.746\n",
      "iter 8581/30000  loss         0.103232  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.746\n",
      "iter 8600/30000  loss         0.103201  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.751\n",
      "iter 8601/30000  loss         0.103199  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8620/30000  loss         0.103168  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.755\n",
      "iter 8621/30000  loss         0.103166  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.756\n",
      "iter 8640/30000  loss         0.103135  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.760\n",
      "iter 8641/30000  loss         0.103133  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    3.760\n",
      "iter 8660/30000  loss         0.103102  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.765\n",
      "iter 8661/30000  loss         0.103101  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.765\n",
      "iter 8680/30000  loss         0.103070  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.770\n",
      "iter 8681/30000  loss         0.103068  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.770\n",
      "iter 8700/30000  loss         0.103038  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.774\n",
      "iter 8701/30000  loss         0.103036  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.775\n",
      "iter 8720/30000  loss         0.103005  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.779\n",
      "iter 8721/30000  loss         0.103004  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.779\n",
      "iter 8740/30000  loss         0.102973  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.784\n",
      "iter 8741/30000  loss         0.102972  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.784\n",
      "iter 8760/30000  loss         0.102941  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.789\n",
      "iter 8761/30000  loss         0.102940  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    3.789\n",
      "iter 8780/30000  loss         0.102909  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.793\n",
      "iter 8781/30000  loss         0.102908  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.794\n",
      "iter 8800/30000  loss         0.102878  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.798\n",
      "iter 8801/30000  loss         0.102876  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.798\n",
      "iter 8820/30000  loss         0.102846  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.803\n",
      "iter 8821/30000  loss         0.102844  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.803\n",
      "iter 8840/30000  loss         0.102815  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.807\n",
      "iter 8841/30000  loss         0.102813  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.808\n",
      "iter 8860/30000  loss         0.102783  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.812\n",
      "iter 8861/30000  loss         0.102782  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.812\n",
      "iter 8880/30000  loss         0.102752  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.817\n",
      "iter 8881/30000  loss         0.102750  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    3.817\n",
      "iter 8900/30000  loss         0.102721  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.822\n",
      "iter 8901/30000  loss         0.102719  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.822\n",
      "iter 8920/30000  loss         0.102689  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.826\n",
      "iter 8921/30000  loss         0.102688  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.826\n",
      "iter 8940/30000  loss         0.102658  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.831\n",
      "iter 8941/30000  loss         0.102657  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.831\n",
      "iter 8960/30000  loss         0.102628  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.836\n",
      "iter 8961/30000  loss         0.102626  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.836\n",
      "iter 8980/30000  loss         0.102597  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.840\n",
      "iter 8981/30000  loss         0.102595  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.840\n",
      "iter 9000/30000  loss         0.102566  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.845\n",
      "iter 9001/30000  loss         0.102565  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.845\n",
      "iter 9020/30000  loss         0.102536  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.849\n",
      "iter 9021/30000  loss         0.102534  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    3.850\n",
      "iter 9040/30000  loss         0.102505  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.854\n",
      "iter 9041/30000  loss         0.102504  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.854\n",
      "iter 9060/30000  loss         0.102475  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.859\n",
      "iter 9061/30000  loss         0.102473  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.859\n",
      "iter 9080/30000  loss         0.102444  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.863\n",
      "iter 9081/30000  loss         0.102443  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.864\n",
      "iter 9100/30000  loss         0.102414  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.868\n",
      "iter 9101/30000  loss         0.102413  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.868\n",
      "iter 9120/30000  loss         0.102384  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.873\n",
      "iter 9121/30000  loss         0.102383  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.873\n",
      "iter 9140/30000  loss         0.102354  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.877\n",
      "iter 9141/30000  loss         0.102353  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    3.877\n",
      "iter 9160/30000  loss         0.102324  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.882\n",
      "iter 9161/30000  loss         0.102323  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.882\n",
      "iter 9180/30000  loss         0.102295  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.886\n",
      "iter 9181/30000  loss         0.102293  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.887\n",
      "iter 9200/30000  loss         0.102265  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.891\n",
      "iter 9201/30000  loss         0.102264  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.891\n",
      "iter 9220/30000  loss         0.102236  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.896\n",
      "iter 9221/30000  loss         0.102234  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.896\n",
      "iter 9240/30000  loss         0.102206  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.900\n",
      "iter 9241/30000  loss         0.102205  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.900\n",
      "iter 9260/30000  loss         0.102177  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.905\n",
      "iter 9261/30000  loss         0.102175  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.905\n",
      "iter 9280/30000  loss         0.102148  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.909\n",
      "iter 9281/30000  loss         0.102146  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    3.909\n",
      "iter 9300/30000  loss         0.102118  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.914\n",
      "iter 9301/30000  loss         0.102117  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.914\n",
      "iter 9320/30000  loss         0.102089  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.918\n",
      "iter 9321/30000  loss         0.102088  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.919\n",
      "iter 9340/30000  loss         0.102060  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.923\n",
      "iter 9341/30000  loss         0.102059  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.923\n",
      "iter 9360/30000  loss         0.102032  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.927\n",
      "iter 9361/30000  loss         0.102030  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.928\n",
      "iter 9380/30000  loss         0.102003  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.932\n",
      "iter 9381/30000  loss         0.102001  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.932\n",
      "iter 9400/30000  loss         0.101974  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.936\n",
      "iter 9401/30000  loss         0.101973  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9420/30000  loss         0.101946  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.941\n",
      "iter 9421/30000  loss         0.101944  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    3.941\n",
      "iter 9440/30000  loss         0.101917  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.945\n",
      "iter 9441/30000  loss         0.101916  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.946\n",
      "iter 9460/30000  loss         0.101889  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.950\n",
      "iter 9461/30000  loss         0.101887  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.950\n",
      "iter 9480/30000  loss         0.101861  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.954\n",
      "iter 9481/30000  loss         0.101859  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.955\n",
      "iter 9500/30000  loss         0.101832  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.959\n",
      "iter 9501/30000  loss         0.101831  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.959\n",
      "iter 9520/30000  loss         0.101804  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.963\n",
      "iter 9521/30000  loss         0.101803  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.964\n",
      "iter 9540/30000  loss         0.101776  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.968\n",
      "iter 9541/30000  loss         0.101775  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    3.968\n",
      "iter 9560/30000  loss         0.101749  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.972\n",
      "iter 9561/30000  loss         0.101747  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.973\n",
      "iter 9580/30000  loss         0.101721  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.977\n",
      "iter 9581/30000  loss         0.101719  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.977\n",
      "iter 9600/30000  loss         0.101693  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.981\n",
      "iter 9601/30000  loss         0.101692  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.981\n",
      "iter 9620/30000  loss         0.101665  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.986\n",
      "iter 9621/30000  loss         0.101664  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.986\n",
      "iter 9640/30000  loss         0.101638  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.990\n",
      "iter 9641/30000  loss         0.101636  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.990\n",
      "iter 9660/30000  loss         0.101610  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.995\n",
      "iter 9661/30000  loss         0.101609  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.995\n",
      "iter 9680/30000  loss         0.101583  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.999\n",
      "iter 9681/30000  loss         0.101582  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    3.999\n",
      "iter 9700/30000  loss         0.101556  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.003\n",
      "iter 9701/30000  loss         0.101554  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.004\n",
      "iter 9720/30000  loss         0.101529  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.008\n",
      "iter 9721/30000  loss         0.101527  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.008\n",
      "iter 9740/30000  loss         0.101502  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.012\n",
      "iter 9741/30000  loss         0.101500  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.012\n",
      "iter 9760/30000  loss         0.101475  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.017\n",
      "iter 9761/30000  loss         0.101473  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.017\n",
      "iter 9780/30000  loss         0.101448  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.021\n",
      "iter 9781/30000  loss         0.101446  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.021\n",
      "iter 9800/30000  loss         0.101421  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.025\n",
      "iter 9801/30000  loss         0.101420  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.026\n",
      "iter 9820/30000  loss         0.101394  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.030\n",
      "iter 9821/30000  loss         0.101393  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.030\n",
      "iter 9840/30000  loss         0.101368  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.034\n",
      "iter 9841/30000  loss         0.101366  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.034\n",
      "iter 9860/30000  loss         0.101341  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.039\n",
      "iter 9861/30000  loss         0.101340  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.039\n",
      "iter 9880/30000  loss         0.101315  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.043\n",
      "iter 9881/30000  loss         0.101313  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.043\n",
      "iter 9900/30000  loss         0.101288  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.047\n",
      "iter 9901/30000  loss         0.101287  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.047\n",
      "iter 9920/30000  loss         0.101262  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.052\n",
      "iter 9921/30000  loss         0.101261  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.052\n",
      "iter 9940/30000  loss         0.101236  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.056\n",
      "iter 9941/30000  loss         0.101234  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.056\n",
      "iter 9960/30000  loss         0.101210  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.060\n",
      "iter 9961/30000  loss         0.101208  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.060\n",
      "iter 9980/30000  loss         0.101184  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.065\n",
      "iter 9981/30000  loss         0.101182  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.065\n",
      "iter 10000/30000  loss         0.101158  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.069\n",
      "iter 10001/30000  loss         0.101156  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.069\n",
      "iter 10020/30000  loss         0.101132  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.073\n",
      "iter 10021/30000  loss         0.101130  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.073\n",
      "iter 10040/30000  loss         0.101106  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.078\n",
      "iter 10041/30000  loss         0.101105  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.078\n",
      "iter 10060/30000  loss         0.101080  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.082\n",
      "iter 10061/30000  loss         0.101079  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.082\n",
      "iter 10080/30000  loss         0.101055  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.086\n",
      "iter 10081/30000  loss         0.101053  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.086\n",
      "iter 10100/30000  loss         0.101029  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.090\n",
      "iter 10101/30000  loss         0.101028  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.091\n",
      "iter 10120/30000  loss         0.101004  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.095\n",
      "iter 10121/30000  loss         0.101002  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.095\n",
      "iter 10140/30000  loss         0.100978  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.099\n",
      "iter 10141/30000  loss         0.100977  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.099\n",
      "iter 10160/30000  loss         0.100953  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.103\n",
      "iter 10161/30000  loss         0.100952  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.104\n",
      "iter 10180/30000  loss         0.100928  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.108\n",
      "iter 10181/30000  loss         0.100927  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.108\n",
      "iter 10200/30000  loss         0.100903  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.112\n",
      "iter 10201/30000  loss         0.100901  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10220/30000  loss         0.100878  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.116\n",
      "iter 10221/30000  loss         0.100876  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.116\n",
      "iter 10240/30000  loss         0.100853  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.120\n",
      "iter 10241/30000  loss         0.100851  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.121\n",
      "iter 10260/30000  loss         0.100828  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.125\n",
      "iter 10261/30000  loss         0.100826  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.125\n",
      "iter 10280/30000  loss         0.100803  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.129\n",
      "iter 10281/30000  loss         0.100802  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.129\n",
      "iter 10300/30000  loss         0.100778  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.133\n",
      "iter 10301/30000  loss         0.100777  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.133\n",
      "iter 10320/30000  loss         0.100753  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.137\n",
      "iter 10321/30000  loss         0.100752  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.138\n",
      "iter 10340/30000  loss         0.100729  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.142\n",
      "iter 10341/30000  loss         0.100728  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.142\n",
      "iter 10360/30000  loss         0.100704  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.146\n",
      "iter 10361/30000  loss         0.100703  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.146\n",
      "iter 10380/30000  loss         0.100680  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.150\n",
      "iter 10381/30000  loss         0.100679  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.150\n",
      "iter 10400/30000  loss         0.100656  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.154\n",
      "iter 10401/30000  loss         0.100654  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.154\n",
      "iter 10420/30000  loss         0.100631  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.158\n",
      "iter 10421/30000  loss         0.100630  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.159\n",
      "iter 10440/30000  loss         0.100607  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.163\n",
      "iter 10441/30000  loss         0.100606  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.163\n",
      "iter 10460/30000  loss         0.100583  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.167\n",
      "iter 10461/30000  loss         0.100582  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.167\n",
      "iter 10480/30000  loss         0.100559  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.171\n",
      "iter 10481/30000  loss         0.100558  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.171\n",
      "iter 10500/30000  loss         0.100535  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.175\n",
      "iter 10501/30000  loss         0.100534  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.175\n",
      "iter 10520/30000  loss         0.100511  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.179\n",
      "iter 10521/30000  loss         0.100510  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.180\n",
      "iter 10540/30000  loss         0.100487  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.184\n",
      "iter 10541/30000  loss         0.100486  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.184\n",
      "iter 10560/30000  loss         0.100464  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.188\n",
      "iter 10561/30000  loss         0.100462  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.188\n",
      "iter 10580/30000  loss         0.100440  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.192\n",
      "iter 10581/30000  loss         0.100439  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.192\n",
      "iter 10600/30000  loss         0.100416  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.196\n",
      "iter 10601/30000  loss         0.100415  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.196\n",
      "iter 10620/30000  loss         0.100393  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.200\n",
      "iter 10621/30000  loss         0.100392  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.200\n",
      "iter 10640/30000  loss         0.100369  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.204\n",
      "iter 10641/30000  loss         0.100368  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.205\n",
      "iter 10660/30000  loss         0.100346  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.209\n",
      "iter 10661/30000  loss         0.100345  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.209\n",
      "iter 10680/30000  loss         0.100323  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.213\n",
      "iter 10681/30000  loss         0.100321  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.213\n",
      "iter 10700/30000  loss         0.100299  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.217\n",
      "iter 10701/30000  loss         0.100298  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.217\n",
      "iter 10720/30000  loss         0.100276  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.221\n",
      "iter 10721/30000  loss         0.100275  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.221\n",
      "iter 10740/30000  loss         0.100253  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.225\n",
      "iter 10741/30000  loss         0.100252  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.225\n",
      "iter 10760/30000  loss         0.100230  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.229\n",
      "iter 10761/30000  loss         0.100229  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.229\n",
      "iter 10780/30000  loss         0.100207  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.233\n",
      "iter 10781/30000  loss         0.100206  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.233\n",
      "iter 10800/30000  loss         0.100184  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.237\n",
      "iter 10801/30000  loss         0.100183  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.238\n",
      "iter 10820/30000  loss         0.100161  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.241\n",
      "iter 10821/30000  loss         0.100160  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.242\n",
      "iter 10840/30000  loss         0.100139  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.246\n",
      "iter 10841/30000  loss         0.100138  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.246\n",
      "iter 10860/30000  loss         0.100116  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.250\n",
      "iter 10861/30000  loss         0.100115  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.250\n",
      "iter 10880/30000  loss         0.100093  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.254\n",
      "iter 10881/30000  loss         0.100092  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.254\n",
      "iter 10900/30000  loss         0.100071  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.258\n",
      "iter 10901/30000  loss         0.100070  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.258\n",
      "iter 10920/30000  loss         0.100048  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.262\n",
      "iter 10921/30000  loss         0.100047  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.262\n",
      "iter 10940/30000  loss         0.100026  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.266\n",
      "iter 10941/30000  loss         0.100025  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.266\n",
      "iter 10960/30000  loss         0.100004  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.270\n",
      "iter 10961/30000  loss         0.100002  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.270\n",
      "iter 10980/30000  loss         0.099981  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.274\n",
      "iter 10981/30000  loss         0.099980  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.274\n",
      "iter 11000/30000  loss         0.099959  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.278\n",
      "iter 11001/30000  loss         0.099958  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11020/30000  loss         0.099937  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.282\n",
      "iter 11021/30000  loss         0.099936  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.282\n",
      "iter 11040/30000  loss         0.099915  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.286\n",
      "iter 11041/30000  loss         0.099914  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.286\n",
      "iter 11060/30000  loss         0.099893  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.290\n",
      "iter 11061/30000  loss         0.099892  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.290\n",
      "iter 11080/30000  loss         0.099871  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.294\n",
      "iter 11081/30000  loss         0.099870  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.294\n",
      "iter 11100/30000  loss         0.099849  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.298\n",
      "iter 11101/30000  loss         0.099848  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.298\n",
      "iter 11120/30000  loss         0.099827  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.302\n",
      "iter 11121/30000  loss         0.099826  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.302\n",
      "iter 11140/30000  loss         0.099806  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.306\n",
      "iter 11141/30000  loss         0.099804  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.306\n",
      "iter 11160/30000  loss         0.099784  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.310\n",
      "iter 11161/30000  loss         0.099783  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.310\n",
      "iter 11180/30000  loss         0.099762  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.314\n",
      "iter 11181/30000  loss         0.099761  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.314\n",
      "iter 11200/30000  loss         0.099741  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.318\n",
      "iter 11201/30000  loss         0.099740  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.318\n",
      "iter 11220/30000  loss         0.099719  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.322\n",
      "iter 11221/30000  loss         0.099718  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.322\n",
      "iter 11240/30000  loss         0.099698  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.326\n",
      "iter 11241/30000  loss         0.099697  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.326\n",
      "iter 11260/30000  loss         0.099676  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.330\n",
      "iter 11261/30000  loss         0.099675  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.330\n",
      "iter 11280/30000  loss         0.099655  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.334\n",
      "iter 11281/30000  loss         0.099654  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.334\n",
      "iter 11300/30000  loss         0.099634  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.338\n",
      "iter 11301/30000  loss         0.099633  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.338\n",
      "iter 11320/30000  loss         0.099613  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.342\n",
      "iter 11321/30000  loss         0.099612  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.342\n",
      "iter 11340/30000  loss         0.099592  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.346\n",
      "iter 11341/30000  loss         0.099591  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.346\n",
      "iter 11360/30000  loss         0.099571  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.350\n",
      "iter 11361/30000  loss         0.099570  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.350\n",
      "iter 11380/30000  loss         0.099550  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.354\n",
      "iter 11381/30000  loss         0.099549  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.354\n",
      "iter 11400/30000  loss         0.099529  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.358\n",
      "iter 11401/30000  loss         0.099528  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.358\n",
      "iter 11420/30000  loss         0.099508  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.362\n",
      "iter 11421/30000  loss         0.099507  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.362\n",
      "iter 11440/30000  loss         0.099487  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.366\n",
      "iter 11441/30000  loss         0.099486  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.366\n",
      "iter 11460/30000  loss         0.099466  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.370\n",
      "iter 11461/30000  loss         0.099465  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.370\n",
      "iter 11480/30000  loss         0.099446  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.374\n",
      "iter 11481/30000  loss         0.099445  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.374\n",
      "iter 11500/30000  loss         0.099425  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.377\n",
      "iter 11501/30000  loss         0.099424  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.378\n",
      "iter 11520/30000  loss         0.099405  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.381\n",
      "iter 11521/30000  loss         0.099403  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.382\n",
      "iter 11540/30000  loss         0.099384  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.385\n",
      "iter 11541/30000  loss         0.099383  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.385\n",
      "iter 11560/30000  loss         0.099364  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.389\n",
      "iter 11561/30000  loss         0.099363  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.389\n",
      "iter 11580/30000  loss         0.099343  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.393\n",
      "iter 11581/30000  loss         0.099342  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.393\n",
      "iter 11600/30000  loss         0.099323  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.397\n",
      "iter 11601/30000  loss         0.099322  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.397\n",
      "iter 11620/30000  loss         0.099303  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.401\n",
      "iter 11621/30000  loss         0.099302  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.401\n",
      "iter 11640/30000  loss         0.099282  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.405\n",
      "iter 11641/30000  loss         0.099281  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.405\n",
      "iter 11660/30000  loss         0.099262  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.409\n",
      "iter 11661/30000  loss         0.099261  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.409\n",
      "iter 11680/30000  loss         0.099242  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.412\n",
      "iter 11681/30000  loss         0.099241  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.413\n",
      "iter 11700/30000  loss         0.099222  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.416\n",
      "iter 11701/30000  loss         0.099221  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.417\n",
      "iter 11720/30000  loss         0.099202  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.420\n",
      "iter 11721/30000  loss         0.099201  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.420\n",
      "iter 11740/30000  loss         0.099182  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.424\n",
      "iter 11741/30000  loss         0.099181  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.424\n",
      "iter 11760/30000  loss         0.099162  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.428\n",
      "iter 11761/30000  loss         0.099161  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.428\n",
      "iter 11780/30000  loss         0.099143  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.432\n",
      "iter 11781/30000  loss         0.099142  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.432\n",
      "iter 11800/30000  loss         0.099123  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11801/30000  loss         0.099122  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.436\n",
      "iter 11820/30000  loss         0.099103  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.439\n",
      "iter 11821/30000  loss         0.099102  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.440\n",
      "iter 11840/30000  loss         0.099084  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.443\n",
      "iter 11841/30000  loss         0.099083  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.443\n",
      "iter 11860/30000  loss         0.099064  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.447\n",
      "iter 11861/30000  loss         0.099063  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.447\n",
      "iter 11880/30000  loss         0.099045  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.451\n",
      "iter 11881/30000  loss         0.099044  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.451\n",
      "iter 11900/30000  loss         0.099025  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.455\n",
      "iter 11901/30000  loss         0.099024  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.455\n",
      "iter 11920/30000  loss         0.099006  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.459\n",
      "iter 11921/30000  loss         0.099005  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.459\n",
      "iter 11940/30000  loss         0.098986  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.462\n",
      "iter 11941/30000  loss         0.098985  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.463\n",
      "iter 11960/30000  loss         0.098967  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.466\n",
      "iter 11961/30000  loss         0.098966  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.466\n",
      "iter 11980/30000  loss         0.098948  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.470\n",
      "iter 11981/30000  loss         0.098947  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.470\n",
      "iter 12000/30000  loss         0.098929  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.474\n",
      "iter 12001/30000  loss         0.098928  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.474\n",
      "iter 12020/30000  loss         0.098910  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.477\n",
      "iter 12021/30000  loss         0.098909  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.478\n",
      "iter 12040/30000  loss         0.098890  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.481\n",
      "iter 12041/30000  loss         0.098890  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.481\n",
      "iter 12060/30000  loss         0.098871  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.485\n",
      "iter 12061/30000  loss         0.098871  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.485\n",
      "iter 12080/30000  loss         0.098853  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.489\n",
      "iter 12081/30000  loss         0.098852  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.489\n",
      "iter 12100/30000  loss         0.098834  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.493\n",
      "iter 12101/30000  loss         0.098833  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.493\n",
      "iter 12120/30000  loss         0.098815  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.496\n",
      "iter 12121/30000  loss         0.098814  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.497\n",
      "iter 12140/30000  loss         0.098796  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.500\n",
      "iter 12141/30000  loss         0.098795  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.500\n",
      "iter 12160/30000  loss         0.098777  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.504\n",
      "iter 12161/30000  loss         0.098776  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.504\n",
      "iter 12180/30000  loss         0.098758  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.508\n",
      "iter 12181/30000  loss         0.098758  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.508\n",
      "iter 12200/30000  loss         0.098740  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.511\n",
      "iter 12201/30000  loss         0.098739  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.512\n",
      "iter 12220/30000  loss         0.098721  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.515\n",
      "iter 12221/30000  loss         0.098720  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.515\n",
      "iter 12240/30000  loss         0.098703  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.519\n",
      "iter 12241/30000  loss         0.098702  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.519\n",
      "iter 12260/30000  loss         0.098684  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.523\n",
      "iter 12261/30000  loss         0.098683  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.523\n",
      "iter 12280/30000  loss         0.098666  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.526\n",
      "iter 12281/30000  loss         0.098665  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.526\n",
      "iter 12300/30000  loss         0.098647  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.530\n",
      "iter 12301/30000  loss         0.098646  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.530\n",
      "iter 12320/30000  loss         0.098629  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.534\n",
      "iter 12321/30000  loss         0.098628  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.534\n",
      "iter 12340/30000  loss         0.098611  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.537\n",
      "iter 12341/30000  loss         0.098610  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.538\n",
      "iter 12360/30000  loss         0.098593  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.541\n",
      "iter 12361/30000  loss         0.098592  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.541\n",
      "iter 12380/30000  loss         0.098574  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.545\n",
      "iter 12381/30000  loss         0.098573  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.545\n",
      "iter 12400/30000  loss         0.098556  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.549\n",
      "iter 12401/30000  loss         0.098555  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.549\n",
      "iter 12420/30000  loss         0.098538  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.552\n",
      "iter 12421/30000  loss         0.098537  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.552\n",
      "iter 12440/30000  loss         0.098520  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.556\n",
      "iter 12441/30000  loss         0.098519  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.556\n",
      "iter 12460/30000  loss         0.098502  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.560\n",
      "iter 12461/30000  loss         0.098501  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.560\n",
      "iter 12480/30000  loss         0.098484  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.563\n",
      "iter 12481/30000  loss         0.098483  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.563\n",
      "iter 12500/30000  loss         0.098466  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.567\n",
      "iter 12501/30000  loss         0.098465  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.567\n",
      "iter 12520/30000  loss         0.098448  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.571\n",
      "iter 12521/30000  loss         0.098448  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.571\n",
      "iter 12540/30000  loss         0.098431  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.574\n",
      "iter 12541/30000  loss         0.098430  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.574\n",
      "iter 12560/30000  loss         0.098413  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.578\n",
      "iter 12561/30000  loss         0.098412  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.578\n",
      "iter 12580/30000  loss         0.098395  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.582\n",
      "iter 12581/30000  loss         0.098394  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12600/30000  loss         0.098378  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.585\n",
      "iter 12601/30000  loss         0.098377  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.585\n",
      "iter 12620/30000  loss         0.098360  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.589\n",
      "iter 12621/30000  loss         0.098359  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.589\n",
      "iter 12640/30000  loss         0.098343  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.593\n",
      "iter 12641/30000  loss         0.098342  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.593\n",
      "iter 12660/30000  loss         0.098325  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.596\n",
      "iter 12661/30000  loss         0.098324  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.596\n",
      "iter 12680/30000  loss         0.098308  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.600\n",
      "iter 12681/30000  loss         0.098307  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.600\n",
      "iter 12700/30000  loss         0.098290  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.603\n",
      "iter 12701/30000  loss         0.098289  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.604\n",
      "iter 12720/30000  loss         0.098273  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.607\n",
      "iter 12721/30000  loss         0.098272  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.607\n",
      "iter 12740/30000  loss         0.098256  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.611\n",
      "iter 12741/30000  loss         0.098255  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.611\n",
      "iter 12760/30000  loss         0.098238  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.614\n",
      "iter 12761/30000  loss         0.098237  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.614\n",
      "iter 12780/30000  loss         0.098221  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.618\n",
      "iter 12781/30000  loss         0.098220  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.618\n",
      "iter 12800/30000  loss         0.098204  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.622\n",
      "iter 12801/30000  loss         0.098203  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.622\n",
      "iter 12820/30000  loss         0.098187  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.625\n",
      "iter 12821/30000  loss         0.098186  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.625\n",
      "iter 12840/30000  loss         0.098170  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.629\n",
      "iter 12841/30000  loss         0.098169  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.629\n",
      "iter 12860/30000  loss         0.098153  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.632\n",
      "iter 12861/30000  loss         0.098152  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.632\n",
      "iter 12880/30000  loss         0.098136  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.636\n",
      "iter 12881/30000  loss         0.098135  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.636\n",
      "iter 12900/30000  loss         0.098119  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.639\n",
      "iter 12901/30000  loss         0.098118  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.640\n",
      "iter 12920/30000  loss         0.098102  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.643\n",
      "iter 12921/30000  loss         0.098101  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.643\n",
      "iter 12940/30000  loss         0.098085  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.647\n",
      "iter 12941/30000  loss         0.098084  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.647\n",
      "iter 12960/30000  loss         0.098068  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.650\n",
      "iter 12961/30000  loss         0.098067  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.650\n",
      "iter 12980/30000  loss         0.098052  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.654\n",
      "iter 12981/30000  loss         0.098051  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.654\n",
      "iter 13000/30000  loss         0.098035  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.657\n",
      "iter 13001/30000  loss         0.098034  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.657\n",
      "iter 13020/30000  loss         0.098018  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.661\n",
      "iter 13021/30000  loss         0.098017  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.661\n",
      "iter 13040/30000  loss         0.098002  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.664\n",
      "iter 13041/30000  loss         0.098001  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.665\n",
      "iter 13060/30000  loss         0.097985  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.668\n",
      "iter 13061/30000  loss         0.097984  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.668\n",
      "iter 13080/30000  loss         0.097968  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.672\n",
      "iter 13081/30000  loss         0.097968  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.672\n",
      "iter 13100/30000  loss         0.097952  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.675\n",
      "iter 13101/30000  loss         0.097951  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.675\n",
      "iter 13120/30000  loss         0.097936  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.679\n",
      "iter 13121/30000  loss         0.097935  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.679\n",
      "iter 13140/30000  loss         0.097919  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.682\n",
      "iter 13141/30000  loss         0.097918  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.682\n",
      "iter 13160/30000  loss         0.097903  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.686\n",
      "iter 13161/30000  loss         0.097902  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.686\n",
      "iter 13180/30000  loss         0.097886  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.689\n",
      "iter 13181/30000  loss         0.097886  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.689\n",
      "iter 13200/30000  loss         0.097870  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.693\n",
      "iter 13201/30000  loss         0.097869  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.693\n",
      "iter 13220/30000  loss         0.097854  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.696\n",
      "iter 13221/30000  loss         0.097853  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.696\n",
      "iter 13240/30000  loss         0.097838  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.700\n",
      "iter 13241/30000  loss         0.097837  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.700\n",
      "iter 13260/30000  loss         0.097822  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.703\n",
      "iter 13261/30000  loss         0.097821  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.703\n",
      "iter 13280/30000  loss         0.097806  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.707\n",
      "iter 13281/30000  loss         0.097805  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.707\n",
      "iter 13300/30000  loss         0.097790  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.710\n",
      "iter 13301/30000  loss         0.097789  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.710\n",
      "iter 13320/30000  loss         0.097774  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.714\n",
      "iter 13321/30000  loss         0.097773  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.714\n",
      "iter 13340/30000  loss         0.097758  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.717\n",
      "iter 13341/30000  loss         0.097757  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.717\n",
      "iter 13360/30000  loss         0.097742  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.721\n",
      "iter 13361/30000  loss         0.097741  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.721\n",
      "iter 13380/30000  loss         0.097726  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.724\n",
      "iter 13381/30000  loss         0.097725  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13400/30000  loss         0.097710  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.728\n",
      "iter 13401/30000  loss         0.097709  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.728\n",
      "iter 13420/30000  loss         0.097694  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.731\n",
      "iter 13421/30000  loss         0.097693  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.731\n",
      "iter 13440/30000  loss         0.097678  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.735\n",
      "iter 13441/30000  loss         0.097678  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.735\n",
      "iter 13460/30000  loss         0.097663  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.738\n",
      "iter 13461/30000  loss         0.097662  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.738\n",
      "iter 13480/30000  loss         0.097647  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.742\n",
      "iter 13481/30000  loss         0.097646  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.742\n",
      "iter 13500/30000  loss         0.097631  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.745\n",
      "iter 13501/30000  loss         0.097631  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.745\n",
      "iter 13520/30000  loss         0.097616  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.748\n",
      "iter 13521/30000  loss         0.097615  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.749\n",
      "iter 13540/30000  loss         0.097600  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.752\n",
      "iter 13541/30000  loss         0.097599  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.752\n",
      "iter 13560/30000  loss         0.097585  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.755\n",
      "iter 13561/30000  loss         0.097584  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.755\n",
      "iter 13580/30000  loss         0.097569  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.759\n",
      "iter 13581/30000  loss         0.097569  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.759\n",
      "iter 13600/30000  loss         0.097554  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.762\n",
      "iter 13601/30000  loss         0.097553  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.762\n",
      "iter 13620/30000  loss         0.097538  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.766\n",
      "iter 13621/30000  loss         0.097538  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.766\n",
      "iter 13640/30000  loss         0.097523  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.769\n",
      "iter 13641/30000  loss         0.097522  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.769\n",
      "iter 13660/30000  loss         0.097508  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.772\n",
      "iter 13661/30000  loss         0.097507  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.773\n",
      "iter 13680/30000  loss         0.097493  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.776\n",
      "iter 13681/30000  loss         0.097492  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.776\n",
      "iter 13700/30000  loss         0.097477  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.779\n",
      "iter 13701/30000  loss         0.097477  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.779\n",
      "iter 13720/30000  loss         0.097462  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.783\n",
      "iter 13721/30000  loss         0.097461  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.783\n",
      "iter 13740/30000  loss         0.097447  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.786\n",
      "iter 13741/30000  loss         0.097446  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.786\n",
      "iter 13760/30000  loss         0.097432  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.790\n",
      "iter 13761/30000  loss         0.097431  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.790\n",
      "iter 13780/30000  loss         0.097417  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.793\n",
      "iter 13781/30000  loss         0.097416  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.793\n",
      "iter 13800/30000  loss         0.097402  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.796\n",
      "iter 13801/30000  loss         0.097401  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.796\n",
      "iter 13820/30000  loss         0.097387  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.800\n",
      "iter 13821/30000  loss         0.097386  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.800\n",
      "iter 13840/30000  loss         0.097372  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.803\n",
      "iter 13841/30000  loss         0.097371  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.803\n",
      "iter 13860/30000  loss         0.097357  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.806\n",
      "iter 13861/30000  loss         0.097356  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.807\n",
      "iter 13880/30000  loss         0.097342  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.810\n",
      "iter 13881/30000  loss         0.097341  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.810\n",
      "iter 13900/30000  loss         0.097327  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.813\n",
      "iter 13901/30000  loss         0.097326  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.813\n",
      "iter 13920/30000  loss         0.097312  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.817\n",
      "iter 13921/30000  loss         0.097312  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.817\n",
      "iter 13940/30000  loss         0.097298  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.820\n",
      "iter 13941/30000  loss         0.097297  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.820\n",
      "iter 13960/30000  loss         0.097283  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.823\n",
      "iter 13961/30000  loss         0.097282  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.823\n",
      "iter 13980/30000  loss         0.097268  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.827\n",
      "iter 13981/30000  loss         0.097267  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.827\n",
      "iter 14000/30000  loss         0.097254  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.830\n",
      "iter 14001/30000  loss         0.097253  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.830\n",
      "iter 14020/30000  loss         0.097239  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.833\n",
      "iter 14021/30000  loss         0.097238  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.834\n",
      "iter 14040/30000  loss         0.097224  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.837\n",
      "iter 14041/30000  loss         0.097224  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.837\n",
      "iter 14060/30000  loss         0.097210  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.840\n",
      "iter 14061/30000  loss         0.097209  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.840\n",
      "iter 14080/30000  loss         0.097195  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.843\n",
      "iter 14081/30000  loss         0.097195  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.844\n",
      "iter 14100/30000  loss         0.097181  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.847\n",
      "iter 14101/30000  loss         0.097180  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.847\n",
      "iter 14120/30000  loss         0.097166  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.850\n",
      "iter 14121/30000  loss         0.097166  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.850\n",
      "iter 14140/30000  loss         0.097152  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.853\n",
      "iter 14141/30000  loss         0.097151  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.854\n",
      "iter 14160/30000  loss         0.097138  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.857\n",
      "iter 14161/30000  loss         0.097137  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.857\n",
      "iter 14180/30000  loss         0.097123  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.860\n",
      "iter 14181/30000  loss         0.097123  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14200/30000  loss         0.097109  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.863\n",
      "iter 14201/30000  loss         0.097108  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.864\n",
      "iter 14220/30000  loss         0.097095  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.867\n",
      "iter 14221/30000  loss         0.097094  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.867\n",
      "iter 14240/30000  loss         0.097081  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.870\n",
      "iter 14241/30000  loss         0.097080  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.870\n",
      "iter 14260/30000  loss         0.097066  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.873\n",
      "iter 14261/30000  loss         0.097066  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.873\n",
      "iter 14280/30000  loss         0.097052  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.877\n",
      "iter 14281/30000  loss         0.097052  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.877\n",
      "iter 14300/30000  loss         0.097038  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.880\n",
      "iter 14301/30000  loss         0.097037  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.880\n",
      "iter 14320/30000  loss         0.097024  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.883\n",
      "iter 14321/30000  loss         0.097023  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.883\n",
      "iter 14340/30000  loss         0.097010  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.886\n",
      "iter 14341/30000  loss         0.097009  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.887\n",
      "iter 14360/30000  loss         0.096996  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.890\n",
      "iter 14361/30000  loss         0.096995  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.890\n",
      "iter 14380/30000  loss         0.096982  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.893\n",
      "iter 14381/30000  loss         0.096981  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.893\n",
      "iter 14400/30000  loss         0.096968  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.896\n",
      "iter 14401/30000  loss         0.096968  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.896\n",
      "iter 14420/30000  loss         0.096954  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.900\n",
      "iter 14421/30000  loss         0.096954  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.900\n",
      "iter 14440/30000  loss         0.096940  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.903\n",
      "iter 14441/30000  loss         0.096940  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.903\n",
      "iter 14460/30000  loss         0.096927  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.906\n",
      "iter 14461/30000  loss         0.096926  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.906\n",
      "iter 14480/30000  loss         0.096913  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.909\n",
      "iter 14481/30000  loss         0.096912  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.909\n",
      "iter 14500/30000  loss         0.096899  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.913\n",
      "iter 14501/30000  loss         0.096898  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.913\n",
      "iter 14520/30000  loss         0.096885  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.916\n",
      "iter 14521/30000  loss         0.096885  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.916\n",
      "iter 14540/30000  loss         0.096872  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.919\n",
      "iter 14541/30000  loss         0.096871  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.919\n",
      "iter 14560/30000  loss         0.096858  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.922\n",
      "iter 14561/30000  loss         0.096857  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.922\n",
      "iter 14580/30000  loss         0.096844  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.926\n",
      "iter 14581/30000  loss         0.096844  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.926\n",
      "iter 14600/30000  loss         0.096831  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.929\n",
      "iter 14601/30000  loss         0.096830  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.929\n",
      "iter 14620/30000  loss         0.096817  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.932\n",
      "iter 14621/30000  loss         0.096817  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.932\n",
      "iter 14640/30000  loss         0.096804  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.935\n",
      "iter 14641/30000  loss         0.096803  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.935\n",
      "iter 14660/30000  loss         0.096790  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.939\n",
      "iter 14661/30000  loss         0.096790  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.939\n",
      "iter 14680/30000  loss         0.096777  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.942\n",
      "iter 14681/30000  loss         0.096776  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.942\n",
      "iter 14700/30000  loss         0.096763  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.945\n",
      "iter 14701/30000  loss         0.096763  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.945\n",
      "iter 14720/30000  loss         0.096750  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.948\n",
      "iter 14721/30000  loss         0.096749  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.948\n",
      "iter 14740/30000  loss         0.096737  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.951\n",
      "iter 14741/30000  loss         0.096736  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.952\n",
      "iter 14760/30000  loss         0.096723  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.955\n",
      "iter 14761/30000  loss         0.096723  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    4.955\n",
      "iter 14780/30000  loss         0.096710  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.958\n",
      "iter 14781/30000  loss         0.096709  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.958\n",
      "iter 14800/30000  loss         0.096697  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.961\n",
      "iter 14801/30000  loss         0.096696  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.961\n",
      "iter 14820/30000  loss         0.096684  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.964\n",
      "iter 14821/30000  loss         0.096683  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.964\n",
      "iter 14840/30000  loss         0.096670  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.967\n",
      "iter 14841/30000  loss         0.096670  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.968\n",
      "iter 14860/30000  loss         0.096657  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.971\n",
      "iter 14861/30000  loss         0.096657  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.971\n",
      "iter 14880/30000  loss         0.096644  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.974\n",
      "iter 14881/30000  loss         0.096644  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.974\n",
      "iter 14900/30000  loss         0.096631  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.977\n",
      "iter 14901/30000  loss         0.096630  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.977\n",
      "iter 14920/30000  loss         0.096618  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.980\n",
      "iter 14921/30000  loss         0.096617  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.980\n",
      "iter 14940/30000  loss         0.096605  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.983\n",
      "iter 14941/30000  loss         0.096604  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.983\n",
      "iter 14960/30000  loss         0.096592  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.986\n",
      "iter 14961/30000  loss         0.096591  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.987\n",
      "iter 14980/30000  loss         0.096579  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.990\n",
      "iter 14981/30000  loss         0.096578  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15000/30000  loss         0.096566  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.993\n",
      "iter 15001/30000  loss         0.096565  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.993\n",
      "iter 15020/30000  loss         0.096553  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.996\n",
      "iter 15021/30000  loss         0.096553  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.996\n",
      "iter 15040/30000  loss         0.096540  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.999\n",
      "iter 15041/30000  loss         0.096540  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    4.999\n",
      "iter 15060/30000  loss         0.096528  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.002\n",
      "iter 15061/30000  loss         0.096527  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.002\n",
      "iter 15080/30000  loss         0.096515  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.005\n",
      "iter 15081/30000  loss         0.096514  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.006\n",
      "iter 15100/30000  loss         0.096502  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.009\n",
      "iter 15101/30000  loss         0.096501  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.009\n",
      "iter 15120/30000  loss         0.096489  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.012\n",
      "iter 15121/30000  loss         0.096489  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.012\n",
      "iter 15140/30000  loss         0.096476  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.015\n",
      "iter 15141/30000  loss         0.096476  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.015\n",
      "iter 15160/30000  loss         0.096464  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.018\n",
      "iter 15161/30000  loss         0.096463  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.018\n",
      "iter 15180/30000  loss         0.096451  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.021\n",
      "iter 15181/30000  loss         0.096451  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.021\n",
      "iter 15200/30000  loss         0.096439  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.024\n",
      "iter 15201/30000  loss         0.096438  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.024\n",
      "iter 15220/30000  loss         0.096426  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.027\n",
      "iter 15221/30000  loss         0.096425  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.028\n",
      "iter 15240/30000  loss         0.096413  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.030\n",
      "iter 15241/30000  loss         0.096413  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.031\n",
      "iter 15260/30000  loss         0.096401  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.034\n",
      "iter 15261/30000  loss         0.096400  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.034\n",
      "iter 15280/30000  loss         0.096388  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.037\n",
      "iter 15281/30000  loss         0.096388  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.037\n",
      "iter 15300/30000  loss         0.096376  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.040\n",
      "iter 15301/30000  loss         0.096375  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.040\n",
      "iter 15320/30000  loss         0.096363  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.043\n",
      "iter 15321/30000  loss         0.096363  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.043\n",
      "iter 15340/30000  loss         0.096351  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.046\n",
      "iter 15341/30000  loss         0.096350  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.046\n",
      "iter 15360/30000  loss         0.096339  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.049\n",
      "iter 15361/30000  loss         0.096338  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.049\n",
      "iter 15380/30000  loss         0.096326  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.052\n",
      "iter 15381/30000  loss         0.096326  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.052\n",
      "iter 15400/30000  loss         0.096314  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.055\n",
      "iter 15401/30000  loss         0.096313  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.055\n",
      "iter 15420/30000  loss         0.096302  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.058\n",
      "iter 15421/30000  loss         0.096301  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.059\n",
      "iter 15440/30000  loss         0.096289  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.061\n",
      "iter 15441/30000  loss         0.096289  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.062\n",
      "iter 15460/30000  loss         0.096277  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.065\n",
      "iter 15461/30000  loss         0.096277  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.065\n",
      "iter 15480/30000  loss         0.096265  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.068\n",
      "iter 15481/30000  loss         0.096264  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.068\n",
      "iter 15500/30000  loss         0.096253  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.071\n",
      "iter 15501/30000  loss         0.096252  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.071\n",
      "iter 15520/30000  loss         0.096241  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.074\n",
      "iter 15521/30000  loss         0.096240  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.074\n",
      "iter 15540/30000  loss         0.096229  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.077\n",
      "iter 15541/30000  loss         0.096228  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.077\n",
      "iter 15560/30000  loss         0.096217  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.080\n",
      "iter 15561/30000  loss         0.096216  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.080\n",
      "iter 15580/30000  loss         0.096204  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.083\n",
      "iter 15581/30000  loss         0.096204  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.083\n",
      "iter 15600/30000  loss         0.096192  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.086\n",
      "iter 15601/30000  loss         0.096192  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.086\n",
      "iter 15620/30000  loss         0.096180  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.089\n",
      "iter 15621/30000  loss         0.096180  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.089\n",
      "iter 15640/30000  loss         0.096168  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.092\n",
      "iter 15641/30000  loss         0.096168  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.092\n",
      "iter 15660/30000  loss         0.096157  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.095\n",
      "iter 15661/30000  loss         0.096156  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.095\n",
      "iter 15680/30000  loss         0.096145  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.098\n",
      "iter 15681/30000  loss         0.096144  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.098\n",
      "iter 15700/30000  loss         0.096133  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.101\n",
      "iter 15701/30000  loss         0.096132  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.101\n",
      "iter 15720/30000  loss         0.096121  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.104\n",
      "iter 15721/30000  loss         0.096120  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.104\n",
      "iter 15740/30000  loss         0.096109  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.107\n",
      "iter 15741/30000  loss         0.096108  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.108\n",
      "iter 15760/30000  loss         0.096097  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.110\n",
      "iter 15761/30000  loss         0.096097  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.111\n",
      "iter 15780/30000  loss         0.096085  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.113\n",
      "iter 15781/30000  loss         0.096085  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15800/30000  loss         0.096074  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.116\n",
      "iter 15801/30000  loss         0.096073  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.117\n",
      "iter 15820/30000  loss         0.096062  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.119\n",
      "iter 15821/30000  loss         0.096061  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.120\n",
      "iter 15840/30000  loss         0.096050  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.122\n",
      "iter 15841/30000  loss         0.096050  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.123\n",
      "iter 15860/30000  loss         0.096039  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.125\n",
      "iter 15861/30000  loss         0.096038  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.126\n",
      "iter 15880/30000  loss         0.096027  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.128\n",
      "iter 15881/30000  loss         0.096026  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.129\n",
      "iter 15900/30000  loss         0.096015  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.132\n",
      "iter 15901/30000  loss         0.096015  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.132\n",
      "iter 15920/30000  loss         0.096004  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.135\n",
      "iter 15921/30000  loss         0.096003  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.135\n",
      "iter 15940/30000  loss         0.095992  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.138\n",
      "iter 15941/30000  loss         0.095992  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.138\n",
      "iter 15960/30000  loss         0.095981  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.141\n",
      "iter 15961/30000  loss         0.095980  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.141\n",
      "iter 15980/30000  loss         0.095969  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.143\n",
      "iter 15981/30000  loss         0.095969  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.144\n",
      "iter 16000/30000  loss         0.095958  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.146\n",
      "iter 16001/30000  loss         0.095957  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.147\n",
      "iter 16020/30000  loss         0.095946  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.149\n",
      "iter 16021/30000  loss         0.095946  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.150\n",
      "iter 16040/30000  loss         0.095935  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.152\n",
      "iter 16041/30000  loss         0.095934  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.153\n",
      "iter 16060/30000  loss         0.095924  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.155\n",
      "iter 16061/30000  loss         0.095923  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.156\n",
      "iter 16080/30000  loss         0.095912  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.158\n",
      "iter 16081/30000  loss         0.095912  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.159\n",
      "iter 16100/30000  loss         0.095901  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.161\n",
      "iter 16101/30000  loss         0.095900  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.162\n",
      "iter 16120/30000  loss         0.095890  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.164\n",
      "iter 16121/30000  loss         0.095889  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.165\n",
      "iter 16140/30000  loss         0.095878  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.167\n",
      "iter 16141/30000  loss         0.095878  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.167\n",
      "iter 16160/30000  loss         0.095867  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.170\n",
      "iter 16161/30000  loss         0.095866  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.170\n",
      "iter 16180/30000  loss         0.095856  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.173\n",
      "iter 16181/30000  loss         0.095855  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.173\n",
      "iter 16200/30000  loss         0.095845  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.176\n",
      "iter 16201/30000  loss         0.095844  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.176\n",
      "iter 16220/30000  loss         0.095833  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.179\n",
      "iter 16221/30000  loss         0.095833  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.179\n",
      "iter 16240/30000  loss         0.095822  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.182\n",
      "iter 16241/30000  loss         0.095822  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.182\n",
      "iter 16260/30000  loss         0.095811  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.185\n",
      "iter 16261/30000  loss         0.095810  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.185\n",
      "iter 16280/30000  loss         0.095800  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.188\n",
      "iter 16281/30000  loss         0.095799  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.188\n",
      "iter 16300/30000  loss         0.095789  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.191\n",
      "iter 16301/30000  loss         0.095788  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.191\n",
      "iter 16320/30000  loss         0.095778  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.194\n",
      "iter 16321/30000  loss         0.095777  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.194\n",
      "iter 16340/30000  loss         0.095767  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.197\n",
      "iter 16341/30000  loss         0.095766  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.197\n",
      "iter 16360/30000  loss         0.095756  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.200\n",
      "iter 16361/30000  loss         0.095755  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.200\n",
      "iter 16380/30000  loss         0.095745  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.203\n",
      "iter 16381/30000  loss         0.095744  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.203\n",
      "iter 16400/30000  loss         0.095734  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.206\n",
      "iter 16401/30000  loss         0.095733  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.206\n",
      "iter 16420/30000  loss         0.095723  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.209\n",
      "iter 16421/30000  loss         0.095722  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.209\n",
      "iter 16440/30000  loss         0.095712  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.211\n",
      "iter 16441/30000  loss         0.095711  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.212\n",
      "iter 16460/30000  loss         0.095701  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.214\n",
      "iter 16461/30000  loss         0.095701  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.215\n",
      "iter 16480/30000  loss         0.095690  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.217\n",
      "iter 16481/30000  loss         0.095690  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.217\n",
      "iter 16500/30000  loss         0.095679  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.220\n",
      "iter 16501/30000  loss         0.095679  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.220\n",
      "iter 16520/30000  loss         0.095669  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.223\n",
      "iter 16521/30000  loss         0.095668  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.223\n",
      "iter 16540/30000  loss         0.095658  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.226\n",
      "iter 16541/30000  loss         0.095657  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.226\n",
      "iter 16560/30000  loss         0.095647  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.229\n",
      "iter 16561/30000  loss         0.095647  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.229\n",
      "iter 16580/30000  loss         0.095636  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.232\n",
      "iter 16581/30000  loss         0.095636  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16600/30000  loss         0.095626  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.235\n",
      "iter 16601/30000  loss         0.095625  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.235\n",
      "iter 16620/30000  loss         0.095615  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.238\n",
      "iter 16621/30000  loss         0.095615  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.238\n",
      "iter 16640/30000  loss         0.095604  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.240\n",
      "iter 16641/30000  loss         0.095604  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.241\n",
      "iter 16660/30000  loss         0.095594  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.243\n",
      "iter 16661/30000  loss         0.095593  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.243\n",
      "iter 16680/30000  loss         0.095583  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.246\n",
      "iter 16681/30000  loss         0.095583  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.246\n",
      "iter 16700/30000  loss         0.095573  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.249\n",
      "iter 16701/30000  loss         0.095572  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.249\n",
      "iter 16720/30000  loss         0.095562  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.252\n",
      "iter 16721/30000  loss         0.095561  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.252\n",
      "iter 16740/30000  loss         0.095551  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.255\n",
      "iter 16741/30000  loss         0.095551  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.255\n",
      "iter 16760/30000  loss         0.095541  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.258\n",
      "iter 16761/30000  loss         0.095540  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.258\n",
      "iter 16780/30000  loss         0.095531  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.261\n",
      "iter 16781/30000  loss         0.095530  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.261\n",
      "iter 16800/30000  loss         0.095520  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.263\n",
      "iter 16801/30000  loss         0.095520  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.264\n",
      "iter 16820/30000  loss         0.095510  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.266\n",
      "iter 16821/30000  loss         0.095509  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.266\n",
      "iter 16840/30000  loss         0.095499  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.269\n",
      "iter 16841/30000  loss         0.095499  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.269\n",
      "iter 16860/30000  loss         0.095489  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.272\n",
      "iter 16861/30000  loss         0.095488  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.272\n",
      "iter 16880/30000  loss         0.095478  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.275\n",
      "iter 16881/30000  loss         0.095478  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.275\n",
      "iter 16900/30000  loss         0.095468  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.278\n",
      "iter 16901/30000  loss         0.095468  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.278\n",
      "iter 16920/30000  loss         0.095458  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.281\n",
      "iter 16921/30000  loss         0.095457  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.281\n",
      "iter 16940/30000  loss         0.095447  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.283\n",
      "iter 16941/30000  loss         0.095447  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.284\n",
      "iter 16960/30000  loss         0.095437  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.286\n",
      "iter 16961/30000  loss         0.095437  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.286\n",
      "iter 16980/30000  loss         0.095427  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.289\n",
      "iter 16981/30000  loss         0.095426  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.289\n",
      "iter 17000/30000  loss         0.095417  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.292\n",
      "iter 17001/30000  loss         0.095416  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.292\n",
      "iter 17020/30000  loss         0.095407  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.295\n",
      "iter 17021/30000  loss         0.095406  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.295\n",
      "iter 17040/30000  loss         0.095396  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.298\n",
      "iter 17041/30000  loss         0.095396  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.298\n",
      "iter 17060/30000  loss         0.095386  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.300\n",
      "iter 17061/30000  loss         0.095386  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.300\n",
      "iter 17080/30000  loss         0.095376  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.303\n",
      "iter 17081/30000  loss         0.095376  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.303\n",
      "iter 17100/30000  loss         0.095366  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.306\n",
      "iter 17101/30000  loss         0.095365  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.306\n",
      "iter 17120/30000  loss         0.095356  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.309\n",
      "iter 17121/30000  loss         0.095355  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.309\n",
      "iter 17140/30000  loss         0.095346  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.312\n",
      "iter 17141/30000  loss         0.095345  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.312\n",
      "iter 17160/30000  loss         0.095336  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.314\n",
      "iter 17161/30000  loss         0.095335  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.315\n",
      "iter 17180/30000  loss         0.095326  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.317\n",
      "iter 17181/30000  loss         0.095325  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.317\n",
      "iter 17200/30000  loss         0.095316  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.320\n",
      "iter 17201/30000  loss         0.095315  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.320\n",
      "iter 17220/30000  loss         0.095306  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.323\n",
      "iter 17221/30000  loss         0.095305  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.323\n",
      "iter 17240/30000  loss         0.095296  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.326\n",
      "iter 17241/30000  loss         0.095295  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.326\n",
      "iter 17260/30000  loss         0.095286  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.328\n",
      "iter 17261/30000  loss         0.095285  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.329\n",
      "iter 17280/30000  loss         0.095276  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.331\n",
      "iter 17281/30000  loss         0.095275  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.331\n",
      "iter 17300/30000  loss         0.095266  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.334\n",
      "iter 17301/30000  loss         0.095266  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.334\n",
      "iter 17320/30000  loss         0.095256  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.337\n",
      "iter 17321/30000  loss         0.095256  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.337\n",
      "iter 17340/30000  loss         0.095246  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.340\n",
      "iter 17341/30000  loss         0.095246  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.340\n",
      "iter 17360/30000  loss         0.095237  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.342\n",
      "iter 17361/30000  loss         0.095236  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.342\n",
      "iter 17380/30000  loss         0.095227  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.345\n",
      "iter 17381/30000  loss         0.095226  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17400/30000  loss         0.095217  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.348\n",
      "iter 17401/30000  loss         0.095216  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.348\n",
      "iter 17420/30000  loss         0.095207  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.351\n",
      "iter 17421/30000  loss         0.095207  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.351\n",
      "iter 17440/30000  loss         0.095197  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.353\n",
      "iter 17441/30000  loss         0.095197  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.354\n",
      "iter 17460/30000  loss         0.095188  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.356\n",
      "iter 17461/30000  loss         0.095187  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.356\n",
      "iter 17480/30000  loss         0.095178  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.359\n",
      "iter 17481/30000  loss         0.095178  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.359\n",
      "iter 17500/30000  loss         0.095168  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.362\n",
      "iter 17501/30000  loss         0.095168  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.362\n",
      "iter 17520/30000  loss         0.095159  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.364\n",
      "iter 17521/30000  loss         0.095158  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.365\n",
      "iter 17540/30000  loss         0.095149  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.367\n",
      "iter 17541/30000  loss         0.095149  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.367\n",
      "iter 17560/30000  loss         0.095139  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.370\n",
      "iter 17561/30000  loss         0.095139  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.370\n",
      "iter 17580/30000  loss         0.095130  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.373\n",
      "iter 17581/30000  loss         0.095129  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.373\n",
      "iter 17600/30000  loss         0.095120  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.375\n",
      "iter 17601/30000  loss         0.095120  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.376\n",
      "iter 17620/30000  loss         0.095111  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.378\n",
      "iter 17621/30000  loss         0.095110  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.378\n",
      "iter 17640/30000  loss         0.095101  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.381\n",
      "iter 17641/30000  loss         0.095101  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.381\n",
      "iter 17660/30000  loss         0.095092  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.384\n",
      "iter 17661/30000  loss         0.095091  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.384\n",
      "iter 17680/30000  loss         0.095082  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.386\n",
      "iter 17681/30000  loss         0.095082  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.386\n",
      "iter 17700/30000  loss         0.095073  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.389\n",
      "iter 17701/30000  loss         0.095072  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.389\n",
      "iter 17720/30000  loss         0.095063  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.392\n",
      "iter 17721/30000  loss         0.095063  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.392\n",
      "iter 17740/30000  loss         0.095054  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.394\n",
      "iter 17741/30000  loss         0.095053  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.395\n",
      "iter 17760/30000  loss         0.095045  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.397\n",
      "iter 17761/30000  loss         0.095044  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.397\n",
      "iter 17780/30000  loss         0.095035  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.400\n",
      "iter 17781/30000  loss         0.095035  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.400\n",
      "iter 17800/30000  loss         0.095026  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.403\n",
      "iter 17801/30000  loss         0.095025  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.403\n",
      "iter 17820/30000  loss         0.095016  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.405\n",
      "iter 17821/30000  loss         0.095016  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.405\n",
      "iter 17840/30000  loss         0.095007  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.408\n",
      "iter 17841/30000  loss         0.095007  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.408\n",
      "iter 17860/30000  loss         0.094998  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.411\n",
      "iter 17861/30000  loss         0.094997  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.411\n",
      "iter 17880/30000  loss         0.094989  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.413\n",
      "iter 17881/30000  loss         0.094988  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.414\n",
      "iter 17900/30000  loss         0.094979  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.416\n",
      "iter 17901/30000  loss         0.094979  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.416\n",
      "iter 17920/30000  loss         0.094970  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.419\n",
      "iter 17921/30000  loss         0.094970  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.419\n",
      "iter 17940/30000  loss         0.094961  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.422\n",
      "iter 17941/30000  loss         0.094960  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.422\n",
      "iter 17960/30000  loss         0.094952  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.424\n",
      "iter 17961/30000  loss         0.094951  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.424\n",
      "iter 17980/30000  loss         0.094943  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.427\n",
      "iter 17981/30000  loss         0.094942  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.427\n",
      "iter 18000/30000  loss         0.094933  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.430\n",
      "iter 18001/30000  loss         0.094933  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.430\n",
      "iter 18020/30000  loss         0.094924  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.432\n",
      "iter 18021/30000  loss         0.094924  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.432\n",
      "iter 18040/30000  loss         0.094915  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.435\n",
      "iter 18041/30000  loss         0.094915  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.435\n",
      "iter 18060/30000  loss         0.094906  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.438\n",
      "iter 18061/30000  loss         0.094906  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.438\n",
      "iter 18080/30000  loss         0.094897  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.440\n",
      "iter 18081/30000  loss         0.094896  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.440\n",
      "iter 18100/30000  loss         0.094888  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.443\n",
      "iter 18101/30000  loss         0.094887  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.443\n",
      "iter 18120/30000  loss         0.094879  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.446\n",
      "iter 18121/30000  loss         0.094878  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.446\n",
      "iter 18140/30000  loss         0.094870  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.448\n",
      "iter 18141/30000  loss         0.094869  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.448\n",
      "iter 18160/30000  loss         0.094861  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.451\n",
      "iter 18161/30000  loss         0.094860  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.451\n",
      "iter 18180/30000  loss         0.094852  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.454\n",
      "iter 18181/30000  loss         0.094851  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18200/30000  loss         0.094843  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.456\n",
      "iter 18201/30000  loss         0.094842  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.456\n",
      "iter 18220/30000  loss         0.094834  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.459\n",
      "iter 18221/30000  loss         0.094834  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.459\n",
      "iter 18240/30000  loss         0.094825  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.462\n",
      "iter 18241/30000  loss         0.094825  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.462\n",
      "iter 18260/30000  loss         0.094816  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.464\n",
      "iter 18261/30000  loss         0.094816  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.464\n",
      "iter 18280/30000  loss         0.094807  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.467\n",
      "iter 18281/30000  loss         0.094807  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.467\n",
      "iter 18300/30000  loss         0.094798  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.470\n",
      "iter 18301/30000  loss         0.094798  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.470\n",
      "iter 18320/30000  loss         0.094790  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.472\n",
      "iter 18321/30000  loss         0.094789  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.472\n",
      "iter 18340/30000  loss         0.094781  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.475\n",
      "iter 18341/30000  loss         0.094780  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.475\n",
      "iter 18360/30000  loss         0.094772  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.477\n",
      "iter 18361/30000  loss         0.094771  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.478\n",
      "iter 18380/30000  loss         0.094763  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.480\n",
      "iter 18381/30000  loss         0.094763  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.480\n",
      "iter 18400/30000  loss         0.094754  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.483\n",
      "iter 18401/30000  loss         0.094754  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.483\n",
      "iter 18420/30000  loss         0.094746  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.485\n",
      "iter 18421/30000  loss         0.094745  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.485\n",
      "iter 18440/30000  loss         0.094737  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.488\n",
      "iter 18441/30000  loss         0.094736  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.488\n",
      "iter 18460/30000  loss         0.094728  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.491\n",
      "iter 18461/30000  loss         0.094728  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.491\n",
      "iter 18480/30000  loss         0.094719  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.493\n",
      "iter 18481/30000  loss         0.094719  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.493\n",
      "iter 18500/30000  loss         0.094711  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.496\n",
      "iter 18501/30000  loss         0.094710  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.496\n",
      "iter 18520/30000  loss         0.094702  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.498\n",
      "iter 18521/30000  loss         0.094702  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.499\n",
      "iter 18540/30000  loss         0.094694  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.501\n",
      "iter 18541/30000  loss         0.094693  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.501\n",
      "iter 18560/30000  loss         0.094685  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.504\n",
      "iter 18561/30000  loss         0.094684  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.504\n",
      "iter 18580/30000  loss         0.094676  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.506\n",
      "iter 18581/30000  loss         0.094676  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.506\n",
      "iter 18600/30000  loss         0.094668  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.509\n",
      "iter 18601/30000  loss         0.094667  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.509\n",
      "iter 18620/30000  loss         0.094659  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.511\n",
      "iter 18621/30000  loss         0.094659  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.512\n",
      "iter 18640/30000  loss         0.094651  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.514\n",
      "iter 18641/30000  loss         0.094650  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.514\n",
      "iter 18660/30000  loss         0.094642  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.517\n",
      "iter 18661/30000  loss         0.094642  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.517\n",
      "iter 18680/30000  loss         0.094634  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.519\n",
      "iter 18681/30000  loss         0.094633  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.519\n",
      "iter 18700/30000  loss         0.094625  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.522\n",
      "iter 18701/30000  loss         0.094625  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.522\n",
      "iter 18720/30000  loss         0.094617  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.524\n",
      "iter 18721/30000  loss         0.094616  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.524\n",
      "iter 18740/30000  loss         0.094608  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.527\n",
      "iter 18741/30000  loss         0.094608  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.527\n",
      "iter 18760/30000  loss         0.094600  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.530\n",
      "iter 18761/30000  loss         0.094599  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.530\n",
      "iter 18780/30000  loss         0.094591  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.532\n",
      "iter 18781/30000  loss         0.094591  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.532\n",
      "iter 18800/30000  loss         0.094583  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.535\n",
      "iter 18801/30000  loss         0.094582  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.535\n",
      "iter 18820/30000  loss         0.094575  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.537\n",
      "iter 18821/30000  loss         0.094574  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.537\n",
      "iter 18840/30000  loss         0.094566  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.540\n",
      "iter 18841/30000  loss         0.094566  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.540\n",
      "iter 18860/30000  loss         0.094558  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.542\n",
      "iter 18861/30000  loss         0.094557  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.542\n",
      "iter 18880/30000  loss         0.094549  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.545\n",
      "iter 18881/30000  loss         0.094549  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.545\n",
      "iter 18900/30000  loss         0.094541  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.547\n",
      "iter 18901/30000  loss         0.094541  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.548\n",
      "iter 18920/30000  loss         0.094533  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.550\n",
      "iter 18921/30000  loss         0.094532  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.550\n",
      "iter 18940/30000  loss         0.094525  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.553\n",
      "iter 18941/30000  loss         0.094524  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.553\n",
      "iter 18960/30000  loss         0.094516  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.555\n",
      "iter 18961/30000  loss         0.094516  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.555\n",
      "iter 18980/30000  loss         0.094508  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.558\n",
      "iter 18981/30000  loss         0.094508  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19000/30000  loss         0.094500  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.560\n",
      "iter 19001/30000  loss         0.094499  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.560\n",
      "iter 19020/30000  loss         0.094492  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.563\n",
      "iter 19021/30000  loss         0.094491  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.563\n",
      "iter 19040/30000  loss         0.094483  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.565\n",
      "iter 19041/30000  loss         0.094483  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.565\n",
      "iter 19060/30000  loss         0.094475  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.568\n",
      "iter 19061/30000  loss         0.094475  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.568\n",
      "iter 19080/30000  loss         0.094467  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.570\n",
      "iter 19081/30000  loss         0.094467  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.571\n",
      "iter 19100/30000  loss         0.094459  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.573\n",
      "iter 19101/30000  loss         0.094459  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.573\n",
      "iter 19120/30000  loss         0.094451  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.575\n",
      "iter 19121/30000  loss         0.094450  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.576\n",
      "iter 19140/30000  loss         0.094443  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.578\n",
      "iter 19141/30000  loss         0.094442  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.578\n",
      "iter 19160/30000  loss         0.094435  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.581\n",
      "iter 19161/30000  loss         0.094434  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.581\n",
      "iter 19180/30000  loss         0.094427  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.583\n",
      "iter 19181/30000  loss         0.094426  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.583\n",
      "iter 19200/30000  loss         0.094419  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.586\n",
      "iter 19201/30000  loss         0.094418  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.586\n",
      "iter 19220/30000  loss         0.094411  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.588\n",
      "iter 19221/30000  loss         0.094410  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.588\n",
      "iter 19240/30000  loss         0.094402  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.591\n",
      "iter 19241/30000  loss         0.094402  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.591\n",
      "iter 19260/30000  loss         0.094394  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.593\n",
      "iter 19261/30000  loss         0.094394  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.593\n",
      "iter 19280/30000  loss         0.094387  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.596\n",
      "iter 19281/30000  loss         0.094386  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.596\n",
      "iter 19300/30000  loss         0.094379  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.598\n",
      "iter 19301/30000  loss         0.094378  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.598\n",
      "iter 19320/30000  loss         0.094371  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.601\n",
      "iter 19321/30000  loss         0.094370  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.601\n",
      "iter 19340/30000  loss         0.094363  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.603\n",
      "iter 19341/30000  loss         0.094362  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.603\n",
      "iter 19360/30000  loss         0.094355  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.606\n",
      "iter 19361/30000  loss         0.094354  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.606\n",
      "iter 19380/30000  loss         0.094347  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.608\n",
      "iter 19381/30000  loss         0.094346  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.608\n",
      "iter 19400/30000  loss         0.094339  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.611\n",
      "iter 19401/30000  loss         0.094339  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.611\n",
      "iter 19420/30000  loss         0.094331  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.613\n",
      "iter 19421/30000  loss         0.094331  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.613\n",
      "iter 19440/30000  loss         0.094323  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.616\n",
      "iter 19441/30000  loss         0.094323  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.616\n",
      "iter 19460/30000  loss         0.094315  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.618\n",
      "iter 19461/30000  loss         0.094315  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.618\n",
      "iter 19480/30000  loss         0.094308  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.621\n",
      "iter 19481/30000  loss         0.094307  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.621\n",
      "iter 19500/30000  loss         0.094300  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.623\n",
      "iter 19501/30000  loss         0.094299  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.623\n",
      "iter 19520/30000  loss         0.094292  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.626\n",
      "iter 19521/30000  loss         0.094292  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.626\n",
      "iter 19540/30000  loss         0.094284  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.628\n",
      "iter 19541/30000  loss         0.094284  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.628\n",
      "iter 19560/30000  loss         0.094276  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.631\n",
      "iter 19561/30000  loss         0.094276  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.631\n",
      "iter 19580/30000  loss         0.094269  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.633\n",
      "iter 19581/30000  loss         0.094268  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.633\n",
      "iter 19600/30000  loss         0.094261  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.635\n",
      "iter 19601/30000  loss         0.094261  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.636\n",
      "iter 19620/30000  loss         0.094253  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.638\n",
      "iter 19621/30000  loss         0.094253  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.638\n",
      "iter 19640/30000  loss         0.094246  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.640\n",
      "iter 19641/30000  loss         0.094245  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.641\n",
      "iter 19660/30000  loss         0.094238  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.643\n",
      "iter 19661/30000  loss         0.094237  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.643\n",
      "iter 19680/30000  loss         0.094230  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.645\n",
      "iter 19681/30000  loss         0.094230  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.645\n",
      "iter 19700/30000  loss         0.094223  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.648\n",
      "iter 19701/30000  loss         0.094222  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.648\n",
      "iter 19720/30000  loss         0.094215  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.650\n",
      "iter 19721/30000  loss         0.094215  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.650\n",
      "iter 19740/30000  loss         0.094207  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.653\n",
      "iter 19741/30000  loss         0.094207  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.653\n",
      "iter 19760/30000  loss         0.094200  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.655\n",
      "iter 19761/30000  loss         0.094199  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.655\n",
      "iter 19780/30000  loss         0.094192  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.658\n",
      "iter 19781/30000  loss         0.094192  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19800/30000  loss         0.094185  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.660\n",
      "iter 19801/30000  loss         0.094184  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.660\n",
      "iter 19820/30000  loss         0.094177  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.662\n",
      "iter 19821/30000  loss         0.094177  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.663\n",
      "iter 19840/30000  loss         0.094169  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.665\n",
      "iter 19841/30000  loss         0.094169  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.665\n",
      "iter 19860/30000  loss         0.094162  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.667\n",
      "iter 19861/30000  loss         0.094162  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.667\n",
      "iter 19880/30000  loss         0.094154  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.670\n",
      "iter 19881/30000  loss         0.094154  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.670\n",
      "iter 19900/30000  loss         0.094147  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.672\n",
      "iter 19901/30000  loss         0.094147  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.672\n",
      "iter 19920/30000  loss         0.094139  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.675\n",
      "iter 19921/30000  loss         0.094139  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.675\n",
      "iter 19940/30000  loss         0.094132  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.677\n",
      "iter 19941/30000  loss         0.094132  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.677\n",
      "iter 19960/30000  loss         0.094125  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.679\n",
      "iter 19961/30000  loss         0.094124  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.680\n",
      "iter 19980/30000  loss         0.094117  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.682\n",
      "iter 19981/30000  loss         0.094117  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.682\n",
      "iter 20000/30000  loss         0.094110  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.684\n",
      "iter 20001/30000  loss         0.094109  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.684\n",
      "iter 20020/30000  loss         0.094102  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.687\n",
      "iter 20021/30000  loss         0.094102  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.687\n",
      "iter 20040/30000  loss         0.094095  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.689\n",
      "iter 20041/30000  loss         0.094095  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.689\n",
      "iter 20060/30000  loss         0.094088  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.692\n",
      "iter 20061/30000  loss         0.094087  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.692\n",
      "iter 20080/30000  loss         0.094080  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.694\n",
      "iter 20081/30000  loss         0.094080  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.694\n",
      "iter 20100/30000  loss         0.094073  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.696\n",
      "iter 20101/30000  loss         0.094073  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.697\n",
      "iter 20120/30000  loss         0.094066  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.699\n",
      "iter 20121/30000  loss         0.094065  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.699\n",
      "iter 20140/30000  loss         0.094058  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.701\n",
      "iter 20141/30000  loss         0.094058  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.701\n",
      "iter 20160/30000  loss         0.094051  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.704\n",
      "iter 20161/30000  loss         0.094051  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.704\n",
      "iter 20180/30000  loss         0.094044  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.706\n",
      "iter 20181/30000  loss         0.094043  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.706\n",
      "iter 20200/30000  loss         0.094036  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.708\n",
      "iter 20201/30000  loss         0.094036  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.709\n",
      "iter 20220/30000  loss         0.094029  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.711\n",
      "iter 20221/30000  loss         0.094029  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.711\n",
      "iter 20240/30000  loss         0.094022  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.713\n",
      "iter 20241/30000  loss         0.094022  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.713\n",
      "iter 20260/30000  loss         0.094015  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.716\n",
      "iter 20261/30000  loss         0.094014  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.716\n",
      "iter 20280/30000  loss         0.094007  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.718\n",
      "iter 20281/30000  loss         0.094007  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.718\n",
      "iter 20300/30000  loss         0.094000  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.720\n",
      "iter 20301/30000  loss         0.094000  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.720\n",
      "iter 20320/30000  loss         0.093993  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.723\n",
      "iter 20321/30000  loss         0.093993  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.723\n",
      "iter 20340/30000  loss         0.093986  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.725\n",
      "iter 20341/30000  loss         0.093986  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.725\n",
      "iter 20360/30000  loss         0.093979  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.728\n",
      "iter 20361/30000  loss         0.093978  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.728\n",
      "iter 20380/30000  loss         0.093972  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.730\n",
      "iter 20381/30000  loss         0.093971  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.730\n",
      "iter 20400/30000  loss         0.093965  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.732\n",
      "iter 20401/30000  loss         0.093964  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.732\n",
      "iter 20420/30000  loss         0.093957  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.735\n",
      "iter 20421/30000  loss         0.093957  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.735\n",
      "iter 20440/30000  loss         0.093950  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.737\n",
      "iter 20441/30000  loss         0.093950  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.737\n",
      "iter 20460/30000  loss         0.093943  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.739\n",
      "iter 20461/30000  loss         0.093943  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.739\n",
      "iter 20480/30000  loss         0.093936  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.742\n",
      "iter 20481/30000  loss         0.093936  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.742\n",
      "iter 20500/30000  loss         0.093929  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.744\n",
      "iter 20501/30000  loss         0.093929  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.744\n",
      "iter 20520/30000  loss         0.093922  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.746\n",
      "iter 20521/30000  loss         0.093922  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.747\n",
      "iter 20540/30000  loss         0.093915  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.749\n",
      "iter 20541/30000  loss         0.093915  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.749\n",
      "iter 20560/30000  loss         0.093908  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.751\n",
      "iter 20561/30000  loss         0.093908  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.751\n",
      "iter 20580/30000  loss         0.093901  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.754\n",
      "iter 20581/30000  loss         0.093901  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20600/30000  loss         0.093894  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.756\n",
      "iter 20601/30000  loss         0.093894  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.756\n",
      "iter 20620/30000  loss         0.093887  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.758\n",
      "iter 20621/30000  loss         0.093887  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.758\n",
      "iter 20640/30000  loss         0.093880  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.761\n",
      "iter 20641/30000  loss         0.093880  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.761\n",
      "iter 20660/30000  loss         0.093873  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.763\n",
      "iter 20661/30000  loss         0.093873  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.763\n",
      "iter 20680/30000  loss         0.093866  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.765\n",
      "iter 20681/30000  loss         0.093866  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.765\n",
      "iter 20700/30000  loss         0.093859  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.768\n",
      "iter 20701/30000  loss         0.093859  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.768\n",
      "iter 20720/30000  loss         0.093853  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.770\n",
      "iter 20721/30000  loss         0.093852  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.770\n",
      "iter 20740/30000  loss         0.093846  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.772\n",
      "iter 20741/30000  loss         0.093845  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.772\n",
      "iter 20760/30000  loss         0.093839  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.775\n",
      "iter 20761/30000  loss         0.093838  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.775\n",
      "iter 20780/30000  loss         0.093832  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.777\n",
      "iter 20781/30000  loss         0.093832  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.777\n",
      "iter 20800/30000  loss         0.093825  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.779\n",
      "iter 20801/30000  loss         0.093825  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.779\n",
      "iter 20820/30000  loss         0.093818  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.782\n",
      "iter 20821/30000  loss         0.093818  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.782\n",
      "iter 20840/30000  loss         0.093812  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.784\n",
      "iter 20841/30000  loss         0.093811  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.784\n",
      "iter 20860/30000  loss         0.093805  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.786\n",
      "iter 20861/30000  loss         0.093804  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.786\n",
      "iter 20880/30000  loss         0.093798  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.789\n",
      "iter 20881/30000  loss         0.093798  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.789\n",
      "iter 20900/30000  loss         0.093791  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.791\n",
      "iter 20901/30000  loss         0.093791  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.791\n",
      "iter 20920/30000  loss         0.093784  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.793\n",
      "iter 20921/30000  loss         0.093784  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.793\n",
      "iter 20940/30000  loss         0.093778  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.795\n",
      "iter 20941/30000  loss         0.093777  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.796\n",
      "iter 20960/30000  loss         0.093771  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.798\n",
      "iter 20961/30000  loss         0.093771  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.798\n",
      "iter 20980/30000  loss         0.093764  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.800\n",
      "iter 20981/30000  loss         0.093764  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.800\n",
      "iter 21000/30000  loss         0.093758  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.802\n",
      "iter 21001/30000  loss         0.093757  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.802\n",
      "iter 21020/30000  loss         0.093751  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.805\n",
      "iter 21021/30000  loss         0.093750  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.805\n",
      "iter 21040/30000  loss         0.093744  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.807\n",
      "iter 21041/30000  loss         0.093744  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.807\n",
      "iter 21060/30000  loss         0.093737  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.809\n",
      "iter 21061/30000  loss         0.093737  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.809\n",
      "iter 21080/30000  loss         0.093731  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.812\n",
      "iter 21081/30000  loss         0.093730  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.812\n",
      "iter 21100/30000  loss         0.093724  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.814\n",
      "iter 21101/30000  loss         0.093724  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.814\n",
      "iter 21120/30000  loss         0.093718  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.816\n",
      "iter 21121/30000  loss         0.093717  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.816\n",
      "iter 21140/30000  loss         0.093711  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.818\n",
      "iter 21141/30000  loss         0.093711  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.819\n",
      "iter 21160/30000  loss         0.093704  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.821\n",
      "iter 21161/30000  loss         0.093704  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.821\n",
      "iter 21180/30000  loss         0.093698  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.823\n",
      "iter 21181/30000  loss         0.093697  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.823\n",
      "iter 21200/30000  loss         0.093691  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.825\n",
      "iter 21201/30000  loss         0.093691  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.825\n",
      "iter 21220/30000  loss         0.093685  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.828\n",
      "iter 21221/30000  loss         0.093684  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.828\n",
      "iter 21240/30000  loss         0.093678  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.830\n",
      "iter 21241/30000  loss         0.093678  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.830\n",
      "iter 21260/30000  loss         0.093672  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.832\n",
      "iter 21261/30000  loss         0.093671  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.832\n",
      "iter 21280/30000  loss         0.093665  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.834\n",
      "iter 21281/30000  loss         0.093665  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.834\n",
      "iter 21300/30000  loss         0.093659  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.837\n",
      "iter 21301/30000  loss         0.093658  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.837\n",
      "iter 21320/30000  loss         0.093652  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.839\n",
      "iter 21321/30000  loss         0.093652  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.839\n",
      "iter 21340/30000  loss         0.093646  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.841\n",
      "iter 21341/30000  loss         0.093645  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.841\n",
      "iter 21360/30000  loss         0.093639  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.843\n",
      "iter 21361/30000  loss         0.093639  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.844\n",
      "iter 21380/30000  loss         0.093633  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.846\n",
      "iter 21381/30000  loss         0.093632  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21400/30000  loss         0.093626  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.848\n",
      "iter 21401/30000  loss         0.093626  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.848\n",
      "iter 21420/30000  loss         0.093620  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.850\n",
      "iter 21421/30000  loss         0.093619  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.850\n",
      "iter 21440/30000  loss         0.093613  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.852\n",
      "iter 21441/30000  loss         0.093613  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.853\n",
      "iter 21460/30000  loss         0.093607  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.855\n",
      "iter 21461/30000  loss         0.093607  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.855\n",
      "iter 21480/30000  loss         0.093601  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.857\n",
      "iter 21481/30000  loss         0.093600  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.857\n",
      "iter 21500/30000  loss         0.093594  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.859\n",
      "iter 21501/30000  loss         0.093594  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.859\n",
      "iter 21520/30000  loss         0.093588  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.861\n",
      "iter 21521/30000  loss         0.093587  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.862\n",
      "iter 21540/30000  loss         0.093581  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.864\n",
      "iter 21541/30000  loss         0.093581  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.864\n",
      "iter 21560/30000  loss         0.093575  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.866\n",
      "iter 21561/30000  loss         0.093575  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.866\n",
      "iter 21580/30000  loss         0.093569  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.868\n",
      "iter 21581/30000  loss         0.093568  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.868\n",
      "iter 21600/30000  loss         0.093562  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.870\n",
      "iter 21601/30000  loss         0.093562  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.871\n",
      "iter 21620/30000  loss         0.093556  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.873\n",
      "iter 21621/30000  loss         0.093556  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.873\n",
      "iter 21640/30000  loss         0.093550  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.875\n",
      "iter 21641/30000  loss         0.093550  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.875\n",
      "iter 21660/30000  loss         0.093544  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.877\n",
      "iter 21661/30000  loss         0.093543  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.877\n",
      "iter 21680/30000  loss         0.093537  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.879\n",
      "iter 21681/30000  loss         0.093537  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.879\n",
      "iter 21700/30000  loss         0.093531  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.882\n",
      "iter 21701/30000  loss         0.093531  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.882\n",
      "iter 21720/30000  loss         0.093525  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.884\n",
      "iter 21721/30000  loss         0.093524  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.884\n",
      "iter 21740/30000  loss         0.093519  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.886\n",
      "iter 21741/30000  loss         0.093518  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.886\n",
      "iter 21760/30000  loss         0.093512  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.888\n",
      "iter 21761/30000  loss         0.093512  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.888\n",
      "iter 21780/30000  loss         0.093506  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.890\n",
      "iter 21781/30000  loss         0.093506  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.891\n",
      "iter 21800/30000  loss         0.093500  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.893\n",
      "iter 21801/30000  loss         0.093500  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.893\n",
      "iter 21820/30000  loss         0.093494  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.895\n",
      "iter 21821/30000  loss         0.093493  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.895\n",
      "iter 21840/30000  loss         0.093488  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.897\n",
      "iter 21841/30000  loss         0.093487  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.897\n",
      "iter 21860/30000  loss         0.093481  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.899\n",
      "iter 21861/30000  loss         0.093481  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.899\n",
      "iter 21880/30000  loss         0.093475  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.902\n",
      "iter 21881/30000  loss         0.093475  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.902\n",
      "iter 21900/30000  loss         0.093469  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.904\n",
      "iter 21901/30000  loss         0.093469  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.904\n",
      "iter 21920/30000  loss         0.093463  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.906\n",
      "iter 21921/30000  loss         0.093463  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.906\n",
      "iter 21940/30000  loss         0.093457  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.908\n",
      "iter 21941/30000  loss         0.093457  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.908\n",
      "iter 21960/30000  loss         0.093451  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.910\n",
      "iter 21961/30000  loss         0.093451  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.910\n",
      "iter 21980/30000  loss         0.093445  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.913\n",
      "iter 21981/30000  loss         0.093444  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.913\n",
      "iter 22000/30000  loss         0.093439  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.915\n",
      "iter 22001/30000  loss         0.093438  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.915\n",
      "iter 22020/30000  loss         0.093433  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.917\n",
      "iter 22021/30000  loss         0.093432  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.917\n",
      "iter 22040/30000  loss         0.093427  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.919\n",
      "iter 22041/30000  loss         0.093426  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.919\n",
      "iter 22060/30000  loss         0.093420  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.921\n",
      "iter 22061/30000  loss         0.093420  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.921\n",
      "iter 22080/30000  loss         0.093414  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.923\n",
      "iter 22081/30000  loss         0.093414  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.924\n",
      "iter 22100/30000  loss         0.093408  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.926\n",
      "iter 22101/30000  loss         0.093408  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.926\n",
      "iter 22120/30000  loss         0.093402  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.928\n",
      "iter 22121/30000  loss         0.093402  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.928\n",
      "iter 22140/30000  loss         0.093396  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.930\n",
      "iter 22141/30000  loss         0.093396  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.930\n",
      "iter 22160/30000  loss         0.093390  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.932\n",
      "iter 22161/30000  loss         0.093390  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.932\n",
      "iter 22180/30000  loss         0.093384  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.934\n",
      "iter 22181/30000  loss         0.093384  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22200/30000  loss         0.093379  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.937\n",
      "iter 22201/30000  loss         0.093378  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.937\n",
      "iter 22220/30000  loss         0.093373  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.939\n",
      "iter 22221/30000  loss         0.093372  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.939\n",
      "iter 22240/30000  loss         0.093367  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.941\n",
      "iter 22241/30000  loss         0.093366  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.941\n",
      "iter 22260/30000  loss         0.093361  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.943\n",
      "iter 22261/30000  loss         0.093360  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.943\n",
      "iter 22280/30000  loss         0.093355  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.945\n",
      "iter 22281/30000  loss         0.093355  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.945\n",
      "iter 22300/30000  loss         0.093349  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.947\n",
      "iter 22301/30000  loss         0.093349  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.947\n",
      "iter 22320/30000  loss         0.093343  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.950\n",
      "iter 22321/30000  loss         0.093343  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.950\n",
      "iter 22340/30000  loss         0.093337  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.952\n",
      "iter 22341/30000  loss         0.093337  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.952\n",
      "iter 22360/30000  loss         0.093331  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.954\n",
      "iter 22361/30000  loss         0.093331  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.954\n",
      "iter 22380/30000  loss         0.093325  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.956\n",
      "iter 22381/30000  loss         0.093325  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.956\n",
      "iter 22400/30000  loss         0.093320  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.958\n",
      "iter 22401/30000  loss         0.093319  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.958\n",
      "iter 22420/30000  loss         0.093314  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.960\n",
      "iter 22421/30000  loss         0.093313  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.960\n",
      "iter 22440/30000  loss         0.093308  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.962\n",
      "iter 22441/30000  loss         0.093308  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.963\n",
      "iter 22460/30000  loss         0.093302  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.965\n",
      "iter 22461/30000  loss         0.093302  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.965\n",
      "iter 22480/30000  loss         0.093296  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.967\n",
      "iter 22481/30000  loss         0.093296  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.967\n",
      "iter 22500/30000  loss         0.093291  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.969\n",
      "iter 22501/30000  loss         0.093290  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.969\n",
      "iter 22520/30000  loss         0.093285  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.971\n",
      "iter 22521/30000  loss         0.093284  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.971\n",
      "iter 22540/30000  loss         0.093279  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.973\n",
      "iter 22541/30000  loss         0.093279  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.973\n",
      "iter 22560/30000  loss         0.093273  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.975\n",
      "iter 22561/30000  loss         0.093273  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.975\n",
      "iter 22580/30000  loss         0.093267  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.977\n",
      "iter 22581/30000  loss         0.093267  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.978\n",
      "iter 22600/30000  loss         0.093262  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.980\n",
      "iter 22601/30000  loss         0.093261  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.980\n",
      "iter 22620/30000  loss         0.093256  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.982\n",
      "iter 22621/30000  loss         0.093256  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.982\n",
      "iter 22640/30000  loss         0.093250  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.984\n",
      "iter 22641/30000  loss         0.093250  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.984\n",
      "iter 22660/30000  loss         0.093245  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.986\n",
      "iter 22661/30000  loss         0.093244  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.986\n",
      "iter 22680/30000  loss         0.093239  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.988\n",
      "iter 22681/30000  loss         0.093239  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.988\n",
      "iter 22700/30000  loss         0.093233  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.990\n",
      "iter 22701/30000  loss         0.093233  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.990\n",
      "iter 22720/30000  loss         0.093228  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.992\n",
      "iter 22721/30000  loss         0.093227  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.992\n",
      "iter 22740/30000  loss         0.093222  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.994\n",
      "iter 22741/30000  loss         0.093222  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.995\n",
      "iter 22760/30000  loss         0.093216  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.997\n",
      "iter 22761/30000  loss         0.093216  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.997\n",
      "iter 22780/30000  loss         0.093211  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.999\n",
      "iter 22781/30000  loss         0.093210  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.999\n",
      "iter 22800/30000  loss         0.093205  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.001\n",
      "iter 22801/30000  loss         0.093205  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.001\n",
      "iter 22820/30000  loss         0.093199  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.003\n",
      "iter 22821/30000  loss         0.093199  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.003\n",
      "iter 22840/30000  loss         0.093194  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.005\n",
      "iter 22841/30000  loss         0.093193  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.005\n",
      "iter 22860/30000  loss         0.093188  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.007\n",
      "iter 22861/30000  loss         0.093188  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.007\n",
      "iter 22880/30000  loss         0.093183  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.009\n",
      "iter 22881/30000  loss         0.093182  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.009\n",
      "iter 22900/30000  loss         0.093177  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.011\n",
      "iter 22901/30000  loss         0.093177  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.011\n",
      "iter 22920/30000  loss         0.093171  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.013\n",
      "iter 22921/30000  loss         0.093171  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.013\n",
      "iter 22940/30000  loss         0.093166  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.015\n",
      "iter 22941/30000  loss         0.093166  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.016\n",
      "iter 22960/30000  loss         0.093160  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.018\n",
      "iter 22961/30000  loss         0.093160  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.018\n",
      "iter 22980/30000  loss         0.093155  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.020\n",
      "iter 22981/30000  loss         0.093155  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    6.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23000/30000  loss         0.093149  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.022\n",
      "iter 23001/30000  loss         0.093149  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.022\n",
      "iter 23020/30000  loss         0.093144  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.024\n",
      "iter 23021/30000  loss         0.093143  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.024\n",
      "iter 23040/30000  loss         0.093138  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.026\n",
      "iter 23041/30000  loss         0.093138  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.026\n",
      "iter 23060/30000  loss         0.093133  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.028\n",
      "iter 23061/30000  loss         0.093133  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.028\n",
      "iter 23080/30000  loss         0.093127  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.030\n",
      "iter 23081/30000  loss         0.093127  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.030\n",
      "iter 23100/30000  loss         0.093122  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.032\n",
      "iter 23101/30000  loss         0.093122  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.032\n",
      "iter 23120/30000  loss         0.093116  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.034\n",
      "iter 23121/30000  loss         0.093116  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.034\n",
      "iter 23140/30000  loss         0.093111  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.036\n",
      "iter 23141/30000  loss         0.093111  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.036\n",
      "iter 23160/30000  loss         0.093105  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.038\n",
      "iter 23161/30000  loss         0.093105  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.039\n",
      "iter 23180/30000  loss         0.093100  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.041\n",
      "iter 23181/30000  loss         0.093100  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.041\n",
      "iter 23200/30000  loss         0.093095  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.043\n",
      "iter 23201/30000  loss         0.093094  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.043\n",
      "iter 23220/30000  loss         0.093089  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.045\n",
      "iter 23221/30000  loss         0.093089  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.045\n",
      "iter 23240/30000  loss         0.093084  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.047\n",
      "iter 23241/30000  loss         0.093084  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.047\n",
      "iter 23260/30000  loss         0.093078  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.049\n",
      "iter 23261/30000  loss         0.093078  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.049\n",
      "iter 23280/30000  loss         0.093073  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.051\n",
      "iter 23281/30000  loss         0.093073  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.051\n",
      "iter 23300/30000  loss         0.093068  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.053\n",
      "iter 23301/30000  loss         0.093067  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.053\n",
      "iter 23320/30000  loss         0.093062  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.055\n",
      "iter 23321/30000  loss         0.093062  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.055\n",
      "iter 23340/30000  loss         0.093057  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.057\n",
      "iter 23341/30000  loss         0.093057  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.057\n",
      "iter 23360/30000  loss         0.093052  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.059\n",
      "iter 23361/30000  loss         0.093051  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.059\n",
      "iter 23380/30000  loss         0.093046  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.061\n",
      "iter 23381/30000  loss         0.093046  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.061\n",
      "iter 23400/30000  loss         0.093041  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.063\n",
      "iter 23401/30000  loss         0.093041  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.063\n",
      "iter 23420/30000  loss         0.093036  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.065\n",
      "iter 23421/30000  loss         0.093035  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.065\n",
      "iter 23440/30000  loss         0.093030  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.067\n",
      "iter 23441/30000  loss         0.093030  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.067\n",
      "iter 23460/30000  loss         0.093025  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.069\n",
      "iter 23461/30000  loss         0.093025  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.069\n",
      "iter 23480/30000  loss         0.093020  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.071\n",
      "iter 23481/30000  loss         0.093020  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.071\n",
      "iter 23500/30000  loss         0.093015  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.073\n",
      "iter 23501/30000  loss         0.093014  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.074\n",
      "iter 23520/30000  loss         0.093009  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.075\n",
      "iter 23521/30000  loss         0.093009  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.076\n",
      "iter 23540/30000  loss         0.093004  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.078\n",
      "iter 23541/30000  loss         0.093004  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.078\n",
      "iter 23560/30000  loss         0.092999  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.080\n",
      "iter 23561/30000  loss         0.092999  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.080\n",
      "iter 23580/30000  loss         0.092994  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.082\n",
      "iter 23581/30000  loss         0.092993  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    6.082\n",
      "iter 23600/30000  loss         0.092988  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.084\n",
      "iter 23601/30000  loss         0.092988  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.084\n",
      "iter 23620/30000  loss         0.092983  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.086\n",
      "iter 23621/30000  loss         0.092983  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.086\n",
      "iter 23640/30000  loss         0.092978  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.088\n",
      "iter 23641/30000  loss         0.092978  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.088\n",
      "iter 23660/30000  loss         0.092973  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.090\n",
      "iter 23661/30000  loss         0.092973  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.090\n",
      "iter 23680/30000  loss         0.092968  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.092\n",
      "iter 23681/30000  loss         0.092967  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.092\n",
      "iter 23700/30000  loss         0.092962  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.094\n",
      "iter 23701/30000  loss         0.092962  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.094\n",
      "iter 23720/30000  loss         0.092957  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.096\n",
      "iter 23721/30000  loss         0.092957  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.096\n",
      "iter 23740/30000  loss         0.092952  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.098\n",
      "iter 23741/30000  loss         0.092952  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.098\n",
      "iter 23760/30000  loss         0.092947  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.100\n",
      "iter 23761/30000  loss         0.092947  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.100\n",
      "iter 23780/30000  loss         0.092942  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.102\n",
      "iter 23781/30000  loss         0.092942  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23800/30000  loss         0.092937  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.104\n",
      "iter 23801/30000  loss         0.092936  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.104\n",
      "iter 23820/30000  loss         0.092932  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.106\n",
      "iter 23821/30000  loss         0.092931  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.106\n",
      "iter 23840/30000  loss         0.092927  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.108\n",
      "iter 23841/30000  loss         0.092926  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.108\n",
      "iter 23860/30000  loss         0.092921  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.110\n",
      "iter 23861/30000  loss         0.092921  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.110\n",
      "iter 23880/30000  loss         0.092916  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.112\n",
      "iter 23881/30000  loss         0.092916  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.112\n",
      "iter 23900/30000  loss         0.092911  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.114\n",
      "iter 23901/30000  loss         0.092911  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.114\n",
      "iter 23920/30000  loss         0.092906  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.116\n",
      "iter 23921/30000  loss         0.092906  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.116\n",
      "iter 23940/30000  loss         0.092901  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.118\n",
      "iter 23941/30000  loss         0.092901  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.118\n",
      "iter 23960/30000  loss         0.092896  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.120\n",
      "iter 23961/30000  loss         0.092896  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.120\n",
      "iter 23980/30000  loss         0.092891  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.122\n",
      "iter 23981/30000  loss         0.092891  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.122\n",
      "iter 24000/30000  loss         0.092886  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.124\n",
      "iter 24001/30000  loss         0.092886  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.124\n",
      "iter 24020/30000  loss         0.092881  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.126\n",
      "iter 24021/30000  loss         0.092881  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.126\n",
      "iter 24040/30000  loss         0.092876  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.128\n",
      "iter 24041/30000  loss         0.092876  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.128\n",
      "iter 24060/30000  loss         0.092871  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.130\n",
      "iter 24061/30000  loss         0.092871  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.130\n",
      "iter 24080/30000  loss         0.092866  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.132\n",
      "iter 24081/30000  loss         0.092866  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.132\n",
      "iter 24100/30000  loss         0.092861  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.134\n",
      "iter 24101/30000  loss         0.092861  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.134\n",
      "iter 24120/30000  loss         0.092856  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.136\n",
      "iter 24121/30000  loss         0.092856  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.136\n",
      "iter 24140/30000  loss         0.092851  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.138\n",
      "iter 24141/30000  loss         0.092851  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.138\n",
      "iter 24160/30000  loss         0.092846  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.140\n",
      "iter 24161/30000  loss         0.092846  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.140\n",
      "iter 24180/30000  loss         0.092841  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.142\n",
      "iter 24181/30000  loss         0.092841  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.142\n",
      "iter 24200/30000  loss         0.092836  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.144\n",
      "iter 24201/30000  loss         0.092836  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    6.144\n",
      "iter 24220/30000  loss         0.092831  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.146\n",
      "iter 24221/30000  loss         0.092831  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.146\n",
      "iter 24240/30000  loss         0.092826  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.148\n",
      "iter 24241/30000  loss         0.092826  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.148\n",
      "iter 24260/30000  loss         0.092822  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.150\n",
      "iter 24261/30000  loss         0.092821  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.150\n",
      "iter 24280/30000  loss         0.092817  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.152\n",
      "iter 24281/30000  loss         0.092816  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.152\n",
      "iter 24300/30000  loss         0.092812  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.154\n",
      "iter 24301/30000  loss         0.092811  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.154\n",
      "iter 24320/30000  loss         0.092807  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.156\n",
      "iter 24321/30000  loss         0.092807  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.156\n",
      "iter 24340/30000  loss         0.092802  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.158\n",
      "iter 24341/30000  loss         0.092802  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.158\n",
      "iter 24360/30000  loss         0.092797  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.160\n",
      "iter 24361/30000  loss         0.092797  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.160\n",
      "iter 24380/30000  loss         0.092792  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.161\n",
      "iter 24381/30000  loss         0.092792  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.162\n",
      "iter 24400/30000  loss         0.092787  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.163\n",
      "iter 24401/30000  loss         0.092787  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.164\n",
      "iter 24420/30000  loss         0.092783  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.165\n",
      "iter 24421/30000  loss         0.092782  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.165\n",
      "iter 24440/30000  loss         0.092778  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.167\n",
      "iter 24441/30000  loss         0.092778  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.167\n",
      "iter 24460/30000  loss         0.092773  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.169\n",
      "iter 24461/30000  loss         0.092773  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.169\n",
      "iter 24480/30000  loss         0.092768  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.171\n",
      "iter 24481/30000  loss         0.092768  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.171\n",
      "iter 24500/30000  loss         0.092763  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.173\n",
      "iter 24501/30000  loss         0.092763  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.173\n",
      "iter 24520/30000  loss         0.092759  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.175\n",
      "iter 24521/30000  loss         0.092758  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.175\n",
      "iter 24540/30000  loss         0.092754  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.177\n",
      "iter 24541/30000  loss         0.092754  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.177\n",
      "iter 24560/30000  loss         0.092749  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.179\n",
      "iter 24561/30000  loss         0.092749  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.179\n",
      "iter 24580/30000  loss         0.092744  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.181\n",
      "iter 24581/30000  loss         0.092744  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24600/30000  loss         0.092739  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.183\n",
      "iter 24601/30000  loss         0.092739  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.183\n",
      "iter 24620/30000  loss         0.092735  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.185\n",
      "iter 24621/30000  loss         0.092734  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.185\n",
      "iter 24640/30000  loss         0.092730  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.187\n",
      "iter 24641/30000  loss         0.092730  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.187\n",
      "iter 24660/30000  loss         0.092725  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.189\n",
      "iter 24661/30000  loss         0.092725  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.189\n",
      "iter 24680/30000  loss         0.092721  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.191\n",
      "iter 24681/30000  loss         0.092720  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.191\n",
      "iter 24700/30000  loss         0.092716  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.193\n",
      "iter 24701/30000  loss         0.092716  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.193\n",
      "iter 24720/30000  loss         0.092711  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.195\n",
      "iter 24721/30000  loss         0.092711  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.195\n",
      "iter 24740/30000  loss         0.092706  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.196\n",
      "iter 24741/30000  loss         0.092706  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.197\n",
      "iter 24760/30000  loss         0.092702  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.198\n",
      "iter 24761/30000  loss         0.092701  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.198\n",
      "iter 24780/30000  loss         0.092697  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.200\n",
      "iter 24781/30000  loss         0.092697  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.200\n",
      "iter 24800/30000  loss         0.092692  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.202\n",
      "iter 24801/30000  loss         0.092692  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.202\n",
      "iter 24820/30000  loss         0.092688  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.204\n",
      "iter 24821/30000  loss         0.092687  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.204\n",
      "iter 24840/30000  loss         0.092683  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.206\n",
      "iter 24841/30000  loss         0.092683  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.206\n",
      "iter 24860/30000  loss         0.092678  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.208\n",
      "iter 24861/30000  loss         0.092678  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    6.208\n",
      "iter 24880/30000  loss         0.092674  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.210\n",
      "iter 24881/30000  loss         0.092674  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.210\n",
      "iter 24900/30000  loss         0.092669  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.212\n",
      "iter 24901/30000  loss         0.092669  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.212\n",
      "iter 24920/30000  loss         0.092665  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.214\n",
      "iter 24921/30000  loss         0.092664  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.214\n",
      "iter 24940/30000  loss         0.092660  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.216\n",
      "iter 24941/30000  loss         0.092660  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.216\n",
      "iter 24960/30000  loss         0.092655  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.218\n",
      "iter 24961/30000  loss         0.092655  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.218\n",
      "iter 24980/30000  loss         0.092651  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.219\n",
      "iter 24981/30000  loss         0.092650  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.220\n",
      "iter 25000/30000  loss         0.092646  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.221\n",
      "iter 25001/30000  loss         0.092646  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.221\n",
      "iter 25020/30000  loss         0.092642  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.223\n",
      "iter 25021/30000  loss         0.092641  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.223\n",
      "iter 25040/30000  loss         0.092637  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.225\n",
      "iter 25041/30000  loss         0.092637  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.225\n",
      "iter 25060/30000  loss         0.092632  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.227\n",
      "iter 25061/30000  loss         0.092632  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.227\n",
      "iter 25080/30000  loss         0.092628  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.229\n",
      "iter 25081/30000  loss         0.092628  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.229\n",
      "iter 25100/30000  loss         0.092623  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.231\n",
      "iter 25101/30000  loss         0.092623  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.231\n",
      "iter 25120/30000  loss         0.092619  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.233\n",
      "iter 25121/30000  loss         0.092619  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.233\n",
      "iter 25140/30000  loss         0.092614  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.235\n",
      "iter 25141/30000  loss         0.092614  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.235\n",
      "iter 25160/30000  loss         0.092610  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.237\n",
      "iter 25161/30000  loss         0.092609  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.237\n",
      "iter 25180/30000  loss         0.092605  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.238\n",
      "iter 25181/30000  loss         0.092605  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.239\n",
      "iter 25200/30000  loss         0.092601  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.240\n",
      "iter 25201/30000  loss         0.092600  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.240\n",
      "iter 25220/30000  loss         0.092596  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.242\n",
      "iter 25221/30000  loss         0.092596  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.242\n",
      "iter 25240/30000  loss         0.092592  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.244\n",
      "iter 25241/30000  loss         0.092591  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.244\n",
      "iter 25260/30000  loss         0.092587  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.246\n",
      "iter 25261/30000  loss         0.092587  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.246\n",
      "iter 25280/30000  loss         0.092583  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.248\n",
      "iter 25281/30000  loss         0.092583  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.248\n",
      "iter 25300/30000  loss         0.092578  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.250\n",
      "iter 25301/30000  loss         0.092578  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.250\n",
      "iter 25320/30000  loss         0.092574  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.252\n",
      "iter 25321/30000  loss         0.092574  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.252\n",
      "iter 25340/30000  loss         0.092569  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.254\n",
      "iter 25341/30000  loss         0.092569  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.254\n",
      "iter 25360/30000  loss         0.092565  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.255\n",
      "iter 25361/30000  loss         0.092565  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.255\n",
      "iter 25380/30000  loss         0.092561  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.257\n",
      "iter 25381/30000  loss         0.092560  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25400/30000  loss         0.092556  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.259\n",
      "iter 25401/30000  loss         0.092556  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.259\n",
      "iter 25420/30000  loss         0.092552  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.261\n",
      "iter 25421/30000  loss         0.092551  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.261\n",
      "iter 25440/30000  loss         0.092547  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.263\n",
      "iter 25441/30000  loss         0.092547  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.263\n",
      "iter 25460/30000  loss         0.092543  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.265\n",
      "iter 25461/30000  loss         0.092543  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.265\n",
      "iter 25480/30000  loss         0.092538  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.267\n",
      "iter 25481/30000  loss         0.092538  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.267\n",
      "iter 25500/30000  loss         0.092534  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.268\n",
      "iter 25501/30000  loss         0.092534  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.269\n",
      "iter 25520/30000  loss         0.092530  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.270\n",
      "iter 25521/30000  loss         0.092529  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.270\n",
      "iter 25540/30000  loss         0.092525  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.272\n",
      "iter 25541/30000  loss         0.092525  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    6.272\n",
      "iter 25560/30000  loss         0.092521  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.274\n",
      "iter 25561/30000  loss         0.092521  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.274\n",
      "iter 25580/30000  loss         0.092517  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.276\n",
      "iter 25581/30000  loss         0.092516  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.276\n",
      "iter 25600/30000  loss         0.092512  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.278\n",
      "iter 25601/30000  loss         0.092512  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.278\n",
      "iter 25620/30000  loss         0.092508  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.280\n",
      "iter 25621/30000  loss         0.092508  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.280\n",
      "iter 25640/30000  loss         0.092504  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.281\n",
      "iter 25641/30000  loss         0.092503  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.282\n",
      "iter 25660/30000  loss         0.092499  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.283\n",
      "iter 25661/30000  loss         0.092499  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.283\n",
      "iter 25680/30000  loss         0.092495  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.285\n",
      "iter 25681/30000  loss         0.092495  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.285\n",
      "iter 25700/30000  loss         0.092491  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.287\n",
      "iter 25701/30000  loss         0.092490  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.287\n",
      "iter 25720/30000  loss         0.092486  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.289\n",
      "iter 25721/30000  loss         0.092486  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.289\n",
      "iter 25740/30000  loss         0.092482  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.291\n",
      "iter 25741/30000  loss         0.092482  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.291\n",
      "iter 25760/30000  loss         0.092478  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.293\n",
      "iter 25761/30000  loss         0.092478  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.293\n",
      "iter 25780/30000  loss         0.092474  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.294\n",
      "iter 25781/30000  loss         0.092473  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.294\n",
      "iter 25800/30000  loss         0.092469  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.296\n",
      "iter 25801/30000  loss         0.092469  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.296\n",
      "iter 25820/30000  loss         0.092465  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.298\n",
      "iter 25821/30000  loss         0.092465  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.298\n",
      "iter 25840/30000  loss         0.092461  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.300\n",
      "iter 25841/30000  loss         0.092461  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.300\n",
      "iter 25860/30000  loss         0.092456  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.302\n",
      "iter 25861/30000  loss         0.092456  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.302\n",
      "iter 25880/30000  loss         0.092452  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.304\n",
      "iter 25881/30000  loss         0.092452  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.304\n",
      "iter 25900/30000  loss         0.092448  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.305\n",
      "iter 25901/30000  loss         0.092448  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.305\n",
      "iter 25920/30000  loss         0.092444  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.307\n",
      "iter 25921/30000  loss         0.092444  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.307\n",
      "iter 25940/30000  loss         0.092440  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.309\n",
      "iter 25941/30000  loss         0.092439  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.309\n",
      "iter 25960/30000  loss         0.092435  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.311\n",
      "iter 25961/30000  loss         0.092435  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.311\n",
      "iter 25980/30000  loss         0.092431  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.313\n",
      "iter 25981/30000  loss         0.092431  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.313\n",
      "iter 26000/30000  loss         0.092427  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.315\n",
      "iter 26001/30000  loss         0.092427  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.315\n",
      "iter 26020/30000  loss         0.092423  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.316\n",
      "iter 26021/30000  loss         0.092423  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.316\n",
      "iter 26040/30000  loss         0.092419  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.318\n",
      "iter 26041/30000  loss         0.092418  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.318\n",
      "iter 26060/30000  loss         0.092414  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.320\n",
      "iter 26061/30000  loss         0.092414  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.320\n",
      "iter 26080/30000  loss         0.092410  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.322\n",
      "iter 26081/30000  loss         0.092410  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.322\n",
      "iter 26100/30000  loss         0.092406  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.324\n",
      "iter 26101/30000  loss         0.092406  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.324\n",
      "iter 26120/30000  loss         0.092402  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.325\n",
      "iter 26121/30000  loss         0.092402  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.326\n",
      "iter 26140/30000  loss         0.092398  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.327\n",
      "iter 26141/30000  loss         0.092398  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.327\n",
      "iter 26160/30000  loss         0.092394  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.329\n",
      "iter 26161/30000  loss         0.092393  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.329\n",
      "iter 26180/30000  loss         0.092390  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.331\n",
      "iter 26181/30000  loss         0.092389  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26200/30000  loss         0.092385  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.333\n",
      "iter 26201/30000  loss         0.092385  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.333\n",
      "iter 26220/30000  loss         0.092381  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.334\n",
      "iter 26221/30000  loss         0.092381  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.335\n",
      "iter 26240/30000  loss         0.092377  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.336\n",
      "iter 26241/30000  loss         0.092377  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.336\n",
      "iter 26260/30000  loss         0.092373  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.338\n",
      "iter 26261/30000  loss         0.092373  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    6.338\n",
      "iter 26280/30000  loss         0.092369  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.340\n",
      "iter 26281/30000  loss         0.092369  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.340\n",
      "iter 26300/30000  loss         0.092365  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.342\n",
      "iter 26301/30000  loss         0.092365  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.342\n",
      "iter 26320/30000  loss         0.092361  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.343\n",
      "iter 26321/30000  loss         0.092361  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.344\n",
      "iter 26340/30000  loss         0.092357  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.345\n",
      "iter 26341/30000  loss         0.092357  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.345\n",
      "iter 26360/30000  loss         0.092353  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.347\n",
      "iter 26361/30000  loss         0.092353  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.347\n",
      "iter 26380/30000  loss         0.092349  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.349\n",
      "iter 26381/30000  loss         0.092349  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.349\n",
      "iter 26400/30000  loss         0.092345  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.351\n",
      "iter 26401/30000  loss         0.092344  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.351\n",
      "iter 26420/30000  loss         0.092341  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.352\n",
      "iter 26421/30000  loss         0.092340  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.352\n",
      "iter 26440/30000  loss         0.092337  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.354\n",
      "iter 26441/30000  loss         0.092336  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.354\n",
      "iter 26460/30000  loss         0.092333  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.356\n",
      "iter 26461/30000  loss         0.092332  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.356\n",
      "iter 26480/30000  loss         0.092329  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.358\n",
      "iter 26481/30000  loss         0.092328  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.358\n",
      "iter 26500/30000  loss         0.092325  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.360\n",
      "iter 26501/30000  loss         0.092324  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.360\n",
      "iter 26520/30000  loss         0.092321  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.361\n",
      "iter 26521/30000  loss         0.092320  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.361\n",
      "iter 26540/30000  loss         0.092317  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.363\n",
      "iter 26541/30000  loss         0.092316  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.363\n",
      "iter 26560/30000  loss         0.092313  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.365\n",
      "iter 26561/30000  loss         0.092312  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.365\n",
      "iter 26580/30000  loss         0.092309  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.367\n",
      "iter 26581/30000  loss         0.092308  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.367\n",
      "iter 26600/30000  loss         0.092305  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.368\n",
      "iter 26601/30000  loss         0.092304  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.369\n",
      "iter 26620/30000  loss         0.092301  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.370\n",
      "iter 26621/30000  loss         0.092300  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.370\n",
      "iter 26640/30000  loss         0.092297  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.372\n",
      "iter 26641/30000  loss         0.092296  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.372\n",
      "iter 26660/30000  loss         0.092293  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.374\n",
      "iter 26661/30000  loss         0.092292  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.374\n",
      "iter 26680/30000  loss         0.092289  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.376\n",
      "iter 26681/30000  loss         0.092289  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.376\n",
      "iter 26700/30000  loss         0.092285  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.377\n",
      "iter 26701/30000  loss         0.092285  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.377\n",
      "iter 26720/30000  loss         0.092281  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.379\n",
      "iter 26721/30000  loss         0.092281  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.379\n",
      "iter 26740/30000  loss         0.092277  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.381\n",
      "iter 26741/30000  loss         0.092277  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.381\n",
      "iter 26760/30000  loss         0.092273  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.383\n",
      "iter 26761/30000  loss         0.092273  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.383\n",
      "iter 26780/30000  loss         0.092269  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.384\n",
      "iter 26781/30000  loss         0.092269  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.384\n",
      "iter 26800/30000  loss         0.092265  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.386\n",
      "iter 26801/30000  loss         0.092265  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.386\n",
      "iter 26820/30000  loss         0.092261  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.388\n",
      "iter 26821/30000  loss         0.092261  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.388\n",
      "iter 26840/30000  loss         0.092257  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.390\n",
      "iter 26841/30000  loss         0.092257  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.390\n",
      "iter 26860/30000  loss         0.092254  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.391\n",
      "iter 26861/30000  loss         0.092253  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.391\n",
      "iter 26880/30000  loss         0.092250  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.393\n",
      "iter 26881/30000  loss         0.092249  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.393\n",
      "iter 26900/30000  loss         0.092246  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.395\n",
      "iter 26901/30000  loss         0.092246  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.395\n",
      "iter 26920/30000  loss         0.092242  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.397\n",
      "iter 26921/30000  loss         0.092242  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.397\n",
      "iter 26940/30000  loss         0.092238  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.398\n",
      "iter 26941/30000  loss         0.092238  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.398\n",
      "iter 26960/30000  loss         0.092234  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.400\n",
      "iter 26961/30000  loss         0.092234  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.400\n",
      "iter 26980/30000  loss         0.092230  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.402\n",
      "iter 26981/30000  loss         0.092230  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27000/30000  loss         0.092227  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.404\n",
      "iter 27001/30000  loss         0.092226  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.404\n",
      "iter 27020/30000  loss         0.092223  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.405\n",
      "iter 27021/30000  loss         0.092222  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    6.405\n",
      "iter 27040/30000  loss         0.092219  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.407\n",
      "iter 27041/30000  loss         0.092219  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.407\n",
      "iter 27060/30000  loss         0.092215  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.409\n",
      "iter 27061/30000  loss         0.092215  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.409\n",
      "iter 27080/30000  loss         0.092211  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.411\n",
      "iter 27081/30000  loss         0.092211  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.411\n",
      "iter 27100/30000  loss         0.092207  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.412\n",
      "iter 27101/30000  loss         0.092207  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.412\n",
      "iter 27120/30000  loss         0.092204  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.414\n",
      "iter 27121/30000  loss         0.092203  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.414\n",
      "iter 27140/30000  loss         0.092200  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.416\n",
      "iter 27141/30000  loss         0.092200  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.416\n",
      "iter 27160/30000  loss         0.092196  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.417\n",
      "iter 27161/30000  loss         0.092196  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.418\n",
      "iter 27180/30000  loss         0.092192  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.419\n",
      "iter 27181/30000  loss         0.092192  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.419\n",
      "iter 27200/30000  loss         0.092188  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.421\n",
      "iter 27201/30000  loss         0.092188  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.421\n",
      "iter 27220/30000  loss         0.092185  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.423\n",
      "iter 27221/30000  loss         0.092185  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.423\n",
      "iter 27240/30000  loss         0.092181  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.424\n",
      "iter 27241/30000  loss         0.092181  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.424\n",
      "iter 27260/30000  loss         0.092177  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.426\n",
      "iter 27261/30000  loss         0.092177  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.426\n",
      "iter 27280/30000  loss         0.092173  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.428\n",
      "iter 27281/30000  loss         0.092173  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.428\n",
      "iter 27300/30000  loss         0.092170  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.430\n",
      "iter 27301/30000  loss         0.092170  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.430\n",
      "iter 27320/30000  loss         0.092166  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.431\n",
      "iter 27321/30000  loss         0.092166  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.431\n",
      "iter 27340/30000  loss         0.092162  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.433\n",
      "iter 27341/30000  loss         0.092162  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.433\n",
      "iter 27360/30000  loss         0.092159  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.435\n",
      "iter 27361/30000  loss         0.092158  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.435\n",
      "iter 27380/30000  loss         0.092155  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.436\n",
      "iter 27381/30000  loss         0.092155  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.436\n",
      "iter 27400/30000  loss         0.092151  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.438\n",
      "iter 27401/30000  loss         0.092151  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.438\n",
      "iter 27420/30000  loss         0.092147  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.440\n",
      "iter 27421/30000  loss         0.092147  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.440\n",
      "iter 27440/30000  loss         0.092144  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.442\n",
      "iter 27441/30000  loss         0.092144  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.442\n",
      "iter 27460/30000  loss         0.092140  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.443\n",
      "iter 27461/30000  loss         0.092140  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.443\n",
      "iter 27480/30000  loss         0.092136  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.445\n",
      "iter 27481/30000  loss         0.092136  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.445\n",
      "iter 27500/30000  loss         0.092133  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.447\n",
      "iter 27501/30000  loss         0.092132  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.447\n",
      "iter 27520/30000  loss         0.092129  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.448\n",
      "iter 27521/30000  loss         0.092129  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.448\n",
      "iter 27540/30000  loss         0.092125  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.450\n",
      "iter 27541/30000  loss         0.092125  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.450\n",
      "iter 27560/30000  loss         0.092122  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.452\n",
      "iter 27561/30000  loss         0.092121  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.452\n",
      "iter 27580/30000  loss         0.092118  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.453\n",
      "iter 27581/30000  loss         0.092118  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.454\n",
      "iter 27600/30000  loss         0.092114  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.455\n",
      "iter 27601/30000  loss         0.092114  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.455\n",
      "iter 27620/30000  loss         0.092111  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.457\n",
      "iter 27621/30000  loss         0.092111  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.457\n",
      "iter 27640/30000  loss         0.092107  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.459\n",
      "iter 27641/30000  loss         0.092107  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.459\n",
      "iter 27660/30000  loss         0.092103  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.460\n",
      "iter 27661/30000  loss         0.092103  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.460\n",
      "iter 27680/30000  loss         0.092100  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.462\n",
      "iter 27681/30000  loss         0.092100  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.462\n",
      "iter 27700/30000  loss         0.092096  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.464\n",
      "iter 27701/30000  loss         0.092096  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.464\n",
      "iter 27720/30000  loss         0.092093  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.465\n",
      "iter 27721/30000  loss         0.092092  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.465\n",
      "iter 27740/30000  loss         0.092089  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.467\n",
      "iter 27741/30000  loss         0.092089  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.467\n",
      "iter 27760/30000  loss         0.092085  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.469\n",
      "iter 27761/30000  loss         0.092085  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.469\n",
      "iter 27780/30000  loss         0.092082  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.470\n",
      "iter 27781/30000  loss         0.092082  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27800/30000  loss         0.092078  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.472\n",
      "iter 27801/30000  loss         0.092078  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    6.472\n",
      "iter 27820/30000  loss         0.092075  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.474\n",
      "iter 27821/30000  loss         0.092075  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.474\n",
      "iter 27840/30000  loss         0.092071  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.475\n",
      "iter 27841/30000  loss         0.092071  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.475\n",
      "iter 27860/30000  loss         0.092068  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.477\n",
      "iter 27861/30000  loss         0.092067  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.477\n",
      "iter 27880/30000  loss         0.092064  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.479\n",
      "iter 27881/30000  loss         0.092064  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.479\n",
      "iter 27900/30000  loss         0.092060  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.480\n",
      "iter 27901/30000  loss         0.092060  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.481\n",
      "iter 27920/30000  loss         0.092057  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.482\n",
      "iter 27921/30000  loss         0.092057  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.482\n",
      "iter 27940/30000  loss         0.092053  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.484\n",
      "iter 27941/30000  loss         0.092053  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.484\n",
      "iter 27960/30000  loss         0.092050  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.485\n",
      "iter 27961/30000  loss         0.092050  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.486\n",
      "iter 27980/30000  loss         0.092046  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.487\n",
      "iter 27981/30000  loss         0.092046  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.487\n",
      "iter 28000/30000  loss         0.092043  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.489\n",
      "iter 28001/30000  loss         0.092043  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.489\n",
      "iter 28020/30000  loss         0.092039  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.490\n",
      "iter 28021/30000  loss         0.092039  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.491\n",
      "iter 28040/30000  loss         0.092036  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.492\n",
      "iter 28041/30000  loss         0.092036  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.492\n",
      "iter 28060/30000  loss         0.092032  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.494\n",
      "iter 28061/30000  loss         0.092032  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.494\n",
      "iter 28080/30000  loss         0.092029  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.495\n",
      "iter 28081/30000  loss         0.092029  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.496\n",
      "iter 28100/30000  loss         0.092025  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.497\n",
      "iter 28101/30000  loss         0.092025  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.497\n",
      "iter 28120/30000  loss         0.092022  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.499\n",
      "iter 28121/30000  loss         0.092022  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.499\n",
      "iter 28140/30000  loss         0.092018  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.500\n",
      "iter 28141/30000  loss         0.092018  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.501\n",
      "iter 28160/30000  loss         0.092015  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.502\n",
      "iter 28161/30000  loss         0.092015  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.502\n",
      "iter 28180/30000  loss         0.092011  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.504\n",
      "iter 28181/30000  loss         0.092011  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.504\n",
      "iter 28200/30000  loss         0.092008  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.505\n",
      "iter 28201/30000  loss         0.092008  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.505\n",
      "iter 28220/30000  loss         0.092004  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.507\n",
      "iter 28221/30000  loss         0.092004  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.507\n",
      "iter 28240/30000  loss         0.092001  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.509\n",
      "iter 28241/30000  loss         0.092001  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.509\n",
      "iter 28260/30000  loss         0.091998  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.510\n",
      "iter 28261/30000  loss         0.091997  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.510\n",
      "iter 28280/30000  loss         0.091994  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.512\n",
      "iter 28281/30000  loss         0.091994  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.512\n",
      "iter 28300/30000  loss         0.091991  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.514\n",
      "iter 28301/30000  loss         0.091990  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.514\n",
      "iter 28320/30000  loss         0.091987  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.515\n",
      "iter 28321/30000  loss         0.091987  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.515\n",
      "iter 28340/30000  loss         0.091984  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.517\n",
      "iter 28341/30000  loss         0.091984  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.517\n",
      "iter 28360/30000  loss         0.091980  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.519\n",
      "iter 28361/30000  loss         0.091980  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.519\n",
      "iter 28380/30000  loss         0.091977  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.520\n",
      "iter 28381/30000  loss         0.091977  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.520\n",
      "iter 28400/30000  loss         0.091974  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.522\n",
      "iter 28401/30000  loss         0.091973  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.522\n",
      "iter 28420/30000  loss         0.091970  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.523\n",
      "iter 28421/30000  loss         0.091970  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.524\n",
      "iter 28440/30000  loss         0.091967  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.525\n",
      "iter 28441/30000  loss         0.091967  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.525\n",
      "iter 28460/30000  loss         0.091963  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.527\n",
      "iter 28461/30000  loss         0.091963  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.527\n",
      "iter 28480/30000  loss         0.091960  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.528\n",
      "iter 28481/30000  loss         0.091960  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.528\n",
      "iter 28500/30000  loss         0.091957  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.530\n",
      "iter 28501/30000  loss         0.091956  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.530\n",
      "iter 28520/30000  loss         0.091953  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.532\n",
      "iter 28521/30000  loss         0.091953  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.532\n",
      "iter 28540/30000  loss         0.091950  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.533\n",
      "iter 28541/30000  loss         0.091950  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.533\n",
      "iter 28560/30000  loss         0.091947  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.535\n",
      "iter 28561/30000  loss         0.091946  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.535\n",
      "iter 28580/30000  loss         0.091943  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.537\n",
      "iter 28581/30000  loss         0.091943  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28600/30000  loss         0.091940  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.538\n",
      "iter 28601/30000  loss         0.091940  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.538\n",
      "iter 28620/30000  loss         0.091937  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.540\n",
      "iter 28621/30000  loss         0.091936  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.540\n",
      "iter 28640/30000  loss         0.091933  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.541\n",
      "iter 28641/30000  loss         0.091933  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    6.541\n",
      "iter 28660/30000  loss         0.091930  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.543\n",
      "iter 28661/30000  loss         0.091930  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.543\n",
      "iter 28680/30000  loss         0.091927  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.545\n",
      "iter 28681/30000  loss         0.091926  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.545\n",
      "iter 28700/30000  loss         0.091923  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.546\n",
      "iter 28701/30000  loss         0.091923  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.546\n",
      "iter 28720/30000  loss         0.091920  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.548\n",
      "iter 28721/30000  loss         0.091920  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.548\n",
      "iter 28740/30000  loss         0.091917  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.549\n",
      "iter 28741/30000  loss         0.091916  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.550\n",
      "iter 28760/30000  loss         0.091913  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.551\n",
      "iter 28761/30000  loss         0.091913  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.551\n",
      "iter 28780/30000  loss         0.091910  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.553\n",
      "iter 28781/30000  loss         0.091910  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.553\n",
      "iter 28800/30000  loss         0.091907  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.554\n",
      "iter 28801/30000  loss         0.091907  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.554\n",
      "iter 28820/30000  loss         0.091903  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.556\n",
      "iter 28821/30000  loss         0.091903  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.556\n",
      "iter 28840/30000  loss         0.091900  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.558\n",
      "iter 28841/30000  loss         0.091900  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.558\n",
      "iter 28860/30000  loss         0.091897  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.559\n",
      "iter 28861/30000  loss         0.091897  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.559\n",
      "iter 28880/30000  loss         0.091894  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.561\n",
      "iter 28881/30000  loss         0.091893  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.561\n",
      "iter 28900/30000  loss         0.091890  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.562\n",
      "iter 28901/30000  loss         0.091890  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.562\n",
      "iter 28920/30000  loss         0.091887  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.564\n",
      "iter 28921/30000  loss         0.091887  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.564\n",
      "iter 28940/30000  loss         0.091884  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.566\n",
      "iter 28941/30000  loss         0.091884  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.566\n",
      "iter 28960/30000  loss         0.091881  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.567\n",
      "iter 28961/30000  loss         0.091880  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.567\n",
      "iter 28980/30000  loss         0.091877  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.569\n",
      "iter 28981/30000  loss         0.091877  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.569\n",
      "iter 29000/30000  loss         0.091874  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.570\n",
      "iter 29001/30000  loss         0.091874  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.570\n",
      "iter 29020/30000  loss         0.091871  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.572\n",
      "iter 29021/30000  loss         0.091871  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.572\n",
      "iter 29040/30000  loss         0.091868  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.574\n",
      "iter 29041/30000  loss         0.091868  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.574\n",
      "iter 29060/30000  loss         0.091864  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.575\n",
      "iter 29061/30000  loss         0.091864  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.575\n",
      "iter 29080/30000  loss         0.091861  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.577\n",
      "iter 29081/30000  loss         0.091861  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.577\n",
      "iter 29100/30000  loss         0.091858  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.578\n",
      "iter 29101/30000  loss         0.091858  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.578\n",
      "iter 29120/30000  loss         0.091855  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.580\n",
      "iter 29121/30000  loss         0.091855  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.580\n",
      "iter 29140/30000  loss         0.091852  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.581\n",
      "iter 29141/30000  loss         0.091851  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.582\n",
      "iter 29160/30000  loss         0.091848  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.583\n",
      "iter 29161/30000  loss         0.091848  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.583\n",
      "iter 29180/30000  loss         0.091845  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.585\n",
      "iter 29181/30000  loss         0.091845  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.585\n",
      "iter 29200/30000  loss         0.091842  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.586\n",
      "iter 29201/30000  loss         0.091842  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.586\n",
      "iter 29220/30000  loss         0.091839  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.588\n",
      "iter 29221/30000  loss         0.091839  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.588\n",
      "iter 29240/30000  loss         0.091836  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.589\n",
      "iter 29241/30000  loss         0.091836  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.589\n",
      "iter 29260/30000  loss         0.091833  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.591\n",
      "iter 29261/30000  loss         0.091832  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.591\n",
      "iter 29280/30000  loss         0.091829  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.593\n",
      "iter 29281/30000  loss         0.091829  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.593\n",
      "iter 29300/30000  loss         0.091826  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.594\n",
      "iter 29301/30000  loss         0.091826  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.594\n",
      "iter 29320/30000  loss         0.091823  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.596\n",
      "iter 29321/30000  loss         0.091823  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.596\n",
      "iter 29340/30000  loss         0.091820  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.597\n",
      "iter 29341/30000  loss         0.091820  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.597\n",
      "iter 29360/30000  loss         0.091817  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.599\n",
      "iter 29361/30000  loss         0.091817  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.599\n",
      "iter 29380/30000  loss         0.091814  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.600\n",
      "iter 29381/30000  loss         0.091814  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29400/30000  loss         0.091811  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.602\n",
      "iter 29401/30000  loss         0.091810  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.602\n",
      "iter 29420/30000  loss         0.091807  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.604\n",
      "iter 29421/30000  loss         0.091807  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.604\n",
      "iter 29440/30000  loss         0.091804  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.605\n",
      "iter 29441/30000  loss         0.091804  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.605\n",
      "iter 29460/30000  loss         0.091801  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.607\n",
      "iter 29461/30000  loss         0.091801  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.607\n",
      "iter 29480/30000  loss         0.091798  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.608\n",
      "iter 29481/30000  loss         0.091798  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.608\n",
      "iter 29500/30000  loss         0.091795  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.610\n",
      "iter 29501/30000  loss         0.091795  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    6.610\n",
      "iter 29520/30000  loss         0.091792  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.611\n",
      "iter 29521/30000  loss         0.091792  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.611\n",
      "iter 29540/30000  loss         0.091789  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.613\n",
      "iter 29541/30000  loss         0.091789  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.613\n",
      "iter 29560/30000  loss         0.091786  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.615\n",
      "iter 29561/30000  loss         0.091786  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.615\n",
      "iter 29580/30000  loss         0.091783  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.616\n",
      "iter 29581/30000  loss         0.091783  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.616\n",
      "iter 29600/30000  loss         0.091780  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.618\n",
      "iter 29601/30000  loss         0.091779  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.618\n",
      "iter 29620/30000  loss         0.091777  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.619\n",
      "iter 29621/30000  loss         0.091776  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.619\n",
      "iter 29640/30000  loss         0.091773  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.621\n",
      "iter 29641/30000  loss         0.091773  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.621\n",
      "iter 29660/30000  loss         0.091770  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.622\n",
      "iter 29661/30000  loss         0.091770  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.622\n",
      "iter 29680/30000  loss         0.091767  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.624\n",
      "iter 29681/30000  loss         0.091767  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.624\n",
      "iter 29700/30000  loss         0.091764  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.625\n",
      "iter 29701/30000  loss         0.091764  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.625\n",
      "iter 29720/30000  loss         0.091761  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.627\n",
      "iter 29721/30000  loss         0.091761  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.627\n",
      "iter 29740/30000  loss         0.091758  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.628\n",
      "iter 29741/30000  loss         0.091758  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.629\n",
      "iter 29760/30000  loss         0.091755  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.630\n",
      "iter 29761/30000  loss         0.091755  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.630\n",
      "iter 29780/30000  loss         0.091752  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.632\n",
      "iter 29781/30000  loss         0.091752  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.632\n",
      "iter 29800/30000  loss         0.091749  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.633\n",
      "iter 29801/30000  loss         0.091749  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.633\n",
      "iter 29820/30000  loss         0.091746  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.635\n",
      "iter 29821/30000  loss         0.091746  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.635\n",
      "iter 29840/30000  loss         0.091743  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.636\n",
      "iter 29841/30000  loss         0.091743  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.636\n",
      "iter 29860/30000  loss         0.091740  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.638\n",
      "iter 29861/30000  loss         0.091740  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.638\n",
      "iter 29880/30000  loss         0.091737  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.639\n",
      "iter 29881/30000  loss         0.091737  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.639\n",
      "iter 29900/30000  loss         0.091734  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.641\n",
      "iter 29901/30000  loss         0.091734  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.641\n",
      "iter 29920/30000  loss         0.091731  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.642\n",
      "iter 29921/30000  loss         0.091731  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.642\n",
      "iter 29940/30000  loss         0.091728  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.644\n",
      "iter 29941/30000  loss         0.091728  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.644\n",
      "iter 29960/30000  loss         0.091725  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.645\n",
      "iter 29961/30000  loss         0.091725  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.646\n",
      "iter 29980/30000  loss         0.091722  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.647\n",
      "iter 29981/30000  loss         0.091722  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.647\n",
      "iter 30000/30000  loss         0.091719  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    6.648\n",
      "Done. Did NOT converge.\n"
     ]
    }
   ],
   "source": [
    "lr_p3_150 = LogisticRegressionGradientDescent(alpha=1, step_size=0.1, init_w_recipe='zeros')\n",
    "lr_p3_150.fit(x_trbw160, y_trbw160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1], dtype=int32)"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros((2,784))\n",
    "lr_p3_150.predict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5812</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>124</td>\n",
       "      <td>5876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        5812   188\n",
       "1.0         124  5876"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lr_p3_150.predict(x_trbw160_2)\n",
    "y_actu = pd.Series(y_tr, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_NF = x_tebw160\n",
    "\n",
    "yproba1_test_N = lr_p3_150.predict_proba(x_tebw160)[:, 1]\n",
    "\n",
    "np.savetxt('yproba1_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.1\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.045961  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         0.944547  avg_L1_norm_grad         0.076056  w[0]    0.000 bias    0.000\n",
      "iter    2/30000  loss         1.351906  avg_L1_norm_grad         0.128900  w[0]    0.000 bias    0.036\n",
      "iter    3/30000  loss         2.200685  avg_L1_norm_grad         0.144238  w[0]    0.000 bias   -0.009\n",
      "iter    4/30000  loss         0.997120  avg_L1_norm_grad         0.095839  w[0]    0.000 bias    0.055\n",
      "iter    5/30000  loss         1.397807  avg_L1_norm_grad         0.108933  w[0]    0.000 bias    0.025\n",
      "iter    6/30000  loss         0.988553  avg_L1_norm_grad         0.097326  w[0]    0.000 bias    0.076\n",
      "iter    7/30000  loss         1.285242  avg_L1_norm_grad         0.099734  w[0]    0.000 bias    0.045\n",
      "iter    8/30000  loss         0.851579  avg_L1_norm_grad         0.082528  w[0]    0.000 bias    0.092\n",
      "iter    9/30000  loss         1.037371  avg_L1_norm_grad         0.084248  w[0]    0.000 bias    0.067\n",
      "iter   10/30000  loss         0.786289  avg_L1_norm_grad         0.077984  w[0]    0.000 bias    0.109\n",
      "iter   11/30000  loss         0.943793  avg_L1_norm_grad         0.079236  w[0]    0.000 bias    0.085\n",
      "iter   12/30000  loss         0.731872  avg_L1_norm_grad         0.074295  w[0]    0.000 bias    0.124\n",
      "iter   13/30000  loss         0.869597  avg_L1_norm_grad         0.075247  w[0]    0.000 bias    0.102\n",
      "iter   14/30000  loss         0.684051  avg_L1_norm_grad         0.070724  w[0]    0.000 bias    0.139\n",
      "iter   15/30000  loss         0.804013  avg_L1_norm_grad         0.071384  w[0]    0.000 bias    0.118\n",
      "iter   16/30000  loss         0.640728  avg_L1_norm_grad         0.066975  w[0]    0.000 bias    0.153\n",
      "iter   17/30000  loss         0.743321  avg_L1_norm_grad         0.067346  w[0]    0.000 bias    0.134\n",
      "iter   18/30000  loss         0.601149  avg_L1_norm_grad         0.062997  w[0]    0.000 bias    0.167\n",
      "iter   19/30000  loss         0.686809  avg_L1_norm_grad         0.063092  w[0]    0.000 bias    0.148\n",
      "iter   20/30000  loss         0.565022  avg_L1_norm_grad         0.058818  w[0]    0.000 bias    0.179\n",
      "iter   21/30000  loss         0.634587  avg_L1_norm_grad         0.058657  w[0]    0.000 bias    0.162\n",
      "iter   40/30000  loss         0.357075  avg_L1_norm_grad         0.016164  w[0]    0.000 bias    0.277\n",
      "iter   41/30000  loss         0.354462  avg_L1_norm_grad         0.014814  w[0]    0.000 bias    0.275\n",
      "iter   60/30000  loss         0.312331  avg_L1_norm_grad         0.002544  w[0]    0.000 bias    0.352\n",
      "iter   61/30000  loss         0.311053  avg_L1_norm_grad         0.002482  w[0]    0.000 bias    0.356\n",
      "iter   80/30000  loss         0.290804  avg_L1_norm_grad         0.002082  w[0]    0.000 bias    0.421\n",
      "iter   81/30000  loss         0.289910  avg_L1_norm_grad         0.002065  w[0]    0.000 bias    0.424\n",
      "iter  100/30000  loss         0.275211  avg_L1_norm_grad         0.001790  w[0]    0.000 bias    0.483\n",
      "iter  101/30000  loss         0.274540  avg_L1_norm_grad         0.001778  w[0]    0.000 bias    0.486\n",
      "iter  120/30000  loss         0.263253  avg_L1_norm_grad         0.001577  w[0]    0.000 bias    0.540\n",
      "iter  121/30000  loss         0.262726  avg_L1_norm_grad         0.001568  w[0]    0.000 bias    0.543\n",
      "iter  140/30000  loss         0.253704  avg_L1_norm_grad         0.001416  w[0]    0.000 bias    0.593\n",
      "iter  141/30000  loss         0.253276  avg_L1_norm_grad         0.001409  w[0]    0.000 bias    0.595\n",
      "iter  160/30000  loss         0.245845  avg_L1_norm_grad         0.001289  w[0]    0.000 bias    0.643\n",
      "iter  161/30000  loss         0.245487  avg_L1_norm_grad         0.001283  w[0]    0.000 bias    0.645\n",
      "iter  180/30000  loss         0.239221  avg_L1_norm_grad         0.001185  w[0]    0.000 bias    0.690\n",
      "iter  181/30000  loss         0.238917  avg_L1_norm_grad         0.001180  w[0]    0.000 bias    0.692\n",
      "iter  200/30000  loss         0.233534  avg_L1_norm_grad         0.001098  w[0]    0.000 bias    0.735\n",
      "iter  201/30000  loss         0.233271  avg_L1_norm_grad         0.001094  w[0]    0.000 bias    0.737\n",
      "iter  220/30000  loss         0.228577  avg_L1_norm_grad         0.001024  w[0]    0.000 bias    0.778\n",
      "iter  221/30000  loss         0.228345  avg_L1_norm_grad         0.001021  w[0]    0.000 bias    0.780\n",
      "iter  240/30000  loss         0.224200  avg_L1_norm_grad         0.000962  w[0]    0.000 bias    0.819\n",
      "iter  241/30000  loss         0.223995  avg_L1_norm_grad         0.000960  w[0]    0.000 bias    0.821\n",
      "iter  260/30000  loss         0.220296  avg_L1_norm_grad         0.000909  w[0]    0.000 bias    0.858\n",
      "iter  261/30000  loss         0.220111  avg_L1_norm_grad         0.000906  w[0]    0.000 bias    0.860\n",
      "iter  280/30000  loss         0.216781  avg_L1_norm_grad         0.000862  w[0]    0.000 bias    0.896\n",
      "iter  281/30000  loss         0.216614  avg_L1_norm_grad         0.000859  w[0]    0.000 bias    0.898\n",
      "iter  300/30000  loss         0.213593  avg_L1_norm_grad         0.000820  w[0]    0.000 bias    0.933\n",
      "iter  301/30000  loss         0.213441  avg_L1_norm_grad         0.000818  w[0]    0.000 bias    0.935\n",
      "iter  320/30000  loss         0.210682  avg_L1_norm_grad         0.000784  w[0]    0.000 bias    0.969\n",
      "iter  321/30000  loss         0.210543  avg_L1_norm_grad         0.000782  w[0]    0.000 bias    0.971\n",
      "iter  340/30000  loss         0.208009  avg_L1_norm_grad         0.000751  w[0]    0.000 bias    1.003\n",
      "iter  341/30000  loss         0.207881  avg_L1_norm_grad         0.000749  w[0]    0.000 bias    1.005\n",
      "iter  360/30000  loss         0.205541  avg_L1_norm_grad         0.000722  w[0]    0.000 bias    1.037\n",
      "iter  361/30000  loss         0.205422  avg_L1_norm_grad         0.000720  w[0]    0.000 bias    1.039\n",
      "iter  380/30000  loss         0.203252  avg_L1_norm_grad         0.000695  w[0]    0.000 bias    1.070\n",
      "iter  381/30000  loss         0.203142  avg_L1_norm_grad         0.000693  w[0]    0.000 bias    1.072\n",
      "iter  400/30000  loss         0.201122  avg_L1_norm_grad         0.000670  w[0]    0.000 bias    1.102\n",
      "iter  401/30000  loss         0.201020  avg_L1_norm_grad         0.000669  w[0]    0.000 bias    1.104\n",
      "iter  420/30000  loss         0.199132  avg_L1_norm_grad         0.000648  w[0]    0.000 bias    1.133\n",
      "iter  421/30000  loss         0.199036  avg_L1_norm_grad         0.000647  w[0]    0.000 bias    1.135\n",
      "iter  440/30000  loss         0.197266  avg_L1_norm_grad         0.000628  w[0]    0.000 bias    1.164\n",
      "iter  441/30000  loss         0.197176  avg_L1_norm_grad         0.000627  w[0]    0.000 bias    1.165\n",
      "iter  460/30000  loss         0.195513  avg_L1_norm_grad         0.000609  w[0]    0.000 bias    1.193\n",
      "iter  461/30000  loss         0.195428  avg_L1_norm_grad         0.000608  w[0]    0.000 bias    1.195\n",
      "iter  480/30000  loss         0.193860  avg_L1_norm_grad         0.000591  w[0]    0.000 bias    1.223\n",
      "iter  481/30000  loss         0.193779  avg_L1_norm_grad         0.000590  w[0]    0.000 bias    1.224\n",
      "iter  500/30000  loss         0.192298  avg_L1_norm_grad         0.000575  w[0]    0.000 bias    1.251\n",
      "iter  501/30000  loss         0.192222  avg_L1_norm_grad         0.000574  w[0]    0.000 bias    1.253\n",
      "iter  520/30000  loss         0.190818  avg_L1_norm_grad         0.000560  w[0]    0.000 bias    1.279\n",
      "iter  521/30000  loss         0.190747  avg_L1_norm_grad         0.000559  w[0]    0.000 bias    1.281\n",
      "iter  540/30000  loss         0.189415  avg_L1_norm_grad         0.000546  w[0]    0.000 bias    1.307\n",
      "iter  541/30000  loss         0.189347  avg_L1_norm_grad         0.000545  w[0]    0.000 bias    1.308\n",
      "iter  560/30000  loss         0.188080  avg_L1_norm_grad         0.000533  w[0]    0.000 bias    1.334\n",
      "iter  561/30000  loss         0.188015  avg_L1_norm_grad         0.000532  w[0]    0.000 bias    1.335\n",
      "iter  580/30000  loss         0.186810  avg_L1_norm_grad         0.000520  w[0]    0.000 bias    1.360\n",
      "iter  581/30000  loss         0.186748  avg_L1_norm_grad         0.000520  w[0]    0.000 bias    1.361\n",
      "iter  600/30000  loss         0.185597  avg_L1_norm_grad         0.000509  w[0]    0.000 bias    1.386\n",
      "iter  601/30000  loss         0.185538  avg_L1_norm_grad         0.000508  w[0]    0.000 bias    1.387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.184439  avg_L1_norm_grad         0.000497  w[0]    0.000 bias    1.412\n",
      "iter  621/30000  loss         0.184382  avg_L1_norm_grad         0.000497  w[0]    0.000 bias    1.413\n",
      "iter  640/30000  loss         0.183331  avg_L1_norm_grad         0.000487  w[0]    0.000 bias    1.437\n",
      "iter  641/30000  loss         0.183277  avg_L1_norm_grad         0.000486  w[0]    0.000 bias    1.438\n",
      "iter  660/30000  loss         0.182269  avg_L1_norm_grad         0.000477  w[0]    0.000 bias    1.462\n",
      "iter  661/30000  loss         0.182217  avg_L1_norm_grad         0.000476  w[0]    0.000 bias    1.463\n",
      "iter  680/30000  loss         0.181251  avg_L1_norm_grad         0.000467  w[0]    0.000 bias    1.486\n",
      "iter  681/30000  loss         0.181201  avg_L1_norm_grad         0.000467  w[0]    0.000 bias    1.487\n",
      "iter  700/30000  loss         0.180273  avg_L1_norm_grad         0.000458  w[0]    0.000 bias    1.510\n",
      "iter  701/30000  loss         0.180225  avg_L1_norm_grad         0.000458  w[0]    0.000 bias    1.511\n",
      "iter  720/30000  loss         0.179332  avg_L1_norm_grad         0.000449  w[0]    0.000 bias    1.534\n",
      "iter  721/30000  loss         0.179286  avg_L1_norm_grad         0.000449  w[0]    0.000 bias    1.535\n",
      "iter  740/30000  loss         0.178427  avg_L1_norm_grad         0.000441  w[0]    0.000 bias    1.557\n",
      "iter  741/30000  loss         0.178383  avg_L1_norm_grad         0.000441  w[0]    0.000 bias    1.558\n",
      "iter  760/30000  loss         0.177556  avg_L1_norm_grad         0.000433  w[0]    0.000 bias    1.580\n",
      "iter  761/30000  loss         0.177513  avg_L1_norm_grad         0.000433  w[0]    0.000 bias    1.581\n",
      "iter  780/30000  loss         0.176715  avg_L1_norm_grad         0.000425  w[0]    0.000 bias    1.602\n",
      "iter  781/30000  loss         0.176674  avg_L1_norm_grad         0.000425  w[0]    0.000 bias    1.603\n",
      "iter  800/30000  loss         0.175903  avg_L1_norm_grad         0.000418  w[0]    0.000 bias    1.625\n",
      "iter  801/30000  loss         0.175864  avg_L1_norm_grad         0.000418  w[0]    0.000 bias    1.626\n",
      "iter  820/30000  loss         0.175120  avg_L1_norm_grad         0.000411  w[0]    0.000 bias    1.647\n",
      "iter  821/30000  loss         0.175081  avg_L1_norm_grad         0.000411  w[0]    0.000 bias    1.648\n",
      "iter  840/30000  loss         0.174362  avg_L1_norm_grad         0.000404  w[0]    0.000 bias    1.668\n",
      "iter  841/30000  loss         0.174325  avg_L1_norm_grad         0.000404  w[0]    0.000 bias    1.669\n",
      "iter  860/30000  loss         0.173629  avg_L1_norm_grad         0.000398  w[0]    0.000 bias    1.690\n",
      "iter  861/30000  loss         0.173593  avg_L1_norm_grad         0.000398  w[0]    0.000 bias    1.691\n",
      "iter  880/30000  loss         0.172919  avg_L1_norm_grad         0.000392  w[0]    0.000 bias    1.711\n",
      "iter  881/30000  loss         0.172884  avg_L1_norm_grad         0.000391  w[0]    0.000 bias    1.712\n",
      "iter  900/30000  loss         0.172232  avg_L1_norm_grad         0.000386  w[0]    0.000 bias    1.732\n",
      "iter  901/30000  loss         0.172198  avg_L1_norm_grad         0.000386  w[0]    0.000 bias    1.733\n",
      "iter  920/30000  loss         0.171565  avg_L1_norm_grad         0.000380  w[0]    0.000 bias    1.752\n",
      "iter  921/30000  loss         0.171532  avg_L1_norm_grad         0.000380  w[0]    0.000 bias    1.753\n",
      "iter  940/30000  loss         0.170918  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.773\n",
      "iter  941/30000  loss         0.170886  avg_L1_norm_grad         0.000374  w[0]    0.000 bias    1.774\n",
      "iter  960/30000  loss         0.170290  avg_L1_norm_grad         0.000369  w[0]    0.000 bias    1.793\n",
      "iter  961/30000  loss         0.170259  avg_L1_norm_grad         0.000369  w[0]    0.000 bias    1.794\n",
      "iter  980/30000  loss         0.169680  avg_L1_norm_grad         0.000364  w[0]    0.000 bias    1.813\n",
      "iter  981/30000  loss         0.169650  avg_L1_norm_grad         0.000364  w[0]    0.000 bias    1.814\n",
      "iter 1000/30000  loss         0.169088  avg_L1_norm_grad         0.000359  w[0]    0.000 bias    1.832\n",
      "iter 1001/30000  loss         0.169059  avg_L1_norm_grad         0.000359  w[0]    0.000 bias    1.833\n",
      "iter 1020/30000  loss         0.168512  avg_L1_norm_grad         0.000354  w[0]    0.000 bias    1.852\n",
      "iter 1021/30000  loss         0.168483  avg_L1_norm_grad         0.000354  w[0]    0.000 bias    1.853\n",
      "iter 1040/30000  loss         0.167951  avg_L1_norm_grad         0.000349  w[0]    0.000 bias    1.871\n",
      "iter 1041/30000  loss         0.167923  avg_L1_norm_grad         0.000349  w[0]    0.000 bias    1.872\n",
      "iter 1060/30000  loss         0.167405  avg_L1_norm_grad         0.000345  w[0]    0.000 bias    1.890\n",
      "iter 1061/30000  loss         0.167378  avg_L1_norm_grad         0.000345  w[0]    0.000 bias    1.891\n",
      "iter 1080/30000  loss         0.166874  avg_L1_norm_grad         0.000341  w[0]    0.000 bias    1.908\n",
      "iter 1081/30000  loss         0.166847  avg_L1_norm_grad         0.000340  w[0]    0.000 bias    1.909\n",
      "iter 1100/30000  loss         0.166356  avg_L1_norm_grad         0.000336  w[0]    0.000 bias    1.927\n",
      "iter 1101/30000  loss         0.166330  avg_L1_norm_grad         0.000336  w[0]    0.000 bias    1.928\n",
      "iter 1120/30000  loss         0.165851  avg_L1_norm_grad         0.000332  w[0]    0.000 bias    1.945\n",
      "iter 1121/30000  loss         0.165826  avg_L1_norm_grad         0.000332  w[0]    0.000 bias    1.946\n",
      "iter 1140/30000  loss         0.165359  avg_L1_norm_grad         0.000328  w[0]    0.000 bias    1.964\n",
      "iter 1141/30000  loss         0.165335  avg_L1_norm_grad         0.000328  w[0]    0.000 bias    1.964\n",
      "iter 1160/30000  loss         0.164879  avg_L1_norm_grad         0.000325  w[0]    0.000 bias    1.981\n",
      "iter 1161/30000  loss         0.164855  avg_L1_norm_grad         0.000324  w[0]    0.000 bias    1.982\n",
      "iter 1180/30000  loss         0.164410  avg_L1_norm_grad         0.000321  w[0]    0.000 bias    1.999\n",
      "iter 1181/30000  loss         0.164387  avg_L1_norm_grad         0.000321  w[0]    0.000 bias    2.000\n",
      "iter 1200/30000  loss         0.163953  avg_L1_norm_grad         0.000317  w[0]    0.000 bias    2.017\n",
      "iter 1201/30000  loss         0.163930  avg_L1_norm_grad         0.000317  w[0]    0.000 bias    2.018\n",
      "iter 1220/30000  loss         0.163506  avg_L1_norm_grad         0.000314  w[0]    0.000 bias    2.034\n",
      "iter 1221/30000  loss         0.163484  avg_L1_norm_grad         0.000313  w[0]    0.000 bias    2.035\n",
      "iter 1240/30000  loss         0.163069  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    2.052\n",
      "iter 1241/30000  loss         0.163048  avg_L1_norm_grad         0.000310  w[0]    0.000 bias    2.052\n",
      "iter 1260/30000  loss         0.162642  avg_L1_norm_grad         0.000307  w[0]    0.000 bias    2.069\n",
      "iter 1261/30000  loss         0.162621  avg_L1_norm_grad         0.000307  w[0]    0.000 bias    2.069\n",
      "iter 1280/30000  loss         0.162225  avg_L1_norm_grad         0.000304  w[0]    0.000 bias    2.086\n",
      "iter 1281/30000  loss         0.162205  avg_L1_norm_grad         0.000303  w[0]    0.000 bias    2.086\n",
      "iter 1300/30000  loss         0.161817  avg_L1_norm_grad         0.000300  w[0]    0.000 bias    2.102\n",
      "iter 1301/30000  loss         0.161797  avg_L1_norm_grad         0.000300  w[0]    0.000 bias    2.103\n",
      "iter 1320/30000  loss         0.161418  avg_L1_norm_grad         0.000297  w[0]    0.000 bias    2.119\n",
      "iter 1321/30000  loss         0.161398  avg_L1_norm_grad         0.000297  w[0]    0.000 bias    2.120\n",
      "iter 1340/30000  loss         0.161027  avg_L1_norm_grad         0.000294  w[0]    0.000 bias    2.135\n",
      "iter 1341/30000  loss         0.161008  avg_L1_norm_grad         0.000294  w[0]    0.000 bias    2.136\n",
      "iter 1360/30000  loss         0.160645  avg_L1_norm_grad         0.000291  w[0]    0.000 bias    2.152\n",
      "iter 1361/30000  loss         0.160626  avg_L1_norm_grad         0.000291  w[0]    0.000 bias    2.152\n",
      "iter 1380/30000  loss         0.160270  avg_L1_norm_grad         0.000289  w[0]    0.000 bias    2.168\n",
      "iter 1381/30000  loss         0.160251  avg_L1_norm_grad         0.000288  w[0]    0.000 bias    2.169\n",
      "iter 1400/30000  loss         0.159903  avg_L1_norm_grad         0.000286  w[0]    0.000 bias    2.184\n",
      "iter 1401/30000  loss         0.159885  avg_L1_norm_grad         0.000286  w[0]    0.000 bias    2.185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.159543  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    2.200\n",
      "iter 1421/30000  loss         0.159526  avg_L1_norm_grad         0.000283  w[0]    0.000 bias    2.200\n",
      "iter 1440/30000  loss         0.159191  avg_L1_norm_grad         0.000280  w[0]    0.000 bias    2.215\n",
      "iter 1441/30000  loss         0.159174  avg_L1_norm_grad         0.000280  w[0]    0.000 bias    2.216\n",
      "iter 1460/30000  loss         0.158846  avg_L1_norm_grad         0.000278  w[0]    0.000 bias    2.231\n",
      "iter 1461/30000  loss         0.158828  avg_L1_norm_grad         0.000278  w[0]    0.000 bias    2.232\n",
      "iter 1480/30000  loss         0.158507  avg_L1_norm_grad         0.000275  w[0]    0.000 bias    2.246\n",
      "iter 1481/30000  loss         0.158490  avg_L1_norm_grad         0.000275  w[0]    0.000 bias    2.247\n",
      "iter 1500/30000  loss         0.158175  avg_L1_norm_grad         0.000273  w[0]    0.000 bias    2.262\n",
      "iter 1501/30000  loss         0.158158  avg_L1_norm_grad         0.000273  w[0]    0.000 bias    2.262\n",
      "iter 1520/30000  loss         0.157849  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    2.277\n",
      "iter 1521/30000  loss         0.157832  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    2.277\n",
      "iter 1540/30000  loss         0.157529  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    2.292\n",
      "iter 1541/30000  loss         0.157513  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    2.292\n",
      "iter 1560/30000  loss         0.157215  avg_L1_norm_grad         0.000266  w[0]    0.000 bias    2.307\n",
      "iter 1561/30000  loss         0.157199  avg_L1_norm_grad         0.000265  w[0]    0.000 bias    2.307\n",
      "iter 1580/30000  loss         0.156907  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    2.321\n",
      "iter 1581/30000  loss         0.156891  avg_L1_norm_grad         0.000263  w[0]    0.000 bias    2.322\n",
      "iter 1600/30000  loss         0.156604  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    2.336\n",
      "iter 1601/30000  loss         0.156589  avg_L1_norm_grad         0.000261  w[0]    0.000 bias    2.337\n",
      "iter 1620/30000  loss         0.156307  avg_L1_norm_grad         0.000259  w[0]    0.000 bias    2.351\n",
      "iter 1621/30000  loss         0.156292  avg_L1_norm_grad         0.000259  w[0]    0.000 bias    2.351\n",
      "iter 1640/30000  loss         0.156015  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    2.365\n",
      "iter 1641/30000  loss         0.156000  avg_L1_norm_grad         0.000257  w[0]    0.000 bias    2.366\n",
      "iter 1660/30000  loss         0.155728  avg_L1_norm_grad         0.000255  w[0]    0.000 bias    2.379\n",
      "iter 1661/30000  loss         0.155714  avg_L1_norm_grad         0.000255  w[0]    0.000 bias    2.380\n",
      "iter 1680/30000  loss         0.155446  avg_L1_norm_grad         0.000253  w[0]    0.000 bias    2.393\n",
      "iter 1681/30000  loss         0.155432  avg_L1_norm_grad         0.000252  w[0]    0.000 bias    2.394\n",
      "iter 1700/30000  loss         0.155169  avg_L1_norm_grad         0.000251  w[0]    0.000 bias    2.407\n",
      "iter 1701/30000  loss         0.155155  avg_L1_norm_grad         0.000250  w[0]    0.000 bias    2.408\n",
      "iter 1720/30000  loss         0.154896  avg_L1_norm_grad         0.000249  w[0]    0.000 bias    2.421\n",
      "iter 1721/30000  loss         0.154883  avg_L1_norm_grad         0.000248  w[0]    0.000 bias    2.422\n",
      "iter 1740/30000  loss         0.154628  avg_L1_norm_grad         0.000247  w[0]    0.000 bias    2.435\n",
      "iter 1741/30000  loss         0.154615  avg_L1_norm_grad         0.000246  w[0]    0.000 bias    2.436\n",
      "iter 1760/30000  loss         0.154365  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.449\n",
      "iter 1761/30000  loss         0.154352  avg_L1_norm_grad         0.000245  w[0]    0.000 bias    2.450\n",
      "iter 1780/30000  loss         0.154106  avg_L1_norm_grad         0.000243  w[0]    0.000 bias    2.463\n",
      "iter 1781/30000  loss         0.154093  avg_L1_norm_grad         0.000243  w[0]    0.000 bias    2.463\n",
      "iter 1800/30000  loss         0.153851  avg_L1_norm_grad         0.000241  w[0]    0.000 bias    2.476\n",
      "iter 1801/30000  loss         0.153838  avg_L1_norm_grad         0.000241  w[0]    0.000 bias    2.477\n",
      "iter 1820/30000  loss         0.153600  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    2.489\n",
      "iter 1821/30000  loss         0.153587  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    2.490\n",
      "iter 1840/30000  loss         0.153353  avg_L1_norm_grad         0.000237  w[0]    0.000 bias    2.503\n",
      "iter 1841/30000  loss         0.153341  avg_L1_norm_grad         0.000237  w[0]    0.000 bias    2.503\n",
      "iter 1860/30000  loss         0.153110  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    2.516\n",
      "iter 1861/30000  loss         0.153098  avg_L1_norm_grad         0.000235  w[0]    0.000 bias    2.517\n",
      "iter 1880/30000  loss         0.152871  avg_L1_norm_grad         0.000234  w[0]    0.000 bias    2.529\n",
      "iter 1881/30000  loss         0.152859  avg_L1_norm_grad         0.000234  w[0]    0.000 bias    2.530\n",
      "iter 1900/30000  loss         0.152635  avg_L1_norm_grad         0.000232  w[0]    0.000 bias    2.542\n",
      "iter 1901/30000  loss         0.152623  avg_L1_norm_grad         0.000232  w[0]    0.000 bias    2.543\n",
      "iter 1920/30000  loss         0.152403  avg_L1_norm_grad         0.000230  w[0]    0.000 bias    2.555\n",
      "iter 1921/30000  loss         0.152392  avg_L1_norm_grad         0.000230  w[0]    0.000 bias    2.556\n",
      "iter 1940/30000  loss         0.152175  avg_L1_norm_grad         0.000229  w[0]    0.000 bias    2.568\n",
      "iter 1941/30000  loss         0.152163  avg_L1_norm_grad         0.000229  w[0]    0.000 bias    2.568\n",
      "iter 1960/30000  loss         0.151950  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    2.581\n",
      "iter 1961/30000  loss         0.151938  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    2.581\n",
      "iter 1980/30000  loss         0.151728  avg_L1_norm_grad         0.000226  w[0]    0.000 bias    2.593\n",
      "iter 1981/30000  loss         0.151717  avg_L1_norm_grad         0.000225  w[0]    0.000 bias    2.594\n",
      "iter 2000/30000  loss         0.151509  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    2.606\n",
      "iter 2001/30000  loss         0.151499  avg_L1_norm_grad         0.000224  w[0]    0.000 bias    2.606\n",
      "iter 2020/30000  loss         0.151294  avg_L1_norm_grad         0.000222  w[0]    0.000 bias    2.618\n",
      "iter 2021/30000  loss         0.151284  avg_L1_norm_grad         0.000222  w[0]    0.000 bias    2.619\n",
      "iter 2040/30000  loss         0.151082  avg_L1_norm_grad         0.000221  w[0]    0.000 bias    2.630\n",
      "iter 2041/30000  loss         0.151072  avg_L1_norm_grad         0.000221  w[0]    0.000 bias    2.631\n",
      "iter 2060/30000  loss         0.150873  avg_L1_norm_grad         0.000219  w[0]    0.000 bias    2.643\n",
      "iter 2061/30000  loss         0.150863  avg_L1_norm_grad         0.000219  w[0]    0.000 bias    2.643\n",
      "iter 2080/30000  loss         0.150667  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.655\n",
      "iter 2081/30000  loss         0.150657  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.656\n",
      "iter 2100/30000  loss         0.150464  avg_L1_norm_grad         0.000216  w[0]    0.000 bias    2.667\n",
      "iter 2101/30000  loss         0.150454  avg_L1_norm_grad         0.000216  w[0]    0.000 bias    2.668\n",
      "iter 2120/30000  loss         0.150263  avg_L1_norm_grad         0.000215  w[0]    0.000 bias    2.679\n",
      "iter 2121/30000  loss         0.150253  avg_L1_norm_grad         0.000215  w[0]    0.000 bias    2.680\n",
      "iter 2140/30000  loss         0.150066  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    2.691\n",
      "iter 2141/30000  loss         0.150056  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    2.692\n",
      "iter 2160/30000  loss         0.149871  avg_L1_norm_grad         0.000212  w[0]    0.000 bias    2.703\n",
      "iter 2161/30000  loss         0.149861  avg_L1_norm_grad         0.000212  w[0]    0.000 bias    2.703\n",
      "iter 2180/30000  loss         0.149678  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.715\n",
      "iter 2181/30000  loss         0.149669  avg_L1_norm_grad         0.000211  w[0]    0.000 bias    2.715\n",
      "iter 2200/30000  loss         0.149489  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    2.726\n",
      "iter 2201/30000  loss         0.149479  avg_L1_norm_grad         0.000210  w[0]    0.000 bias    2.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.149302  avg_L1_norm_grad         0.000208  w[0]    0.000 bias    2.738\n",
      "iter 2221/30000  loss         0.149292  avg_L1_norm_grad         0.000208  w[0]    0.000 bias    2.738\n",
      "iter 2240/30000  loss         0.149117  avg_L1_norm_grad         0.000207  w[0]    0.000 bias    2.749\n",
      "iter 2241/30000  loss         0.149108  avg_L1_norm_grad         0.000207  w[0]    0.000 bias    2.750\n",
      "iter 2260/30000  loss         0.148935  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    2.761\n",
      "iter 2261/30000  loss         0.148926  avg_L1_norm_grad         0.000206  w[0]    0.000 bias    2.761\n",
      "iter 2280/30000  loss         0.148755  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    2.772\n",
      "iter 2281/30000  loss         0.148746  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    2.773\n",
      "iter 2300/30000  loss         0.148577  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    2.783\n",
      "iter 2301/30000  loss         0.148569  avg_L1_norm_grad         0.000203  w[0]    0.000 bias    2.784\n",
      "iter 2320/30000  loss         0.148402  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    2.795\n",
      "iter 2321/30000  loss         0.148394  avg_L1_norm_grad         0.000202  w[0]    0.000 bias    2.795\n",
      "iter 2340/30000  loss         0.148229  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    2.806\n",
      "iter 2341/30000  loss         0.148221  avg_L1_norm_grad         0.000201  w[0]    0.000 bias    2.806\n",
      "iter 2360/30000  loss         0.148058  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    2.817\n",
      "iter 2361/30000  loss         0.148050  avg_L1_norm_grad         0.000199  w[0]    0.000 bias    2.817\n",
      "iter 2380/30000  loss         0.147890  avg_L1_norm_grad         0.000198  w[0]    0.000 bias    2.828\n",
      "iter 2381/30000  loss         0.147881  avg_L1_norm_grad         0.000198  w[0]    0.000 bias    2.828\n",
      "iter 2400/30000  loss         0.147723  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    2.839\n",
      "iter 2401/30000  loss         0.147715  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    2.839\n",
      "iter 2420/30000  loss         0.147559  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    2.850\n",
      "iter 2421/30000  loss         0.147551  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    2.850\n",
      "iter 2440/30000  loss         0.147396  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    2.861\n",
      "iter 2441/30000  loss         0.147388  avg_L1_norm_grad         0.000195  w[0]    0.000 bias    2.861\n",
      "iter 2460/30000  loss         0.147236  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.871\n",
      "iter 2461/30000  loss         0.147228  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.872\n",
      "iter 2480/30000  loss         0.147077  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.882\n",
      "iter 2481/30000  loss         0.147070  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.882\n",
      "iter 2500/30000  loss         0.146921  avg_L1_norm_grad         0.000192  w[0]    0.000 bias    2.892\n",
      "iter 2501/30000  loss         0.146913  avg_L1_norm_grad         0.000191  w[0]    0.000 bias    2.893\n",
      "iter 2520/30000  loss         0.146766  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.903\n",
      "iter 2521/30000  loss         0.146758  avg_L1_norm_grad         0.000190  w[0]    0.000 bias    2.904\n",
      "iter 2540/30000  loss         0.146613  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.913\n",
      "iter 2541/30000  loss         0.146606  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.914\n",
      "iter 2560/30000  loss         0.146462  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.924\n",
      "iter 2561/30000  loss         0.146455  avg_L1_norm_grad         0.000188  w[0]    0.000 bias    2.924\n",
      "iter 2580/30000  loss         0.146313  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.934\n",
      "iter 2581/30000  loss         0.146305  avg_L1_norm_grad         0.000187  w[0]    0.000 bias    2.935\n",
      "iter 2600/30000  loss         0.146165  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.944\n",
      "iter 2601/30000  loss         0.146158  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.945\n",
      "iter 2620/30000  loss         0.146019  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.955\n",
      "iter 2621/30000  loss         0.146012  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.955\n",
      "iter 2640/30000  loss         0.145875  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.965\n",
      "iter 2641/30000  loss         0.145868  avg_L1_norm_grad         0.000184  w[0]    0.000 bias    2.965\n",
      "iter 2660/30000  loss         0.145733  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.975\n",
      "iter 2661/30000  loss         0.145725  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.975\n",
      "iter 2680/30000  loss         0.145592  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.985\n",
      "iter 2681/30000  loss         0.145585  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.985\n",
      "iter 2700/30000  loss         0.145452  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.995\n",
      "iter 2701/30000  loss         0.145445  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    2.995\n",
      "iter 2720/30000  loss         0.145314  avg_L1_norm_grad         0.000181  w[0]    0.000 bias    3.005\n",
      "iter 2721/30000  loss         0.145308  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    3.005\n",
      "iter 2740/30000  loss         0.145178  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    3.014\n",
      "iter 2741/30000  loss         0.145171  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    3.015\n",
      "iter 2760/30000  loss         0.145043  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    3.024\n",
      "iter 2761/30000  loss         0.145037  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    3.025\n",
      "iter 2780/30000  loss         0.144910  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    3.034\n",
      "iter 2781/30000  loss         0.144903  avg_L1_norm_grad         0.000178  w[0]    0.000 bias    3.034\n",
      "iter 2800/30000  loss         0.144778  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    3.044\n",
      "iter 2801/30000  loss         0.144772  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    3.044\n",
      "iter 2820/30000  loss         0.144648  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    3.053\n",
      "iter 2821/30000  loss         0.144641  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    3.054\n",
      "iter 2840/30000  loss         0.144519  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    3.063\n",
      "iter 2841/30000  loss         0.144512  avg_L1_norm_grad         0.000175  w[0]    0.000 bias    3.063\n",
      "iter 2860/30000  loss         0.144391  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    3.072\n",
      "iter 2861/30000  loss         0.144385  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    3.073\n",
      "iter 2880/30000  loss         0.144265  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.082\n",
      "iter 2881/30000  loss         0.144259  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.082\n",
      "iter 2900/30000  loss         0.144140  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.091\n",
      "iter 2901/30000  loss         0.144134  avg_L1_norm_grad         0.000173  w[0]    0.000 bias    3.092\n",
      "iter 2920/30000  loss         0.144016  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.100\n",
      "iter 2921/30000  loss         0.144010  avg_L1_norm_grad         0.000172  w[0]    0.000 bias    3.101\n",
      "iter 2940/30000  loss         0.143894  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    3.110\n",
      "iter 2941/30000  loss         0.143888  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    3.110\n",
      "iter 2960/30000  loss         0.143773  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.119\n",
      "iter 2961/30000  loss         0.143767  avg_L1_norm_grad         0.000170  w[0]    0.000 bias    3.119\n",
      "iter 2980/30000  loss         0.143653  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.128\n",
      "iter 2981/30000  loss         0.143647  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.128\n",
      "iter 3000/30000  loss         0.143535  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    3.137\n",
      "iter 3001/30000  loss         0.143529  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    3.138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.143417  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    3.146\n",
      "iter 3021/30000  loss         0.143411  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    3.147\n",
      "iter 3040/30000  loss         0.143301  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.155\n",
      "iter 3041/30000  loss         0.143295  avg_L1_norm_grad         0.000167  w[0]    0.000 bias    3.156\n",
      "iter 3060/30000  loss         0.143186  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    3.164\n",
      "iter 3061/30000  loss         0.143180  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    3.165\n",
      "iter 3080/30000  loss         0.143072  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.173\n",
      "iter 3081/30000  loss         0.143066  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.173\n",
      "iter 3100/30000  loss         0.142959  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.182\n",
      "iter 3101/30000  loss         0.142954  avg_L1_norm_grad         0.000165  w[0]    0.000 bias    3.182\n",
      "iter 3120/30000  loss         0.142848  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.191\n",
      "iter 3121/30000  loss         0.142842  avg_L1_norm_grad         0.000164  w[0]    0.000 bias    3.191\n",
      "iter 3140/30000  loss         0.142737  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    3.199\n",
      "iter 3141/30000  loss         0.142732  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    3.200\n",
      "iter 3160/30000  loss         0.142628  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.208\n",
      "iter 3161/30000  loss         0.142622  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.209\n",
      "iter 3180/30000  loss         0.142519  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.217\n",
      "iter 3181/30000  loss         0.142514  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    3.217\n",
      "iter 3200/30000  loss         0.142412  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.225\n",
      "iter 3201/30000  loss         0.142407  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    3.226\n",
      "iter 3220/30000  loss         0.142306  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.234\n",
      "iter 3221/30000  loss         0.142301  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.234\n",
      "iter 3240/30000  loss         0.142201  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.243\n",
      "iter 3241/30000  loss         0.142195  avg_L1_norm_grad         0.000160  w[0]    0.000 bias    3.243\n",
      "iter 3260/30000  loss         0.142096  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.251\n",
      "iter 3261/30000  loss         0.142091  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    3.251\n",
      "iter 3280/30000  loss         0.141993  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.259\n",
      "iter 3281/30000  loss         0.141988  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.260\n",
      "iter 3300/30000  loss         0.141891  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.268\n",
      "iter 3301/30000  loss         0.141886  avg_L1_norm_grad         0.000158  w[0]    0.000 bias    3.268\n",
      "iter 3320/30000  loss         0.141789  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.276\n",
      "iter 3321/30000  loss         0.141784  avg_L1_norm_grad         0.000157  w[0]    0.000 bias    3.277\n",
      "iter 3340/30000  loss         0.141689  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.284\n",
      "iter 3341/30000  loss         0.141684  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.285\n",
      "iter 3360/30000  loss         0.141590  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.293\n",
      "iter 3361/30000  loss         0.141585  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    3.293\n",
      "iter 3380/30000  loss         0.141491  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.301\n",
      "iter 3381/30000  loss         0.141486  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    3.301\n",
      "iter 3400/30000  loss         0.141394  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.309\n",
      "iter 3401/30000  loss         0.141389  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.309\n",
      "iter 3420/30000  loss         0.141297  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.317\n",
      "iter 3421/30000  loss         0.141292  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    3.318\n",
      "iter 3440/30000  loss         0.141201  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.325\n",
      "iter 3441/30000  loss         0.141196  avg_L1_norm_grad         0.000153  w[0]    0.000 bias    3.326\n",
      "iter 3460/30000  loss         0.141106  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.333\n",
      "iter 3461/30000  loss         0.141101  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.334\n",
      "iter 3480/30000  loss         0.141012  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.341\n",
      "iter 3481/30000  loss         0.141007  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    3.342\n",
      "iter 3500/30000  loss         0.140919  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.349\n",
      "iter 3501/30000  loss         0.140914  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.350\n",
      "iter 3520/30000  loss         0.140826  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.357\n",
      "iter 3521/30000  loss         0.140822  avg_L1_norm_grad         0.000151  w[0]    0.000 bias    3.357\n",
      "iter 3540/30000  loss         0.140735  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.365\n",
      "iter 3541/30000  loss         0.140730  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    3.365\n",
      "iter 3560/30000  loss         0.140644  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.373\n",
      "iter 3561/30000  loss         0.140639  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.373\n",
      "iter 3580/30000  loss         0.140554  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.381\n",
      "iter 3581/30000  loss         0.140549  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    3.381\n",
      "iter 3600/30000  loss         0.140464  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.388\n",
      "iter 3601/30000  loss         0.140460  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.389\n",
      "iter 3620/30000  loss         0.140376  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.396\n",
      "iter 3621/30000  loss         0.140372  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    3.396\n",
      "iter 3640/30000  loss         0.140288  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.404\n",
      "iter 3641/30000  loss         0.140284  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.404\n",
      "iter 3660/30000  loss         0.140201  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.411\n",
      "iter 3661/30000  loss         0.140197  avg_L1_norm_grad         0.000147  w[0]    0.000 bias    3.412\n",
      "iter 3680/30000  loss         0.140115  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.419\n",
      "iter 3681/30000  loss         0.140111  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    3.419\n",
      "iter 3700/30000  loss         0.140030  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.426\n",
      "iter 3701/30000  loss         0.140026  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.427\n",
      "iter 3720/30000  loss         0.139945  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.434\n",
      "iter 3721/30000  loss         0.139941  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    3.434\n",
      "iter 3740/30000  loss         0.139861  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.441\n",
      "iter 3741/30000  loss         0.139857  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.442\n",
      "iter 3760/30000  loss         0.139778  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.449\n",
      "iter 3761/30000  loss         0.139774  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    3.449\n",
      "iter 3780/30000  loss         0.139695  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.456\n",
      "iter 3781/30000  loss         0.139691  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.456\n",
      "iter 3800/30000  loss         0.139613  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.463\n",
      "iter 3801/30000  loss         0.139609  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    3.464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.139532  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.471\n",
      "iter 3821/30000  loss         0.139528  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.471\n",
      "iter 3840/30000  loss         0.139452  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.478\n",
      "iter 3841/30000  loss         0.139448  avg_L1_norm_grad         0.000142  w[0]    0.000 bias    3.478\n",
      "iter 3860/30000  loss         0.139372  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.485\n",
      "iter 3861/30000  loss         0.139368  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.486\n",
      "iter 3880/30000  loss         0.139293  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.493\n",
      "iter 3881/30000  loss         0.139289  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    3.493\n",
      "iter 3900/30000  loss         0.139214  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.500\n",
      "iter 3901/30000  loss         0.139210  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.500\n",
      "iter 3920/30000  loss         0.139136  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.507\n",
      "iter 3921/30000  loss         0.139132  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    3.507\n",
      "iter 3940/30000  loss         0.139059  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.514\n",
      "iter 3941/30000  loss         0.139055  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.514\n",
      "iter 3960/30000  loss         0.138982  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.521\n",
      "iter 3961/30000  loss         0.138978  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    3.521\n",
      "iter 3980/30000  loss         0.138906  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.528\n",
      "iter 3981/30000  loss         0.138902  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.528\n",
      "iter 4000/30000  loss         0.138831  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.535\n",
      "iter 4001/30000  loss         0.138827  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    3.535\n",
      "iter 4020/30000  loss         0.138756  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.542\n",
      "iter 4021/30000  loss         0.138752  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.542\n",
      "iter 4040/30000  loss         0.138681  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.549\n",
      "iter 4041/30000  loss         0.138678  avg_L1_norm_grad         0.000137  w[0]    0.000 bias    3.549\n",
      "iter 4060/30000  loss         0.138608  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.556\n",
      "iter 4061/30000  loss         0.138604  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.556\n",
      "iter 4080/30000  loss         0.138535  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.563\n",
      "iter 4081/30000  loss         0.138531  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    3.563\n",
      "iter 4100/30000  loss         0.138462  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.569\n",
      "iter 4101/30000  loss         0.138459  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.570\n",
      "iter 4120/30000  loss         0.138390  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.576\n",
      "iter 4121/30000  loss         0.138387  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    3.577\n",
      "iter 4140/30000  loss         0.138319  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.583\n",
      "iter 4141/30000  loss         0.138315  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.583\n",
      "iter 4160/30000  loss         0.138248  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.590\n",
      "iter 4161/30000  loss         0.138244  avg_L1_norm_grad         0.000134  w[0]    0.000 bias    3.590\n",
      "iter 4180/30000  loss         0.138178  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.596\n",
      "iter 4181/30000  loss         0.138174  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.597\n",
      "iter 4200/30000  loss         0.138108  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.603\n",
      "iter 4201/30000  loss         0.138104  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.603\n",
      "iter 4220/30000  loss         0.138039  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    3.610\n",
      "iter 4221/30000  loss         0.138035  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.610\n",
      "iter 4240/30000  loss         0.137970  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.616\n",
      "iter 4241/30000  loss         0.137967  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.617\n",
      "iter 4260/30000  loss         0.137902  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.623\n",
      "iter 4261/30000  loss         0.137898  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    3.623\n",
      "iter 4280/30000  loss         0.137834  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.629\n",
      "iter 4281/30000  loss         0.137831  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.630\n",
      "iter 4300/30000  loss         0.137767  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.636\n",
      "iter 4301/30000  loss         0.137764  avg_L1_norm_grad         0.000131  w[0]    0.000 bias    3.636\n",
      "iter 4320/30000  loss         0.137700  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.642\n",
      "iter 4321/30000  loss         0.137697  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.643\n",
      "iter 4340/30000  loss         0.137634  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.649\n",
      "iter 4341/30000  loss         0.137631  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    3.649\n",
      "iter 4360/30000  loss         0.137569  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.655\n",
      "iter 4361/30000  loss         0.137565  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.656\n",
      "iter 4380/30000  loss         0.137503  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.662\n",
      "iter 4381/30000  loss         0.137500  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.662\n",
      "iter 4400/30000  loss         0.137439  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.668\n",
      "iter 4401/30000  loss         0.137435  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    3.668\n",
      "iter 4420/30000  loss         0.137374  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.674\n",
      "iter 4421/30000  loss         0.137371  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.675\n",
      "iter 4440/30000  loss         0.137311  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.681\n",
      "iter 4441/30000  loss         0.137308  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    3.681\n",
      "iter 4460/30000  loss         0.137247  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.687\n",
      "iter 4461/30000  loss         0.137244  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.687\n",
      "iter 4480/30000  loss         0.137185  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.693\n",
      "iter 4481/30000  loss         0.137181  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.693\n",
      "iter 4500/30000  loss         0.137122  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.699\n",
      "iter 4501/30000  loss         0.137119  avg_L1_norm_grad         0.000127  w[0]    0.000 bias    3.700\n",
      "iter 4520/30000  loss         0.137060  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.706\n",
      "iter 4521/30000  loss         0.137057  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.706\n",
      "iter 4540/30000  loss         0.136999  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.712\n",
      "iter 4541/30000  loss         0.136996  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    3.712\n",
      "iter 4560/30000  loss         0.136938  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.718\n",
      "iter 4561/30000  loss         0.136935  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.718\n",
      "iter 4580/30000  loss         0.136877  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.724\n",
      "iter 4581/30000  loss         0.136874  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.724\n",
      "iter 4600/30000  loss         0.136817  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.730\n",
      "iter 4601/30000  loss         0.136814  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    3.730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4620/30000  loss         0.136757  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.736\n",
      "iter 4621/30000  loss         0.136754  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.736\n",
      "iter 4640/30000  loss         0.136698  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.742\n",
      "iter 4641/30000  loss         0.136695  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.742\n",
      "iter 4660/30000  loss         0.136639  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.748\n",
      "iter 4661/30000  loss         0.136636  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    3.748\n",
      "iter 4680/30000  loss         0.136580  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.754\n",
      "iter 4681/30000  loss         0.136578  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.754\n",
      "iter 4700/30000  loss         0.136522  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.760\n",
      "iter 4701/30000  loss         0.136520  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    3.760\n",
      "iter 4720/30000  loss         0.136465  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.766\n",
      "iter 4721/30000  loss         0.136462  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.766\n",
      "iter 4740/30000  loss         0.136407  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.772\n",
      "iter 4741/30000  loss         0.136405  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.772\n",
      "iter 4760/30000  loss         0.136351  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.778\n",
      "iter 4761/30000  loss         0.136348  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    3.778\n",
      "iter 4780/30000  loss         0.136294  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.783\n",
      "iter 4781/30000  loss         0.136291  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.784\n",
      "iter 4800/30000  loss         0.136238  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.789\n",
      "iter 4801/30000  loss         0.136235  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.790\n",
      "iter 4820/30000  loss         0.136182  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.795\n",
      "iter 4821/30000  loss         0.136179  avg_L1_norm_grad         0.000121  w[0]    0.000 bias    3.795\n",
      "iter 4840/30000  loss         0.136127  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.801\n",
      "iter 4841/30000  loss         0.136124  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.801\n",
      "iter 4860/30000  loss         0.136072  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.806\n",
      "iter 4861/30000  loss         0.136069  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.807\n",
      "iter 4880/30000  loss         0.136017  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.812\n",
      "iter 4881/30000  loss         0.136014  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    3.812\n",
      "iter 4900/30000  loss         0.135963  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.818\n",
      "iter 4901/30000  loss         0.135960  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.818\n",
      "iter 4920/30000  loss         0.135909  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.824\n",
      "iter 4921/30000  loss         0.135906  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    3.824\n",
      "iter 4940/30000  loss         0.135856  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.829\n",
      "iter 4941/30000  loss         0.135853  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.829\n",
      "iter 4960/30000  loss         0.135802  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.835\n",
      "iter 4961/30000  loss         0.135800  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.835\n",
      "iter 4980/30000  loss         0.135750  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.840\n",
      "iter 4981/30000  loss         0.135747  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    3.841\n",
      "iter 5000/30000  loss         0.135697  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.846\n",
      "iter 5001/30000  loss         0.135694  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.846\n",
      "iter 5020/30000  loss         0.135645  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.851\n",
      "iter 5021/30000  loss         0.135642  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.852\n",
      "iter 5040/30000  loss         0.135593  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.857\n",
      "iter 5041/30000  loss         0.135591  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    3.857\n",
      "iter 5060/30000  loss         0.135542  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.862\n",
      "iter 5061/30000  loss         0.135539  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.863\n",
      "iter 5080/30000  loss         0.135491  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.868\n",
      "iter 5081/30000  loss         0.135488  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.868\n",
      "iter 5100/30000  loss         0.135440  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.873\n",
      "iter 5101/30000  loss         0.135437  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    3.874\n",
      "iter 5120/30000  loss         0.135389  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.879\n",
      "iter 5121/30000  loss         0.135387  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.879\n",
      "iter 5140/30000  loss         0.135339  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.884\n",
      "iter 5141/30000  loss         0.135337  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.884\n",
      "iter 5160/30000  loss         0.135290  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.889\n",
      "iter 5161/30000  loss         0.135287  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.890\n",
      "iter 5180/30000  loss         0.135240  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.895\n",
      "iter 5181/30000  loss         0.135238  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    3.895\n",
      "iter 5200/30000  loss         0.135191  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.900\n",
      "iter 5201/30000  loss         0.135188  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.900\n",
      "iter 5220/30000  loss         0.135142  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.905\n",
      "iter 5221/30000  loss         0.135140  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.906\n",
      "iter 5240/30000  loss         0.135093  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.911\n",
      "iter 5241/30000  loss         0.135091  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    3.911\n",
      "iter 5260/30000  loss         0.135045  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.916\n",
      "iter 5261/30000  loss         0.135043  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.916\n",
      "iter 5280/30000  loss         0.134997  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.921\n",
      "iter 5281/30000  loss         0.134995  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.921\n",
      "iter 5300/30000  loss         0.134950  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.926\n",
      "iter 5301/30000  loss         0.134947  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    3.927\n",
      "iter 5320/30000  loss         0.134902  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.931\n",
      "iter 5321/30000  loss         0.134900  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.932\n",
      "iter 5340/30000  loss         0.134855  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.937\n",
      "iter 5341/30000  loss         0.134853  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.937\n",
      "iter 5360/30000  loss         0.134809  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.942\n",
      "iter 5361/30000  loss         0.134806  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    3.942\n",
      "iter 5380/30000  loss         0.134762  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.947\n",
      "iter 5381/30000  loss         0.134760  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.947\n",
      "iter 5400/30000  loss         0.134716  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.952\n",
      "iter 5401/30000  loss         0.134714  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5420/30000  loss         0.134670  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.957\n",
      "iter 5421/30000  loss         0.134668  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.957\n",
      "iter 5440/30000  loss         0.134624  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.962\n",
      "iter 5441/30000  loss         0.134622  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    3.962\n",
      "iter 5460/30000  loss         0.134579  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.967\n",
      "iter 5461/30000  loss         0.134577  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.967\n",
      "iter 5480/30000  loss         0.134534  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.972\n",
      "iter 5481/30000  loss         0.134532  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.972\n",
      "iter 5500/30000  loss         0.134489  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.977\n",
      "iter 5501/30000  loss         0.134487  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    3.977\n",
      "iter 5520/30000  loss         0.134445  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.982\n",
      "iter 5521/30000  loss         0.134443  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.982\n",
      "iter 5540/30000  loss         0.134401  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.987\n",
      "iter 5541/30000  loss         0.134398  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.987\n",
      "iter 5560/30000  loss         0.134357  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.992\n",
      "iter 5561/30000  loss         0.134354  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.992\n",
      "iter 5580/30000  loss         0.134313  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.997\n",
      "iter 5581/30000  loss         0.134311  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    3.997\n",
      "iter 5600/30000  loss         0.134269  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.002\n",
      "iter 5601/30000  loss         0.134267  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.002\n",
      "iter 5620/30000  loss         0.134226  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.007\n",
      "iter 5621/30000  loss         0.134224  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.007\n",
      "iter 5640/30000  loss         0.134183  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.011\n",
      "iter 5641/30000  loss         0.134181  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    4.012\n",
      "iter 5660/30000  loss         0.134141  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.016\n",
      "iter 5661/30000  loss         0.134139  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.017\n",
      "iter 5680/30000  loss         0.134098  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.021\n",
      "iter 5681/30000  loss         0.134096  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.021\n",
      "iter 5700/30000  loss         0.134056  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.026\n",
      "iter 5701/30000  loss         0.134054  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.026\n",
      "iter 5720/30000  loss         0.134014  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.031\n",
      "iter 5721/30000  loss         0.134012  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    4.031\n",
      "iter 5740/30000  loss         0.133973  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.035\n",
      "iter 5741/30000  loss         0.133970  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.036\n",
      "iter 5760/30000  loss         0.133931  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.040\n",
      "iter 5761/30000  loss         0.133929  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.040\n",
      "iter 5780/30000  loss         0.133890  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.045\n",
      "iter 5781/30000  loss         0.133888  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.045\n",
      "iter 5800/30000  loss         0.133849  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    4.049\n",
      "iter 5801/30000  loss         0.133847  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.050\n",
      "iter 5820/30000  loss         0.133808  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.054\n",
      "iter 5821/30000  loss         0.133806  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.054\n",
      "iter 5840/30000  loss         0.133768  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.059\n",
      "iter 5841/30000  loss         0.133766  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.059\n",
      "iter 5860/30000  loss         0.133728  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.063\n",
      "iter 5861/30000  loss         0.133726  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    4.064\n",
      "iter 5880/30000  loss         0.133688  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.068\n",
      "iter 5881/30000  loss         0.133686  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.068\n",
      "iter 5900/30000  loss         0.133648  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.073\n",
      "iter 5901/30000  loss         0.133646  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.073\n",
      "iter 5920/30000  loss         0.133608  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.077\n",
      "iter 5921/30000  loss         0.133606  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.077\n",
      "iter 5940/30000  loss         0.133569  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.082\n",
      "iter 5941/30000  loss         0.133567  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    4.082\n",
      "iter 5960/30000  loss         0.133530  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.086\n",
      "iter 5961/30000  loss         0.133528  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.087\n",
      "iter 5980/30000  loss         0.133491  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.091\n",
      "iter 5981/30000  loss         0.133489  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.091\n",
      "iter 6000/30000  loss         0.133452  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.095\n",
      "iter 6001/30000  loss         0.133450  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.096\n",
      "iter 6020/30000  loss         0.133414  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.100\n",
      "iter 6021/30000  loss         0.133412  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    4.100\n",
      "iter 6040/30000  loss         0.133376  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.104\n",
      "iter 6041/30000  loss         0.133374  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.105\n",
      "iter 6060/30000  loss         0.133338  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.109\n",
      "iter 6061/30000  loss         0.133336  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.109\n",
      "iter 6080/30000  loss         0.133300  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.113\n",
      "iter 6081/30000  loss         0.133298  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.113\n",
      "iter 6100/30000  loss         0.133262  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.118\n",
      "iter 6101/30000  loss         0.133260  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    4.118\n",
      "iter 6120/30000  loss         0.133225  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.122\n",
      "iter 6121/30000  loss         0.133223  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.122\n",
      "iter 6140/30000  loss         0.133188  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.126\n",
      "iter 6141/30000  loss         0.133186  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.127\n",
      "iter 6160/30000  loss         0.133151  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.131\n",
      "iter 6161/30000  loss         0.133149  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.131\n",
      "iter 6180/30000  loss         0.133114  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.135\n",
      "iter 6181/30000  loss         0.133112  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    4.135\n",
      "iter 6200/30000  loss         0.133077  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.139\n",
      "iter 6201/30000  loss         0.133075  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6220/30000  loss         0.133041  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.144\n",
      "iter 6221/30000  loss         0.133039  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.144\n",
      "iter 6240/30000  loss         0.133005  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.148\n",
      "iter 6241/30000  loss         0.133003  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.148\n",
      "iter 6260/30000  loss         0.132969  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.152\n",
      "iter 6261/30000  loss         0.132967  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.152\n",
      "iter 6280/30000  loss         0.132933  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.157\n",
      "iter 6281/30000  loss         0.132931  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    4.157\n",
      "iter 6300/30000  loss         0.132897  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.161\n",
      "iter 6301/30000  loss         0.132896  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.161\n",
      "iter 6320/30000  loss         0.132862  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.165\n",
      "iter 6321/30000  loss         0.132860  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.165\n",
      "iter 6340/30000  loss         0.132827  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.169\n",
      "iter 6341/30000  loss         0.132825  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.169\n",
      "iter 6360/30000  loss         0.132792  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.173\n",
      "iter 6361/30000  loss         0.132790  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    4.174\n",
      "iter 6380/30000  loss         0.132757  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.178\n",
      "iter 6381/30000  loss         0.132755  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.178\n",
      "iter 6400/30000  loss         0.132722  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.182\n",
      "iter 6401/30000  loss         0.132721  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.182\n",
      "iter 6420/30000  loss         0.132688  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.186\n",
      "iter 6421/30000  loss         0.132686  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.186\n",
      "iter 6440/30000  loss         0.132654  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.190\n",
      "iter 6441/30000  loss         0.132652  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    4.190\n",
      "iter 6460/30000  loss         0.132619  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.194\n",
      "iter 6461/30000  loss         0.132618  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.194\n",
      "iter 6480/30000  loss         0.132585  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.198\n",
      "iter 6481/30000  loss         0.132584  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.198\n",
      "iter 6500/30000  loss         0.132552  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.202\n",
      "iter 6501/30000  loss         0.132550  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.202\n",
      "iter 6520/30000  loss         0.132518  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.206\n",
      "iter 6521/30000  loss         0.132517  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.207\n",
      "iter 6540/30000  loss         0.132485  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.210\n",
      "iter 6541/30000  loss         0.132483  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    4.211\n",
      "iter 6560/30000  loss         0.132452  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.214\n",
      "iter 6561/30000  loss         0.132450  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.215\n",
      "iter 6580/30000  loss         0.132419  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.218\n",
      "iter 6581/30000  loss         0.132417  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.219\n",
      "iter 6600/30000  loss         0.132386  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.222\n",
      "iter 6601/30000  loss         0.132384  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.223\n",
      "iter 6620/30000  loss         0.132353  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.226\n",
      "iter 6621/30000  loss         0.132351  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    4.227\n",
      "iter 6640/30000  loss         0.132320  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.230\n",
      "iter 6641/30000  loss         0.132319  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.231\n",
      "iter 6660/30000  loss         0.132288  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.234\n",
      "iter 6661/30000  loss         0.132286  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.235\n",
      "iter 6680/30000  loss         0.132256  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.238\n",
      "iter 6681/30000  loss         0.132254  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.239\n",
      "iter 6700/30000  loss         0.132224  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.242\n",
      "iter 6701/30000  loss         0.132222  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.242\n",
      "iter 6720/30000  loss         0.132192  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.246\n",
      "iter 6721/30000  loss         0.132190  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    4.246\n",
      "iter 6740/30000  loss         0.132160  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.250\n",
      "iter 6741/30000  loss         0.132159  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.250\n",
      "iter 6760/30000  loss         0.132129  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.254\n",
      "iter 6761/30000  loss         0.132127  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.254\n",
      "iter 6780/30000  loss         0.132097  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.258\n",
      "iter 6781/30000  loss         0.132096  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.258\n",
      "iter 6800/30000  loss         0.132066  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.262\n",
      "iter 6801/30000  loss         0.132065  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.262\n",
      "iter 6820/30000  loss         0.132035  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.266\n",
      "iter 6821/30000  loss         0.132034  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    4.266\n",
      "iter 6840/30000  loss         0.132004  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.269\n",
      "iter 6841/30000  loss         0.132003  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.270\n",
      "iter 6860/30000  loss         0.131973  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.273\n",
      "iter 6861/30000  loss         0.131972  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.273\n",
      "iter 6880/30000  loss         0.131943  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.277\n",
      "iter 6881/30000  loss         0.131941  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.277\n",
      "iter 6900/30000  loss         0.131912  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.281\n",
      "iter 6901/30000  loss         0.131911  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.281\n",
      "iter 6920/30000  loss         0.131882  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.284\n",
      "iter 6921/30000  loss         0.131881  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    4.285\n",
      "iter 6940/30000  loss         0.131852  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.288\n",
      "iter 6941/30000  loss         0.131850  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.288\n",
      "iter 6960/30000  loss         0.131822  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.292\n",
      "iter 6961/30000  loss         0.131820  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.292\n",
      "iter 6980/30000  loss         0.131792  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.296\n",
      "iter 6981/30000  loss         0.131791  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.296\n",
      "iter 7000/30000  loss         0.131762  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.299\n",
      "iter 7001/30000  loss         0.131761  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7020/30000  loss         0.131733  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.303\n",
      "iter 7021/30000  loss         0.131731  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    4.303\n",
      "iter 7040/30000  loss         0.131703  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.307\n",
      "iter 7041/30000  loss         0.131702  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.307\n",
      "iter 7060/30000  loss         0.131674  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.310\n",
      "iter 7061/30000  loss         0.131673  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.311\n",
      "iter 7080/30000  loss         0.131645  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.314\n",
      "iter 7081/30000  loss         0.131644  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.314\n",
      "iter 7100/30000  loss         0.131616  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.318\n",
      "iter 7101/30000  loss         0.131615  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.318\n",
      "iter 7120/30000  loss         0.131587  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.321\n",
      "iter 7121/30000  loss         0.131586  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    4.322\n",
      "iter 7140/30000  loss         0.131559  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.325\n",
      "iter 7141/30000  loss         0.131557  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.325\n",
      "iter 7160/30000  loss         0.131530  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.329\n",
      "iter 7161/30000  loss         0.131529  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.329\n",
      "iter 7180/30000  loss         0.131502  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.332\n",
      "iter 7181/30000  loss         0.131500  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.332\n",
      "iter 7200/30000  loss         0.131473  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.336\n",
      "iter 7201/30000  loss         0.131472  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.336\n",
      "iter 7220/30000  loss         0.131445  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.339\n",
      "iter 7221/30000  loss         0.131444  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    4.340\n",
      "iter 7240/30000  loss         0.131417  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.343\n",
      "iter 7241/30000  loss         0.131416  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.343\n",
      "iter 7260/30000  loss         0.131389  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.346\n",
      "iter 7261/30000  loss         0.131388  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.347\n",
      "iter 7280/30000  loss         0.131361  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.350\n",
      "iter 7281/30000  loss         0.131360  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.350\n",
      "iter 7300/30000  loss         0.131334  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.353\n",
      "iter 7301/30000  loss         0.131332  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.354\n",
      "iter 7320/30000  loss         0.131306  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.357\n",
      "iter 7321/30000  loss         0.131305  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    4.357\n",
      "iter 7340/30000  loss         0.131279  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.360\n",
      "iter 7341/30000  loss         0.131278  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.361\n",
      "iter 7360/30000  loss         0.131252  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.364\n",
      "iter 7361/30000  loss         0.131250  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.364\n",
      "iter 7380/30000  loss         0.131225  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.367\n",
      "iter 7381/30000  loss         0.131223  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.368\n",
      "iter 7400/30000  loss         0.131198  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.371\n",
      "iter 7401/30000  loss         0.131196  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.371\n",
      "iter 7420/30000  loss         0.131171  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.374\n",
      "iter 7421/30000  loss         0.131169  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.374\n",
      "iter 7440/30000  loss         0.131144  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.378\n",
      "iter 7441/30000  loss         0.131143  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    4.378\n",
      "iter 7460/30000  loss         0.131117  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.381\n",
      "iter 7461/30000  loss         0.131116  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.381\n",
      "iter 7480/30000  loss         0.131091  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.385\n",
      "iter 7481/30000  loss         0.131090  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.385\n",
      "iter 7500/30000  loss         0.131065  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.388\n",
      "iter 7501/30000  loss         0.131063  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.388\n",
      "iter 7520/30000  loss         0.131038  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.391\n",
      "iter 7521/30000  loss         0.131037  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.391\n",
      "iter 7540/30000  loss         0.131012  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.395\n",
      "iter 7541/30000  loss         0.131011  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    4.395\n",
      "iter 7560/30000  loss         0.130986  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.398\n",
      "iter 7561/30000  loss         0.130985  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.398\n",
      "iter 7580/30000  loss         0.130960  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.401\n",
      "iter 7581/30000  loss         0.130959  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.402\n",
      "iter 7600/30000  loss         0.130935  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.405\n",
      "iter 7601/30000  loss         0.130933  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.405\n",
      "iter 7620/30000  loss         0.130909  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.408\n",
      "iter 7621/30000  loss         0.130908  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.408\n",
      "iter 7640/30000  loss         0.130884  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.411\n",
      "iter 7641/30000  loss         0.130882  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.411\n",
      "iter 7660/30000  loss         0.130858  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.415\n",
      "iter 7661/30000  loss         0.130857  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    4.415\n",
      "iter 7680/30000  loss         0.130833  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.418\n",
      "iter 7681/30000  loss         0.130832  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.418\n",
      "iter 7700/30000  loss         0.130808  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.421\n",
      "iter 7701/30000  loss         0.130806  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.421\n",
      "iter 7720/30000  loss         0.130783  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.424\n",
      "iter 7721/30000  loss         0.130781  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.425\n",
      "iter 7740/30000  loss         0.130758  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.428\n",
      "iter 7741/30000  loss         0.130756  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.428\n",
      "iter 7760/30000  loss         0.130733  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.431\n",
      "iter 7761/30000  loss         0.130732  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.431\n",
      "iter 7780/30000  loss         0.130708  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.434\n",
      "iter 7781/30000  loss         0.130707  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    4.434\n",
      "iter 7800/30000  loss         0.130684  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.437\n",
      "iter 7801/30000  loss         0.130682  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7820/30000  loss         0.130659  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.440\n",
      "iter 7821/30000  loss         0.130658  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.441\n",
      "iter 7840/30000  loss         0.130635  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.444\n",
      "iter 7841/30000  loss         0.130633  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.444\n",
      "iter 7860/30000  loss         0.130610  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.447\n",
      "iter 7861/30000  loss         0.130609  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.447\n",
      "iter 7880/30000  loss         0.130586  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.450\n",
      "iter 7881/30000  loss         0.130585  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.450\n",
      "iter 7900/30000  loss         0.130562  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.453\n",
      "iter 7901/30000  loss         0.130561  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    4.453\n",
      "iter 7920/30000  loss         0.130538  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.456\n",
      "iter 7921/30000  loss         0.130537  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.456\n",
      "iter 7940/30000  loss         0.130514  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.459\n",
      "iter 7941/30000  loss         0.130513  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.460\n",
      "iter 7960/30000  loss         0.130491  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.463\n",
      "iter 7961/30000  loss         0.130489  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.463\n",
      "iter 7980/30000  loss         0.130467  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.466\n",
      "iter 7981/30000  loss         0.130466  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.466\n",
      "iter 8000/30000  loss         0.130443  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.469\n",
      "iter 8001/30000  loss         0.130442  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.469\n",
      "iter 8020/30000  loss         0.130420  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.472\n",
      "iter 8021/30000  loss         0.130419  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    4.472\n",
      "iter 8040/30000  loss         0.130397  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.475\n",
      "iter 8041/30000  loss         0.130395  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.475\n",
      "iter 8060/30000  loss         0.130373  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.478\n",
      "iter 8061/30000  loss         0.130372  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.478\n",
      "iter 8080/30000  loss         0.130350  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.481\n",
      "iter 8081/30000  loss         0.130349  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.481\n",
      "iter 8100/30000  loss         0.130327  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.484\n",
      "iter 8101/30000  loss         0.130326  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.484\n",
      "iter 8120/30000  loss         0.130304  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.487\n",
      "iter 8121/30000  loss         0.130303  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.487\n",
      "iter 8140/30000  loss         0.130281  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.490\n",
      "iter 8141/30000  loss         0.130280  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.490\n",
      "iter 8160/30000  loss         0.130259  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.493\n",
      "iter 8161/30000  loss         0.130258  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    4.493\n",
      "iter 8180/30000  loss         0.130236  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.496\n",
      "iter 8181/30000  loss         0.130235  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.496\n",
      "iter 8200/30000  loss         0.130214  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.499\n",
      "iter 8201/30000  loss         0.130212  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.499\n",
      "iter 8220/30000  loss         0.130191  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.502\n",
      "iter 8221/30000  loss         0.130190  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.502\n",
      "iter 8240/30000  loss         0.130169  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.505\n",
      "iter 8241/30000  loss         0.130168  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.505\n",
      "iter 8260/30000  loss         0.130147  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.508\n",
      "iter 8261/30000  loss         0.130145  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.508\n",
      "iter 8280/30000  loss         0.130124  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.511\n",
      "iter 8281/30000  loss         0.130123  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    4.511\n",
      "iter 8300/30000  loss         0.130102  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.514\n",
      "iter 8301/30000  loss         0.130101  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.514\n",
      "iter 8320/30000  loss         0.130080  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.517\n",
      "iter 8321/30000  loss         0.130079  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.517\n",
      "iter 8340/30000  loss         0.130058  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.520\n",
      "iter 8341/30000  loss         0.130057  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.520\n",
      "iter 8360/30000  loss         0.130037  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.523\n",
      "iter 8361/30000  loss         0.130036  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.523\n",
      "iter 8380/30000  loss         0.130015  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.526\n",
      "iter 8381/30000  loss         0.130014  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.526\n",
      "iter 8400/30000  loss         0.129993  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.529\n",
      "iter 8401/30000  loss         0.129992  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.529\n",
      "iter 8420/30000  loss         0.129972  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.531\n",
      "iter 8421/30000  loss         0.129971  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    4.532\n",
      "iter 8440/30000  loss         0.129950  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.534\n",
      "iter 8441/30000  loss         0.129949  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.534\n",
      "iter 8460/30000  loss         0.129929  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.537\n",
      "iter 8461/30000  loss         0.129928  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.537\n",
      "iter 8480/30000  loss         0.129908  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.540\n",
      "iter 8481/30000  loss         0.129907  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.540\n",
      "iter 8500/30000  loss         0.129887  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.543\n",
      "iter 8501/30000  loss         0.129886  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.543\n",
      "iter 8520/30000  loss         0.129866  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.546\n",
      "iter 8521/30000  loss         0.129865  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.546\n",
      "iter 8540/30000  loss         0.129845  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.549\n",
      "iter 8541/30000  loss         0.129844  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.549\n",
      "iter 8560/30000  loss         0.129824  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.551\n",
      "iter 8561/30000  loss         0.129823  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    4.551\n",
      "iter 8580/30000  loss         0.129803  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.554\n",
      "iter 8581/30000  loss         0.129802  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.554\n",
      "iter 8600/30000  loss         0.129782  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.557\n",
      "iter 8601/30000  loss         0.129781  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8620/30000  loss         0.129762  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.560\n",
      "iter 8621/30000  loss         0.129761  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.560\n",
      "iter 8640/30000  loss         0.129741  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.562\n",
      "iter 8641/30000  loss         0.129740  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.563\n",
      "iter 8660/30000  loss         0.129721  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.565\n",
      "iter 8661/30000  loss         0.129720  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.565\n",
      "iter 8680/30000  loss         0.129700  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.568\n",
      "iter 8681/30000  loss         0.129699  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.568\n",
      "iter 8700/30000  loss         0.129680  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.571\n",
      "iter 8701/30000  loss         0.129679  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    4.571\n",
      "iter 8720/30000  loss         0.129660  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.574\n",
      "iter 8721/30000  loss         0.129659  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.574\n",
      "iter 8740/30000  loss         0.129639  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.576\n",
      "iter 8741/30000  loss         0.129638  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.576\n",
      "iter 8760/30000  loss         0.129619  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.579\n",
      "iter 8761/30000  loss         0.129618  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.579\n",
      "iter 8780/30000  loss         0.129599  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.582\n",
      "iter 8781/30000  loss         0.129598  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.582\n",
      "iter 8800/30000  loss         0.129580  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.584\n",
      "iter 8801/30000  loss         0.129579  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.585\n",
      "iter 8820/30000  loss         0.129560  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.587\n",
      "iter 8821/30000  loss         0.129559  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.587\n",
      "iter 8840/30000  loss         0.129540  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.590\n",
      "iter 8841/30000  loss         0.129539  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    4.590\n",
      "iter 8860/30000  loss         0.129520  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.592\n",
      "iter 8861/30000  loss         0.129519  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.593\n",
      "iter 8880/30000  loss         0.129501  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.595\n",
      "iter 8881/30000  loss         0.129500  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.595\n",
      "iter 8900/30000  loss         0.129481  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.598\n",
      "iter 8901/30000  loss         0.129480  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.598\n",
      "iter 8920/30000  loss         0.129462  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.600\n",
      "iter 8921/30000  loss         0.129461  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.601\n",
      "iter 8940/30000  loss         0.129442  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.603\n",
      "iter 8941/30000  loss         0.129442  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.603\n",
      "iter 8960/30000  loss         0.129423  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.606\n",
      "iter 8961/30000  loss         0.129422  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.606\n",
      "iter 8980/30000  loss         0.129404  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.608\n",
      "iter 8981/30000  loss         0.129403  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    4.608\n",
      "iter 9000/30000  loss         0.129385  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.611\n",
      "iter 9001/30000  loss         0.129384  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.611\n",
      "iter 9020/30000  loss         0.129366  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.614\n",
      "iter 9021/30000  loss         0.129365  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.614\n",
      "iter 9040/30000  loss         0.129347  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.616\n",
      "iter 9041/30000  loss         0.129346  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.616\n",
      "iter 9060/30000  loss         0.129328  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.619\n",
      "iter 9061/30000  loss         0.129327  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.619\n",
      "iter 9080/30000  loss         0.129309  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.621\n",
      "iter 9081/30000  loss         0.129308  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.621\n",
      "iter 9100/30000  loss         0.129290  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.624\n",
      "iter 9101/30000  loss         0.129289  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.624\n",
      "iter 9120/30000  loss         0.129272  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.626\n",
      "iter 9121/30000  loss         0.129271  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.627\n",
      "iter 9140/30000  loss         0.129253  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.629\n",
      "iter 9141/30000  loss         0.129252  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    4.629\n",
      "iter 9160/30000  loss         0.129235  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.632\n",
      "iter 9161/30000  loss         0.129234  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.632\n",
      "iter 9180/30000  loss         0.129216  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.634\n",
      "iter 9181/30000  loss         0.129215  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.634\n",
      "iter 9200/30000  loss         0.129198  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.637\n",
      "iter 9201/30000  loss         0.129197  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.637\n",
      "iter 9220/30000  loss         0.129179  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.639\n",
      "iter 9221/30000  loss         0.129179  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.639\n",
      "iter 9240/30000  loss         0.129161  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.642\n",
      "iter 9241/30000  loss         0.129160  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.642\n",
      "iter 9260/30000  loss         0.129143  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.644\n",
      "iter 9261/30000  loss         0.129142  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.644\n",
      "iter 9280/30000  loss         0.129125  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.647\n",
      "iter 9281/30000  loss         0.129124  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.647\n",
      "iter 9300/30000  loss         0.129107  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.649\n",
      "iter 9301/30000  loss         0.129106  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    4.649\n",
      "iter 9320/30000  loss         0.129089  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.652\n",
      "iter 9321/30000  loss         0.129088  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.652\n",
      "iter 9340/30000  loss         0.129071  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.654\n",
      "iter 9341/30000  loss         0.129070  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.654\n",
      "iter 9360/30000  loss         0.129053  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.657\n",
      "iter 9361/30000  loss         0.129052  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.657\n",
      "iter 9380/30000  loss         0.129035  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.659\n",
      "iter 9381/30000  loss         0.129035  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.659\n",
      "iter 9400/30000  loss         0.129018  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.661\n",
      "iter 9401/30000  loss         0.129017  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9420/30000  loss         0.129000  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.664\n",
      "iter 9421/30000  loss         0.128999  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.664\n",
      "iter 9440/30000  loss         0.128983  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.666\n",
      "iter 9441/30000  loss         0.128982  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.666\n",
      "iter 9460/30000  loss         0.128965  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.669\n",
      "iter 9461/30000  loss         0.128964  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    4.669\n",
      "iter 9480/30000  loss         0.128948  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.671\n",
      "iter 9481/30000  loss         0.128947  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.671\n",
      "iter 9500/30000  loss         0.128930  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.674\n",
      "iter 9501/30000  loss         0.128929  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.674\n",
      "iter 9520/30000  loss         0.128913  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.676\n",
      "iter 9521/30000  loss         0.128912  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.676\n",
      "iter 9540/30000  loss         0.128896  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.678\n",
      "iter 9541/30000  loss         0.128895  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.678\n",
      "iter 9560/30000  loss         0.128879  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.681\n",
      "iter 9561/30000  loss         0.128878  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.681\n",
      "iter 9580/30000  loss         0.128861  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.683\n",
      "iter 9581/30000  loss         0.128861  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.683\n",
      "iter 9600/30000  loss         0.128844  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.685\n",
      "iter 9601/30000  loss         0.128844  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.686\n",
      "iter 9620/30000  loss         0.128827  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.688\n",
      "iter 9621/30000  loss         0.128827  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    4.688\n",
      "iter 9640/30000  loss         0.128811  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.690\n",
      "iter 9641/30000  loss         0.128810  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.690\n",
      "iter 9660/30000  loss         0.128794  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.693\n",
      "iter 9661/30000  loss         0.128793  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.693\n",
      "iter 9680/30000  loss         0.128777  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.695\n",
      "iter 9681/30000  loss         0.128776  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.695\n",
      "iter 9700/30000  loss         0.128760  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.697\n",
      "iter 9701/30000  loss         0.128759  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.697\n",
      "iter 9720/30000  loss         0.128743  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.700\n",
      "iter 9721/30000  loss         0.128743  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.700\n",
      "iter 9740/30000  loss         0.128727  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.702\n",
      "iter 9741/30000  loss         0.128726  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.702\n",
      "iter 9760/30000  loss         0.128710  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.704\n",
      "iter 9761/30000  loss         0.128710  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.704\n",
      "iter 9780/30000  loss         0.128694  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.706\n",
      "iter 9781/30000  loss         0.128693  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    4.707\n",
      "iter 9800/30000  loss         0.128677  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.709\n",
      "iter 9801/30000  loss         0.128677  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.709\n",
      "iter 9820/30000  loss         0.128661  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.711\n",
      "iter 9821/30000  loss         0.128660  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.711\n",
      "iter 9840/30000  loss         0.128645  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.713\n",
      "iter 9841/30000  loss         0.128644  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.713\n",
      "iter 9860/30000  loss         0.128629  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.716\n",
      "iter 9861/30000  loss         0.128628  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.716\n",
      "iter 9880/30000  loss         0.128612  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.718\n",
      "iter 9881/30000  loss         0.128612  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.718\n",
      "iter 9900/30000  loss         0.128596  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.720\n",
      "iter 9901/30000  loss         0.128595  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.720\n",
      "iter 9920/30000  loss         0.128580  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.722\n",
      "iter 9921/30000  loss         0.128579  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.722\n",
      "iter 9940/30000  loss         0.128564  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.725\n",
      "iter 9941/30000  loss         0.128563  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.725\n",
      "iter 9960/30000  loss         0.128548  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.727\n",
      "iter 9961/30000  loss         0.128547  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    4.727\n",
      "iter 9980/30000  loss         0.128532  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.729\n",
      "iter 9981/30000  loss         0.128531  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.729\n",
      "iter 10000/30000  loss         0.128516  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.731\n",
      "iter 10001/30000  loss         0.128516  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.731\n",
      "iter 10020/30000  loss         0.128501  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.734\n",
      "iter 10021/30000  loss         0.128500  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.734\n",
      "iter 10040/30000  loss         0.128485  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.736\n",
      "iter 10041/30000  loss         0.128484  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.736\n",
      "iter 10060/30000  loss         0.128469  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.738\n",
      "iter 10061/30000  loss         0.128468  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.738\n",
      "iter 10080/30000  loss         0.128454  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.740\n",
      "iter 10081/30000  loss         0.128453  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.740\n",
      "iter 10100/30000  loss         0.128438  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.742\n",
      "iter 10101/30000  loss         0.128437  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.742\n",
      "iter 10120/30000  loss         0.128423  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.744\n",
      "iter 10121/30000  loss         0.128422  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.745\n",
      "iter 10140/30000  loss         0.128407  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.747\n",
      "iter 10141/30000  loss         0.128406  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    4.747\n",
      "iter 10160/30000  loss         0.128392  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.749\n",
      "iter 10161/30000  loss         0.128391  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.749\n",
      "iter 10180/30000  loss         0.128376  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.751\n",
      "iter 10181/30000  loss         0.128376  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.751\n",
      "iter 10200/30000  loss         0.128361  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.753\n",
      "iter 10201/30000  loss         0.128360  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10220/30000  loss         0.128346  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.755\n",
      "iter 10221/30000  loss         0.128345  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.755\n",
      "iter 10240/30000  loss         0.128331  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.757\n",
      "iter 10241/30000  loss         0.128330  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.758\n",
      "iter 10260/30000  loss         0.128316  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.760\n",
      "iter 10261/30000  loss         0.128315  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.760\n",
      "iter 10280/30000  loss         0.128301  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.762\n",
      "iter 10281/30000  loss         0.128300  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.762\n",
      "iter 10300/30000  loss         0.128286  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.764\n",
      "iter 10301/30000  loss         0.128285  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.764\n",
      "iter 10320/30000  loss         0.128271  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.766\n",
      "iter 10321/30000  loss         0.128270  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    4.766\n",
      "iter 10340/30000  loss         0.128256  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.768\n",
      "iter 10341/30000  loss         0.128255  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.768\n",
      "iter 10360/30000  loss         0.128241  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.770\n",
      "iter 10361/30000  loss         0.128240  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.770\n",
      "iter 10380/30000  loss         0.128226  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.772\n",
      "iter 10381/30000  loss         0.128225  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.772\n",
      "iter 10400/30000  loss         0.128211  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.774\n",
      "iter 10401/30000  loss         0.128210  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.774\n",
      "iter 10420/30000  loss         0.128196  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.776\n",
      "iter 10421/30000  loss         0.128196  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.777\n",
      "iter 10440/30000  loss         0.128182  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.778\n",
      "iter 10441/30000  loss         0.128181  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.779\n",
      "iter 10460/30000  loss         0.128167  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.781\n",
      "iter 10461/30000  loss         0.128166  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.781\n",
      "iter 10480/30000  loss         0.128153  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.783\n",
      "iter 10481/30000  loss         0.128152  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.783\n",
      "iter 10500/30000  loss         0.128138  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.785\n",
      "iter 10501/30000  loss         0.128137  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.785\n",
      "iter 10520/30000  loss         0.128124  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.787\n",
      "iter 10521/30000  loss         0.128123  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    4.787\n",
      "iter 10540/30000  loss         0.128109  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.789\n",
      "iter 10541/30000  loss         0.128109  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.789\n",
      "iter 10560/30000  loss         0.128095  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.791\n",
      "iter 10561/30000  loss         0.128094  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.791\n",
      "iter 10580/30000  loss         0.128081  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.793\n",
      "iter 10581/30000  loss         0.128080  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.793\n",
      "iter 10600/30000  loss         0.128066  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.795\n",
      "iter 10601/30000  loss         0.128066  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.795\n",
      "iter 10620/30000  loss         0.128052  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.797\n",
      "iter 10621/30000  loss         0.128051  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.797\n",
      "iter 10640/30000  loss         0.128038  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.799\n",
      "iter 10641/30000  loss         0.128037  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.799\n",
      "iter 10660/30000  loss         0.128024  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.801\n",
      "iter 10661/30000  loss         0.128023  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.801\n",
      "iter 10680/30000  loss         0.128010  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.803\n",
      "iter 10681/30000  loss         0.128009  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.803\n",
      "iter 10700/30000  loss         0.127996  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.805\n",
      "iter 10701/30000  loss         0.127995  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    4.805\n",
      "iter 10720/30000  loss         0.127982  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.807\n",
      "iter 10721/30000  loss         0.127981  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.807\n",
      "iter 10740/30000  loss         0.127968  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.809\n",
      "iter 10741/30000  loss         0.127967  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.809\n",
      "iter 10760/30000  loss         0.127954  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.811\n",
      "iter 10761/30000  loss         0.127953  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.811\n",
      "iter 10780/30000  loss         0.127940  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.813\n",
      "iter 10781/30000  loss         0.127939  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.813\n",
      "iter 10800/30000  loss         0.127926  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.815\n",
      "iter 10801/30000  loss         0.127926  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.815\n",
      "iter 10820/30000  loss         0.127913  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.817\n",
      "iter 10821/30000  loss         0.127912  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.817\n",
      "iter 10840/30000  loss         0.127899  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.819\n",
      "iter 10841/30000  loss         0.127898  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.819\n",
      "iter 10860/30000  loss         0.127885  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.821\n",
      "iter 10861/30000  loss         0.127885  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.821\n",
      "iter 10880/30000  loss         0.127872  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.823\n",
      "iter 10881/30000  loss         0.127871  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.823\n",
      "iter 10900/30000  loss         0.127858  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.824\n",
      "iter 10901/30000  loss         0.127857  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    4.825\n",
      "iter 10920/30000  loss         0.127845  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.826\n",
      "iter 10921/30000  loss         0.127844  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.826\n",
      "iter 10940/30000  loss         0.127831  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.828\n",
      "iter 10941/30000  loss         0.127831  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.828\n",
      "iter 10960/30000  loss         0.127818  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.830\n",
      "iter 10961/30000  loss         0.127817  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.830\n",
      "iter 10980/30000  loss         0.127804  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.832\n",
      "iter 10981/30000  loss         0.127804  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.832\n",
      "iter 11000/30000  loss         0.127791  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.834\n",
      "iter 11001/30000  loss         0.127790  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11020/30000  loss         0.127778  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.836\n",
      "iter 11021/30000  loss         0.127777  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.836\n",
      "iter 11040/30000  loss         0.127764  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.838\n",
      "iter 11041/30000  loss         0.127764  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.838\n",
      "iter 11060/30000  loss         0.127751  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.840\n",
      "iter 11061/30000  loss         0.127751  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.840\n",
      "iter 11080/30000  loss         0.127738  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.842\n",
      "iter 11081/30000  loss         0.127737  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.842\n",
      "iter 11100/30000  loss         0.127725  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.843\n",
      "iter 11101/30000  loss         0.127724  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.844\n",
      "iter 11120/30000  loss         0.127712  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.845\n",
      "iter 11121/30000  loss         0.127711  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    4.845\n",
      "iter 11140/30000  loss         0.127699  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.847\n",
      "iter 11141/30000  loss         0.127698  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.847\n",
      "iter 11160/30000  loss         0.127686  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.849\n",
      "iter 11161/30000  loss         0.127685  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.849\n",
      "iter 11180/30000  loss         0.127673  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.851\n",
      "iter 11181/30000  loss         0.127672  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.851\n",
      "iter 11200/30000  loss         0.127660  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.853\n",
      "iter 11201/30000  loss         0.127659  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.853\n",
      "iter 11220/30000  loss         0.127647  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.855\n",
      "iter 11221/30000  loss         0.127647  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.855\n",
      "iter 11240/30000  loss         0.127634  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.856\n",
      "iter 11241/30000  loss         0.127634  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.857\n",
      "iter 11260/30000  loss         0.127622  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.858\n",
      "iter 11261/30000  loss         0.127621  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.858\n",
      "iter 11280/30000  loss         0.127609  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.860\n",
      "iter 11281/30000  loss         0.127608  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.860\n",
      "iter 11300/30000  loss         0.127596  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.862\n",
      "iter 11301/30000  loss         0.127596  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.862\n",
      "iter 11320/30000  loss         0.127583  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.864\n",
      "iter 11321/30000  loss         0.127583  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    4.864\n",
      "iter 11340/30000  loss         0.127571  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.866\n",
      "iter 11341/30000  loss         0.127570  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.866\n",
      "iter 11360/30000  loss         0.127558  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.867\n",
      "iter 11361/30000  loss         0.127558  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.867\n",
      "iter 11380/30000  loss         0.127546  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.869\n",
      "iter 11381/30000  loss         0.127545  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.869\n",
      "iter 11400/30000  loss         0.127533  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.871\n",
      "iter 11401/30000  loss         0.127533  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.871\n",
      "iter 11420/30000  loss         0.127521  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.873\n",
      "iter 11421/30000  loss         0.127520  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.873\n",
      "iter 11440/30000  loss         0.127508  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.875\n",
      "iter 11441/30000  loss         0.127508  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.875\n",
      "iter 11460/30000  loss         0.127496  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.876\n",
      "iter 11461/30000  loss         0.127495  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.876\n",
      "iter 11480/30000  loss         0.127484  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.878\n",
      "iter 11481/30000  loss         0.127483  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.878\n",
      "iter 11500/30000  loss         0.127471  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.880\n",
      "iter 11501/30000  loss         0.127471  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.880\n",
      "iter 11520/30000  loss         0.127459  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.882\n",
      "iter 11521/30000  loss         0.127458  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.882\n",
      "iter 11540/30000  loss         0.127447  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.883\n",
      "iter 11541/30000  loss         0.127446  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    4.883\n",
      "iter 11560/30000  loss         0.127435  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.885\n",
      "iter 11561/30000  loss         0.127434  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.885\n",
      "iter 11580/30000  loss         0.127423  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.887\n",
      "iter 11581/30000  loss         0.127422  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.887\n",
      "iter 11600/30000  loss         0.127410  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.889\n",
      "iter 11601/30000  loss         0.127410  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.889\n",
      "iter 11620/30000  loss         0.127398  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.890\n",
      "iter 11621/30000  loss         0.127398  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.890\n",
      "iter 11640/30000  loss         0.127386  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.892\n",
      "iter 11641/30000  loss         0.127386  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.892\n",
      "iter 11660/30000  loss         0.127374  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.894\n",
      "iter 11661/30000  loss         0.127374  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.894\n",
      "iter 11680/30000  loss         0.127362  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.895\n",
      "iter 11681/30000  loss         0.127362  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.896\n",
      "iter 11700/30000  loss         0.127350  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.897\n",
      "iter 11701/30000  loss         0.127350  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.897\n",
      "iter 11720/30000  loss         0.127339  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.899\n",
      "iter 11721/30000  loss         0.127338  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.899\n",
      "iter 11740/30000  loss         0.127327  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.901\n",
      "iter 11741/30000  loss         0.127326  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.901\n",
      "iter 11760/30000  loss         0.127315  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.902\n",
      "iter 11761/30000  loss         0.127314  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.902\n",
      "iter 11780/30000  loss         0.127303  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.904\n",
      "iter 11781/30000  loss         0.127303  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    4.904\n",
      "iter 11800/30000  loss         0.127291  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.906\n",
      "iter 11801/30000  loss         0.127291  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11820/30000  loss         0.127280  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.907\n",
      "iter 11821/30000  loss         0.127279  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.907\n",
      "iter 11840/30000  loss         0.127268  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.909\n",
      "iter 11841/30000  loss         0.127267  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.909\n",
      "iter 11860/30000  loss         0.127256  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.911\n",
      "iter 11861/30000  loss         0.127256  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.911\n",
      "iter 11880/30000  loss         0.127245  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.912\n",
      "iter 11881/30000  loss         0.127244  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.912\n",
      "iter 11900/30000  loss         0.127233  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.914\n",
      "iter 11901/30000  loss         0.127233  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.914\n",
      "iter 11920/30000  loss         0.127222  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.916\n",
      "iter 11921/30000  loss         0.127221  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.916\n",
      "iter 11940/30000  loss         0.127210  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.917\n",
      "iter 11941/30000  loss         0.127210  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.917\n",
      "iter 11960/30000  loss         0.127199  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.919\n",
      "iter 11961/30000  loss         0.127198  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.919\n",
      "iter 11980/30000  loss         0.127187  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.921\n",
      "iter 11981/30000  loss         0.127187  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.921\n",
      "iter 12000/30000  loss         0.127176  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.922\n",
      "iter 12001/30000  loss         0.127175  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    4.922\n",
      "iter 12020/30000  loss         0.127165  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.924\n",
      "iter 12021/30000  loss         0.127164  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.924\n",
      "iter 12040/30000  loss         0.127153  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.926\n",
      "iter 12041/30000  loss         0.127153  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.926\n",
      "iter 12060/30000  loss         0.127142  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.927\n",
      "iter 12061/30000  loss         0.127141  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.927\n",
      "iter 12080/30000  loss         0.127131  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.929\n",
      "iter 12081/30000  loss         0.127130  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.929\n",
      "iter 12100/30000  loss         0.127120  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.930\n",
      "iter 12101/30000  loss         0.127119  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.931\n",
      "iter 12120/30000  loss         0.127108  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.932\n",
      "iter 12121/30000  loss         0.127108  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.932\n",
      "iter 12140/30000  loss         0.127097  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.934\n",
      "iter 12141/30000  loss         0.127097  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.934\n",
      "iter 12160/30000  loss         0.127086  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.935\n",
      "iter 12161/30000  loss         0.127086  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.935\n",
      "iter 12180/30000  loss         0.127075  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.937\n",
      "iter 12181/30000  loss         0.127075  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.937\n",
      "iter 12200/30000  loss         0.127064  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.938\n",
      "iter 12201/30000  loss         0.127063  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.939\n",
      "iter 12220/30000  loss         0.127053  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.940\n",
      "iter 12221/30000  loss         0.127052  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.940\n",
      "iter 12240/30000  loss         0.127042  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.942\n",
      "iter 12241/30000  loss         0.127042  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.942\n",
      "iter 12260/30000  loss         0.127031  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.943\n",
      "iter 12261/30000  loss         0.127031  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    4.943\n",
      "iter 12280/30000  loss         0.127020  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.945\n",
      "iter 12281/30000  loss         0.127020  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.945\n",
      "iter 12300/30000  loss         0.127009  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.946\n",
      "iter 12301/30000  loss         0.127009  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.946\n",
      "iter 12320/30000  loss         0.126998  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.948\n",
      "iter 12321/30000  loss         0.126998  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.948\n",
      "iter 12340/30000  loss         0.126988  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.949\n",
      "iter 12341/30000  loss         0.126987  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.950\n",
      "iter 12360/30000  loss         0.126977  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.951\n",
      "iter 12361/30000  loss         0.126976  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.951\n",
      "iter 12380/30000  loss         0.126966  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.953\n",
      "iter 12381/30000  loss         0.126966  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.953\n",
      "iter 12400/30000  loss         0.126955  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.954\n",
      "iter 12401/30000  loss         0.126955  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.954\n",
      "iter 12420/30000  loss         0.126945  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.956\n",
      "iter 12421/30000  loss         0.126944  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.956\n",
      "iter 12440/30000  loss         0.126934  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.957\n",
      "iter 12441/30000  loss         0.126934  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.957\n",
      "iter 12460/30000  loss         0.126923  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.959\n",
      "iter 12461/30000  loss         0.126923  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.959\n",
      "iter 12480/30000  loss         0.126913  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.960\n",
      "iter 12481/30000  loss         0.126912  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.960\n",
      "iter 12500/30000  loss         0.126902  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.962\n",
      "iter 12501/30000  loss         0.126902  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    4.962\n",
      "iter 12520/30000  loss         0.126892  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.963\n",
      "iter 12521/30000  loss         0.126891  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.963\n",
      "iter 12540/30000  loss         0.126881  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.965\n",
      "iter 12541/30000  loss         0.126881  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.965\n",
      "iter 12560/30000  loss         0.126871  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.966\n",
      "iter 12561/30000  loss         0.126870  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.966\n",
      "iter 12580/30000  loss         0.126860  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.968\n",
      "iter 12581/30000  loss         0.126860  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.968\n",
      "iter 12600/30000  loss         0.126850  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.969\n",
      "iter 12601/30000  loss         0.126849  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12620/30000  loss         0.126840  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.971\n",
      "iter 12621/30000  loss         0.126839  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.971\n",
      "iter 12640/30000  loss         0.126829  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.972\n",
      "iter 12641/30000  loss         0.126829  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.972\n",
      "iter 12660/30000  loss         0.126819  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.974\n",
      "iter 12661/30000  loss         0.126818  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.974\n",
      "iter 12680/30000  loss         0.126809  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.975\n",
      "iter 12681/30000  loss         0.126808  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.975\n",
      "iter 12700/30000  loss         0.126798  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.977\n",
      "iter 12701/30000  loss         0.126798  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.977\n",
      "iter 12720/30000  loss         0.126788  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.978\n",
      "iter 12721/30000  loss         0.126788  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.978\n",
      "iter 12740/30000  loss         0.126778  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.980\n",
      "iter 12741/30000  loss         0.126777  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.980\n",
      "iter 12760/30000  loss         0.126768  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.981\n",
      "iter 12761/30000  loss         0.126767  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    4.981\n",
      "iter 12780/30000  loss         0.126758  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.983\n",
      "iter 12781/30000  loss         0.126757  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.983\n",
      "iter 12800/30000  loss         0.126747  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.984\n",
      "iter 12801/30000  loss         0.126747  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.984\n",
      "iter 12820/30000  loss         0.126737  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.986\n",
      "iter 12821/30000  loss         0.126737  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.986\n",
      "iter 12840/30000  loss         0.126727  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.987\n",
      "iter 12841/30000  loss         0.126727  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.987\n",
      "iter 12860/30000  loss         0.126717  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.988\n",
      "iter 12861/30000  loss         0.126717  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.989\n",
      "iter 12880/30000  loss         0.126707  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.990\n",
      "iter 12881/30000  loss         0.126707  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.990\n",
      "iter 12900/30000  loss         0.126697  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.991\n",
      "iter 12901/30000  loss         0.126697  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.991\n",
      "iter 12920/30000  loss         0.126687  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.993\n",
      "iter 12921/30000  loss         0.126687  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.993\n",
      "iter 12940/30000  loss         0.126677  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.994\n",
      "iter 12941/30000  loss         0.126677  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.994\n",
      "iter 12960/30000  loss         0.126668  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.996\n",
      "iter 12961/30000  loss         0.126667  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.996\n",
      "iter 12980/30000  loss         0.126658  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.997\n",
      "iter 12981/30000  loss         0.126657  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.997\n",
      "iter 13000/30000  loss         0.126648  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.998\n",
      "iter 13001/30000  loss         0.126647  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    4.999\n",
      "iter 13020/30000  loss         0.126638  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    5.000\n",
      "iter 13021/30000  loss         0.126638  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    5.000\n",
      "iter 13040/30000  loss         0.126628  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.001\n",
      "iter 13041/30000  loss         0.126628  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.001\n",
      "iter 13060/30000  loss         0.126619  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.003\n",
      "iter 13061/30000  loss         0.126618  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.003\n",
      "iter 13080/30000  loss         0.126609  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.004\n",
      "iter 13081/30000  loss         0.126608  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.004\n",
      "iter 13100/30000  loss         0.126599  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.006\n",
      "iter 13101/30000  loss         0.126599  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.006\n",
      "iter 13120/30000  loss         0.126590  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.007\n",
      "iter 13121/30000  loss         0.126589  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.007\n",
      "iter 13140/30000  loss         0.126580  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.008\n",
      "iter 13141/30000  loss         0.126579  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.008\n",
      "iter 13160/30000  loss         0.126570  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.010\n",
      "iter 13161/30000  loss         0.126570  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.010\n",
      "iter 13180/30000  loss         0.126561  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.011\n",
      "iter 13181/30000  loss         0.126560  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.011\n",
      "iter 13200/30000  loss         0.126551  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.012\n",
      "iter 13201/30000  loss         0.126551  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.013\n",
      "iter 13220/30000  loss         0.126542  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.014\n",
      "iter 13221/30000  loss         0.126541  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.014\n",
      "iter 13240/30000  loss         0.126532  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.015\n",
      "iter 13241/30000  loss         0.126532  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.015\n",
      "iter 13260/30000  loss         0.126523  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.017\n",
      "iter 13261/30000  loss         0.126522  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.017\n",
      "iter 13280/30000  loss         0.126513  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.018\n",
      "iter 13281/30000  loss         0.126513  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.018\n",
      "iter 13300/30000  loss         0.126504  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.019\n",
      "iter 13301/30000  loss         0.126503  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    5.019\n",
      "iter 13320/30000  loss         0.126494  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.021\n",
      "iter 13321/30000  loss         0.126494  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.021\n",
      "iter 13340/30000  loss         0.126485  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.022\n",
      "iter 13341/30000  loss         0.126485  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.022\n",
      "iter 13360/30000  loss         0.126476  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.023\n",
      "iter 13361/30000  loss         0.126475  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.023\n",
      "iter 13380/30000  loss         0.126466  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.025\n",
      "iter 13381/30000  loss         0.126466  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.025\n",
      "iter 13400/30000  loss         0.126457  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.026\n",
      "iter 13401/30000  loss         0.126457  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13420/30000  loss         0.126448  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.027\n",
      "iter 13421/30000  loss         0.126447  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.027\n",
      "iter 13440/30000  loss         0.126438  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.029\n",
      "iter 13441/30000  loss         0.126438  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.029\n",
      "iter 13460/30000  loss         0.126429  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.030\n",
      "iter 13461/30000  loss         0.126429  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.030\n",
      "iter 13480/30000  loss         0.126420  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.031\n",
      "iter 13481/30000  loss         0.126420  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.031\n",
      "iter 13500/30000  loss         0.126411  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.033\n",
      "iter 13501/30000  loss         0.126410  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.033\n",
      "iter 13520/30000  loss         0.126402  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.034\n",
      "iter 13521/30000  loss         0.126401  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.034\n",
      "iter 13540/30000  loss         0.126393  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.035\n",
      "iter 13541/30000  loss         0.126392  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.035\n",
      "iter 13560/30000  loss         0.126384  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.037\n",
      "iter 13561/30000  loss         0.126383  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.037\n",
      "iter 13580/30000  loss         0.126374  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.038\n",
      "iter 13581/30000  loss         0.126374  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    5.038\n",
      "iter 13600/30000  loss         0.126365  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.039\n",
      "iter 13601/30000  loss         0.126365  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.039\n",
      "iter 13620/30000  loss         0.126356  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.041\n",
      "iter 13621/30000  loss         0.126356  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.041\n",
      "iter 13640/30000  loss         0.126347  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.042\n",
      "iter 13641/30000  loss         0.126347  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.042\n",
      "iter 13660/30000  loss         0.126338  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.043\n",
      "iter 13661/30000  loss         0.126338  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.043\n",
      "iter 13680/30000  loss         0.126330  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.044\n",
      "iter 13681/30000  loss         0.126329  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.045\n",
      "iter 13700/30000  loss         0.126321  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.046\n",
      "iter 13701/30000  loss         0.126320  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.046\n",
      "iter 13720/30000  loss         0.126312  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.047\n",
      "iter 13721/30000  loss         0.126311  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.047\n",
      "iter 13740/30000  loss         0.126303  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.048\n",
      "iter 13741/30000  loss         0.126302  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.048\n",
      "iter 13760/30000  loss         0.126294  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.050\n",
      "iter 13761/30000  loss         0.126294  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.050\n",
      "iter 13780/30000  loss         0.126285  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.051\n",
      "iter 13781/30000  loss         0.126285  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.051\n",
      "iter 13800/30000  loss         0.126276  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.052\n",
      "iter 13801/30000  loss         0.126276  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.052\n",
      "iter 13820/30000  loss         0.126268  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.053\n",
      "iter 13821/30000  loss         0.126267  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.053\n",
      "iter 13840/30000  loss         0.126259  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.055\n",
      "iter 13841/30000  loss         0.126258  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.055\n",
      "iter 13860/30000  loss         0.126250  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.056\n",
      "iter 13861/30000  loss         0.126250  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.056\n",
      "iter 13880/30000  loss         0.126241  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.057\n",
      "iter 13881/30000  loss         0.126241  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    5.057\n",
      "iter 13900/30000  loss         0.126233  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.058\n",
      "iter 13901/30000  loss         0.126232  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.058\n",
      "iter 13920/30000  loss         0.126224  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.060\n",
      "iter 13921/30000  loss         0.126224  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.060\n",
      "iter 13940/30000  loss         0.126215  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.061\n",
      "iter 13941/30000  loss         0.126215  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.061\n",
      "iter 13960/30000  loss         0.126207  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.062\n",
      "iter 13961/30000  loss         0.126206  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.062\n",
      "iter 13980/30000  loss         0.126198  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.063\n",
      "iter 13981/30000  loss         0.126198  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.063\n",
      "iter 14000/30000  loss         0.126190  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.065\n",
      "iter 14001/30000  loss         0.126189  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.065\n",
      "iter 14020/30000  loss         0.126181  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.066\n",
      "iter 14021/30000  loss         0.126181  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.066\n",
      "iter 14040/30000  loss         0.126172  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.067\n",
      "iter 14041/30000  loss         0.126172  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.067\n",
      "iter 14060/30000  loss         0.126164  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.068\n",
      "iter 14061/30000  loss         0.126164  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.068\n",
      "iter 14080/30000  loss         0.126155  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.070\n",
      "iter 14081/30000  loss         0.126155  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.070\n",
      "iter 14100/30000  loss         0.126147  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.071\n",
      "iter 14101/30000  loss         0.126147  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.071\n",
      "iter 14120/30000  loss         0.126139  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.072\n",
      "iter 14121/30000  loss         0.126138  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.072\n",
      "iter 14140/30000  loss         0.126130  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.073\n",
      "iter 14141/30000  loss         0.126130  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.073\n",
      "iter 14160/30000  loss         0.126122  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.074\n",
      "iter 14161/30000  loss         0.126121  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.074\n",
      "iter 14180/30000  loss         0.126113  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.076\n",
      "iter 14181/30000  loss         0.126113  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    5.076\n",
      "iter 14200/30000  loss         0.126105  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.077\n",
      "iter 14201/30000  loss         0.126105  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14220/30000  loss         0.126097  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.078\n",
      "iter 14221/30000  loss         0.126096  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.078\n",
      "iter 14240/30000  loss         0.126088  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.079\n",
      "iter 14241/30000  loss         0.126088  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.079\n",
      "iter 14260/30000  loss         0.126080  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.080\n",
      "iter 14261/30000  loss         0.126080  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.080\n",
      "iter 14280/30000  loss         0.126072  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.082\n",
      "iter 14281/30000  loss         0.126071  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.082\n",
      "iter 14300/30000  loss         0.126064  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.083\n",
      "iter 14301/30000  loss         0.126063  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.083\n",
      "iter 14320/30000  loss         0.126055  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.084\n",
      "iter 14321/30000  loss         0.126055  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.084\n",
      "iter 14340/30000  loss         0.126047  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.085\n",
      "iter 14341/30000  loss         0.126047  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.085\n",
      "iter 14360/30000  loss         0.126039  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.086\n",
      "iter 14361/30000  loss         0.126039  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.086\n",
      "iter 14380/30000  loss         0.126031  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.087\n",
      "iter 14381/30000  loss         0.126030  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.087\n",
      "iter 14400/30000  loss         0.126023  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.089\n",
      "iter 14401/30000  loss         0.126022  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.089\n",
      "iter 14420/30000  loss         0.126015  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.090\n",
      "iter 14421/30000  loss         0.126014  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.090\n",
      "iter 14440/30000  loss         0.126007  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.091\n",
      "iter 14441/30000  loss         0.126006  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.091\n",
      "iter 14460/30000  loss         0.125998  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.092\n",
      "iter 14461/30000  loss         0.125998  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.092\n",
      "iter 14480/30000  loss         0.125990  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.093\n",
      "iter 14481/30000  loss         0.125990  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    5.093\n",
      "iter 14500/30000  loss         0.125982  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.094\n",
      "iter 14501/30000  loss         0.125982  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.094\n",
      "iter 14520/30000  loss         0.125974  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.096\n",
      "iter 14521/30000  loss         0.125974  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.096\n",
      "iter 14540/30000  loss         0.125966  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.097\n",
      "iter 14541/30000  loss         0.125966  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.097\n",
      "iter 14560/30000  loss         0.125958  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.098\n",
      "iter 14561/30000  loss         0.125958  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.098\n",
      "iter 14580/30000  loss         0.125950  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.099\n",
      "iter 14581/30000  loss         0.125950  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.099\n",
      "iter 14600/30000  loss         0.125942  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.100\n",
      "iter 14601/30000  loss         0.125942  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.100\n",
      "iter 14620/30000  loss         0.125935  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.101\n",
      "iter 14621/30000  loss         0.125934  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.101\n",
      "iter 14640/30000  loss         0.125927  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.102\n",
      "iter 14641/30000  loss         0.125926  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.102\n",
      "iter 14660/30000  loss         0.125919  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.103\n",
      "iter 14661/30000  loss         0.125918  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.104\n",
      "iter 14680/30000  loss         0.125911  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.105\n",
      "iter 14681/30000  loss         0.125911  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.105\n",
      "iter 14700/30000  loss         0.125903  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.106\n",
      "iter 14701/30000  loss         0.125903  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.106\n",
      "iter 14720/30000  loss         0.125895  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.107\n",
      "iter 14721/30000  loss         0.125895  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.107\n",
      "iter 14740/30000  loss         0.125888  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.108\n",
      "iter 14741/30000  loss         0.125887  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.108\n",
      "iter 14760/30000  loss         0.125880  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.109\n",
      "iter 14761/30000  loss         0.125879  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.109\n",
      "iter 14780/30000  loss         0.125872  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.110\n",
      "iter 14781/30000  loss         0.125872  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.110\n",
      "iter 14800/30000  loss         0.125864  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.111\n",
      "iter 14801/30000  loss         0.125864  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    5.111\n",
      "iter 14820/30000  loss         0.125857  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.112\n",
      "iter 14821/30000  loss         0.125856  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.112\n",
      "iter 14840/30000  loss         0.125849  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.114\n",
      "iter 14841/30000  loss         0.125848  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.114\n",
      "iter 14860/30000  loss         0.125841  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.115\n",
      "iter 14861/30000  loss         0.125841  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.115\n",
      "iter 14880/30000  loss         0.125833  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.116\n",
      "iter 14881/30000  loss         0.125833  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.116\n",
      "iter 14900/30000  loss         0.125826  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.117\n",
      "iter 14901/30000  loss         0.125825  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.117\n",
      "iter 14920/30000  loss         0.125818  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.118\n",
      "iter 14921/30000  loss         0.125818  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.118\n",
      "iter 14940/30000  loss         0.125811  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.119\n",
      "iter 14941/30000  loss         0.125810  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.119\n",
      "iter 14960/30000  loss         0.125803  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.120\n",
      "iter 14961/30000  loss         0.125803  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.120\n",
      "iter 14980/30000  loss         0.125795  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.121\n",
      "iter 14981/30000  loss         0.125795  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.121\n",
      "iter 15000/30000  loss         0.125788  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.122\n",
      "iter 15001/30000  loss         0.125788  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15020/30000  loss         0.125780  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.123\n",
      "iter 15021/30000  loss         0.125780  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.123\n",
      "iter 15040/30000  loss         0.125773  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.124\n",
      "iter 15041/30000  loss         0.125773  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.124\n",
      "iter 15060/30000  loss         0.125765  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.125\n",
      "iter 15061/30000  loss         0.125765  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.125\n",
      "iter 15080/30000  loss         0.125758  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.126\n",
      "iter 15081/30000  loss         0.125758  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.127\n",
      "iter 15100/30000  loss         0.125751  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.128\n",
      "iter 15101/30000  loss         0.125750  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.128\n",
      "iter 15120/30000  loss         0.125743  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.129\n",
      "iter 15121/30000  loss         0.125743  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.129\n",
      "iter 15140/30000  loss         0.125736  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.130\n",
      "iter 15141/30000  loss         0.125735  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    5.130\n",
      "iter 15160/30000  loss         0.125728  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.131\n",
      "iter 15161/30000  loss         0.125728  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.131\n",
      "iter 15180/30000  loss         0.125721  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.132\n",
      "iter 15181/30000  loss         0.125721  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.132\n",
      "iter 15200/30000  loss         0.125714  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.133\n",
      "iter 15201/30000  loss         0.125713  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.133\n",
      "iter 15220/30000  loss         0.125706  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.134\n",
      "iter 15221/30000  loss         0.125706  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.134\n",
      "iter 15240/30000  loss         0.125699  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.135\n",
      "iter 15241/30000  loss         0.125698  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.135\n",
      "iter 15260/30000  loss         0.125692  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.136\n",
      "iter 15261/30000  loss         0.125691  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.136\n",
      "iter 15280/30000  loss         0.125684  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.137\n",
      "iter 15281/30000  loss         0.125684  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.137\n",
      "iter 15300/30000  loss         0.125677  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.138\n",
      "iter 15301/30000  loss         0.125677  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.138\n",
      "iter 15320/30000  loss         0.125670  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.139\n",
      "iter 15321/30000  loss         0.125669  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.139\n",
      "iter 15340/30000  loss         0.125662  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.140\n",
      "iter 15341/30000  loss         0.125662  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.140\n",
      "iter 15360/30000  loss         0.125655  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.141\n",
      "iter 15361/30000  loss         0.125655  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.141\n",
      "iter 15380/30000  loss         0.125648  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.142\n",
      "iter 15381/30000  loss         0.125648  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.142\n",
      "iter 15400/30000  loss         0.125641  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.143\n",
      "iter 15401/30000  loss         0.125641  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.143\n",
      "iter 15420/30000  loss         0.125634  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.144\n",
      "iter 15421/30000  loss         0.125633  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.144\n",
      "iter 15440/30000  loss         0.125627  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.145\n",
      "iter 15441/30000  loss         0.125626  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.145\n",
      "iter 15460/30000  loss         0.125619  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.146\n",
      "iter 15461/30000  loss         0.125619  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.146\n",
      "iter 15480/30000  loss         0.125612  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.147\n",
      "iter 15481/30000  loss         0.125612  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    5.147\n",
      "iter 15500/30000  loss         0.125605  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.148\n",
      "iter 15501/30000  loss         0.125605  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.148\n",
      "iter 15520/30000  loss         0.125598  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.149\n",
      "iter 15521/30000  loss         0.125598  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.149\n",
      "iter 15540/30000  loss         0.125591  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.150\n",
      "iter 15541/30000  loss         0.125591  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.150\n",
      "iter 15560/30000  loss         0.125584  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.151\n",
      "iter 15561/30000  loss         0.125584  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.151\n",
      "iter 15580/30000  loss         0.125577  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.152\n",
      "iter 15581/30000  loss         0.125577  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.152\n",
      "iter 15600/30000  loss         0.125570  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.153\n",
      "iter 15601/30000  loss         0.125570  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.153\n",
      "iter 15620/30000  loss         0.125563  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.154\n",
      "iter 15621/30000  loss         0.125563  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.154\n",
      "iter 15640/30000  loss         0.125556  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.155\n",
      "iter 15641/30000  loss         0.125556  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.155\n",
      "iter 15660/30000  loss         0.125549  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.156\n",
      "iter 15661/30000  loss         0.125549  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.156\n",
      "iter 15680/30000  loss         0.125542  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.157\n",
      "iter 15681/30000  loss         0.125542  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.157\n",
      "iter 15700/30000  loss         0.125535  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.158\n",
      "iter 15701/30000  loss         0.125535  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.158\n",
      "iter 15720/30000  loss         0.125528  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.159\n",
      "iter 15721/30000  loss         0.125528  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.159\n",
      "iter 15740/30000  loss         0.125521  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.160\n",
      "iter 15741/30000  loss         0.125521  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.160\n",
      "iter 15760/30000  loss         0.125514  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.161\n",
      "iter 15761/30000  loss         0.125514  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.161\n",
      "iter 15780/30000  loss         0.125508  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.162\n",
      "iter 15781/30000  loss         0.125507  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.162\n",
      "iter 15800/30000  loss         0.125501  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.163\n",
      "iter 15801/30000  loss         0.125500  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15820/30000  loss         0.125494  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.164\n",
      "iter 15821/30000  loss         0.125494  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.164\n",
      "iter 15840/30000  loss         0.125487  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.165\n",
      "iter 15841/30000  loss         0.125487  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    5.165\n",
      "iter 15860/30000  loss         0.125480  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.166\n",
      "iter 15861/30000  loss         0.125480  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.166\n",
      "iter 15880/30000  loss         0.125474  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.167\n",
      "iter 15881/30000  loss         0.125473  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.167\n",
      "iter 15900/30000  loss         0.125467  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.168\n",
      "iter 15901/30000  loss         0.125466  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.168\n",
      "iter 15920/30000  loss         0.125460  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.169\n",
      "iter 15921/30000  loss         0.125460  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.169\n",
      "iter 15940/30000  loss         0.125453  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.170\n",
      "iter 15941/30000  loss         0.125453  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.170\n",
      "iter 15960/30000  loss         0.125447  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.171\n",
      "iter 15961/30000  loss         0.125446  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.171\n",
      "iter 15980/30000  loss         0.125440  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.172\n",
      "iter 15981/30000  loss         0.125440  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.172\n",
      "iter 16000/30000  loss         0.125433  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.173\n",
      "iter 16001/30000  loss         0.125433  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.173\n",
      "iter 16020/30000  loss         0.125426  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.173\n",
      "iter 16021/30000  loss         0.125426  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.174\n",
      "iter 16040/30000  loss         0.125420  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.174\n",
      "iter 16041/30000  loss         0.125419  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.174\n",
      "iter 16060/30000  loss         0.125413  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.175\n",
      "iter 16061/30000  loss         0.125413  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.175\n",
      "iter 16080/30000  loss         0.125407  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.176\n",
      "iter 16081/30000  loss         0.125406  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.176\n",
      "iter 16100/30000  loss         0.125400  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.177\n",
      "iter 16101/30000  loss         0.125400  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.177\n",
      "iter 16120/30000  loss         0.125393  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.178\n",
      "iter 16121/30000  loss         0.125393  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.178\n",
      "iter 16140/30000  loss         0.125387  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.179\n",
      "iter 16141/30000  loss         0.125386  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.179\n",
      "iter 16160/30000  loss         0.125380  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.180\n",
      "iter 16161/30000  loss         0.125380  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.180\n",
      "iter 16180/30000  loss         0.125374  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.181\n",
      "iter 16181/30000  loss         0.125373  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.181\n",
      "iter 16200/30000  loss         0.125367  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.182\n",
      "iter 16201/30000  loss         0.125367  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.182\n",
      "iter 16220/30000  loss         0.125361  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.183\n",
      "iter 16221/30000  loss         0.125360  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    5.183\n",
      "iter 16240/30000  loss         0.125354  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.184\n",
      "iter 16241/30000  loss         0.125354  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.184\n",
      "iter 16260/30000  loss         0.125348  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.185\n",
      "iter 16261/30000  loss         0.125347  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.185\n",
      "iter 16280/30000  loss         0.125341  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.186\n",
      "iter 16281/30000  loss         0.125341  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.186\n",
      "iter 16300/30000  loss         0.125335  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.186\n",
      "iter 16301/30000  loss         0.125334  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.186\n",
      "iter 16320/30000  loss         0.125328  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.187\n",
      "iter 16321/30000  loss         0.125328  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.187\n",
      "iter 16340/30000  loss         0.125322  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.188\n",
      "iter 16341/30000  loss         0.125321  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.188\n",
      "iter 16360/30000  loss         0.125315  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.189\n",
      "iter 16361/30000  loss         0.125315  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.189\n",
      "iter 16380/30000  loss         0.125309  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.190\n",
      "iter 16381/30000  loss         0.125309  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.190\n",
      "iter 16400/30000  loss         0.125303  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.191\n",
      "iter 16401/30000  loss         0.125302  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.191\n",
      "iter 16420/30000  loss         0.125296  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.192\n",
      "iter 16421/30000  loss         0.125296  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.192\n",
      "iter 16440/30000  loss         0.125290  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.193\n",
      "iter 16441/30000  loss         0.125289  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.193\n",
      "iter 16460/30000  loss         0.125283  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.194\n",
      "iter 16461/30000  loss         0.125283  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.194\n",
      "iter 16480/30000  loss         0.125277  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.194\n",
      "iter 16481/30000  loss         0.125277  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.195\n",
      "iter 16500/30000  loss         0.125271  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.195\n",
      "iter 16501/30000  loss         0.125271  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.195\n",
      "iter 16520/30000  loss         0.125265  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.196\n",
      "iter 16521/30000  loss         0.125264  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.196\n",
      "iter 16540/30000  loss         0.125258  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.197\n",
      "iter 16541/30000  loss         0.125258  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.197\n",
      "iter 16560/30000  loss         0.125252  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.198\n",
      "iter 16561/30000  loss         0.125252  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.198\n",
      "iter 16580/30000  loss         0.125246  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.199\n",
      "iter 16581/30000  loss         0.125245  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.199\n",
      "iter 16600/30000  loss         0.125239  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.200\n",
      "iter 16601/30000  loss         0.125239  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    5.200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16620/30000  loss         0.125233  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.201\n",
      "iter 16621/30000  loss         0.125233  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.201\n",
      "iter 16640/30000  loss         0.125227  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.201\n",
      "iter 16641/30000  loss         0.125227  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.202\n",
      "iter 16660/30000  loss         0.125221  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.202\n",
      "iter 16661/30000  loss         0.125220  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.202\n",
      "iter 16680/30000  loss         0.125215  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.203\n",
      "iter 16681/30000  loss         0.125214  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.203\n",
      "iter 16700/30000  loss         0.125208  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.204\n",
      "iter 16701/30000  loss         0.125208  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.204\n",
      "iter 16720/30000  loss         0.125202  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.205\n",
      "iter 16721/30000  loss         0.125202  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.205\n",
      "iter 16740/30000  loss         0.125196  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.206\n",
      "iter 16741/30000  loss         0.125196  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.206\n",
      "iter 16760/30000  loss         0.125190  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.207\n",
      "iter 16761/30000  loss         0.125190  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.207\n",
      "iter 16780/30000  loss         0.125184  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.207\n",
      "iter 16781/30000  loss         0.125184  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.208\n",
      "iter 16800/30000  loss         0.125178  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.208\n",
      "iter 16801/30000  loss         0.125177  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.208\n",
      "iter 16820/30000  loss         0.125172  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.209\n",
      "iter 16821/30000  loss         0.125171  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.209\n",
      "iter 16840/30000  loss         0.125166  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.210\n",
      "iter 16841/30000  loss         0.125165  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.210\n",
      "iter 16860/30000  loss         0.125160  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.211\n",
      "iter 16861/30000  loss         0.125159  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.211\n",
      "iter 16880/30000  loss         0.125153  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.212\n",
      "iter 16881/30000  loss         0.125153  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.212\n",
      "iter 16900/30000  loss         0.125147  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.213\n",
      "iter 16901/30000  loss         0.125147  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.213\n",
      "iter 16920/30000  loss         0.125141  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.213\n",
      "iter 16921/30000  loss         0.125141  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.213\n",
      "iter 16940/30000  loss         0.125135  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.214\n",
      "iter 16941/30000  loss         0.125135  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.214\n",
      "iter 16960/30000  loss         0.125129  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.215\n",
      "iter 16961/30000  loss         0.125129  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.215\n",
      "iter 16980/30000  loss         0.125123  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.216\n",
      "iter 16981/30000  loss         0.125123  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.216\n",
      "iter 17000/30000  loss         0.125117  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.217\n",
      "iter 17001/30000  loss         0.125117  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    5.217\n",
      "iter 17020/30000  loss         0.125111  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.218\n",
      "iter 17021/30000  loss         0.125111  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.218\n",
      "iter 17040/30000  loss         0.125106  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.218\n",
      "iter 17041/30000  loss         0.125105  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.218\n",
      "iter 17060/30000  loss         0.125100  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.219\n",
      "iter 17061/30000  loss         0.125099  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.219\n",
      "iter 17080/30000  loss         0.125094  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.220\n",
      "iter 17081/30000  loss         0.125093  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.220\n",
      "iter 17100/30000  loss         0.125088  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.221\n",
      "iter 17101/30000  loss         0.125087  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.221\n",
      "iter 17120/30000  loss         0.125082  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.222\n",
      "iter 17121/30000  loss         0.125082  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.222\n",
      "iter 17140/30000  loss         0.125076  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.222\n",
      "iter 17141/30000  loss         0.125076  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.222\n",
      "iter 17160/30000  loss         0.125070  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.223\n",
      "iter 17161/30000  loss         0.125070  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.223\n",
      "iter 17180/30000  loss         0.125064  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.224\n",
      "iter 17181/30000  loss         0.125064  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.224\n",
      "iter 17200/30000  loss         0.125058  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.225\n",
      "iter 17201/30000  loss         0.125058  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.225\n",
      "iter 17220/30000  loss         0.125053  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.226\n",
      "iter 17221/30000  loss         0.125052  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.226\n",
      "iter 17240/30000  loss         0.125047  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.226\n",
      "iter 17241/30000  loss         0.125046  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.227\n",
      "iter 17260/30000  loss         0.125041  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.227\n",
      "iter 17261/30000  loss         0.125041  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.227\n",
      "iter 17280/30000  loss         0.125035  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.228\n",
      "iter 17281/30000  loss         0.125035  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.228\n",
      "iter 17300/30000  loss         0.125029  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.229\n",
      "iter 17301/30000  loss         0.125029  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.229\n",
      "iter 17320/30000  loss         0.125024  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.230\n",
      "iter 17321/30000  loss         0.125023  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.230\n",
      "iter 17340/30000  loss         0.125018  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.230\n",
      "iter 17341/30000  loss         0.125018  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.231\n",
      "iter 17360/30000  loss         0.125012  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.231\n",
      "iter 17361/30000  loss         0.125012  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.231\n",
      "iter 17380/30000  loss         0.125006  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.232\n",
      "iter 17381/30000  loss         0.125006  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.232\n",
      "iter 17400/30000  loss         0.125001  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.233\n",
      "iter 17401/30000  loss         0.125000  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17420/30000  loss         0.124995  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.234\n",
      "iter 17421/30000  loss         0.124995  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    5.234\n",
      "iter 17440/30000  loss         0.124989  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.234\n",
      "iter 17441/30000  loss         0.124989  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.234\n",
      "iter 17460/30000  loss         0.124984  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.235\n",
      "iter 17461/30000  loss         0.124983  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.235\n",
      "iter 17480/30000  loss         0.124978  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.236\n",
      "iter 17481/30000  loss         0.124978  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.236\n",
      "iter 17500/30000  loss         0.124972  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.237\n",
      "iter 17501/30000  loss         0.124972  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.237\n",
      "iter 17520/30000  loss         0.124967  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.238\n",
      "iter 17521/30000  loss         0.124966  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.238\n",
      "iter 17540/30000  loss         0.124961  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.238\n",
      "iter 17541/30000  loss         0.124961  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.238\n",
      "iter 17560/30000  loss         0.124955  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.239\n",
      "iter 17561/30000  loss         0.124955  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.239\n",
      "iter 17580/30000  loss         0.124950  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.240\n",
      "iter 17581/30000  loss         0.124949  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.240\n",
      "iter 17600/30000  loss         0.124944  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.241\n",
      "iter 17601/30000  loss         0.124944  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.241\n",
      "iter 17620/30000  loss         0.124939  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.241\n",
      "iter 17621/30000  loss         0.124938  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.241\n",
      "iter 17640/30000  loss         0.124933  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.242\n",
      "iter 17641/30000  loss         0.124933  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.242\n",
      "iter 17660/30000  loss         0.124927  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.243\n",
      "iter 17661/30000  loss         0.124927  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.243\n",
      "iter 17680/30000  loss         0.124922  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.244\n",
      "iter 17681/30000  loss         0.124922  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.244\n",
      "iter 17700/30000  loss         0.124916  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.244\n",
      "iter 17701/30000  loss         0.124916  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.244\n",
      "iter 17720/30000  loss         0.124911  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.245\n",
      "iter 17721/30000  loss         0.124911  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.245\n",
      "iter 17740/30000  loss         0.124905  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.246\n",
      "iter 17741/30000  loss         0.124905  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.246\n",
      "iter 17760/30000  loss         0.124900  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.247\n",
      "iter 17761/30000  loss         0.124900  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.247\n",
      "iter 17780/30000  loss         0.124894  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.247\n",
      "iter 17781/30000  loss         0.124894  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.247\n",
      "iter 17800/30000  loss         0.124889  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.248\n",
      "iter 17801/30000  loss         0.124889  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.248\n",
      "iter 17820/30000  loss         0.124883  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.249\n",
      "iter 17821/30000  loss         0.124883  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.249\n",
      "iter 17840/30000  loss         0.124878  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.250\n",
      "iter 17841/30000  loss         0.124878  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.250\n",
      "iter 17860/30000  loss         0.124872  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.250\n",
      "iter 17861/30000  loss         0.124872  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    5.250\n",
      "iter 17880/30000  loss         0.124867  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.251\n",
      "iter 17881/30000  loss         0.124867  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.251\n",
      "iter 17900/30000  loss         0.124862  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.252\n",
      "iter 17901/30000  loss         0.124861  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.252\n",
      "iter 17920/30000  loss         0.124856  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.253\n",
      "iter 17921/30000  loss         0.124856  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.253\n",
      "iter 17940/30000  loss         0.124851  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.253\n",
      "iter 17941/30000  loss         0.124850  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.253\n",
      "iter 17960/30000  loss         0.124845  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.254\n",
      "iter 17961/30000  loss         0.124845  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.254\n",
      "iter 17980/30000  loss         0.124840  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.255\n",
      "iter 17981/30000  loss         0.124840  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.255\n",
      "iter 18000/30000  loss         0.124835  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.256\n",
      "iter 18001/30000  loss         0.124834  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.256\n",
      "iter 18020/30000  loss         0.124829  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.256\n",
      "iter 18021/30000  loss         0.124829  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.256\n",
      "iter 18040/30000  loss         0.124824  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.257\n",
      "iter 18041/30000  loss         0.124824  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.257\n",
      "iter 18060/30000  loss         0.124819  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.258\n",
      "iter 18061/30000  loss         0.124818  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.258\n",
      "iter 18080/30000  loss         0.124813  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.258\n",
      "iter 18081/30000  loss         0.124813  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.258\n",
      "iter 18100/30000  loss         0.124808  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.259\n",
      "iter 18101/30000  loss         0.124808  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.259\n",
      "iter 18120/30000  loss         0.124803  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.260\n",
      "iter 18121/30000  loss         0.124802  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.260\n",
      "iter 18140/30000  loss         0.124797  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.261\n",
      "iter 18141/30000  loss         0.124797  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.261\n",
      "iter 18160/30000  loss         0.124792  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.261\n",
      "iter 18161/30000  loss         0.124792  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.261\n",
      "iter 18180/30000  loss         0.124787  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.262\n",
      "iter 18181/30000  loss         0.124787  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.262\n",
      "iter 18200/30000  loss         0.124782  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.263\n",
      "iter 18201/30000  loss         0.124781  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18220/30000  loss         0.124776  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.263\n",
      "iter 18221/30000  loss         0.124776  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.264\n",
      "iter 18240/30000  loss         0.124771  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.264\n",
      "iter 18241/30000  loss         0.124771  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.264\n",
      "iter 18260/30000  loss         0.124766  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.265\n",
      "iter 18261/30000  loss         0.124766  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.265\n",
      "iter 18280/30000  loss         0.124761  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.266\n",
      "iter 18281/30000  loss         0.124760  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.266\n",
      "iter 18300/30000  loss         0.124755  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.266\n",
      "iter 18301/30000  loss         0.124755  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.266\n",
      "iter 18320/30000  loss         0.124750  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    5.267\n",
      "iter 18321/30000  loss         0.124750  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.267\n",
      "iter 18340/30000  loss         0.124745  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.268\n",
      "iter 18341/30000  loss         0.124745  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.268\n",
      "iter 18360/30000  loss         0.124740  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.268\n",
      "iter 18361/30000  loss         0.124740  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.268\n",
      "iter 18380/30000  loss         0.124735  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.269\n",
      "iter 18381/30000  loss         0.124735  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.269\n",
      "iter 18400/30000  loss         0.124730  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.270\n",
      "iter 18401/30000  loss         0.124729  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.270\n",
      "iter 18420/30000  loss         0.124724  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.270\n",
      "iter 18421/30000  loss         0.124724  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.271\n",
      "iter 18440/30000  loss         0.124719  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.271\n",
      "iter 18441/30000  loss         0.124719  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.271\n",
      "iter 18460/30000  loss         0.124714  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.272\n",
      "iter 18461/30000  loss         0.124714  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.272\n",
      "iter 18480/30000  loss         0.124709  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.273\n",
      "iter 18481/30000  loss         0.124709  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.273\n",
      "iter 18500/30000  loss         0.124704  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.273\n",
      "iter 18501/30000  loss         0.124704  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.273\n",
      "iter 18520/30000  loss         0.124699  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.274\n",
      "iter 18521/30000  loss         0.124699  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.274\n",
      "iter 18540/30000  loss         0.124694  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.275\n",
      "iter 18541/30000  loss         0.124694  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.275\n",
      "iter 18560/30000  loss         0.124689  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.275\n",
      "iter 18561/30000  loss         0.124689  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.275\n",
      "iter 18580/30000  loss         0.124684  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.276\n",
      "iter 18581/30000  loss         0.124683  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.276\n",
      "iter 18600/30000  loss         0.124679  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.277\n",
      "iter 18601/30000  loss         0.124678  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.277\n",
      "iter 18620/30000  loss         0.124674  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.277\n",
      "iter 18621/30000  loss         0.124673  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.277\n",
      "iter 18640/30000  loss         0.124669  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.278\n",
      "iter 18641/30000  loss         0.124668  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.278\n",
      "iter 18660/30000  loss         0.124664  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.279\n",
      "iter 18661/30000  loss         0.124663  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.279\n",
      "iter 18680/30000  loss         0.124659  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.279\n",
      "iter 18681/30000  loss         0.124658  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.279\n",
      "iter 18700/30000  loss         0.124654  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.280\n",
      "iter 18701/30000  loss         0.124653  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.280\n",
      "iter 18720/30000  loss         0.124649  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.281\n",
      "iter 18721/30000  loss         0.124648  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.281\n",
      "iter 18740/30000  loss         0.124644  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.281\n",
      "iter 18741/30000  loss         0.124643  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.281\n",
      "iter 18760/30000  loss         0.124639  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.282\n",
      "iter 18761/30000  loss         0.124638  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.282\n",
      "iter 18780/30000  loss         0.124634  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.283\n",
      "iter 18781/30000  loss         0.124633  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    5.283\n",
      "iter 18800/30000  loss         0.124629  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.283\n",
      "iter 18801/30000  loss         0.124629  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.283\n",
      "iter 18820/30000  loss         0.124624  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.284\n",
      "iter 18821/30000  loss         0.124624  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.284\n",
      "iter 18840/30000  loss         0.124619  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.285\n",
      "iter 18841/30000  loss         0.124619  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.285\n",
      "iter 18860/30000  loss         0.124614  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.285\n",
      "iter 18861/30000  loss         0.124614  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.285\n",
      "iter 18880/30000  loss         0.124609  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.286\n",
      "iter 18881/30000  loss         0.124609  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.286\n",
      "iter 18900/30000  loss         0.124604  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.287\n",
      "iter 18901/30000  loss         0.124604  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.287\n",
      "iter 18920/30000  loss         0.124599  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.287\n",
      "iter 18921/30000  loss         0.124599  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.287\n",
      "iter 18940/30000  loss         0.124594  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.288\n",
      "iter 18941/30000  loss         0.124594  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.288\n",
      "iter 18960/30000  loss         0.124590  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.289\n",
      "iter 18961/30000  loss         0.124589  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.289\n",
      "iter 18980/30000  loss         0.124585  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.289\n",
      "iter 18981/30000  loss         0.124584  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.289\n",
      "iter 19000/30000  loss         0.124580  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.290\n",
      "iter 19001/30000  loss         0.124580  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19020/30000  loss         0.124575  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.290\n",
      "iter 19021/30000  loss         0.124575  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.290\n",
      "iter 19040/30000  loss         0.124570  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.291\n",
      "iter 19041/30000  loss         0.124570  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.291\n",
      "iter 19060/30000  loss         0.124565  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.292\n",
      "iter 19061/30000  loss         0.124565  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.292\n",
      "iter 19080/30000  loss         0.124561  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.292\n",
      "iter 19081/30000  loss         0.124560  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.292\n",
      "iter 19100/30000  loss         0.124556  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.293\n",
      "iter 19101/30000  loss         0.124556  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.293\n",
      "iter 19120/30000  loss         0.124551  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.294\n",
      "iter 19121/30000  loss         0.124551  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.294\n",
      "iter 19140/30000  loss         0.124546  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.294\n",
      "iter 19141/30000  loss         0.124546  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.294\n",
      "iter 19160/30000  loss         0.124541  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.295\n",
      "iter 19161/30000  loss         0.124541  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.295\n",
      "iter 19180/30000  loss         0.124537  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.296\n",
      "iter 19181/30000  loss         0.124536  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.296\n",
      "iter 19200/30000  loss         0.124532  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.296\n",
      "iter 19201/30000  loss         0.124532  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.296\n",
      "iter 19220/30000  loss         0.124527  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.297\n",
      "iter 19221/30000  loss         0.124527  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.297\n",
      "iter 19240/30000  loss         0.124522  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.297\n",
      "iter 19241/30000  loss         0.124522  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.297\n",
      "iter 19260/30000  loss         0.124518  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.298\n",
      "iter 19261/30000  loss         0.124518  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.298\n",
      "iter 19280/30000  loss         0.124513  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.299\n",
      "iter 19281/30000  loss         0.124513  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    5.299\n",
      "iter 19300/30000  loss         0.124508  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.299\n",
      "iter 19301/30000  loss         0.124508  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.299\n",
      "iter 19320/30000  loss         0.124504  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.300\n",
      "iter 19321/30000  loss         0.124503  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.300\n",
      "iter 19340/30000  loss         0.124499  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.301\n",
      "iter 19341/30000  loss         0.124499  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.301\n",
      "iter 19360/30000  loss         0.124494  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.301\n",
      "iter 19361/30000  loss         0.124494  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.301\n",
      "iter 19380/30000  loss         0.124490  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.302\n",
      "iter 19381/30000  loss         0.124489  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.302\n",
      "iter 19400/30000  loss         0.124485  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.302\n",
      "iter 19401/30000  loss         0.124485  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.302\n",
      "iter 19420/30000  loss         0.124480  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.303\n",
      "iter 19421/30000  loss         0.124480  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.303\n",
      "iter 19440/30000  loss         0.124476  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.304\n",
      "iter 19441/30000  loss         0.124475  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.304\n",
      "iter 19460/30000  loss         0.124471  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.304\n",
      "iter 19461/30000  loss         0.124471  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.304\n",
      "iter 19480/30000  loss         0.124466  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.305\n",
      "iter 19481/30000  loss         0.124466  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.305\n",
      "iter 19500/30000  loss         0.124462  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.305\n",
      "iter 19501/30000  loss         0.124462  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.305\n",
      "iter 19520/30000  loss         0.124457  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.306\n",
      "iter 19521/30000  loss         0.124457  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.306\n",
      "iter 19540/30000  loss         0.124453  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.307\n",
      "iter 19541/30000  loss         0.124452  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.307\n",
      "iter 19560/30000  loss         0.124448  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.307\n",
      "iter 19561/30000  loss         0.124448  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.307\n",
      "iter 19580/30000  loss         0.124443  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.308\n",
      "iter 19581/30000  loss         0.124443  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.308\n",
      "iter 19600/30000  loss         0.124439  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.308\n",
      "iter 19601/30000  loss         0.124439  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.308\n",
      "iter 19620/30000  loss         0.124434  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.309\n",
      "iter 19621/30000  loss         0.124434  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.309\n",
      "iter 19640/30000  loss         0.124430  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.310\n",
      "iter 19641/30000  loss         0.124429  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.310\n",
      "iter 19660/30000  loss         0.124425  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.310\n",
      "iter 19661/30000  loss         0.124425  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.310\n",
      "iter 19680/30000  loss         0.124421  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.311\n",
      "iter 19681/30000  loss         0.124420  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.311\n",
      "iter 19700/30000  loss         0.124416  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.311\n",
      "iter 19701/30000  loss         0.124416  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.311\n",
      "iter 19720/30000  loss         0.124412  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.312\n",
      "iter 19721/30000  loss         0.124411  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.312\n",
      "iter 19740/30000  loss         0.124407  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.313\n",
      "iter 19741/30000  loss         0.124407  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.313\n",
      "iter 19760/30000  loss         0.124403  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.313\n",
      "iter 19761/30000  loss         0.124402  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.313\n",
      "iter 19780/30000  loss         0.124398  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.314\n",
      "iter 19781/30000  loss         0.124398  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    5.314\n",
      "iter 19800/30000  loss         0.124394  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.314\n",
      "iter 19801/30000  loss         0.124393  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19820/30000  loss         0.124389  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.315\n",
      "iter 19821/30000  loss         0.124389  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.315\n",
      "iter 19840/30000  loss         0.124385  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.315\n",
      "iter 19841/30000  loss         0.124384  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.316\n",
      "iter 19860/30000  loss         0.124380  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.316\n",
      "iter 19861/30000  loss         0.124380  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.316\n",
      "iter 19880/30000  loss         0.124376  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.317\n",
      "iter 19881/30000  loss         0.124376  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.317\n",
      "iter 19900/30000  loss         0.124371  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.317\n",
      "iter 19901/30000  loss         0.124371  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.317\n",
      "iter 19920/30000  loss         0.124367  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.318\n",
      "iter 19921/30000  loss         0.124367  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.318\n",
      "iter 19940/30000  loss         0.124362  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.318\n",
      "iter 19941/30000  loss         0.124362  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.318\n",
      "iter 19960/30000  loss         0.124358  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.319\n",
      "iter 19961/30000  loss         0.124358  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.319\n",
      "iter 19980/30000  loss         0.124354  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.320\n",
      "iter 19981/30000  loss         0.124353  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.320\n",
      "iter 20000/30000  loss         0.124349  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.320\n",
      "iter 20001/30000  loss         0.124349  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.320\n",
      "iter 20020/30000  loss         0.124345  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.321\n",
      "iter 20021/30000  loss         0.124345  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.321\n",
      "iter 20040/30000  loss         0.124340  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.321\n",
      "iter 20041/30000  loss         0.124340  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.321\n",
      "iter 20060/30000  loss         0.124336  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.322\n",
      "iter 20061/30000  loss         0.124336  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.322\n",
      "iter 20080/30000  loss         0.124332  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.322\n",
      "iter 20081/30000  loss         0.124332  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.322\n",
      "iter 20100/30000  loss         0.124327  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.323\n",
      "iter 20101/30000  loss         0.124327  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.323\n",
      "iter 20120/30000  loss         0.124323  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.323\n",
      "iter 20121/30000  loss         0.124323  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.324\n",
      "iter 20140/30000  loss         0.124319  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.324\n",
      "iter 20141/30000  loss         0.124318  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.324\n",
      "iter 20160/30000  loss         0.124314  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.325\n",
      "iter 20161/30000  loss         0.124314  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.325\n",
      "iter 20180/30000  loss         0.124310  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.325\n",
      "iter 20181/30000  loss         0.124310  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.325\n",
      "iter 20200/30000  loss         0.124306  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.326\n",
      "iter 20201/30000  loss         0.124305  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.326\n",
      "iter 20220/30000  loss         0.124301  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.326\n",
      "iter 20221/30000  loss         0.124301  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.326\n",
      "iter 20240/30000  loss         0.124297  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.327\n",
      "iter 20241/30000  loss         0.124297  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.327\n",
      "iter 20260/30000  loss         0.124293  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.327\n",
      "iter 20261/30000  loss         0.124293  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.327\n",
      "iter 20280/30000  loss         0.124289  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.328\n",
      "iter 20281/30000  loss         0.124288  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.328\n",
      "iter 20300/30000  loss         0.124284  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.328\n",
      "iter 20301/30000  loss         0.124284  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.329\n",
      "iter 20320/30000  loss         0.124280  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.329\n",
      "iter 20321/30000  loss         0.124280  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    5.329\n",
      "iter 20340/30000  loss         0.124276  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.330\n",
      "iter 20341/30000  loss         0.124275  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.330\n",
      "iter 20360/30000  loss         0.124271  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.330\n",
      "iter 20361/30000  loss         0.124271  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.330\n",
      "iter 20380/30000  loss         0.124267  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.331\n",
      "iter 20381/30000  loss         0.124267  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.331\n",
      "iter 20400/30000  loss         0.124263  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.331\n",
      "iter 20401/30000  loss         0.124263  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.331\n",
      "iter 20420/30000  loss         0.124259  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.332\n",
      "iter 20421/30000  loss         0.124259  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.332\n",
      "iter 20440/30000  loss         0.124255  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.332\n",
      "iter 20441/30000  loss         0.124254  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.332\n",
      "iter 20460/30000  loss         0.124250  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.333\n",
      "iter 20461/30000  loss         0.124250  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.333\n",
      "iter 20480/30000  loss         0.124246  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.333\n",
      "iter 20481/30000  loss         0.124246  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.333\n",
      "iter 20500/30000  loss         0.124242  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.334\n",
      "iter 20501/30000  loss         0.124242  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.334\n",
      "iter 20520/30000  loss         0.124238  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.334\n",
      "iter 20521/30000  loss         0.124238  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.334\n",
      "iter 20540/30000  loss         0.124234  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.335\n",
      "iter 20541/30000  loss         0.124233  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.335\n",
      "iter 20560/30000  loss         0.124229  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.336\n",
      "iter 20561/30000  loss         0.124229  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.336\n",
      "iter 20580/30000  loss         0.124225  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.336\n",
      "iter 20581/30000  loss         0.124225  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.336\n",
      "iter 20600/30000  loss         0.124221  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.337\n",
      "iter 20601/30000  loss         0.124221  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20620/30000  loss         0.124217  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.337\n",
      "iter 20621/30000  loss         0.124217  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.337\n",
      "iter 20640/30000  loss         0.124213  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.338\n",
      "iter 20641/30000  loss         0.124213  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.338\n",
      "iter 20660/30000  loss         0.124209  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.338\n",
      "iter 20661/30000  loss         0.124208  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.338\n",
      "iter 20680/30000  loss         0.124204  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.339\n",
      "iter 20681/30000  loss         0.124204  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.339\n",
      "iter 20700/30000  loss         0.124200  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.339\n",
      "iter 20701/30000  loss         0.124200  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.339\n",
      "iter 20720/30000  loss         0.124196  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.340\n",
      "iter 20721/30000  loss         0.124196  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.340\n",
      "iter 20740/30000  loss         0.124192  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.340\n",
      "iter 20741/30000  loss         0.124192  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.340\n",
      "iter 20760/30000  loss         0.124188  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.341\n",
      "iter 20761/30000  loss         0.124188  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.341\n",
      "iter 20780/30000  loss         0.124184  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.341\n",
      "iter 20781/30000  loss         0.124184  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.341\n",
      "iter 20800/30000  loss         0.124180  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.342\n",
      "iter 20801/30000  loss         0.124180  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.342\n",
      "iter 20820/30000  loss         0.124176  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.342\n",
      "iter 20821/30000  loss         0.124176  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.342\n",
      "iter 20840/30000  loss         0.124172  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.343\n",
      "iter 20841/30000  loss         0.124172  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.343\n",
      "iter 20860/30000  loss         0.124168  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.343\n",
      "iter 20861/30000  loss         0.124167  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.343\n",
      "iter 20880/30000  loss         0.124164  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.344\n",
      "iter 20881/30000  loss         0.124163  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.344\n",
      "iter 20900/30000  loss         0.124160  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.344\n",
      "iter 20901/30000  loss         0.124159  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    5.344\n",
      "iter 20920/30000  loss         0.124156  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.345\n",
      "iter 20921/30000  loss         0.124155  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.345\n",
      "iter 20940/30000  loss         0.124151  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.345\n",
      "iter 20941/30000  loss         0.124151  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.345\n",
      "iter 20960/30000  loss         0.124147  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.346\n",
      "iter 20961/30000  loss         0.124147  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.346\n",
      "iter 20980/30000  loss         0.124143  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.346\n",
      "iter 20981/30000  loss         0.124143  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.346\n",
      "iter 21000/30000  loss         0.124139  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.347\n",
      "iter 21001/30000  loss         0.124139  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.347\n",
      "iter 21020/30000  loss         0.124135  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.347\n",
      "iter 21021/30000  loss         0.124135  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.347\n",
      "iter 21040/30000  loss         0.124131  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.348\n",
      "iter 21041/30000  loss         0.124131  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.348\n",
      "iter 21060/30000  loss         0.124127  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.348\n",
      "iter 21061/30000  loss         0.124127  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.348\n",
      "iter 21080/30000  loss         0.124123  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.349\n",
      "iter 21081/30000  loss         0.124123  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.349\n",
      "iter 21100/30000  loss         0.124119  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.349\n",
      "iter 21101/30000  loss         0.124119  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.349\n",
      "iter 21120/30000  loss         0.124116  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.350\n",
      "iter 21121/30000  loss         0.124115  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.350\n",
      "iter 21140/30000  loss         0.124112  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.350\n",
      "iter 21141/30000  loss         0.124111  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.350\n",
      "iter 21160/30000  loss         0.124108  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.351\n",
      "iter 21161/30000  loss         0.124107  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.351\n",
      "iter 21180/30000  loss         0.124104  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.351\n",
      "iter 21181/30000  loss         0.124103  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.351\n",
      "iter 21200/30000  loss         0.124100  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.352\n",
      "iter 21201/30000  loss         0.124100  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.352\n",
      "iter 21220/30000  loss         0.124096  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.352\n",
      "iter 21221/30000  loss         0.124096  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.352\n",
      "iter 21240/30000  loss         0.124092  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.353\n",
      "iter 21241/30000  loss         0.124092  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.353\n",
      "iter 21260/30000  loss         0.124088  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.353\n",
      "iter 21261/30000  loss         0.124088  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.353\n",
      "iter 21280/30000  loss         0.124084  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.354\n",
      "iter 21281/30000  loss         0.124084  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.354\n",
      "iter 21300/30000  loss         0.124080  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.354\n",
      "iter 21301/30000  loss         0.124080  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.354\n",
      "iter 21320/30000  loss         0.124076  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.355\n",
      "iter 21321/30000  loss         0.124076  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.355\n",
      "iter 21340/30000  loss         0.124072  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.355\n",
      "iter 21341/30000  loss         0.124072  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.355\n",
      "iter 21360/30000  loss         0.124068  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.356\n",
      "iter 21361/30000  loss         0.124068  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.356\n",
      "iter 21380/30000  loss         0.124065  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.356\n",
      "iter 21381/30000  loss         0.124064  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.356\n",
      "iter 21400/30000  loss         0.124061  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.357\n",
      "iter 21401/30000  loss         0.124061  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21420/30000  loss         0.124057  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.357\n",
      "iter 21421/30000  loss         0.124057  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.357\n",
      "iter 21440/30000  loss         0.124053  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.358\n",
      "iter 21441/30000  loss         0.124053  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.358\n",
      "iter 21460/30000  loss         0.124049  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.358\n",
      "iter 21461/30000  loss         0.124049  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.358\n",
      "iter 21480/30000  loss         0.124045  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.359\n",
      "iter 21481/30000  loss         0.124045  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    5.359\n",
      "iter 21500/30000  loss         0.124041  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.359\n",
      "iter 21501/30000  loss         0.124041  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.359\n",
      "iter 21520/30000  loss         0.124038  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.360\n",
      "iter 21521/30000  loss         0.124037  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.360\n",
      "iter 21540/30000  loss         0.124034  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.360\n",
      "iter 21541/30000  loss         0.124034  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.360\n",
      "iter 21560/30000  loss         0.124030  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.361\n",
      "iter 21561/30000  loss         0.124030  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.361\n",
      "iter 21580/30000  loss         0.124026  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.361\n",
      "iter 21581/30000  loss         0.124026  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.361\n",
      "iter 21600/30000  loss         0.124022  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.362\n",
      "iter 21601/30000  loss         0.124022  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.362\n",
      "iter 21620/30000  loss         0.124019  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.362\n",
      "iter 21621/30000  loss         0.124018  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.362\n",
      "iter 21640/30000  loss         0.124015  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.362\n",
      "iter 21641/30000  loss         0.124015  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.363\n",
      "iter 21660/30000  loss         0.124011  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.363\n",
      "iter 21661/30000  loss         0.124011  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.363\n",
      "iter 21680/30000  loss         0.124007  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.363\n",
      "iter 21681/30000  loss         0.124007  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.363\n",
      "iter 21700/30000  loss         0.124004  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.364\n",
      "iter 21701/30000  loss         0.124003  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.364\n",
      "iter 21720/30000  loss         0.124000  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.364\n",
      "iter 21721/30000  loss         0.124000  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.364\n",
      "iter 21740/30000  loss         0.123996  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.365\n",
      "iter 21741/30000  loss         0.123996  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.365\n",
      "iter 21760/30000  loss         0.123992  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.365\n",
      "iter 21761/30000  loss         0.123992  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.365\n",
      "iter 21780/30000  loss         0.123989  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.366\n",
      "iter 21781/30000  loss         0.123988  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.366\n",
      "iter 21800/30000  loss         0.123985  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.366\n",
      "iter 21801/30000  loss         0.123985  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.366\n",
      "iter 21820/30000  loss         0.123981  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.367\n",
      "iter 21821/30000  loss         0.123981  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.367\n",
      "iter 21840/30000  loss         0.123977  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.367\n",
      "iter 21841/30000  loss         0.123977  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.367\n",
      "iter 21860/30000  loss         0.123974  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.368\n",
      "iter 21861/30000  loss         0.123973  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.368\n",
      "iter 21880/30000  loss         0.123970  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.368\n",
      "iter 21881/30000  loss         0.123970  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.368\n",
      "iter 21900/30000  loss         0.123966  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.368\n",
      "iter 21901/30000  loss         0.123966  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.369\n",
      "iter 21920/30000  loss         0.123963  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.369\n",
      "iter 21921/30000  loss         0.123962  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.369\n",
      "iter 21940/30000  loss         0.123959  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.369\n",
      "iter 21941/30000  loss         0.123959  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.369\n",
      "iter 21960/30000  loss         0.123955  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.370\n",
      "iter 21961/30000  loss         0.123955  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.370\n",
      "iter 21980/30000  loss         0.123951  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.370\n",
      "iter 21981/30000  loss         0.123951  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.370\n",
      "iter 22000/30000  loss         0.123948  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.371\n",
      "iter 22001/30000  loss         0.123948  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.371\n",
      "iter 22020/30000  loss         0.123944  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.371\n",
      "iter 22021/30000  loss         0.123944  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.371\n",
      "iter 22040/30000  loss         0.123940  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.372\n",
      "iter 22041/30000  loss         0.123940  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.372\n",
      "iter 22060/30000  loss         0.123937  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.372\n",
      "iter 22061/30000  loss         0.123937  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.372\n",
      "iter 22080/30000  loss         0.123933  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.373\n",
      "iter 22081/30000  loss         0.123933  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.373\n",
      "iter 22100/30000  loss         0.123930  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.373\n",
      "iter 22101/30000  loss         0.123929  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.373\n",
      "iter 22120/30000  loss         0.123926  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.373\n",
      "iter 22121/30000  loss         0.123926  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    5.373\n",
      "iter 22140/30000  loss         0.123922  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.374\n",
      "iter 22141/30000  loss         0.123922  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.374\n",
      "iter 22160/30000  loss         0.123919  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.374\n",
      "iter 22161/30000  loss         0.123918  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.374\n",
      "iter 22180/30000  loss         0.123915  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.375\n",
      "iter 22181/30000  loss         0.123915  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.375\n",
      "iter 22200/30000  loss         0.123911  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.375\n",
      "iter 22201/30000  loss         0.123911  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22220/30000  loss         0.123908  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.376\n",
      "iter 22221/30000  loss         0.123908  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.376\n",
      "iter 22240/30000  loss         0.123904  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.376\n",
      "iter 22241/30000  loss         0.123904  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.376\n",
      "iter 22260/30000  loss         0.123901  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.376\n",
      "iter 22261/30000  loss         0.123900  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.377\n",
      "iter 22280/30000  loss         0.123897  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.377\n",
      "iter 22281/30000  loss         0.123897  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.377\n",
      "iter 22300/30000  loss         0.123893  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.377\n",
      "iter 22301/30000  loss         0.123893  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.377\n",
      "iter 22320/30000  loss         0.123890  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.378\n",
      "iter 22321/30000  loss         0.123890  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.378\n",
      "iter 22340/30000  loss         0.123886  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.378\n",
      "iter 22341/30000  loss         0.123886  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.378\n",
      "iter 22360/30000  loss         0.123883  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.379\n",
      "iter 22361/30000  loss         0.123883  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.379\n",
      "iter 22380/30000  loss         0.123879  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.379\n",
      "iter 22381/30000  loss         0.123879  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.379\n",
      "iter 22400/30000  loss         0.123876  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.380\n",
      "iter 22401/30000  loss         0.123876  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.380\n",
      "iter 22420/30000  loss         0.123872  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.380\n",
      "iter 22421/30000  loss         0.123872  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.380\n",
      "iter 22440/30000  loss         0.123869  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.380\n",
      "iter 22441/30000  loss         0.123868  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.380\n",
      "iter 22460/30000  loss         0.123865  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.381\n",
      "iter 22461/30000  loss         0.123865  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.381\n",
      "iter 22480/30000  loss         0.123862  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.381\n",
      "iter 22481/30000  loss         0.123861  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.381\n",
      "iter 22500/30000  loss         0.123858  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.382\n",
      "iter 22501/30000  loss         0.123858  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.382\n",
      "iter 22520/30000  loss         0.123855  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.382\n",
      "iter 22521/30000  loss         0.123854  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.382\n",
      "iter 22540/30000  loss         0.123851  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.382\n",
      "iter 22541/30000  loss         0.123851  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.383\n",
      "iter 22560/30000  loss         0.123848  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.383\n",
      "iter 22561/30000  loss         0.123847  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.383\n",
      "iter 22580/30000  loss         0.123844  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.383\n",
      "iter 22581/30000  loss         0.123844  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.383\n",
      "iter 22600/30000  loss         0.123841  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.384\n",
      "iter 22601/30000  loss         0.123840  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.384\n",
      "iter 22620/30000  loss         0.123837  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.384\n",
      "iter 22621/30000  loss         0.123837  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.384\n",
      "iter 22640/30000  loss         0.123834  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.385\n",
      "iter 22641/30000  loss         0.123833  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.385\n",
      "iter 22660/30000  loss         0.123830  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.385\n",
      "iter 22661/30000  loss         0.123830  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.385\n",
      "iter 22680/30000  loss         0.123827  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.385\n",
      "iter 22681/30000  loss         0.123827  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.385\n",
      "iter 22700/30000  loss         0.123823  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.386\n",
      "iter 22701/30000  loss         0.123823  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.386\n",
      "iter 22720/30000  loss         0.123820  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.386\n",
      "iter 22721/30000  loss         0.123820  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.386\n",
      "iter 22740/30000  loss         0.123816  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.387\n",
      "iter 22741/30000  loss         0.123816  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.387\n",
      "iter 22760/30000  loss         0.123813  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.387\n",
      "iter 22761/30000  loss         0.123813  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.387\n",
      "iter 22780/30000  loss         0.123809  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.387\n",
      "iter 22781/30000  loss         0.123809  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    5.387\n",
      "iter 22800/30000  loss         0.123806  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.388\n",
      "iter 22801/30000  loss         0.123806  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.388\n",
      "iter 22820/30000  loss         0.123803  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.388\n",
      "iter 22821/30000  loss         0.123802  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.388\n",
      "iter 22840/30000  loss         0.123799  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.389\n",
      "iter 22841/30000  loss         0.123799  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.389\n",
      "iter 22860/30000  loss         0.123796  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.389\n",
      "iter 22861/30000  loss         0.123796  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.389\n",
      "iter 22880/30000  loss         0.123792  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.390\n",
      "iter 22881/30000  loss         0.123792  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.390\n",
      "iter 22900/30000  loss         0.123789  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.390\n",
      "iter 22901/30000  loss         0.123789  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.390\n",
      "iter 22920/30000  loss         0.123786  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.390\n",
      "iter 22921/30000  loss         0.123785  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.390\n",
      "iter 22940/30000  loss         0.123782  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.391\n",
      "iter 22941/30000  loss         0.123782  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.391\n",
      "iter 22960/30000  loss         0.123779  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.391\n",
      "iter 22961/30000  loss         0.123779  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.391\n",
      "iter 22980/30000  loss         0.123775  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.392\n",
      "iter 22981/30000  loss         0.123775  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.392\n",
      "iter 23000/30000  loss         0.123772  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.392\n",
      "iter 23001/30000  loss         0.123772  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23020/30000  loss         0.123769  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.392\n",
      "iter 23021/30000  loss         0.123769  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.392\n",
      "iter 23040/30000  loss         0.123765  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.393\n",
      "iter 23041/30000  loss         0.123765  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.393\n",
      "iter 23060/30000  loss         0.123762  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.393\n",
      "iter 23061/30000  loss         0.123762  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.393\n",
      "iter 23080/30000  loss         0.123759  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.394\n",
      "iter 23081/30000  loss         0.123758  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.394\n",
      "iter 23100/30000  loss         0.123755  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.394\n",
      "iter 23101/30000  loss         0.123755  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.394\n",
      "iter 23120/30000  loss         0.123752  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.394\n",
      "iter 23121/30000  loss         0.123752  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.394\n",
      "iter 23140/30000  loss         0.123749  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.395\n",
      "iter 23141/30000  loss         0.123748  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.395\n",
      "iter 23160/30000  loss         0.123745  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.395\n",
      "iter 23161/30000  loss         0.123745  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.395\n",
      "iter 23180/30000  loss         0.123742  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.395\n",
      "iter 23181/30000  loss         0.123742  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.396\n",
      "iter 23200/30000  loss         0.123739  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.396\n",
      "iter 23201/30000  loss         0.123739  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.396\n",
      "iter 23220/30000  loss         0.123735  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.396\n",
      "iter 23221/30000  loss         0.123735  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.396\n",
      "iter 23240/30000  loss         0.123732  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.397\n",
      "iter 23241/30000  loss         0.123732  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.397\n",
      "iter 23260/30000  loss         0.123729  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.397\n",
      "iter 23261/30000  loss         0.123729  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.397\n",
      "iter 23280/30000  loss         0.123725  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.397\n",
      "iter 23281/30000  loss         0.123725  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.397\n",
      "iter 23300/30000  loss         0.123722  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.398\n",
      "iter 23301/30000  loss         0.123722  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.398\n",
      "iter 23320/30000  loss         0.123719  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.398\n",
      "iter 23321/30000  loss         0.123719  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.398\n",
      "iter 23340/30000  loss         0.123716  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.399\n",
      "iter 23341/30000  loss         0.123715  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.399\n",
      "iter 23360/30000  loss         0.123712  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.399\n",
      "iter 23361/30000  loss         0.123712  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.399\n",
      "iter 23380/30000  loss         0.123709  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.399\n",
      "iter 23381/30000  loss         0.123709  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.399\n",
      "iter 23400/30000  loss         0.123706  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.400\n",
      "iter 23401/30000  loss         0.123706  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.400\n",
      "iter 23420/30000  loss         0.123703  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.400\n",
      "iter 23421/30000  loss         0.123702  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.400\n",
      "iter 23440/30000  loss         0.123699  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.400\n",
      "iter 23441/30000  loss         0.123699  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.401\n",
      "iter 23460/30000  loss         0.123696  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.401\n",
      "iter 23461/30000  loss         0.123696  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.401\n",
      "iter 23480/30000  loss         0.123693  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.401\n",
      "iter 23481/30000  loss         0.123693  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    5.401\n",
      "iter 23500/30000  loss         0.123690  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.402\n",
      "iter 23501/30000  loss         0.123689  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.402\n",
      "iter 23520/30000  loss         0.123686  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.402\n",
      "iter 23521/30000  loss         0.123686  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.402\n",
      "iter 23540/30000  loss         0.123683  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.402\n",
      "iter 23541/30000  loss         0.123683  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.402\n",
      "iter 23560/30000  loss         0.123680  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.403\n",
      "iter 23561/30000  loss         0.123680  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.403\n",
      "iter 23580/30000  loss         0.123677  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.403\n",
      "iter 23581/30000  loss         0.123677  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.403\n",
      "iter 23600/30000  loss         0.123674  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.404\n",
      "iter 23601/30000  loss         0.123673  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.404\n",
      "iter 23620/30000  loss         0.123670  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.404\n",
      "iter 23621/30000  loss         0.123670  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.404\n",
      "iter 23640/30000  loss         0.123667  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.404\n",
      "iter 23641/30000  loss         0.123667  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.404\n",
      "iter 23660/30000  loss         0.123664  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.405\n",
      "iter 23661/30000  loss         0.123664  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.405\n",
      "iter 23680/30000  loss         0.123661  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.405\n",
      "iter 23681/30000  loss         0.123661  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.405\n",
      "iter 23700/30000  loss         0.123658  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.405\n",
      "iter 23701/30000  loss         0.123657  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.405\n",
      "iter 23720/30000  loss         0.123654  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.406\n",
      "iter 23721/30000  loss         0.123654  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.406\n",
      "iter 23740/30000  loss         0.123651  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.406\n",
      "iter 23741/30000  loss         0.123651  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.406\n",
      "iter 23760/30000  loss         0.123648  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.406\n",
      "iter 23761/30000  loss         0.123648  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.406\n",
      "iter 23780/30000  loss         0.123645  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.407\n",
      "iter 23781/30000  loss         0.123645  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.407\n",
      "iter 23800/30000  loss         0.123642  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.407\n",
      "iter 23801/30000  loss         0.123642  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23820/30000  loss         0.123639  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.408\n",
      "iter 23821/30000  loss         0.123638  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.408\n",
      "iter 23840/30000  loss         0.123635  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.408\n",
      "iter 23841/30000  loss         0.123635  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.408\n",
      "iter 23860/30000  loss         0.123632  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.408\n",
      "iter 23861/30000  loss         0.123632  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.408\n",
      "iter 23880/30000  loss         0.123629  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.409\n",
      "iter 23881/30000  loss         0.123629  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.409\n",
      "iter 23900/30000  loss         0.123626  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.409\n",
      "iter 23901/30000  loss         0.123626  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.409\n",
      "iter 23920/30000  loss         0.123623  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.409\n",
      "iter 23921/30000  loss         0.123623  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.409\n",
      "iter 23940/30000  loss         0.123620  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.410\n",
      "iter 23941/30000  loss         0.123620  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.410\n",
      "iter 23960/30000  loss         0.123617  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.410\n",
      "iter 23961/30000  loss         0.123617  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.410\n",
      "iter 23980/30000  loss         0.123614  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.410\n",
      "iter 23981/30000  loss         0.123613  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.410\n",
      "iter 24000/30000  loss         0.123610  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.411\n",
      "iter 24001/30000  loss         0.123610  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.411\n",
      "iter 24020/30000  loss         0.123607  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.411\n",
      "iter 24021/30000  loss         0.123607  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.411\n",
      "iter 24040/30000  loss         0.123604  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.412\n",
      "iter 24041/30000  loss         0.123604  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.412\n",
      "iter 24060/30000  loss         0.123601  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.412\n",
      "iter 24061/30000  loss         0.123601  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.412\n",
      "iter 24080/30000  loss         0.123598  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.412\n",
      "iter 24081/30000  loss         0.123598  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.412\n",
      "iter 24100/30000  loss         0.123595  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.413\n",
      "iter 24101/30000  loss         0.123595  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.413\n",
      "iter 24120/30000  loss         0.123592  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.413\n",
      "iter 24121/30000  loss         0.123592  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.413\n",
      "iter 24140/30000  loss         0.123589  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.413\n",
      "iter 24141/30000  loss         0.123589  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.413\n",
      "iter 24160/30000  loss         0.123586  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.414\n",
      "iter 24161/30000  loss         0.123586  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.414\n",
      "iter 24180/30000  loss         0.123583  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.414\n",
      "iter 24181/30000  loss         0.123583  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.414\n",
      "iter 24200/30000  loss         0.123580  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.414\n",
      "iter 24201/30000  loss         0.123580  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    5.414\n",
      "iter 24220/30000  loss         0.123577  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.415\n",
      "iter 24221/30000  loss         0.123576  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.415\n",
      "iter 24240/30000  loss         0.123574  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.415\n",
      "iter 24241/30000  loss         0.123573  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.415\n",
      "iter 24260/30000  loss         0.123571  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.415\n",
      "iter 24261/30000  loss         0.123570  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.415\n",
      "iter 24280/30000  loss         0.123567  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.416\n",
      "iter 24281/30000  loss         0.123567  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.416\n",
      "iter 24300/30000  loss         0.123564  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.416\n",
      "iter 24301/30000  loss         0.123564  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.416\n",
      "iter 24320/30000  loss         0.123561  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.416\n",
      "iter 24321/30000  loss         0.123561  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.416\n",
      "iter 24340/30000  loss         0.123558  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.417\n",
      "iter 24341/30000  loss         0.123558  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.417\n",
      "iter 24360/30000  loss         0.123555  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.417\n",
      "iter 24361/30000  loss         0.123555  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.417\n",
      "iter 24380/30000  loss         0.123552  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.417\n",
      "iter 24381/30000  loss         0.123552  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.417\n",
      "iter 24400/30000  loss         0.123549  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.418\n",
      "iter 24401/30000  loss         0.123549  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.418\n",
      "iter 24420/30000  loss         0.123546  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.418\n",
      "iter 24421/30000  loss         0.123546  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.418\n",
      "iter 24440/30000  loss         0.123543  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.418\n",
      "iter 24441/30000  loss         0.123543  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.418\n",
      "iter 24460/30000  loss         0.123540  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.419\n",
      "iter 24461/30000  loss         0.123540  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.419\n",
      "iter 24480/30000  loss         0.123537  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.419\n",
      "iter 24481/30000  loss         0.123537  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.419\n",
      "iter 24500/30000  loss         0.123534  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.419\n",
      "iter 24501/30000  loss         0.123534  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.419\n",
      "iter 24520/30000  loss         0.123531  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.420\n",
      "iter 24521/30000  loss         0.123531  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.420\n",
      "iter 24540/30000  loss         0.123528  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.420\n",
      "iter 24541/30000  loss         0.123528  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.420\n",
      "iter 24560/30000  loss         0.123525  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.420\n",
      "iter 24561/30000  loss         0.123525  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.420\n",
      "iter 24580/30000  loss         0.123523  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.421\n",
      "iter 24581/30000  loss         0.123522  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.421\n",
      "iter 24600/30000  loss         0.123520  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.421\n",
      "iter 24601/30000  loss         0.123519  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24620/30000  loss         0.123517  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.421\n",
      "iter 24621/30000  loss         0.123516  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.421\n",
      "iter 24640/30000  loss         0.123514  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.422\n",
      "iter 24641/30000  loss         0.123514  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.422\n",
      "iter 24660/30000  loss         0.123511  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.422\n",
      "iter 24661/30000  loss         0.123511  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.422\n",
      "iter 24680/30000  loss         0.123508  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.422\n",
      "iter 24681/30000  loss         0.123508  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.422\n",
      "iter 24700/30000  loss         0.123505  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.423\n",
      "iter 24701/30000  loss         0.123505  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.423\n",
      "iter 24720/30000  loss         0.123502  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.423\n",
      "iter 24721/30000  loss         0.123502  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.423\n",
      "iter 24740/30000  loss         0.123499  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.423\n",
      "iter 24741/30000  loss         0.123499  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.423\n",
      "iter 24760/30000  loss         0.123496  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.424\n",
      "iter 24761/30000  loss         0.123496  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.424\n",
      "iter 24780/30000  loss         0.123493  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.424\n",
      "iter 24781/30000  loss         0.123493  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.424\n",
      "iter 24800/30000  loss         0.123490  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.424\n",
      "iter 24801/30000  loss         0.123490  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.424\n",
      "iter 24820/30000  loss         0.123487  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.425\n",
      "iter 24821/30000  loss         0.123487  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.425\n",
      "iter 24840/30000  loss         0.123484  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.425\n",
      "iter 24841/30000  loss         0.123484  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.425\n",
      "iter 24860/30000  loss         0.123481  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.425\n",
      "iter 24861/30000  loss         0.123481  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.425\n",
      "iter 24880/30000  loss         0.123479  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.426\n",
      "iter 24881/30000  loss         0.123478  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.426\n",
      "iter 24900/30000  loss         0.123476  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.426\n",
      "iter 24901/30000  loss         0.123476  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.426\n",
      "iter 24920/30000  loss         0.123473  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.426\n",
      "iter 24921/30000  loss         0.123473  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.426\n",
      "iter 24940/30000  loss         0.123470  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.427\n",
      "iter 24941/30000  loss         0.123470  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.427\n",
      "iter 24960/30000  loss         0.123467  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.427\n",
      "iter 24961/30000  loss         0.123467  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.427\n",
      "iter 24980/30000  loss         0.123464  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.427\n",
      "iter 24981/30000  loss         0.123464  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    5.427\n",
      "iter 25000/30000  loss         0.123461  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.428\n",
      "iter 25001/30000  loss         0.123461  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.428\n",
      "iter 25020/30000  loss         0.123458  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.428\n",
      "iter 25021/30000  loss         0.123458  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.428\n",
      "iter 25040/30000  loss         0.123456  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.428\n",
      "iter 25041/30000  loss         0.123455  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.428\n",
      "iter 25060/30000  loss         0.123453  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.429\n",
      "iter 25061/30000  loss         0.123453  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.429\n",
      "iter 25080/30000  loss         0.123450  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.429\n",
      "iter 25081/30000  loss         0.123450  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.429\n",
      "iter 25100/30000  loss         0.123447  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.429\n",
      "iter 25101/30000  loss         0.123447  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.429\n",
      "iter 25120/30000  loss         0.123444  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25121/30000  loss         0.123444  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25140/30000  loss         0.123441  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25141/30000  loss         0.123441  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25160/30000  loss         0.123439  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25161/30000  loss         0.123438  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25180/30000  loss         0.123436  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25181/30000  loss         0.123436  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.430\n",
      "iter 25200/30000  loss         0.123433  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.431\n",
      "iter 25201/30000  loss         0.123433  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.431\n",
      "iter 25220/30000  loss         0.123430  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.431\n",
      "iter 25221/30000  loss         0.123430  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.431\n",
      "iter 25240/30000  loss         0.123427  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.431\n",
      "iter 25241/30000  loss         0.123427  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.431\n",
      "iter 25260/30000  loss         0.123424  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.432\n",
      "iter 25261/30000  loss         0.123424  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.432\n",
      "iter 25280/30000  loss         0.123422  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.432\n",
      "iter 25281/30000  loss         0.123421  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.432\n",
      "iter 25300/30000  loss         0.123419  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.432\n",
      "iter 25301/30000  loss         0.123419  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.432\n",
      "iter 25320/30000  loss         0.123416  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.433\n",
      "iter 25321/30000  loss         0.123416  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.433\n",
      "iter 25340/30000  loss         0.123413  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.433\n",
      "iter 25341/30000  loss         0.123413  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.433\n",
      "iter 25360/30000  loss         0.123410  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.433\n",
      "iter 25361/30000  loss         0.123410  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.433\n",
      "iter 25380/30000  loss         0.123408  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n",
      "iter 25381/30000  loss         0.123407  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n",
      "iter 25400/30000  loss         0.123405  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n",
      "iter 25401/30000  loss         0.123405  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25420/30000  loss         0.123402  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n",
      "iter 25421/30000  loss         0.123402  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n",
      "iter 25440/30000  loss         0.123399  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n",
      "iter 25441/30000  loss         0.123399  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.434\n",
      "iter 25460/30000  loss         0.123397  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.435\n",
      "iter 25461/30000  loss         0.123396  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.435\n",
      "iter 25480/30000  loss         0.123394  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.435\n",
      "iter 25481/30000  loss         0.123394  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.435\n",
      "iter 25500/30000  loss         0.123391  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.435\n",
      "iter 25501/30000  loss         0.123391  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.435\n",
      "iter 25520/30000  loss         0.123388  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.436\n",
      "iter 25521/30000  loss         0.123388  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.436\n",
      "iter 25540/30000  loss         0.123385  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.436\n",
      "iter 25541/30000  loss         0.123385  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.436\n",
      "iter 25560/30000  loss         0.123383  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.436\n",
      "iter 25561/30000  loss         0.123383  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.436\n",
      "iter 25580/30000  loss         0.123380  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25581/30000  loss         0.123380  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25600/30000  loss         0.123377  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25601/30000  loss         0.123377  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25620/30000  loss         0.123375  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25621/30000  loss         0.123374  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25640/30000  loss         0.123372  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25641/30000  loss         0.123372  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.437\n",
      "iter 25660/30000  loss         0.123369  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.438\n",
      "iter 25661/30000  loss         0.123369  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.438\n",
      "iter 25680/30000  loss         0.123366  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.438\n",
      "iter 25681/30000  loss         0.123366  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.438\n",
      "iter 25700/30000  loss         0.123364  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.438\n",
      "iter 25701/30000  loss         0.123363  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.438\n",
      "iter 25720/30000  loss         0.123361  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.439\n",
      "iter 25721/30000  loss         0.123361  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.439\n",
      "iter 25740/30000  loss         0.123358  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.439\n",
      "iter 25741/30000  loss         0.123358  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.439\n",
      "iter 25760/30000  loss         0.123355  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.439\n",
      "iter 25761/30000  loss         0.123355  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.439\n",
      "iter 25780/30000  loss         0.123353  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.440\n",
      "iter 25781/30000  loss         0.123353  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.440\n",
      "iter 25800/30000  loss         0.123350  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.440\n",
      "iter 25801/30000  loss         0.123350  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    5.440\n",
      "iter 25820/30000  loss         0.123347  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.440\n",
      "iter 25821/30000  loss         0.123347  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.440\n",
      "iter 25840/30000  loss         0.123345  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.440\n",
      "iter 25841/30000  loss         0.123345  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.440\n",
      "iter 25860/30000  loss         0.123342  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.441\n",
      "iter 25861/30000  loss         0.123342  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.441\n",
      "iter 25880/30000  loss         0.123339  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.441\n",
      "iter 25881/30000  loss         0.123339  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.441\n",
      "iter 25900/30000  loss         0.123337  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.441\n",
      "iter 25901/30000  loss         0.123337  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.441\n",
      "iter 25920/30000  loss         0.123334  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 25921/30000  loss         0.123334  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 25940/30000  loss         0.123331  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 25941/30000  loss         0.123331  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 25960/30000  loss         0.123329  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 25961/30000  loss         0.123328  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 25980/30000  loss         0.123326  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 25981/30000  loss         0.123326  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.442\n",
      "iter 26000/30000  loss         0.123323  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.443\n",
      "iter 26001/30000  loss         0.123323  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.443\n",
      "iter 26020/30000  loss         0.123321  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.443\n",
      "iter 26021/30000  loss         0.123321  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.443\n",
      "iter 26040/30000  loss         0.123318  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.443\n",
      "iter 26041/30000  loss         0.123318  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.443\n",
      "iter 26060/30000  loss         0.123315  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26061/30000  loss         0.123315  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26080/30000  loss         0.123313  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26081/30000  loss         0.123313  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26100/30000  loss         0.123310  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26101/30000  loss         0.123310  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26120/30000  loss         0.123307  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26121/30000  loss         0.123307  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.444\n",
      "iter 26140/30000  loss         0.123305  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.445\n",
      "iter 26141/30000  loss         0.123305  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.445\n",
      "iter 26160/30000  loss         0.123302  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.445\n",
      "iter 26161/30000  loss         0.123302  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.445\n",
      "iter 26180/30000  loss         0.123300  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.445\n",
      "iter 26181/30000  loss         0.123299  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.445\n",
      "iter 26200/30000  loss         0.123297  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.445\n",
      "iter 26201/30000  loss         0.123297  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26220/30000  loss         0.123294  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.446\n",
      "iter 26221/30000  loss         0.123294  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.446\n",
      "iter 26240/30000  loss         0.123292  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.446\n",
      "iter 26241/30000  loss         0.123292  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.446\n",
      "iter 26260/30000  loss         0.123289  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.446\n",
      "iter 26261/30000  loss         0.123289  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.446\n",
      "iter 26280/30000  loss         0.123287  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26281/30000  loss         0.123286  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26300/30000  loss         0.123284  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26301/30000  loss         0.123284  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26320/30000  loss         0.123281  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26321/30000  loss         0.123281  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26340/30000  loss         0.123279  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26341/30000  loss         0.123279  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.447\n",
      "iter 26360/30000  loss         0.123276  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.448\n",
      "iter 26361/30000  loss         0.123276  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.448\n",
      "iter 26380/30000  loss         0.123274  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.448\n",
      "iter 26381/30000  loss         0.123273  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.448\n",
      "iter 26400/30000  loss         0.123271  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.448\n",
      "iter 26401/30000  loss         0.123271  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.448\n",
      "iter 26420/30000  loss         0.123268  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26421/30000  loss         0.123268  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26440/30000  loss         0.123266  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26441/30000  loss         0.123266  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26460/30000  loss         0.123263  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26461/30000  loss         0.123263  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26480/30000  loss         0.123261  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26481/30000  loss         0.123261  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.449\n",
      "iter 26500/30000  loss         0.123258  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26501/30000  loss         0.123258  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26520/30000  loss         0.123256  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26521/30000  loss         0.123255  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26540/30000  loss         0.123253  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26541/30000  loss         0.123253  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26560/30000  loss         0.123250  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26561/30000  loss         0.123250  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.450\n",
      "iter 26580/30000  loss         0.123248  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26581/30000  loss         0.123248  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26600/30000  loss         0.123245  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26601/30000  loss         0.123245  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26620/30000  loss         0.123243  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26621/30000  loss         0.123243  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26640/30000  loss         0.123240  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26641/30000  loss         0.123240  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.451\n",
      "iter 26660/30000  loss         0.123238  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.452\n",
      "iter 26661/30000  loss         0.123238  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    5.452\n",
      "iter 26680/30000  loss         0.123235  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.452\n",
      "iter 26681/30000  loss         0.123235  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.452\n",
      "iter 26700/30000  loss         0.123233  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.452\n",
      "iter 26701/30000  loss         0.123233  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.452\n",
      "iter 26720/30000  loss         0.123230  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26721/30000  loss         0.123230  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26740/30000  loss         0.123228  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26741/30000  loss         0.123228  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26760/30000  loss         0.123225  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26761/30000  loss         0.123225  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26780/30000  loss         0.123223  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26781/30000  loss         0.123223  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.453\n",
      "iter 26800/30000  loss         0.123220  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26801/30000  loss         0.123220  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26820/30000  loss         0.123218  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26821/30000  loss         0.123218  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26840/30000  loss         0.123215  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26841/30000  loss         0.123215  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26860/30000  loss         0.123213  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26861/30000  loss         0.123213  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.454\n",
      "iter 26880/30000  loss         0.123210  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26881/30000  loss         0.123210  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26900/30000  loss         0.123208  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26901/30000  loss         0.123208  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26920/30000  loss         0.123205  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26921/30000  loss         0.123205  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26940/30000  loss         0.123203  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26941/30000  loss         0.123203  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.455\n",
      "iter 26960/30000  loss         0.123200  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n",
      "iter 26961/30000  loss         0.123200  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n",
      "iter 26980/30000  loss         0.123198  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n",
      "iter 26981/30000  loss         0.123198  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n",
      "iter 27000/30000  loss         0.123195  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n",
      "iter 27001/30000  loss         0.123195  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27020/30000  loss         0.123193  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n",
      "iter 27021/30000  loss         0.123193  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.456\n",
      "iter 27040/30000  loss         0.123190  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27041/30000  loss         0.123190  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27060/30000  loss         0.123188  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27061/30000  loss         0.123188  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27080/30000  loss         0.123185  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27081/30000  loss         0.123185  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27100/30000  loss         0.123183  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27101/30000  loss         0.123183  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.457\n",
      "iter 27120/30000  loss         0.123181  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27121/30000  loss         0.123180  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27140/30000  loss         0.123178  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27141/30000  loss         0.123178  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27160/30000  loss         0.123176  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27161/30000  loss         0.123176  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27180/30000  loss         0.123173  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27181/30000  loss         0.123173  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.458\n",
      "iter 27200/30000  loss         0.123171  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27201/30000  loss         0.123171  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27220/30000  loss         0.123168  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27221/30000  loss         0.123168  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27240/30000  loss         0.123166  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27241/30000  loss         0.123166  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27260/30000  loss         0.123164  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27261/30000  loss         0.123163  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.459\n",
      "iter 27280/30000  loss         0.123161  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27281/30000  loss         0.123161  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27300/30000  loss         0.123159  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27301/30000  loss         0.123159  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27320/30000  loss         0.123156  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27321/30000  loss         0.123156  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27340/30000  loss         0.123154  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27341/30000  loss         0.123154  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.460\n",
      "iter 27360/30000  loss         0.123151  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27361/30000  loss         0.123151  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27380/30000  loss         0.123149  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27381/30000  loss         0.123149  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27400/30000  loss         0.123147  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27401/30000  loss         0.123147  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27420/30000  loss         0.123144  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27421/30000  loss         0.123144  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.461\n",
      "iter 27440/30000  loss         0.123142  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27441/30000  loss         0.123142  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27460/30000  loss         0.123139  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27461/30000  loss         0.123139  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27480/30000  loss         0.123137  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27481/30000  loss         0.123137  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27500/30000  loss         0.123135  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27501/30000  loss         0.123135  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.462\n",
      "iter 27520/30000  loss         0.123132  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27521/30000  loss         0.123132  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27540/30000  loss         0.123130  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27541/30000  loss         0.123130  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27560/30000  loss         0.123128  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27561/30000  loss         0.123127  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27580/30000  loss         0.123125  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27581/30000  loss         0.123125  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    5.463\n",
      "iter 27600/30000  loss         0.123123  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27601/30000  loss         0.123123  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27620/30000  loss         0.123121  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27621/30000  loss         0.123120  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27640/30000  loss         0.123118  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27641/30000  loss         0.123118  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27660/30000  loss         0.123116  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27661/30000  loss         0.123116  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27680/30000  loss         0.123113  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.464\n",
      "iter 27681/30000  loss         0.123113  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27700/30000  loss         0.123111  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27701/30000  loss         0.123111  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27720/30000  loss         0.123109  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27721/30000  loss         0.123109  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27740/30000  loss         0.123106  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27741/30000  loss         0.123106  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27760/30000  loss         0.123104  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27761/30000  loss         0.123104  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.465\n",
      "iter 27780/30000  loss         0.123102  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n",
      "iter 27781/30000  loss         0.123102  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n",
      "iter 27800/30000  loss         0.123099  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n",
      "iter 27801/30000  loss         0.123099  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27820/30000  loss         0.123097  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n",
      "iter 27821/30000  loss         0.123097  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n",
      "iter 27840/30000  loss         0.123095  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n",
      "iter 27841/30000  loss         0.123095  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.466\n",
      "iter 27860/30000  loss         0.123092  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27861/30000  loss         0.123092  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27880/30000  loss         0.123090  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27881/30000  loss         0.123090  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27900/30000  loss         0.123088  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27901/30000  loss         0.123088  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27920/30000  loss         0.123086  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27921/30000  loss         0.123085  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.467\n",
      "iter 27940/30000  loss         0.123083  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 27941/30000  loss         0.123083  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 27960/30000  loss         0.123081  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 27961/30000  loss         0.123081  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 27980/30000  loss         0.123079  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 27981/30000  loss         0.123078  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 28000/30000  loss         0.123076  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 28001/30000  loss         0.123076  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 28020/30000  loss         0.123074  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 28021/30000  loss         0.123074  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.468\n",
      "iter 28040/30000  loss         0.123072  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28041/30000  loss         0.123072  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28060/30000  loss         0.123069  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28061/30000  loss         0.123069  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28080/30000  loss         0.123067  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28081/30000  loss         0.123067  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28100/30000  loss         0.123065  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28101/30000  loss         0.123065  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.469\n",
      "iter 28120/30000  loss         0.123063  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28121/30000  loss         0.123062  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28140/30000  loss         0.123060  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28141/30000  loss         0.123060  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28160/30000  loss         0.123058  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28161/30000  loss         0.123058  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28180/30000  loss         0.123056  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28181/30000  loss         0.123056  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28200/30000  loss         0.123053  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28201/30000  loss         0.123053  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.470\n",
      "iter 28220/30000  loss         0.123051  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28221/30000  loss         0.123051  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28240/30000  loss         0.123049  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28241/30000  loss         0.123049  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28260/30000  loss         0.123047  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28261/30000  loss         0.123047  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28280/30000  loss         0.123044  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28281/30000  loss         0.123044  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.471\n",
      "iter 28300/30000  loss         0.123042  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28301/30000  loss         0.123042  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28320/30000  loss         0.123040  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28321/30000  loss         0.123040  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28340/30000  loss         0.123038  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28341/30000  loss         0.123038  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28360/30000  loss         0.123035  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28361/30000  loss         0.123035  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28380/30000  loss         0.123033  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28381/30000  loss         0.123033  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.472\n",
      "iter 28400/30000  loss         0.123031  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28401/30000  loss         0.123031  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28420/30000  loss         0.123029  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28421/30000  loss         0.123029  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28440/30000  loss         0.123027  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28441/30000  loss         0.123026  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28460/30000  loss         0.123024  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28461/30000  loss         0.123024  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.473\n",
      "iter 28480/30000  loss         0.123022  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28481/30000  loss         0.123022  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28500/30000  loss         0.123020  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28501/30000  loss         0.123020  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28520/30000  loss         0.123018  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28521/30000  loss         0.123018  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28540/30000  loss         0.123015  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28541/30000  loss         0.123015  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28560/30000  loss         0.123013  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28561/30000  loss         0.123013  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    5.474\n",
      "iter 28580/30000  loss         0.123011  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28581/30000  loss         0.123011  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28600/30000  loss         0.123009  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28601/30000  loss         0.123009  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28620/30000  loss         0.123007  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28621/30000  loss         0.123007  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28640/30000  loss         0.123004  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28641/30000  loss         0.123004  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28660/30000  loss         0.123002  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28661/30000  loss         0.123002  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.475\n",
      "iter 28680/30000  loss         0.123000  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28681/30000  loss         0.123000  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28700/30000  loss         0.122998  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28701/30000  loss         0.122998  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28720/30000  loss         0.122996  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28721/30000  loss         0.122996  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28740/30000  loss         0.122994  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28741/30000  loss         0.122993  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.476\n",
      "iter 28760/30000  loss         0.122991  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28761/30000  loss         0.122991  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28780/30000  loss         0.122989  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28781/30000  loss         0.122989  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28800/30000  loss         0.122987  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28801/30000  loss         0.122987  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28820/30000  loss         0.122985  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28821/30000  loss         0.122985  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28840/30000  loss         0.122983  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28841/30000  loss         0.122983  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.477\n",
      "iter 28860/30000  loss         0.122980  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28861/30000  loss         0.122980  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28880/30000  loss         0.122978  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28881/30000  loss         0.122978  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28900/30000  loss         0.122976  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28901/30000  loss         0.122976  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28920/30000  loss         0.122974  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28921/30000  loss         0.122974  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28940/30000  loss         0.122972  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28941/30000  loss         0.122972  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.478\n",
      "iter 28960/30000  loss         0.122970  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 28961/30000  loss         0.122970  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 28980/30000  loss         0.122968  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 28981/30000  loss         0.122967  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 29000/30000  loss         0.122965  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 29001/30000  loss         0.122965  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 29020/30000  loss         0.122963  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 29021/30000  loss         0.122963  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 29040/30000  loss         0.122961  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 29041/30000  loss         0.122961  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.479\n",
      "iter 29060/30000  loss         0.122959  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29061/30000  loss         0.122959  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29080/30000  loss         0.122957  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29081/30000  loss         0.122957  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29100/30000  loss         0.122955  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29101/30000  loss         0.122955  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29120/30000  loss         0.122953  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29121/30000  loss         0.122953  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29140/30000  loss         0.122951  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29141/30000  loss         0.122950  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.480\n",
      "iter 29160/30000  loss         0.122948  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29161/30000  loss         0.122948  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29180/30000  loss         0.122946  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29181/30000  loss         0.122946  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29200/30000  loss         0.122944  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29201/30000  loss         0.122944  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29220/30000  loss         0.122942  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29221/30000  loss         0.122942  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29240/30000  loss         0.122940  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29241/30000  loss         0.122940  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.481\n",
      "iter 29260/30000  loss         0.122938  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29261/30000  loss         0.122938  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29280/30000  loss         0.122936  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29281/30000  loss         0.122936  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29300/30000  loss         0.122934  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29301/30000  loss         0.122934  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29320/30000  loss         0.122932  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29321/30000  loss         0.122931  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29340/30000  loss         0.122929  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29341/30000  loss         0.122929  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.482\n",
      "iter 29360/30000  loss         0.122927  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29361/30000  loss         0.122927  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29380/30000  loss         0.122925  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29381/30000  loss         0.122925  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29400/30000  loss         0.122923  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29401/30000  loss         0.122923  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29420/30000  loss         0.122921  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29421/30000  loss         0.122921  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29440/30000  loss         0.122919  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29441/30000  loss         0.122919  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.483\n",
      "iter 29460/30000  loss         0.122917  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29461/30000  loss         0.122917  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29480/30000  loss         0.122915  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29481/30000  loss         0.122915  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29500/30000  loss         0.122913  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29501/30000  loss         0.122913  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29520/30000  loss         0.122911  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29521/30000  loss         0.122911  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29540/30000  loss         0.122909  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29541/30000  loss         0.122909  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.484\n",
      "iter 29560/30000  loss         0.122907  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.485\n",
      "iter 29561/30000  loss         0.122906  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.485\n",
      "iter 29580/30000  loss         0.122905  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.485\n",
      "iter 29581/30000  loss         0.122904  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.485\n",
      "iter 29600/30000  loss         0.122902  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.485\n",
      "iter 29601/30000  loss         0.122902  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    5.485\n",
      "iter 29620/30000  loss         0.122900  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.485\n",
      "iter 29621/30000  loss         0.122900  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.485\n",
      "iter 29640/30000  loss         0.122898  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.485\n",
      "iter 29641/30000  loss         0.122898  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.485\n",
      "iter 29660/30000  loss         0.122896  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29661/30000  loss         0.122896  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29680/30000  loss         0.122894  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29681/30000  loss         0.122894  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29700/30000  loss         0.122892  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29701/30000  loss         0.122892  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29720/30000  loss         0.122890  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29721/30000  loss         0.122890  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29740/30000  loss         0.122888  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29741/30000  loss         0.122888  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.486\n",
      "iter 29760/30000  loss         0.122886  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29761/30000  loss         0.122886  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29780/30000  loss         0.122884  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29781/30000  loss         0.122884  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29800/30000  loss         0.122882  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29801/30000  loss         0.122882  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29820/30000  loss         0.122880  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29821/30000  loss         0.122880  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29840/30000  loss         0.122878  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29841/30000  loss         0.122878  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29860/30000  loss         0.122876  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29861/30000  loss         0.122876  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.487\n",
      "iter 29880/30000  loss         0.122874  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29881/30000  loss         0.122874  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29900/30000  loss         0.122872  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29901/30000  loss         0.122872  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29920/30000  loss         0.122870  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29921/30000  loss         0.122870  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29940/30000  loss         0.122868  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29941/30000  loss         0.122868  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29960/30000  loss         0.122866  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29961/30000  loss         0.122866  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.488\n",
      "iter 29980/30000  loss         0.122864  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.489\n",
      "iter 29981/30000  loss         0.122864  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.489\n",
      "iter 30000/30000  loss         0.122862  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    5.489\n",
      "Done. Did NOT converge.\n"
     ]
    }
   ],
   "source": [
    "lr_2 = LogisticRegressionGradientDescent(alpha=1, step_size=0.1, init_w_recipe='zeros')\n",
    "lr_2.fit(x_trbw, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_NF = x_tebw\n",
    "\n",
    "yproba1_test_N = lr.predict_proba(x_tebw_29_3)[:, 1]\n",
    "\n",
    "np.savetxt('yproba3_test.txt', yproba1_test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5857</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>196</td>\n",
       "      <td>5804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        5857   143\n",
       "1.0         196  5804"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lr_2.predict(x_trbw)\n",
    "y_actu = pd.Series(y_tr, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04, ...,\n",
       "        7.96260777e-02, 9.82817114e-01, 1.81612851e-01],\n",
       "       [8.11858698e-01, 8.74961645e-01, 6.88413252e-01, ...,\n",
       "        3.77082599e-01, 2.66278847e-02, 1.10920369e-01],\n",
       "       [6.74564024e-01, 7.99776537e-01, 8.05295271e-02, ...,\n",
       "        5.52767597e-01, 8.89131089e-01, 3.54952851e-01],\n",
       "       ...,\n",
       "       [1.50901322e-01, 6.51542740e-01, 1.42803772e-01, ...,\n",
       "        1.77559727e-01, 7.79021186e-01, 5.94109396e-02],\n",
       "       [6.95665012e-01, 8.84282243e-01, 7.43374491e-01, ...,\n",
       "        5.52912768e-01, 3.11197213e-01, 8.04303725e-01],\n",
       "       [8.20619230e-01, 6.78628601e-02, 8.63862058e-01, ...,\n",
       "        9.18034672e-01, 7.38040818e-02, 8.76249555e-01]])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_rad = np.random.rand(784,300)\n",
    "x_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_comp_bw = np.dot(x_trbw, x_rad)\n",
    "for i in range(12000):\n",
    "    x_tr_comp_bw[i] = (x_tr_comp_bw[i]-np.min(x_tr_comp_bw[i])) / (np.max(x_tr_comp_bw[i])-np.min(x_tr_comp_bw[i]))\n",
    "    \n",
    "x_te_comp_bw = np.dot(x_tebw, x_rad)\n",
    "for i in range(2000):\n",
    "    x_te_comp_bw[i] = (x_te_comp_bw[i]-np.min(x_te_comp_bw[i])) / (np.max(x_te_comp_bw[i])-np.min(x_te_comp_bw[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_tr_comp_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 301 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.1\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.017132  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         0.986641  avg_L1_norm_grad         0.017026  w[0]    0.001 bias    0.000\n",
      "iter    2/30000  loss         0.973916  avg_L1_norm_grad         0.016867  w[0]    0.002 bias    0.001\n",
      "iter    3/30000  loss         0.961884  avg_L1_norm_grad         0.017677  w[0]    0.002 bias    0.000\n",
      "iter    4/30000  loss         0.950743  avg_L1_norm_grad         0.019062  w[0]    0.003 bias    0.003\n",
      "iter    5/30000  loss         0.941033  avg_L1_norm_grad         0.024526  w[0]    0.003 bias    0.000\n",
      "iter    6/30000  loss         0.934106  avg_L1_norm_grad         0.031637  w[0]    0.005 bias    0.005\n",
      "iter    7/30000  loss         0.933249  avg_L1_norm_grad         0.053089  w[0]    0.003 bias   -0.001\n",
      "iter    8/30000  loss         0.945781  avg_L1_norm_grad         0.074271  w[0]    0.008 bias    0.010\n",
      "iter    9/30000  loss         0.986156  avg_L1_norm_grad         0.117521  w[0]    0.001 bias   -0.006\n",
      "iter   10/30000  loss         1.072728  avg_L1_norm_grad         0.157226  w[0]    0.013 bias    0.019\n",
      "iter   11/30000  loss         1.206417  avg_L1_norm_grad         0.206108  w[0]   -0.003 bias   -0.014\n",
      "iter   12/30000  loss         1.328531  avg_L1_norm_grad         0.228262  w[0]    0.019 bias    0.029\n",
      "iter   13/30000  loss         1.391836  avg_L1_norm_grad         0.245380  w[0]   -0.005 bias   -0.019\n",
      "iter   14/30000  loss         1.392059  avg_L1_norm_grad         0.241520  w[0]    0.021 bias    0.033\n",
      "iter   15/30000  loss         1.389498  avg_L1_norm_grad         0.247167  w[0]   -0.004 bias   -0.019\n",
      "iter   16/30000  loss         1.367383  avg_L1_norm_grad         0.239935  w[0]    0.022 bias    0.034\n",
      "iter   17/30000  loss         1.360528  avg_L1_norm_grad         0.244629  w[0]   -0.003 bias   -0.018\n",
      "iter   18/30000  loss         1.336917  avg_L1_norm_grad         0.237204  w[0]    0.023 bias    0.034\n",
      "iter   19/30000  loss         1.330181  avg_L1_norm_grad         0.241575  w[0]   -0.002 bias   -0.016\n",
      "iter   20/30000  loss         1.305798  avg_L1_norm_grad         0.234099  w[0]    0.023 bias    0.035\n",
      "iter   21/30000  loss         1.299619  avg_L1_norm_grad         0.238176  w[0]   -0.001 bias   -0.015\n",
      "iter   40/30000  loss         0.992734  avg_L1_norm_grad         0.185620  w[0]    0.024 bias    0.043\n",
      "iter   41/30000  loss         0.994676  avg_L1_norm_grad         0.187016  w[0]    0.004 bias    0.003\n",
      "iter   60/30000  loss         0.745009  avg_L1_norm_grad         0.115058  w[0]    0.013 bias    0.051\n",
      "iter   61/30000  loss         0.742613  avg_L1_norm_grad         0.114704  w[0]    0.000 bias    0.027\n",
      "iter   80/30000  loss         0.609069  avg_L1_norm_grad         0.043025  w[0]   -0.003 bias    0.061\n",
      "iter   81/30000  loss         0.605661  avg_L1_norm_grad         0.042469  w[0]   -0.009 bias    0.052\n",
      "iter  100/30000  loss         0.561797  avg_L1_norm_grad         0.007093  w[0]   -0.019 bias    0.074\n",
      "iter  101/30000  loss         0.560331  avg_L1_norm_grad         0.007028  w[0]   -0.020 bias    0.074\n",
      "iter  120/30000  loss         0.535768  avg_L1_norm_grad         0.004950  w[0]   -0.032 bias    0.091\n",
      "iter  121/30000  loss         0.534593  avg_L1_norm_grad         0.004931  w[0]   -0.033 bias    0.092\n",
      "iter  140/30000  loss         0.514014  avg_L1_norm_grad         0.004563  w[0]   -0.044 bias    0.108\n",
      "iter  141/30000  loss         0.513015  avg_L1_norm_grad         0.004546  w[0]   -0.044 bias    0.109\n",
      "iter  160/30000  loss         0.495391  avg_L1_norm_grad         0.004235  w[0]   -0.055 bias    0.125\n",
      "iter  161/30000  loss         0.494530  avg_L1_norm_grad         0.004219  w[0]   -0.055 bias    0.126\n",
      "iter  180/30000  loss         0.479259  avg_L1_norm_grad         0.003952  w[0]   -0.065 bias    0.142\n",
      "iter  181/30000  loss         0.478508  avg_L1_norm_grad         0.003939  w[0]   -0.065 bias    0.143\n",
      "iter  200/30000  loss         0.465140  avg_L1_norm_grad         0.003706  w[0]   -0.074 bias    0.158\n",
      "iter  201/30000  loss         0.464480  avg_L1_norm_grad         0.003695  w[0]   -0.074 bias    0.159\n",
      "iter  220/30000  loss         0.452670  avg_L1_norm_grad         0.003491  w[0]   -0.083 bias    0.175\n",
      "iter  221/30000  loss         0.452084  avg_L1_norm_grad         0.003481  w[0]   -0.083 bias    0.175\n",
      "iter  240/30000  loss         0.441568  avg_L1_norm_grad         0.003300  w[0]   -0.091 bias    0.190\n",
      "iter  241/30000  loss         0.441044  avg_L1_norm_grad         0.003291  w[0]   -0.091 bias    0.191\n",
      "iter  260/30000  loss         0.431613  avg_L1_norm_grad         0.003130  w[0]   -0.098 bias    0.206\n",
      "iter  261/30000  loss         0.431142  avg_L1_norm_grad         0.003122  w[0]   -0.099 bias    0.207\n",
      "iter  280/30000  loss         0.422630  avg_L1_norm_grad         0.002978  w[0]   -0.106 bias    0.222\n",
      "iter  281/30000  loss         0.422204  avg_L1_norm_grad         0.002971  w[0]   -0.106 bias    0.222\n",
      "iter  300/30000  loss         0.414477  avg_L1_norm_grad         0.002841  w[0]   -0.112 bias    0.237\n",
      "iter  301/30000  loss         0.414089  avg_L1_norm_grad         0.002835  w[0]   -0.113 bias    0.237\n",
      "iter  320/30000  loss         0.407040  avg_L1_norm_grad         0.002717  w[0]   -0.119 bias    0.252\n",
      "iter  321/30000  loss         0.406685  avg_L1_norm_grad         0.002712  w[0]   -0.119 bias    0.252\n",
      "iter  340/30000  loss         0.400224  avg_L1_norm_grad         0.002605  w[0]   -0.125 bias    0.266\n",
      "iter  341/30000  loss         0.399898  avg_L1_norm_grad         0.002599  w[0]   -0.125 bias    0.267\n",
      "iter  360/30000  loss         0.393950  avg_L1_norm_grad         0.002502  w[0]   -0.131 bias    0.281\n",
      "iter  361/30000  loss         0.393649  avg_L1_norm_grad         0.002497  w[0]   -0.131 bias    0.282\n",
      "iter  380/30000  loss         0.388153  avg_L1_norm_grad         0.002407  w[0]   -0.136 bias    0.295\n",
      "iter  381/30000  loss         0.387874  avg_L1_norm_grad         0.002402  w[0]   -0.137 bias    0.296\n",
      "iter  400/30000  loss         0.382778  avg_L1_norm_grad         0.002319  w[0]   -0.142 bias    0.309\n",
      "iter  401/30000  loss         0.382519  avg_L1_norm_grad         0.002315  w[0]   -0.142 bias    0.310\n",
      "iter  420/30000  loss         0.377777  avg_L1_norm_grad         0.002239  w[0]   -0.147 bias    0.323\n",
      "iter  421/30000  loss         0.377536  avg_L1_norm_grad         0.002235  w[0]   -0.147 bias    0.324\n",
      "iter  440/30000  loss         0.373111  avg_L1_norm_grad         0.002164  w[0]   -0.152 bias    0.337\n",
      "iter  441/30000  loss         0.372886  avg_L1_norm_grad         0.002160  w[0]   -0.152 bias    0.338\n",
      "iter  460/30000  loss         0.368746  avg_L1_norm_grad         0.002095  w[0]   -0.156 bias    0.350\n",
      "iter  461/30000  loss         0.368535  avg_L1_norm_grad         0.002091  w[0]   -0.157 bias    0.351\n",
      "iter  480/30000  loss         0.364651  avg_L1_norm_grad         0.002030  w[0]   -0.161 bias    0.364\n",
      "iter  481/30000  loss         0.364452  avg_L1_norm_grad         0.002027  w[0]   -0.161 bias    0.364\n",
      "iter  500/30000  loss         0.360800  avg_L1_norm_grad         0.001969  w[0]   -0.165 bias    0.377\n",
      "iter  501/30000  loss         0.360613  avg_L1_norm_grad         0.001966  w[0]   -0.166 bias    0.378\n",
      "iter  520/30000  loss         0.357170  avg_L1_norm_grad         0.001912  w[0]   -0.170 bias    0.390\n",
      "iter  521/30000  loss         0.356994  avg_L1_norm_grad         0.001910  w[0]   -0.170 bias    0.391\n",
      "iter  540/30000  loss         0.353743  avg_L1_norm_grad         0.001859  w[0]   -0.174 bias    0.403\n",
      "iter  541/30000  loss         0.353577  avg_L1_norm_grad         0.001856  w[0]   -0.174 bias    0.404\n",
      "iter  560/30000  loss         0.350500  avg_L1_norm_grad         0.001809  w[0]   -0.178 bias    0.416\n",
      "iter  561/30000  loss         0.350342  avg_L1_norm_grad         0.001806  w[0]   -0.178 bias    0.416\n",
      "iter  580/30000  loss         0.347426  avg_L1_norm_grad         0.001761  w[0]   -0.181 bias    0.428\n",
      "iter  581/30000  loss         0.347276  avg_L1_norm_grad         0.001759  w[0]   -0.182 bias    0.429\n",
      "iter  600/30000  loss         0.344506  avg_L1_norm_grad         0.001717  w[0]   -0.185 bias    0.441\n",
      "iter  601/30000  loss         0.344364  avg_L1_norm_grad         0.001715  w[0]   -0.185 bias    0.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.341730  avg_L1_norm_grad         0.001675  w[0]   -0.189 bias    0.453\n",
      "iter  621/30000  loss         0.341594  avg_L1_norm_grad         0.001673  w[0]   -0.189 bias    0.454\n",
      "iter  640/30000  loss         0.339085  avg_L1_norm_grad         0.001635  w[0]   -0.192 bias    0.465\n",
      "iter  641/30000  loss         0.338956  avg_L1_norm_grad         0.001634  w[0]   -0.192 bias    0.466\n",
      "iter  660/30000  loss         0.336562  avg_L1_norm_grad         0.001598  w[0]   -0.195 bias    0.477\n",
      "iter  661/30000  loss         0.336438  avg_L1_norm_grad         0.001596  w[0]   -0.196 bias    0.478\n",
      "iter  680/30000  loss         0.334151  avg_L1_norm_grad         0.001562  w[0]   -0.199 bias    0.489\n",
      "iter  681/30000  loss         0.334034  avg_L1_norm_grad         0.001560  w[0]   -0.199 bias    0.490\n",
      "iter  700/30000  loss         0.331846  avg_L1_norm_grad         0.001528  w[0]   -0.202 bias    0.501\n",
      "iter  701/30000  loss         0.331733  avg_L1_norm_grad         0.001526  w[0]   -0.202 bias    0.501\n",
      "iter  720/30000  loss         0.329638  avg_L1_norm_grad         0.001496  w[0]   -0.205 bias    0.513\n",
      "iter  721/30000  loss         0.329530  avg_L1_norm_grad         0.001494  w[0]   -0.205 bias    0.513\n",
      "iter  740/30000  loss         0.327522  avg_L1_norm_grad         0.001465  w[0]   -0.208 bias    0.524\n",
      "iter  741/30000  loss         0.327418  avg_L1_norm_grad         0.001463  w[0]   -0.208 bias    0.525\n",
      "iter  760/30000  loss         0.325490  avg_L1_norm_grad         0.001435  w[0]   -0.211 bias    0.536\n",
      "iter  761/30000  loss         0.325391  avg_L1_norm_grad         0.001434  w[0]   -0.211 bias    0.536\n",
      "iter  780/30000  loss         0.323538  avg_L1_norm_grad         0.001407  w[0]   -0.214 bias    0.547\n",
      "iter  781/30000  loss         0.323442  avg_L1_norm_grad         0.001405  w[0]   -0.214 bias    0.548\n",
      "iter  800/30000  loss         0.321661  avg_L1_norm_grad         0.001379  w[0]   -0.217 bias    0.558\n",
      "iter  801/30000  loss         0.321569  avg_L1_norm_grad         0.001378  w[0]   -0.217 bias    0.559\n",
      "iter  820/30000  loss         0.319853  avg_L1_norm_grad         0.001353  w[0]   -0.219 bias    0.569\n",
      "iter  821/30000  loss         0.319765  avg_L1_norm_grad         0.001352  w[0]   -0.219 bias    0.570\n",
      "iter  840/30000  loss         0.318111  avg_L1_norm_grad         0.001328  w[0]   -0.222 bias    0.580\n",
      "iter  841/30000  loss         0.318026  avg_L1_norm_grad         0.001327  w[0]   -0.222 bias    0.581\n",
      "iter  860/30000  loss         0.316432  avg_L1_norm_grad         0.001304  w[0]   -0.225 bias    0.591\n",
      "iter  861/30000  loss         0.316349  avg_L1_norm_grad         0.001303  w[0]   -0.225 bias    0.592\n",
      "iter  880/30000  loss         0.314810  avg_L1_norm_grad         0.001281  w[0]   -0.227 bias    0.602\n",
      "iter  881/30000  loss         0.314731  avg_L1_norm_grad         0.001280  w[0]   -0.227 bias    0.603\n",
      "iter  900/30000  loss         0.313244  avg_L1_norm_grad         0.001258  w[0]   -0.230 bias    0.613\n",
      "iter  901/30000  loss         0.313167  avg_L1_norm_grad         0.001257  w[0]   -0.230 bias    0.613\n",
      "iter  920/30000  loss         0.311730  avg_L1_norm_grad         0.001237  w[0]   -0.232 bias    0.624\n",
      "iter  921/30000  loss         0.311656  avg_L1_norm_grad         0.001236  w[0]   -0.232 bias    0.624\n",
      "iter  940/30000  loss         0.310265  avg_L1_norm_grad         0.001216  w[0]   -0.234 bias    0.634\n",
      "iter  941/30000  loss         0.310193  avg_L1_norm_grad         0.001215  w[0]   -0.235 bias    0.635\n",
      "iter  960/30000  loss         0.308847  avg_L1_norm_grad         0.001196  w[0]   -0.237 bias    0.645\n",
      "iter  961/30000  loss         0.308777  avg_L1_norm_grad         0.001195  w[0]   -0.237 bias    0.645\n",
      "iter  980/30000  loss         0.307473  avg_L1_norm_grad         0.001177  w[0]   -0.239 bias    0.655\n",
      "iter  981/30000  loss         0.307405  avg_L1_norm_grad         0.001176  w[0]   -0.239 bias    0.656\n",
      "iter 1000/30000  loss         0.306141  avg_L1_norm_grad         0.001159  w[0]   -0.241 bias    0.665\n",
      "iter 1001/30000  loss         0.306075  avg_L1_norm_grad         0.001158  w[0]   -0.241 bias    0.666\n",
      "iter 1020/30000  loss         0.304849  avg_L1_norm_grad         0.001141  w[0]   -0.243 bias    0.676\n",
      "iter 1021/30000  loss         0.304785  avg_L1_norm_grad         0.001140  w[0]   -0.244 bias    0.676\n",
      "iter 1040/30000  loss         0.303594  avg_L1_norm_grad         0.001124  w[0]   -0.246 bias    0.686\n",
      "iter 1041/30000  loss         0.303533  avg_L1_norm_grad         0.001123  w[0]   -0.246 bias    0.686\n",
      "iter 1060/30000  loss         0.302376  avg_L1_norm_grad         0.001107  w[0]   -0.248 bias    0.696\n",
      "iter 1061/30000  loss         0.302316  avg_L1_norm_grad         0.001107  w[0]   -0.248 bias    0.696\n",
      "iter 1080/30000  loss         0.301193  avg_L1_norm_grad         0.001092  w[0]   -0.250 bias    0.706\n",
      "iter 1081/30000  loss         0.301134  avg_L1_norm_grad         0.001091  w[0]   -0.250 bias    0.706\n",
      "iter 1100/30000  loss         0.300042  avg_L1_norm_grad         0.001076  w[0]   -0.252 bias    0.716\n",
      "iter 1101/30000  loss         0.299985  avg_L1_norm_grad         0.001076  w[0]   -0.252 bias    0.716\n",
      "iter 1120/30000  loss         0.298922  avg_L1_norm_grad         0.001062  w[0]   -0.254 bias    0.726\n",
      "iter 1121/30000  loss         0.298867  avg_L1_norm_grad         0.001061  w[0]   -0.254 bias    0.726\n",
      "iter 1140/30000  loss         0.297833  avg_L1_norm_grad         0.001047  w[0]   -0.256 bias    0.735\n",
      "iter 1141/30000  loss         0.297779  avg_L1_norm_grad         0.001047  w[0]   -0.256 bias    0.736\n",
      "iter 1160/30000  loss         0.296772  avg_L1_norm_grad         0.001033  w[0]   -0.258 bias    0.745\n",
      "iter 1161/30000  loss         0.296719  avg_L1_norm_grad         0.001033  w[0]   -0.258 bias    0.746\n",
      "iter 1180/30000  loss         0.295738  avg_L1_norm_grad         0.001020  w[0]   -0.259 bias    0.755\n",
      "iter 1181/30000  loss         0.295687  avg_L1_norm_grad         0.001019  w[0]   -0.260 bias    0.755\n",
      "iter 1200/30000  loss         0.294730  avg_L1_norm_grad         0.001007  w[0]   -0.261 bias    0.764\n",
      "iter 1201/30000  loss         0.294681  avg_L1_norm_grad         0.001006  w[0]   -0.261 bias    0.765\n",
      "iter 1220/30000  loss         0.293748  avg_L1_norm_grad         0.000994  w[0]   -0.263 bias    0.774\n",
      "iter 1221/30000  loss         0.293700  avg_L1_norm_grad         0.000994  w[0]   -0.263 bias    0.774\n",
      "iter 1240/30000  loss         0.292790  avg_L1_norm_grad         0.000982  w[0]   -0.265 bias    0.783\n",
      "iter 1241/30000  loss         0.292742  avg_L1_norm_grad         0.000981  w[0]   -0.265 bias    0.784\n",
      "iter 1260/30000  loss         0.291855  avg_L1_norm_grad         0.000970  w[0]   -0.267 bias    0.793\n",
      "iter 1261/30000  loss         0.291808  avg_L1_norm_grad         0.000970  w[0]   -0.267 bias    0.793\n",
      "iter 1280/30000  loss         0.290941  avg_L1_norm_grad         0.000959  w[0]   -0.268 bias    0.802\n",
      "iter 1281/30000  loss         0.290896  avg_L1_norm_grad         0.000958  w[0]   -0.268 bias    0.803\n",
      "iter 1300/30000  loss         0.290050  avg_L1_norm_grad         0.000947  w[0]   -0.270 bias    0.811\n",
      "iter 1301/30000  loss         0.290006  avg_L1_norm_grad         0.000947  w[0]   -0.270 bias    0.812\n",
      "iter 1320/30000  loss         0.289178  avg_L1_norm_grad         0.000936  w[0]   -0.272 bias    0.821\n",
      "iter 1321/30000  loss         0.289135  avg_L1_norm_grad         0.000936  w[0]   -0.272 bias    0.821\n",
      "iter 1340/30000  loss         0.288327  avg_L1_norm_grad         0.000926  w[0]   -0.273 bias    0.830\n",
      "iter 1341/30000  loss         0.288285  avg_L1_norm_grad         0.000925  w[0]   -0.273 bias    0.830\n",
      "iter 1360/30000  loss         0.287494  avg_L1_norm_grad         0.000915  w[0]   -0.275 bias    0.839\n",
      "iter 1361/30000  loss         0.287453  avg_L1_norm_grad         0.000915  w[0]   -0.275 bias    0.839\n",
      "iter 1380/30000  loss         0.286680  avg_L1_norm_grad         0.000905  w[0]   -0.276 bias    0.848\n",
      "iter 1381/30000  loss         0.286639  avg_L1_norm_grad         0.000905  w[0]   -0.277 bias    0.848\n",
      "iter 1400/30000  loss         0.285883  avg_L1_norm_grad         0.000895  w[0]   -0.278 bias    0.857\n",
      "iter 1401/30000  loss         0.285843  avg_L1_norm_grad         0.000895  w[0]   -0.278 bias    0.857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.285103  avg_L1_norm_grad         0.000886  w[0]   -0.280 bias    0.866\n",
      "iter 1421/30000  loss         0.285064  avg_L1_norm_grad         0.000885  w[0]   -0.280 bias    0.866\n",
      "iter 1440/30000  loss         0.284339  avg_L1_norm_grad         0.000876  w[0]   -0.281 bias    0.875\n",
      "iter 1441/30000  loss         0.284301  avg_L1_norm_grad         0.000876  w[0]   -0.281 bias    0.875\n",
      "iter 1460/30000  loss         0.283591  avg_L1_norm_grad         0.000867  w[0]   -0.283 bias    0.883\n",
      "iter 1461/30000  loss         0.283554  avg_L1_norm_grad         0.000867  w[0]   -0.283 bias    0.884\n",
      "iter 1480/30000  loss         0.282859  avg_L1_norm_grad         0.000858  w[0]   -0.284 bias    0.892\n",
      "iter 1481/30000  loss         0.282822  avg_L1_norm_grad         0.000858  w[0]   -0.284 bias    0.893\n",
      "iter 1500/30000  loss         0.282141  avg_L1_norm_grad         0.000849  w[0]   -0.285 bias    0.901\n",
      "iter 1501/30000  loss         0.282105  avg_L1_norm_grad         0.000849  w[0]   -0.286 bias    0.901\n",
      "iter 1520/30000  loss         0.281437  avg_L1_norm_grad         0.000841  w[0]   -0.287 bias    0.910\n",
      "iter 1521/30000  loss         0.281402  avg_L1_norm_grad         0.000840  w[0]   -0.287 bias    0.910\n",
      "iter 1540/30000  loss         0.280747  avg_L1_norm_grad         0.000832  w[0]   -0.288 bias    0.918\n",
      "iter 1541/30000  loss         0.280713  avg_L1_norm_grad         0.000832  w[0]   -0.288 bias    0.919\n",
      "iter 1560/30000  loss         0.280070  avg_L1_norm_grad         0.000824  w[0]   -0.290 bias    0.927\n",
      "iter 1561/30000  loss         0.280037  avg_L1_norm_grad         0.000824  w[0]   -0.290 bias    0.927\n",
      "iter 1580/30000  loss         0.279407  avg_L1_norm_grad         0.000816  w[0]   -0.291 bias    0.935\n",
      "iter 1581/30000  loss         0.279374  avg_L1_norm_grad         0.000816  w[0]   -0.291 bias    0.936\n",
      "iter 1600/30000  loss         0.278755  avg_L1_norm_grad         0.000808  w[0]   -0.292 bias    0.944\n",
      "iter 1601/30000  loss         0.278723  avg_L1_norm_grad         0.000808  w[0]   -0.292 bias    0.944\n",
      "iter 1620/30000  loss         0.278116  avg_L1_norm_grad         0.000801  w[0]   -0.294 bias    0.952\n",
      "iter 1621/30000  loss         0.278084  avg_L1_norm_grad         0.000800  w[0]   -0.294 bias    0.953\n",
      "iter 1640/30000  loss         0.277488  avg_L1_norm_grad         0.000793  w[0]   -0.295 bias    0.961\n",
      "iter 1641/30000  loss         0.277457  avg_L1_norm_grad         0.000793  w[0]   -0.295 bias    0.961\n",
      "iter 1660/30000  loss         0.276872  avg_L1_norm_grad         0.000786  w[0]   -0.296 bias    0.969\n",
      "iter 1661/30000  loss         0.276841  avg_L1_norm_grad         0.000786  w[0]   -0.296 bias    0.969\n",
      "iter 1680/30000  loss         0.276266  avg_L1_norm_grad         0.000779  w[0]   -0.298 bias    0.977\n",
      "iter 1681/30000  loss         0.276236  avg_L1_norm_grad         0.000779  w[0]   -0.298 bias    0.978\n",
      "iter 1700/30000  loss         0.275671  avg_L1_norm_grad         0.000772  w[0]   -0.299 bias    0.985\n",
      "iter 1701/30000  loss         0.275642  avg_L1_norm_grad         0.000772  w[0]   -0.299 bias    0.986\n",
      "iter 1720/30000  loss         0.275087  avg_L1_norm_grad         0.000766  w[0]   -0.300 bias    0.994\n",
      "iter 1721/30000  loss         0.275058  avg_L1_norm_grad         0.000765  w[0]   -0.300 bias    0.994\n",
      "iter 1740/30000  loss         0.274513  avg_L1_norm_grad         0.000759  w[0]   -0.301 bias    1.002\n",
      "iter 1741/30000  loss         0.274484  avg_L1_norm_grad         0.000759  w[0]   -0.301 bias    1.002\n",
      "iter 1760/30000  loss         0.273948  avg_L1_norm_grad         0.000753  w[0]   -0.302 bias    1.010\n",
      "iter 1761/30000  loss         0.273920  avg_L1_norm_grad         0.000752  w[0]   -0.302 bias    1.010\n",
      "iter 1780/30000  loss         0.273393  avg_L1_norm_grad         0.000746  w[0]   -0.304 bias    1.018\n",
      "iter 1781/30000  loss         0.273365  avg_L1_norm_grad         0.000746  w[0]   -0.304 bias    1.018\n",
      "iter 1800/30000  loss         0.272847  avg_L1_norm_grad         0.000740  w[0]   -0.305 bias    1.026\n",
      "iter 1801/30000  loss         0.272820  avg_L1_norm_grad         0.000740  w[0]   -0.305 bias    1.026\n",
      "iter 1820/30000  loss         0.272309  avg_L1_norm_grad         0.000734  w[0]   -0.306 bias    1.034\n",
      "iter 1821/30000  loss         0.272283  avg_L1_norm_grad         0.000734  w[0]   -0.306 bias    1.034\n",
      "iter 1840/30000  loss         0.271781  avg_L1_norm_grad         0.000728  w[0]   -0.307 bias    1.042\n",
      "iter 1841/30000  loss         0.271755  avg_L1_norm_grad         0.000728  w[0]   -0.307 bias    1.042\n",
      "iter 1860/30000  loss         0.271261  avg_L1_norm_grad         0.000722  w[0]   -0.308 bias    1.050\n",
      "iter 1861/30000  loss         0.271235  avg_L1_norm_grad         0.000722  w[0]   -0.308 bias    1.050\n",
      "iter 1880/30000  loss         0.270749  avg_L1_norm_grad         0.000716  w[0]   -0.309 bias    1.058\n",
      "iter 1881/30000  loss         0.270724  avg_L1_norm_grad         0.000716  w[0]   -0.309 bias    1.058\n",
      "iter 1900/30000  loss         0.270245  avg_L1_norm_grad         0.000711  w[0]   -0.310 bias    1.065\n",
      "iter 1901/30000  loss         0.270220  avg_L1_norm_grad         0.000710  w[0]   -0.310 bias    1.066\n",
      "iter 1920/30000  loss         0.269749  avg_L1_norm_grad         0.000705  w[0]   -0.311 bias    1.073\n",
      "iter 1921/30000  loss         0.269725  avg_L1_norm_grad         0.000705  w[0]   -0.312 bias    1.074\n",
      "iter 1940/30000  loss         0.269261  avg_L1_norm_grad         0.000700  w[0]   -0.313 bias    1.081\n",
      "iter 1941/30000  loss         0.269237  avg_L1_norm_grad         0.000699  w[0]   -0.313 bias    1.081\n",
      "iter 1960/30000  loss         0.268780  avg_L1_norm_grad         0.000694  w[0]   -0.314 bias    1.089\n",
      "iter 1961/30000  loss         0.268756  avg_L1_norm_grad         0.000694  w[0]   -0.314 bias    1.089\n",
      "iter 1980/30000  loss         0.268306  avg_L1_norm_grad         0.000689  w[0]   -0.315 bias    1.096\n",
      "iter 1981/30000  loss         0.268282  avg_L1_norm_grad         0.000689  w[0]   -0.315 bias    1.097\n",
      "iter 2000/30000  loss         0.267839  avg_L1_norm_grad         0.000684  w[0]   -0.316 bias    1.104\n",
      "iter 2001/30000  loss         0.267816  avg_L1_norm_grad         0.000684  w[0]   -0.316 bias    1.104\n",
      "iter 2020/30000  loss         0.267379  avg_L1_norm_grad         0.000679  w[0]   -0.317 bias    1.112\n",
      "iter 2021/30000  loss         0.267356  avg_L1_norm_grad         0.000679  w[0]   -0.317 bias    1.112\n",
      "iter 2040/30000  loss         0.266926  avg_L1_norm_grad         0.000674  w[0]   -0.318 bias    1.119\n",
      "iter 2041/30000  loss         0.266903  avg_L1_norm_grad         0.000674  w[0]   -0.318 bias    1.120\n",
      "iter 2060/30000  loss         0.266479  avg_L1_norm_grad         0.000669  w[0]   -0.319 bias    1.127\n",
      "iter 2061/30000  loss         0.266457  avg_L1_norm_grad         0.000669  w[0]   -0.319 bias    1.127\n",
      "iter 2080/30000  loss         0.266038  avg_L1_norm_grad         0.000664  w[0]   -0.320 bias    1.134\n",
      "iter 2081/30000  loss         0.266016  avg_L1_norm_grad         0.000664  w[0]   -0.320 bias    1.135\n",
      "iter 2100/30000  loss         0.265604  avg_L1_norm_grad         0.000660  w[0]   -0.321 bias    1.142\n",
      "iter 2101/30000  loss         0.265582  avg_L1_norm_grad         0.000659  w[0]   -0.321 bias    1.142\n",
      "iter 2120/30000  loss         0.265175  avg_L1_norm_grad         0.000655  w[0]   -0.322 bias    1.149\n",
      "iter 2121/30000  loss         0.265154  avg_L1_norm_grad         0.000655  w[0]   -0.322 bias    1.150\n",
      "iter 2140/30000  loss         0.264753  avg_L1_norm_grad         0.000651  w[0]   -0.322 bias    1.157\n",
      "iter 2141/30000  loss         0.264732  avg_L1_norm_grad         0.000650  w[0]   -0.323 bias    1.157\n",
      "iter 2160/30000  loss         0.264336  avg_L1_norm_grad         0.000646  w[0]   -0.323 bias    1.164\n",
      "iter 2161/30000  loss         0.264316  avg_L1_norm_grad         0.000646  w[0]   -0.323 bias    1.164\n",
      "iter 2180/30000  loss         0.263925  avg_L1_norm_grad         0.000642  w[0]   -0.324 bias    1.171\n",
      "iter 2181/30000  loss         0.263905  avg_L1_norm_grad         0.000641  w[0]   -0.324 bias    1.172\n",
      "iter 2200/30000  loss         0.263520  avg_L1_norm_grad         0.000637  w[0]   -0.325 bias    1.179\n",
      "iter 2201/30000  loss         0.263499  avg_L1_norm_grad         0.000637  w[0]   -0.325 bias    1.179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.263119  avg_L1_norm_grad         0.000633  w[0]   -0.326 bias    1.186\n",
      "iter 2221/30000  loss         0.263099  avg_L1_norm_grad         0.000633  w[0]   -0.326 bias    1.186\n",
      "iter 2240/30000  loss         0.262724  avg_L1_norm_grad         0.000629  w[0]   -0.327 bias    1.193\n",
      "iter 2241/30000  loss         0.262705  avg_L1_norm_grad         0.000629  w[0]   -0.327 bias    1.193\n",
      "iter 2260/30000  loss         0.262335  avg_L1_norm_grad         0.000625  w[0]   -0.328 bias    1.200\n",
      "iter 2261/30000  loss         0.262315  avg_L1_norm_grad         0.000625  w[0]   -0.328 bias    1.201\n",
      "iter 2280/30000  loss         0.261950  avg_L1_norm_grad         0.000621  w[0]   -0.329 bias    1.207\n",
      "iter 2281/30000  loss         0.261931  avg_L1_norm_grad         0.000620  w[0]   -0.329 bias    1.208\n",
      "iter 2300/30000  loss         0.261570  avg_L1_norm_grad         0.000617  w[0]   -0.330 bias    1.215\n",
      "iter 2301/30000  loss         0.261551  avg_L1_norm_grad         0.000617  w[0]   -0.330 bias    1.215\n",
      "iter 2320/30000  loss         0.261195  avg_L1_norm_grad         0.000613  w[0]   -0.331 bias    1.222\n",
      "iter 2321/30000  loss         0.261176  avg_L1_norm_grad         0.000613  w[0]   -0.331 bias    1.222\n",
      "iter 2340/30000  loss         0.260824  avg_L1_norm_grad         0.000609  w[0]   -0.331 bias    1.229\n",
      "iter 2341/30000  loss         0.260806  avg_L1_norm_grad         0.000609  w[0]   -0.331 bias    1.229\n",
      "iter 2360/30000  loss         0.260458  avg_L1_norm_grad         0.000605  w[0]   -0.332 bias    1.236\n",
      "iter 2361/30000  loss         0.260440  avg_L1_norm_grad         0.000605  w[0]   -0.332 bias    1.236\n",
      "iter 2380/30000  loss         0.260097  avg_L1_norm_grad         0.000601  w[0]   -0.333 bias    1.243\n",
      "iter 2381/30000  loss         0.260079  avg_L1_norm_grad         0.000601  w[0]   -0.333 bias    1.243\n",
      "iter 2400/30000  loss         0.259740  avg_L1_norm_grad         0.000598  w[0]   -0.334 bias    1.250\n",
      "iter 2401/30000  loss         0.259722  avg_L1_norm_grad         0.000597  w[0]   -0.334 bias    1.250\n",
      "iter 2420/30000  loss         0.259387  avg_L1_norm_grad         0.000594  w[0]   -0.335 bias    1.257\n",
      "iter 2421/30000  loss         0.259370  avg_L1_norm_grad         0.000594  w[0]   -0.335 bias    1.257\n",
      "iter 2440/30000  loss         0.259039  avg_L1_norm_grad         0.000590  w[0]   -0.335 bias    1.264\n",
      "iter 2441/30000  loss         0.259022  avg_L1_norm_grad         0.000590  w[0]   -0.335 bias    1.264\n",
      "iter 2460/30000  loss         0.258695  avg_L1_norm_grad         0.000587  w[0]   -0.336 bias    1.271\n",
      "iter 2461/30000  loss         0.258677  avg_L1_norm_grad         0.000587  w[0]   -0.336 bias    1.271\n",
      "iter 2480/30000  loss         0.258354  avg_L1_norm_grad         0.000583  w[0]   -0.337 bias    1.277\n",
      "iter 2481/30000  loss         0.258337  avg_L1_norm_grad         0.000583  w[0]   -0.337 bias    1.278\n",
      "iter 2500/30000  loss         0.258018  avg_L1_norm_grad         0.000580  w[0]   -0.338 bias    1.284\n",
      "iter 2501/30000  loss         0.258001  avg_L1_norm_grad         0.000580  w[0]   -0.338 bias    1.285\n",
      "iter 2520/30000  loss         0.257686  avg_L1_norm_grad         0.000576  w[0]   -0.339 bias    1.291\n",
      "iter 2521/30000  loss         0.257669  avg_L1_norm_grad         0.000576  w[0]   -0.339 bias    1.291\n",
      "iter 2540/30000  loss         0.257357  avg_L1_norm_grad         0.000573  w[0]   -0.339 bias    1.298\n",
      "iter 2541/30000  loss         0.257341  avg_L1_norm_grad         0.000573  w[0]   -0.339 bias    1.298\n",
      "iter 2560/30000  loss         0.257032  avg_L1_norm_grad         0.000570  w[0]   -0.340 bias    1.305\n",
      "iter 2561/30000  loss         0.257016  avg_L1_norm_grad         0.000569  w[0]   -0.340 bias    1.305\n",
      "iter 2580/30000  loss         0.256711  avg_L1_norm_grad         0.000566  w[0]   -0.341 bias    1.311\n",
      "iter 2581/30000  loss         0.256695  avg_L1_norm_grad         0.000566  w[0]   -0.341 bias    1.312\n",
      "iter 2600/30000  loss         0.256393  avg_L1_norm_grad         0.000563  w[0]   -0.342 bias    1.318\n",
      "iter 2601/30000  loss         0.256378  avg_L1_norm_grad         0.000563  w[0]   -0.342 bias    1.318\n",
      "iter 2620/30000  loss         0.256079  avg_L1_norm_grad         0.000560  w[0]   -0.342 bias    1.325\n",
      "iter 2621/30000  loss         0.256064  avg_L1_norm_grad         0.000560  w[0]   -0.342 bias    1.325\n",
      "iter 2640/30000  loss         0.255769  avg_L1_norm_grad         0.000557  w[0]   -0.343 bias    1.332\n",
      "iter 2641/30000  loss         0.255753  avg_L1_norm_grad         0.000557  w[0]   -0.343 bias    1.332\n",
      "iter 2660/30000  loss         0.255461  avg_L1_norm_grad         0.000554  w[0]   -0.344 bias    1.338\n",
      "iter 2661/30000  loss         0.255446  avg_L1_norm_grad         0.000554  w[0]   -0.344 bias    1.338\n",
      "iter 2680/30000  loss         0.255158  avg_L1_norm_grad         0.000551  w[0]   -0.344 bias    1.345\n",
      "iter 2681/30000  loss         0.255142  avg_L1_norm_grad         0.000551  w[0]   -0.344 bias    1.345\n",
      "iter 2700/30000  loss         0.254857  avg_L1_norm_grad         0.000548  w[0]   -0.345 bias    1.351\n",
      "iter 2701/30000  loss         0.254842  avg_L1_norm_grad         0.000548  w[0]   -0.345 bias    1.352\n",
      "iter 2720/30000  loss         0.254560  avg_L1_norm_grad         0.000545  w[0]   -0.346 bias    1.358\n",
      "iter 2721/30000  loss         0.254545  avg_L1_norm_grad         0.000545  w[0]   -0.346 bias    1.358\n",
      "iter 2740/30000  loss         0.254265  avg_L1_norm_grad         0.000542  w[0]   -0.346 bias    1.364\n",
      "iter 2741/30000  loss         0.254251  avg_L1_norm_grad         0.000542  w[0]   -0.346 bias    1.365\n",
      "iter 2760/30000  loss         0.253974  avg_L1_norm_grad         0.000539  w[0]   -0.347 bias    1.371\n",
      "iter 2761/30000  loss         0.253960  avg_L1_norm_grad         0.000539  w[0]   -0.347 bias    1.371\n",
      "iter 2780/30000  loss         0.253686  avg_L1_norm_grad         0.000537  w[0]   -0.348 bias    1.377\n",
      "iter 2781/30000  loss         0.253672  avg_L1_norm_grad         0.000537  w[0]   -0.348 bias    1.378\n",
      "iter 2800/30000  loss         0.253401  avg_L1_norm_grad         0.000534  w[0]   -0.348 bias    1.384\n",
      "iter 2801/30000  loss         0.253387  avg_L1_norm_grad         0.000534  w[0]   -0.348 bias    1.384\n",
      "iter 2820/30000  loss         0.253118  avg_L1_norm_grad         0.000531  w[0]   -0.349 bias    1.390\n",
      "iter 2821/30000  loss         0.253104  avg_L1_norm_grad         0.000531  w[0]   -0.349 bias    1.391\n",
      "iter 2840/30000  loss         0.252839  avg_L1_norm_grad         0.000529  w[0]   -0.350 bias    1.397\n",
      "iter 2841/30000  loss         0.252825  avg_L1_norm_grad         0.000528  w[0]   -0.350 bias    1.397\n",
      "iter 2860/30000  loss         0.252563  avg_L1_norm_grad         0.000526  w[0]   -0.350 bias    1.403\n",
      "iter 2861/30000  loss         0.252549  avg_L1_norm_grad         0.000526  w[0]   -0.350 bias    1.403\n",
      "iter 2880/30000  loss         0.252289  avg_L1_norm_grad         0.000523  w[0]   -0.351 bias    1.410\n",
      "iter 2881/30000  loss         0.252275  avg_L1_norm_grad         0.000523  w[0]   -0.351 bias    1.410\n",
      "iter 2900/30000  loss         0.252018  avg_L1_norm_grad         0.000521  w[0]   -0.352 bias    1.416\n",
      "iter 2901/30000  loss         0.252004  avg_L1_norm_grad         0.000521  w[0]   -0.352 bias    1.416\n",
      "iter 2920/30000  loss         0.251749  avg_L1_norm_grad         0.000518  w[0]   -0.352 bias    1.422\n",
      "iter 2921/30000  loss         0.251736  avg_L1_norm_grad         0.000518  w[0]   -0.352 bias    1.423\n",
      "iter 2940/30000  loss         0.251484  avg_L1_norm_grad         0.000516  w[0]   -0.353 bias    1.429\n",
      "iter 2941/30000  loss         0.251470  avg_L1_norm_grad         0.000516  w[0]   -0.353 bias    1.429\n",
      "iter 2960/30000  loss         0.251221  avg_L1_norm_grad         0.000513  w[0]   -0.354 bias    1.435\n",
      "iter 2961/30000  loss         0.251207  avg_L1_norm_grad         0.000513  w[0]   -0.354 bias    1.435\n",
      "iter 2980/30000  loss         0.250960  avg_L1_norm_grad         0.000511  w[0]   -0.354 bias    1.441\n",
      "iter 2981/30000  loss         0.250947  avg_L1_norm_grad         0.000511  w[0]   -0.354 bias    1.441\n",
      "iter 3000/30000  loss         0.250702  avg_L1_norm_grad         0.000509  w[0]   -0.355 bias    1.447\n",
      "iter 3001/30000  loss         0.250689  avg_L1_norm_grad         0.000509  w[0]   -0.355 bias    1.448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.250446  avg_L1_norm_grad         0.000506  w[0]   -0.355 bias    1.454\n",
      "iter 3021/30000  loss         0.250434  avg_L1_norm_grad         0.000506  w[0]   -0.355 bias    1.454\n",
      "iter 3040/30000  loss         0.250193  avg_L1_norm_grad         0.000504  w[0]   -0.356 bias    1.460\n",
      "iter 3041/30000  loss         0.250181  avg_L1_norm_grad         0.000504  w[0]   -0.356 bias    1.460\n",
      "iter 3060/30000  loss         0.249942  avg_L1_norm_grad         0.000502  w[0]   -0.357 bias    1.466\n",
      "iter 3061/30000  loss         0.249930  avg_L1_norm_grad         0.000502  w[0]   -0.357 bias    1.466\n",
      "iter 3080/30000  loss         0.249694  avg_L1_norm_grad         0.000499  w[0]   -0.357 bias    1.472\n",
      "iter 3081/30000  loss         0.249682  avg_L1_norm_grad         0.000499  w[0]   -0.357 bias    1.472\n",
      "iter 3100/30000  loss         0.249448  avg_L1_norm_grad         0.000497  w[0]   -0.358 bias    1.478\n",
      "iter 3101/30000  loss         0.249436  avg_L1_norm_grad         0.000497  w[0]   -0.358 bias    1.478\n",
      "iter 3120/30000  loss         0.249204  avg_L1_norm_grad         0.000495  w[0]   -0.358 bias    1.484\n",
      "iter 3121/30000  loss         0.249192  avg_L1_norm_grad         0.000495  w[0]   -0.358 bias    1.485\n",
      "iter 3140/30000  loss         0.248962  avg_L1_norm_grad         0.000493  w[0]   -0.359 bias    1.490\n",
      "iter 3141/30000  loss         0.248950  avg_L1_norm_grad         0.000493  w[0]   -0.359 bias    1.491\n",
      "iter 3160/30000  loss         0.248723  avg_L1_norm_grad         0.000491  w[0]   -0.359 bias    1.496\n",
      "iter 3161/30000  loss         0.248711  avg_L1_norm_grad         0.000491  w[0]   -0.359 bias    1.497\n",
      "iter 3180/30000  loss         0.248486  avg_L1_norm_grad         0.000488  w[0]   -0.360 bias    1.502\n",
      "iter 3181/30000  loss         0.248474  avg_L1_norm_grad         0.000488  w[0]   -0.360 bias    1.503\n",
      "iter 3200/30000  loss         0.248250  avg_L1_norm_grad         0.000486  w[0]   -0.360 bias    1.509\n",
      "iter 3201/30000  loss         0.248239  avg_L1_norm_grad         0.000486  w[0]   -0.361 bias    1.509\n",
      "iter 3220/30000  loss         0.248017  avg_L1_norm_grad         0.000484  w[0]   -0.361 bias    1.515\n",
      "iter 3221/30000  loss         0.248006  avg_L1_norm_grad         0.000484  w[0]   -0.361 bias    1.515\n",
      "iter 3240/30000  loss         0.247787  avg_L1_norm_grad         0.000482  w[0]   -0.362 bias    1.521\n",
      "iter 3241/30000  loss         0.247775  avg_L1_norm_grad         0.000482  w[0]   -0.362 bias    1.521\n",
      "iter 3260/30000  loss         0.247558  avg_L1_norm_grad         0.000480  w[0]   -0.362 bias    1.526\n",
      "iter 3261/30000  loss         0.247546  avg_L1_norm_grad         0.000480  w[0]   -0.362 bias    1.527\n",
      "iter 3280/30000  loss         0.247331  avg_L1_norm_grad         0.000478  w[0]   -0.363 bias    1.532\n",
      "iter 3281/30000  loss         0.247319  avg_L1_norm_grad         0.000478  w[0]   -0.363 bias    1.533\n",
      "iter 3300/30000  loss         0.247106  avg_L1_norm_grad         0.000476  w[0]   -0.363 bias    1.538\n",
      "iter 3301/30000  loss         0.247095  avg_L1_norm_grad         0.000476  w[0]   -0.363 bias    1.539\n",
      "iter 3320/30000  loss         0.246883  avg_L1_norm_grad         0.000474  w[0]   -0.364 bias    1.544\n",
      "iter 3321/30000  loss         0.246872  avg_L1_norm_grad         0.000474  w[0]   -0.364 bias    1.545\n",
      "iter 3340/30000  loss         0.246662  avg_L1_norm_grad         0.000472  w[0]   -0.364 bias    1.550\n",
      "iter 3341/30000  loss         0.246651  avg_L1_norm_grad         0.000472  w[0]   -0.364 bias    1.550\n",
      "iter 3360/30000  loss         0.246443  avg_L1_norm_grad         0.000470  w[0]   -0.365 bias    1.556\n",
      "iter 3361/30000  loss         0.246432  avg_L1_norm_grad         0.000470  w[0]   -0.365 bias    1.556\n",
      "iter 3380/30000  loss         0.246225  avg_L1_norm_grad         0.000468  w[0]   -0.365 bias    1.562\n",
      "iter 3381/30000  loss         0.246214  avg_L1_norm_grad         0.000468  w[0]   -0.365 bias    1.562\n",
      "iter 3400/30000  loss         0.246010  avg_L1_norm_grad         0.000466  w[0]   -0.366 bias    1.568\n",
      "iter 3401/30000  loss         0.245999  avg_L1_norm_grad         0.000466  w[0]   -0.366 bias    1.568\n",
      "iter 3420/30000  loss         0.245796  avg_L1_norm_grad         0.000464  w[0]   -0.366 bias    1.574\n",
      "iter 3421/30000  loss         0.245785  avg_L1_norm_grad         0.000464  w[0]   -0.366 bias    1.574\n",
      "iter 3440/30000  loss         0.245584  avg_L1_norm_grad         0.000462  w[0]   -0.367 bias    1.579\n",
      "iter 3441/30000  loss         0.245574  avg_L1_norm_grad         0.000462  w[0]   -0.367 bias    1.580\n",
      "iter 3460/30000  loss         0.245374  avg_L1_norm_grad         0.000461  w[0]   -0.367 bias    1.585\n",
      "iter 3461/30000  loss         0.245364  avg_L1_norm_grad         0.000460  w[0]   -0.367 bias    1.585\n",
      "iter 3480/30000  loss         0.245166  avg_L1_norm_grad         0.000459  w[0]   -0.368 bias    1.591\n",
      "iter 3481/30000  loss         0.245155  avg_L1_norm_grad         0.000459  w[0]   -0.368 bias    1.591\n",
      "iter 3500/30000  loss         0.244959  avg_L1_norm_grad         0.000457  w[0]   -0.368 bias    1.597\n",
      "iter 3501/30000  loss         0.244949  avg_L1_norm_grad         0.000457  w[0]   -0.368 bias    1.597\n",
      "iter 3520/30000  loss         0.244754  avg_L1_norm_grad         0.000455  w[0]   -0.369 bias    1.602\n",
      "iter 3521/30000  loss         0.244744  avg_L1_norm_grad         0.000455  w[0]   -0.369 bias    1.603\n",
      "iter 3540/30000  loss         0.244551  avg_L1_norm_grad         0.000453  w[0]   -0.369 bias    1.608\n",
      "iter 3541/30000  loss         0.244541  avg_L1_norm_grad         0.000453  w[0]   -0.369 bias    1.608\n",
      "iter 3560/30000  loss         0.244349  avg_L1_norm_grad         0.000452  w[0]   -0.370 bias    1.614\n",
      "iter 3561/30000  loss         0.244339  avg_L1_norm_grad         0.000451  w[0]   -0.370 bias    1.614\n",
      "iter 3580/30000  loss         0.244149  avg_L1_norm_grad         0.000450  w[0]   -0.370 bias    1.619\n",
      "iter 3581/30000  loss         0.244139  avg_L1_norm_grad         0.000450  w[0]   -0.370 bias    1.620\n",
      "iter 3600/30000  loss         0.243950  avg_L1_norm_grad         0.000448  w[0]   -0.371 bias    1.625\n",
      "iter 3601/30000  loss         0.243941  avg_L1_norm_grad         0.000448  w[0]   -0.371 bias    1.625\n",
      "iter 3620/30000  loss         0.243754  avg_L1_norm_grad         0.000446  w[0]   -0.371 bias    1.631\n",
      "iter 3621/30000  loss         0.243744  avg_L1_norm_grad         0.000446  w[0]   -0.371 bias    1.631\n",
      "iter 3640/30000  loss         0.243558  avg_L1_norm_grad         0.000445  w[0]   -0.371 bias    1.636\n",
      "iter 3641/30000  loss         0.243548  avg_L1_norm_grad         0.000445  w[0]   -0.372 bias    1.637\n",
      "iter 3660/30000  loss         0.243364  avg_L1_norm_grad         0.000443  w[0]   -0.372 bias    1.642\n",
      "iter 3661/30000  loss         0.243355  avg_L1_norm_grad         0.000443  w[0]   -0.372 bias    1.642\n",
      "iter 3680/30000  loss         0.243172  avg_L1_norm_grad         0.000441  w[0]   -0.372 bias    1.648\n",
      "iter 3681/30000  loss         0.243162  avg_L1_norm_grad         0.000441  w[0]   -0.372 bias    1.648\n",
      "iter 3700/30000  loss         0.242981  avg_L1_norm_grad         0.000440  w[0]   -0.373 bias    1.653\n",
      "iter 3701/30000  loss         0.242972  avg_L1_norm_grad         0.000440  w[0]   -0.373 bias    1.653\n",
      "iter 3720/30000  loss         0.242792  avg_L1_norm_grad         0.000438  w[0]   -0.373 bias    1.659\n",
      "iter 3721/30000  loss         0.242782  avg_L1_norm_grad         0.000438  w[0]   -0.373 bias    1.659\n",
      "iter 3740/30000  loss         0.242604  avg_L1_norm_grad         0.000436  w[0]   -0.374 bias    1.664\n",
      "iter 3741/30000  loss         0.242595  avg_L1_norm_grad         0.000436  w[0]   -0.374 bias    1.665\n",
      "iter 3760/30000  loss         0.242418  avg_L1_norm_grad         0.000435  w[0]   -0.374 bias    1.670\n",
      "iter 3761/30000  loss         0.242408  avg_L1_norm_grad         0.000435  w[0]   -0.374 bias    1.670\n",
      "iter 3780/30000  loss         0.242232  avg_L1_norm_grad         0.000433  w[0]   -0.375 bias    1.675\n",
      "iter 3781/30000  loss         0.242223  avg_L1_norm_grad         0.000433  w[0]   -0.375 bias    1.676\n",
      "iter 3800/30000  loss         0.242049  avg_L1_norm_grad         0.000432  w[0]   -0.375 bias    1.681\n",
      "iter 3801/30000  loss         0.242040  avg_L1_norm_grad         0.000432  w[0]   -0.375 bias    1.681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.241867  avg_L1_norm_grad         0.000430  w[0]   -0.375 bias    1.686\n",
      "iter 3821/30000  loss         0.241857  avg_L1_norm_grad         0.000430  w[0]   -0.375 bias    1.687\n",
      "iter 3840/30000  loss         0.241686  avg_L1_norm_grad         0.000428  w[0]   -0.376 bias    1.692\n",
      "iter 3841/30000  loss         0.241677  avg_L1_norm_grad         0.000428  w[0]   -0.376 bias    1.692\n",
      "iter 3860/30000  loss         0.241506  avg_L1_norm_grad         0.000427  w[0]   -0.376 bias    1.697\n",
      "iter 3861/30000  loss         0.241497  avg_L1_norm_grad         0.000427  w[0]   -0.376 bias    1.697\n",
      "iter 3880/30000  loss         0.241328  avg_L1_norm_grad         0.000425  w[0]   -0.377 bias    1.703\n",
      "iter 3881/30000  loss         0.241319  avg_L1_norm_grad         0.000425  w[0]   -0.377 bias    1.703\n",
      "iter 3900/30000  loss         0.241151  avg_L1_norm_grad         0.000424  w[0]   -0.377 bias    1.708\n",
      "iter 3901/30000  loss         0.241142  avg_L1_norm_grad         0.000424  w[0]   -0.377 bias    1.708\n",
      "iter 3920/30000  loss         0.240975  avg_L1_norm_grad         0.000422  w[0]   -0.378 bias    1.713\n",
      "iter 3921/30000  loss         0.240966  avg_L1_norm_grad         0.000422  w[0]   -0.378 bias    1.714\n",
      "iter 3940/30000  loss         0.240801  avg_L1_norm_grad         0.000421  w[0]   -0.378 bias    1.719\n",
      "iter 3941/30000  loss         0.240792  avg_L1_norm_grad         0.000421  w[0]   -0.378 bias    1.719\n",
      "iter 3960/30000  loss         0.240628  avg_L1_norm_grad         0.000419  w[0]   -0.378 bias    1.724\n",
      "iter 3961/30000  loss         0.240619  avg_L1_norm_grad         0.000419  w[0]   -0.378 bias    1.724\n",
      "iter 3980/30000  loss         0.240456  avg_L1_norm_grad         0.000418  w[0]   -0.379 bias    1.730\n",
      "iter 3981/30000  loss         0.240447  avg_L1_norm_grad         0.000418  w[0]   -0.379 bias    1.730\n",
      "iter 4000/30000  loss         0.240285  avg_L1_norm_grad         0.000416  w[0]   -0.379 bias    1.735\n",
      "iter 4001/30000  loss         0.240277  avg_L1_norm_grad         0.000416  w[0]   -0.379 bias    1.735\n",
      "iter 4020/30000  loss         0.240116  avg_L1_norm_grad         0.000415  w[0]   -0.380 bias    1.740\n",
      "iter 4021/30000  loss         0.240107  avg_L1_norm_grad         0.000415  w[0]   -0.380 bias    1.740\n",
      "iter 4040/30000  loss         0.239948  avg_L1_norm_grad         0.000414  w[0]   -0.380 bias    1.746\n",
      "iter 4041/30000  loss         0.239939  avg_L1_norm_grad         0.000413  w[0]   -0.380 bias    1.746\n",
      "iter 4060/30000  loss         0.239780  avg_L1_norm_grad         0.000412  w[0]   -0.380 bias    1.751\n",
      "iter 4061/30000  loss         0.239772  avg_L1_norm_grad         0.000412  w[0]   -0.380 bias    1.751\n",
      "iter 4080/30000  loss         0.239615  avg_L1_norm_grad         0.000411  w[0]   -0.381 bias    1.756\n",
      "iter 4081/30000  loss         0.239606  avg_L1_norm_grad         0.000411  w[0]   -0.381 bias    1.756\n",
      "iter 4100/30000  loss         0.239450  avg_L1_norm_grad         0.000409  w[0]   -0.381 bias    1.761\n",
      "iter 4101/30000  loss         0.239442  avg_L1_norm_grad         0.000409  w[0]   -0.381 bias    1.762\n",
      "iter 4120/30000  loss         0.239286  avg_L1_norm_grad         0.000408  w[0]   -0.381 bias    1.767\n",
      "iter 4121/30000  loss         0.239278  avg_L1_norm_grad         0.000408  w[0]   -0.381 bias    1.767\n",
      "iter 4140/30000  loss         0.239124  avg_L1_norm_grad         0.000406  w[0]   -0.382 bias    1.772\n",
      "iter 4141/30000  loss         0.239116  avg_L1_norm_grad         0.000406  w[0]   -0.382 bias    1.772\n",
      "iter 4160/30000  loss         0.238963  avg_L1_norm_grad         0.000405  w[0]   -0.382 bias    1.777\n",
      "iter 4161/30000  loss         0.238955  avg_L1_norm_grad         0.000405  w[0]   -0.382 bias    1.777\n",
      "iter 4180/30000  loss         0.238802  avg_L1_norm_grad         0.000404  w[0]   -0.383 bias    1.782\n",
      "iter 4181/30000  loss         0.238794  avg_L1_norm_grad         0.000404  w[0]   -0.383 bias    1.783\n",
      "iter 4200/30000  loss         0.238643  avg_L1_norm_grad         0.000402  w[0]   -0.383 bias    1.787\n",
      "iter 4201/30000  loss         0.238635  avg_L1_norm_grad         0.000402  w[0]   -0.383 bias    1.788\n",
      "iter 4220/30000  loss         0.238485  avg_L1_norm_grad         0.000401  w[0]   -0.383 bias    1.793\n",
      "iter 4221/30000  loss         0.238477  avg_L1_norm_grad         0.000401  w[0]   -0.383 bias    1.793\n",
      "iter 4240/30000  loss         0.238328  avg_L1_norm_grad         0.000400  w[0]   -0.384 bias    1.798\n",
      "iter 4241/30000  loss         0.238321  avg_L1_norm_grad         0.000400  w[0]   -0.384 bias    1.798\n",
      "iter 4260/30000  loss         0.238172  avg_L1_norm_grad         0.000398  w[0]   -0.384 bias    1.803\n",
      "iter 4261/30000  loss         0.238165  avg_L1_norm_grad         0.000398  w[0]   -0.384 bias    1.803\n",
      "iter 4280/30000  loss         0.238018  avg_L1_norm_grad         0.000397  w[0]   -0.384 bias    1.808\n",
      "iter 4281/30000  loss         0.238010  avg_L1_norm_grad         0.000397  w[0]   -0.384 bias    1.808\n",
      "iter 4300/30000  loss         0.237864  avg_L1_norm_grad         0.000396  w[0]   -0.385 bias    1.813\n",
      "iter 4301/30000  loss         0.237856  avg_L1_norm_grad         0.000396  w[0]   -0.385 bias    1.813\n",
      "iter 4320/30000  loss         0.237711  avg_L1_norm_grad         0.000394  w[0]   -0.385 bias    1.818\n",
      "iter 4321/30000  loss         0.237703  avg_L1_norm_grad         0.000394  w[0]   -0.385 bias    1.819\n",
      "iter 4340/30000  loss         0.237559  avg_L1_norm_grad         0.000393  w[0]   -0.385 bias    1.823\n",
      "iter 4341/30000  loss         0.237552  avg_L1_norm_grad         0.000393  w[0]   -0.385 bias    1.824\n",
      "iter 4360/30000  loss         0.237408  avg_L1_norm_grad         0.000392  w[0]   -0.386 bias    1.829\n",
      "iter 4361/30000  loss         0.237401  avg_L1_norm_grad         0.000392  w[0]   -0.386 bias    1.829\n",
      "iter 4380/30000  loss         0.237259  avg_L1_norm_grad         0.000391  w[0]   -0.386 bias    1.834\n",
      "iter 4381/30000  loss         0.237251  avg_L1_norm_grad         0.000391  w[0]   -0.386 bias    1.834\n",
      "iter 4400/30000  loss         0.237110  avg_L1_norm_grad         0.000389  w[0]   -0.386 bias    1.839\n",
      "iter 4401/30000  loss         0.237102  avg_L1_norm_grad         0.000389  w[0]   -0.386 bias    1.839\n",
      "iter 4420/30000  loss         0.236962  avg_L1_norm_grad         0.000388  w[0]   -0.387 bias    1.844\n",
      "iter 4421/30000  loss         0.236954  avg_L1_norm_grad         0.000388  w[0]   -0.387 bias    1.844\n",
      "iter 4440/30000  loss         0.236815  avg_L1_norm_grad         0.000387  w[0]   -0.387 bias    1.849\n",
      "iter 4441/30000  loss         0.236808  avg_L1_norm_grad         0.000387  w[0]   -0.387 bias    1.849\n",
      "iter 4460/30000  loss         0.236669  avg_L1_norm_grad         0.000386  w[0]   -0.387 bias    1.854\n",
      "iter 4461/30000  loss         0.236662  avg_L1_norm_grad         0.000386  w[0]   -0.388 bias    1.854\n",
      "iter 4480/30000  loss         0.236524  avg_L1_norm_grad         0.000385  w[0]   -0.388 bias    1.859\n",
      "iter 4481/30000  loss         0.236517  avg_L1_norm_grad         0.000384  w[0]   -0.388 bias    1.859\n",
      "iter 4500/30000  loss         0.236380  avg_L1_norm_grad         0.000383  w[0]   -0.388 bias    1.864\n",
      "iter 4501/30000  loss         0.236373  avg_L1_norm_grad         0.000383  w[0]   -0.388 bias    1.864\n",
      "iter 4520/30000  loss         0.236237  avg_L1_norm_grad         0.000382  w[0]   -0.388 bias    1.869\n",
      "iter 4521/30000  loss         0.236229  avg_L1_norm_grad         0.000382  w[0]   -0.389 bias    1.869\n",
      "iter 4540/30000  loss         0.236094  avg_L1_norm_grad         0.000381  w[0]   -0.389 bias    1.874\n",
      "iter 4541/30000  loss         0.236087  avg_L1_norm_grad         0.000381  w[0]   -0.389 bias    1.874\n",
      "iter 4560/30000  loss         0.235953  avg_L1_norm_grad         0.000380  w[0]   -0.389 bias    1.879\n",
      "iter 4561/30000  loss         0.235946  avg_L1_norm_grad         0.000380  w[0]   -0.389 bias    1.879\n",
      "iter 4580/30000  loss         0.235812  avg_L1_norm_grad         0.000379  w[0]   -0.389 bias    1.884\n",
      "iter 4581/30000  loss         0.235805  avg_L1_norm_grad         0.000379  w[0]   -0.389 bias    1.884\n",
      "iter 4600/30000  loss         0.235673  avg_L1_norm_grad         0.000377  w[0]   -0.390 bias    1.889\n",
      "iter 4601/30000  loss         0.235666  avg_L1_norm_grad         0.000377  w[0]   -0.390 bias    1.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4620/30000  loss         0.235534  avg_L1_norm_grad         0.000376  w[0]   -0.390 bias    1.893\n",
      "iter 4621/30000  loss         0.235527  avg_L1_norm_grad         0.000376  w[0]   -0.390 bias    1.894\n",
      "iter 4640/30000  loss         0.235396  avg_L1_norm_grad         0.000375  w[0]   -0.390 bias    1.898\n",
      "iter 4641/30000  loss         0.235389  avg_L1_norm_grad         0.000375  w[0]   -0.390 bias    1.899\n",
      "iter 4660/30000  loss         0.235259  avg_L1_norm_grad         0.000374  w[0]   -0.391 bias    1.903\n",
      "iter 4661/30000  loss         0.235252  avg_L1_norm_grad         0.000374  w[0]   -0.391 bias    1.904\n",
      "iter 4680/30000  loss         0.235123  avg_L1_norm_grad         0.000373  w[0]   -0.391 bias    1.908\n",
      "iter 4681/30000  loss         0.235116  avg_L1_norm_grad         0.000373  w[0]   -0.391 bias    1.908\n",
      "iter 4700/30000  loss         0.234987  avg_L1_norm_grad         0.000372  w[0]   -0.391 bias    1.913\n",
      "iter 4701/30000  loss         0.234981  avg_L1_norm_grad         0.000372  w[0]   -0.391 bias    1.913\n",
      "iter 4720/30000  loss         0.234853  avg_L1_norm_grad         0.000371  w[0]   -0.392 bias    1.918\n",
      "iter 4721/30000  loss         0.234846  avg_L1_norm_grad         0.000371  w[0]   -0.392 bias    1.918\n",
      "iter 4740/30000  loss         0.234719  avg_L1_norm_grad         0.000369  w[0]   -0.392 bias    1.923\n",
      "iter 4741/30000  loss         0.234712  avg_L1_norm_grad         0.000369  w[0]   -0.392 bias    1.923\n",
      "iter 4760/30000  loss         0.234586  avg_L1_norm_grad         0.000368  w[0]   -0.392 bias    1.928\n",
      "iter 4761/30000  loss         0.234580  avg_L1_norm_grad         0.000368  w[0]   -0.392 bias    1.928\n",
      "iter 4780/30000  loss         0.234454  avg_L1_norm_grad         0.000367  w[0]   -0.393 bias    1.932\n",
      "iter 4781/30000  loss         0.234447  avg_L1_norm_grad         0.000367  w[0]   -0.393 bias    1.933\n",
      "iter 4800/30000  loss         0.234323  avg_L1_norm_grad         0.000366  w[0]   -0.393 bias    1.937\n",
      "iter 4801/30000  loss         0.234316  avg_L1_norm_grad         0.000366  w[0]   -0.393 bias    1.937\n",
      "iter 4820/30000  loss         0.234192  avg_L1_norm_grad         0.000365  w[0]   -0.393 bias    1.942\n",
      "iter 4821/30000  loss         0.234186  avg_L1_norm_grad         0.000365  w[0]   -0.393 bias    1.942\n",
      "iter 4840/30000  loss         0.234062  avg_L1_norm_grad         0.000364  w[0]   -0.393 bias    1.947\n",
      "iter 4841/30000  loss         0.234056  avg_L1_norm_grad         0.000364  w[0]   -0.393 bias    1.947\n",
      "iter 4860/30000  loss         0.233933  avg_L1_norm_grad         0.000363  w[0]   -0.394 bias    1.952\n",
      "iter 4861/30000  loss         0.233927  avg_L1_norm_grad         0.000363  w[0]   -0.394 bias    1.952\n",
      "iter 4880/30000  loss         0.233805  avg_L1_norm_grad         0.000362  w[0]   -0.394 bias    1.956\n",
      "iter 4881/30000  loss         0.233799  avg_L1_norm_grad         0.000362  w[0]   -0.394 bias    1.957\n",
      "iter 4900/30000  loss         0.233677  avg_L1_norm_grad         0.000361  w[0]   -0.394 bias    1.961\n",
      "iter 4901/30000  loss         0.233671  avg_L1_norm_grad         0.000361  w[0]   -0.394 bias    1.961\n",
      "iter 4920/30000  loss         0.233551  avg_L1_norm_grad         0.000360  w[0]   -0.395 bias    1.966\n",
      "iter 4921/30000  loss         0.233544  avg_L1_norm_grad         0.000360  w[0]   -0.395 bias    1.966\n",
      "iter 4940/30000  loss         0.233425  avg_L1_norm_grad         0.000359  w[0]   -0.395 bias    1.971\n",
      "iter 4941/30000  loss         0.233418  avg_L1_norm_grad         0.000359  w[0]   -0.395 bias    1.971\n",
      "iter 4960/30000  loss         0.233299  avg_L1_norm_grad         0.000358  w[0]   -0.395 bias    1.975\n",
      "iter 4961/30000  loss         0.233293  avg_L1_norm_grad         0.000358  w[0]   -0.395 bias    1.976\n",
      "iter 4980/30000  loss         0.233175  avg_L1_norm_grad         0.000357  w[0]   -0.395 bias    1.980\n",
      "iter 4981/30000  loss         0.233169  avg_L1_norm_grad         0.000357  w[0]   -0.396 bias    1.980\n",
      "iter 5000/30000  loss         0.233051  avg_L1_norm_grad         0.000356  w[0]   -0.396 bias    1.985\n",
      "iter 5001/30000  loss         0.233045  avg_L1_norm_grad         0.000356  w[0]   -0.396 bias    1.985\n",
      "iter 5020/30000  loss         0.232928  avg_L1_norm_grad         0.000355  w[0]   -0.396 bias    1.989\n",
      "iter 5021/30000  loss         0.232922  avg_L1_norm_grad         0.000355  w[0]   -0.396 bias    1.990\n",
      "iter 5040/30000  loss         0.232805  avg_L1_norm_grad         0.000354  w[0]   -0.396 bias    1.994\n",
      "iter 5041/30000  loss         0.232799  avg_L1_norm_grad         0.000354  w[0]   -0.396 bias    1.994\n",
      "iter 5060/30000  loss         0.232684  avg_L1_norm_grad         0.000353  w[0]   -0.397 bias    1.999\n",
      "iter 5061/30000  loss         0.232678  avg_L1_norm_grad         0.000353  w[0]   -0.397 bias    1.999\n",
      "iter 5080/30000  loss         0.232563  avg_L1_norm_grad         0.000352  w[0]   -0.397 bias    2.003\n",
      "iter 5081/30000  loss         0.232557  avg_L1_norm_grad         0.000352  w[0]   -0.397 bias    2.004\n",
      "iter 5100/30000  loss         0.232442  avg_L1_norm_grad         0.000351  w[0]   -0.397 bias    2.008\n",
      "iter 5101/30000  loss         0.232436  avg_L1_norm_grad         0.000351  w[0]   -0.397 bias    2.008\n",
      "iter 5120/30000  loss         0.232323  avg_L1_norm_grad         0.000350  w[0]   -0.397 bias    2.013\n",
      "iter 5121/30000  loss         0.232317  avg_L1_norm_grad         0.000350  w[0]   -0.397 bias    2.013\n",
      "iter 5140/30000  loss         0.232203  avg_L1_norm_grad         0.000349  w[0]   -0.398 bias    2.017\n",
      "iter 5141/30000  loss         0.232198  avg_L1_norm_grad         0.000349  w[0]   -0.398 bias    2.018\n",
      "iter 5160/30000  loss         0.232085  avg_L1_norm_grad         0.000348  w[0]   -0.398 bias    2.022\n",
      "iter 5161/30000  loss         0.232079  avg_L1_norm_grad         0.000348  w[0]   -0.398 bias    2.022\n",
      "iter 5180/30000  loss         0.231967  avg_L1_norm_grad         0.000347  w[0]   -0.398 bias    2.027\n",
      "iter 5181/30000  loss         0.231962  avg_L1_norm_grad         0.000347  w[0]   -0.398 bias    2.027\n",
      "iter 5200/30000  loss         0.231850  avg_L1_norm_grad         0.000346  w[0]   -0.399 bias    2.031\n",
      "iter 5201/30000  loss         0.231845  avg_L1_norm_grad         0.000346  w[0]   -0.399 bias    2.031\n",
      "iter 5220/30000  loss         0.231734  avg_L1_norm_grad         0.000345  w[0]   -0.399 bias    2.036\n",
      "iter 5221/30000  loss         0.231728  avg_L1_norm_grad         0.000345  w[0]   -0.399 bias    2.036\n",
      "iter 5240/30000  loss         0.231618  avg_L1_norm_grad         0.000344  w[0]   -0.399 bias    2.040\n",
      "iter 5241/30000  loss         0.231612  avg_L1_norm_grad         0.000344  w[0]   -0.399 bias    2.041\n",
      "iter 5260/30000  loss         0.231503  avg_L1_norm_grad         0.000343  w[0]   -0.399 bias    2.045\n",
      "iter 5261/30000  loss         0.231497  avg_L1_norm_grad         0.000343  w[0]   -0.399 bias    2.045\n",
      "iter 5280/30000  loss         0.231389  avg_L1_norm_grad         0.000342  w[0]   -0.400 bias    2.049\n",
      "iter 5281/30000  loss         0.231383  avg_L1_norm_grad         0.000342  w[0]   -0.400 bias    2.050\n",
      "iter 5300/30000  loss         0.231275  avg_L1_norm_grad         0.000341  w[0]   -0.400 bias    2.054\n",
      "iter 5301/30000  loss         0.231269  avg_L1_norm_grad         0.000341  w[0]   -0.400 bias    2.054\n",
      "iter 5320/30000  loss         0.231161  avg_L1_norm_grad         0.000340  w[0]   -0.400 bias    2.059\n",
      "iter 5321/30000  loss         0.231156  avg_L1_norm_grad         0.000340  w[0]   -0.400 bias    2.059\n",
      "iter 5340/30000  loss         0.231049  avg_L1_norm_grad         0.000339  w[0]   -0.400 bias    2.063\n",
      "iter 5341/30000  loss         0.231043  avg_L1_norm_grad         0.000339  w[0]   -0.400 bias    2.063\n",
      "iter 5360/30000  loss         0.230937  avg_L1_norm_grad         0.000338  w[0]   -0.401 bias    2.068\n",
      "iter 5361/30000  loss         0.230931  avg_L1_norm_grad         0.000338  w[0]   -0.401 bias    2.068\n",
      "iter 5380/30000  loss         0.230825  avg_L1_norm_grad         0.000337  w[0]   -0.401 bias    2.072\n",
      "iter 5381/30000  loss         0.230820  avg_L1_norm_grad         0.000337  w[0]   -0.401 bias    2.072\n",
      "iter 5400/30000  loss         0.230715  avg_L1_norm_grad         0.000337  w[0]   -0.401 bias    2.077\n",
      "iter 5401/30000  loss         0.230709  avg_L1_norm_grad         0.000337  w[0]   -0.401 bias    2.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5420/30000  loss         0.230604  avg_L1_norm_grad         0.000336  w[0]   -0.401 bias    2.081\n",
      "iter 5421/30000  loss         0.230599  avg_L1_norm_grad         0.000336  w[0]   -0.401 bias    2.081\n",
      "iter 5440/30000  loss         0.230495  avg_L1_norm_grad         0.000335  w[0]   -0.402 bias    2.085\n",
      "iter 5441/30000  loss         0.230489  avg_L1_norm_grad         0.000335  w[0]   -0.402 bias    2.086\n",
      "iter 5460/30000  loss         0.230386  avg_L1_norm_grad         0.000334  w[0]   -0.402 bias    2.090\n",
      "iter 5461/30000  loss         0.230380  avg_L1_norm_grad         0.000334  w[0]   -0.402 bias    2.090\n",
      "iter 5480/30000  loss         0.230277  avg_L1_norm_grad         0.000333  w[0]   -0.402 bias    2.094\n",
      "iter 5481/30000  loss         0.230272  avg_L1_norm_grad         0.000333  w[0]   -0.402 bias    2.095\n",
      "iter 5500/30000  loss         0.230169  avg_L1_norm_grad         0.000332  w[0]   -0.402 bias    2.099\n",
      "iter 5501/30000  loss         0.230164  avg_L1_norm_grad         0.000332  w[0]   -0.402 bias    2.099\n",
      "iter 5520/30000  loss         0.230062  avg_L1_norm_grad         0.000331  w[0]   -0.403 bias    2.103\n",
      "iter 5521/30000  loss         0.230056  avg_L1_norm_grad         0.000331  w[0]   -0.403 bias    2.104\n",
      "iter 5540/30000  loss         0.229955  avg_L1_norm_grad         0.000330  w[0]   -0.403 bias    2.108\n",
      "iter 5541/30000  loss         0.229950  avg_L1_norm_grad         0.000330  w[0]   -0.403 bias    2.108\n",
      "iter 5560/30000  loss         0.229849  avg_L1_norm_grad         0.000330  w[0]   -0.403 bias    2.112\n",
      "iter 5561/30000  loss         0.229843  avg_L1_norm_grad         0.000329  w[0]   -0.403 bias    2.112\n",
      "iter 5580/30000  loss         0.229743  avg_L1_norm_grad         0.000329  w[0]   -0.403 bias    2.117\n",
      "iter 5581/30000  loss         0.229738  avg_L1_norm_grad         0.000329  w[0]   -0.403 bias    2.117\n",
      "iter 5600/30000  loss         0.229638  avg_L1_norm_grad         0.000328  w[0]   -0.403 bias    2.121\n",
      "iter 5601/30000  loss         0.229633  avg_L1_norm_grad         0.000328  w[0]   -0.403 bias    2.121\n",
      "iter 5620/30000  loss         0.229533  avg_L1_norm_grad         0.000327  w[0]   -0.404 bias    2.125\n",
      "iter 5621/30000  loss         0.229528  avg_L1_norm_grad         0.000327  w[0]   -0.404 bias    2.126\n",
      "iter 5640/30000  loss         0.229429  avg_L1_norm_grad         0.000326  w[0]   -0.404 bias    2.130\n",
      "iter 5641/30000  loss         0.229424  avg_L1_norm_grad         0.000326  w[0]   -0.404 bias    2.130\n",
      "iter 5660/30000  loss         0.229326  avg_L1_norm_grad         0.000325  w[0]   -0.404 bias    2.134\n",
      "iter 5661/30000  loss         0.229320  avg_L1_norm_grad         0.000325  w[0]   -0.404 bias    2.134\n",
      "iter 5680/30000  loss         0.229223  avg_L1_norm_grad         0.000324  w[0]   -0.404 bias    2.138\n",
      "iter 5681/30000  loss         0.229217  avg_L1_norm_grad         0.000324  w[0]   -0.404 bias    2.139\n",
      "iter 5700/30000  loss         0.229120  avg_L1_norm_grad         0.000324  w[0]   -0.405 bias    2.143\n",
      "iter 5701/30000  loss         0.229115  avg_L1_norm_grad         0.000324  w[0]   -0.405 bias    2.143\n",
      "iter 5720/30000  loss         0.229018  avg_L1_norm_grad         0.000323  w[0]   -0.405 bias    2.147\n",
      "iter 5721/30000  loss         0.229013  avg_L1_norm_grad         0.000323  w[0]   -0.405 bias    2.147\n",
      "iter 5740/30000  loss         0.228917  avg_L1_norm_grad         0.000322  w[0]   -0.405 bias    2.151\n",
      "iter 5741/30000  loss         0.228912  avg_L1_norm_grad         0.000322  w[0]   -0.405 bias    2.152\n",
      "iter 5760/30000  loss         0.228816  avg_L1_norm_grad         0.000321  w[0]   -0.405 bias    2.156\n",
      "iter 5761/30000  loss         0.228811  avg_L1_norm_grad         0.000321  w[0]   -0.405 bias    2.156\n",
      "iter 5780/30000  loss         0.228715  avg_L1_norm_grad         0.000320  w[0]   -0.406 bias    2.160\n",
      "iter 5781/30000  loss         0.228710  avg_L1_norm_grad         0.000320  w[0]   -0.406 bias    2.160\n",
      "iter 5800/30000  loss         0.228615  avg_L1_norm_grad         0.000320  w[0]   -0.406 bias    2.164\n",
      "iter 5801/30000  loss         0.228610  avg_L1_norm_grad         0.000320  w[0]   -0.406 bias    2.165\n",
      "iter 5820/30000  loss         0.228516  avg_L1_norm_grad         0.000319  w[0]   -0.406 bias    2.169\n",
      "iter 5821/30000  loss         0.228511  avg_L1_norm_grad         0.000319  w[0]   -0.406 bias    2.169\n",
      "iter 5840/30000  loss         0.228417  avg_L1_norm_grad         0.000318  w[0]   -0.406 bias    2.173\n",
      "iter 5841/30000  loss         0.228412  avg_L1_norm_grad         0.000318  w[0]   -0.406 bias    2.173\n",
      "iter 5860/30000  loss         0.228319  avg_L1_norm_grad         0.000317  w[0]   -0.406 bias    2.177\n",
      "iter 5861/30000  loss         0.228314  avg_L1_norm_grad         0.000317  w[0]   -0.406 bias    2.177\n",
      "iter 5880/30000  loss         0.228221  avg_L1_norm_grad         0.000316  w[0]   -0.407 bias    2.182\n",
      "iter 5881/30000  loss         0.228216  avg_L1_norm_grad         0.000316  w[0]   -0.407 bias    2.182\n",
      "iter 5900/30000  loss         0.228123  avg_L1_norm_grad         0.000316  w[0]   -0.407 bias    2.186\n",
      "iter 5901/30000  loss         0.228118  avg_L1_norm_grad         0.000316  w[0]   -0.407 bias    2.186\n",
      "iter 5920/30000  loss         0.228026  avg_L1_norm_grad         0.000315  w[0]   -0.407 bias    2.190\n",
      "iter 5921/30000  loss         0.228021  avg_L1_norm_grad         0.000315  w[0]   -0.407 bias    2.190\n",
      "iter 5940/30000  loss         0.227930  avg_L1_norm_grad         0.000314  w[0]   -0.407 bias    2.194\n",
      "iter 5941/30000  loss         0.227925  avg_L1_norm_grad         0.000314  w[0]   -0.407 bias    2.195\n",
      "iter 5960/30000  loss         0.227834  avg_L1_norm_grad         0.000313  w[0]   -0.408 bias    2.199\n",
      "iter 5961/30000  loss         0.227829  avg_L1_norm_grad         0.000313  w[0]   -0.408 bias    2.199\n",
      "iter 5980/30000  loss         0.227738  avg_L1_norm_grad         0.000313  w[0]   -0.408 bias    2.203\n",
      "iter 5981/30000  loss         0.227733  avg_L1_norm_grad         0.000312  w[0]   -0.408 bias    2.203\n",
      "iter 6000/30000  loss         0.227643  avg_L1_norm_grad         0.000312  w[0]   -0.408 bias    2.207\n",
      "iter 6001/30000  loss         0.227638  avg_L1_norm_grad         0.000312  w[0]   -0.408 bias    2.207\n",
      "iter 6020/30000  loss         0.227548  avg_L1_norm_grad         0.000311  w[0]   -0.408 bias    2.211\n",
      "iter 6021/30000  loss         0.227543  avg_L1_norm_grad         0.000311  w[0]   -0.408 bias    2.211\n",
      "iter 6040/30000  loss         0.227454  avg_L1_norm_grad         0.000310  w[0]   -0.408 bias    2.215\n",
      "iter 6041/30000  loss         0.227449  avg_L1_norm_grad         0.000310  w[0]   -0.408 bias    2.216\n",
      "iter 6060/30000  loss         0.227360  avg_L1_norm_grad         0.000309  w[0]   -0.409 bias    2.220\n",
      "iter 6061/30000  loss         0.227356  avg_L1_norm_grad         0.000309  w[0]   -0.409 bias    2.220\n",
      "iter 6080/30000  loss         0.227267  avg_L1_norm_grad         0.000309  w[0]   -0.409 bias    2.224\n",
      "iter 6081/30000  loss         0.227262  avg_L1_norm_grad         0.000309  w[0]   -0.409 bias    2.224\n",
      "iter 6100/30000  loss         0.227174  avg_L1_norm_grad         0.000308  w[0]   -0.409 bias    2.228\n",
      "iter 6101/30000  loss         0.227169  avg_L1_norm_grad         0.000308  w[0]   -0.409 bias    2.228\n",
      "iter 6120/30000  loss         0.227082  avg_L1_norm_grad         0.000307  w[0]   -0.409 bias    2.232\n",
      "iter 6121/30000  loss         0.227077  avg_L1_norm_grad         0.000307  w[0]   -0.409 bias    2.232\n",
      "iter 6140/30000  loss         0.226990  avg_L1_norm_grad         0.000307  w[0]   -0.409 bias    2.236\n",
      "iter 6141/30000  loss         0.226985  avg_L1_norm_grad         0.000306  w[0]   -0.409 bias    2.236\n",
      "iter 6160/30000  loss         0.226898  avg_L1_norm_grad         0.000306  w[0]   -0.410 bias    2.240\n",
      "iter 6161/30000  loss         0.226894  avg_L1_norm_grad         0.000306  w[0]   -0.410 bias    2.241\n",
      "iter 6180/30000  loss         0.226807  avg_L1_norm_grad         0.000305  w[0]   -0.410 bias    2.245\n",
      "iter 6181/30000  loss         0.226802  avg_L1_norm_grad         0.000305  w[0]   -0.410 bias    2.245\n",
      "iter 6200/30000  loss         0.226716  avg_L1_norm_grad         0.000304  w[0]   -0.410 bias    2.249\n",
      "iter 6201/30000  loss         0.226712  avg_L1_norm_grad         0.000304  w[0]   -0.410 bias    2.249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6220/30000  loss         0.226626  avg_L1_norm_grad         0.000304  w[0]   -0.410 bias    2.253\n",
      "iter 6221/30000  loss         0.226622  avg_L1_norm_grad         0.000304  w[0]   -0.410 bias    2.253\n",
      "iter 6240/30000  loss         0.226536  avg_L1_norm_grad         0.000303  w[0]   -0.410 bias    2.257\n",
      "iter 6241/30000  loss         0.226532  avg_L1_norm_grad         0.000303  w[0]   -0.410 bias    2.257\n",
      "iter 6260/30000  loss         0.226447  avg_L1_norm_grad         0.000302  w[0]   -0.411 bias    2.261\n",
      "iter 6261/30000  loss         0.226442  avg_L1_norm_grad         0.000302  w[0]   -0.411 bias    2.261\n",
      "iter 6280/30000  loss         0.226358  avg_L1_norm_grad         0.000301  w[0]   -0.411 bias    2.265\n",
      "iter 6281/30000  loss         0.226353  avg_L1_norm_grad         0.000301  w[0]   -0.411 bias    2.265\n",
      "iter 6300/30000  loss         0.226269  avg_L1_norm_grad         0.000301  w[0]   -0.411 bias    2.269\n",
      "iter 6301/30000  loss         0.226265  avg_L1_norm_grad         0.000301  w[0]   -0.411 bias    2.269\n",
      "iter 6320/30000  loss         0.226181  avg_L1_norm_grad         0.000300  w[0]   -0.411 bias    2.273\n",
      "iter 6321/30000  loss         0.226177  avg_L1_norm_grad         0.000300  w[0]   -0.411 bias    2.274\n",
      "iter 6340/30000  loss         0.226093  avg_L1_norm_grad         0.000299  w[0]   -0.411 bias    2.277\n",
      "iter 6341/30000  loss         0.226089  avg_L1_norm_grad         0.000299  w[0]   -0.411 bias    2.278\n",
      "iter 6360/30000  loss         0.226006  avg_L1_norm_grad         0.000299  w[0]   -0.412 bias    2.281\n",
      "iter 6361/30000  loss         0.226002  avg_L1_norm_grad         0.000299  w[0]   -0.412 bias    2.282\n",
      "iter 6380/30000  loss         0.225919  avg_L1_norm_grad         0.000298  w[0]   -0.412 bias    2.286\n",
      "iter 6381/30000  loss         0.225915  avg_L1_norm_grad         0.000298  w[0]   -0.412 bias    2.286\n",
      "iter 6400/30000  loss         0.225832  avg_L1_norm_grad         0.000297  w[0]   -0.412 bias    2.290\n",
      "iter 6401/30000  loss         0.225828  avg_L1_norm_grad         0.000297  w[0]   -0.412 bias    2.290\n",
      "iter 6420/30000  loss         0.225746  avg_L1_norm_grad         0.000297  w[0]   -0.412 bias    2.294\n",
      "iter 6421/30000  loss         0.225742  avg_L1_norm_grad         0.000297  w[0]   -0.412 bias    2.294\n",
      "iter 6440/30000  loss         0.225660  avg_L1_norm_grad         0.000296  w[0]   -0.412 bias    2.298\n",
      "iter 6441/30000  loss         0.225656  avg_L1_norm_grad         0.000296  w[0]   -0.412 bias    2.298\n",
      "iter 6460/30000  loss         0.225575  avg_L1_norm_grad         0.000295  w[0]   -0.413 bias    2.302\n",
      "iter 6461/30000  loss         0.225571  avg_L1_norm_grad         0.000295  w[0]   -0.413 bias    2.302\n",
      "iter 6480/30000  loss         0.225490  avg_L1_norm_grad         0.000295  w[0]   -0.413 bias    2.306\n",
      "iter 6481/30000  loss         0.225486  avg_L1_norm_grad         0.000294  w[0]   -0.413 bias    2.306\n",
      "iter 6500/30000  loss         0.225405  avg_L1_norm_grad         0.000294  w[0]   -0.413 bias    2.310\n",
      "iter 6501/30000  loss         0.225401  avg_L1_norm_grad         0.000294  w[0]   -0.413 bias    2.310\n",
      "iter 6520/30000  loss         0.225321  avg_L1_norm_grad         0.000293  w[0]   -0.413 bias    2.314\n",
      "iter 6521/30000  loss         0.225317  avg_L1_norm_grad         0.000293  w[0]   -0.413 bias    2.314\n",
      "iter 6540/30000  loss         0.225237  avg_L1_norm_grad         0.000293  w[0]   -0.413 bias    2.318\n",
      "iter 6541/30000  loss         0.225233  avg_L1_norm_grad         0.000292  w[0]   -0.413 bias    2.318\n",
      "iter 6560/30000  loss         0.225154  avg_L1_norm_grad         0.000292  w[0]   -0.413 bias    2.322\n",
      "iter 6561/30000  loss         0.225150  avg_L1_norm_grad         0.000292  w[0]   -0.413 bias    2.322\n",
      "iter 6580/30000  loss         0.225071  avg_L1_norm_grad         0.000291  w[0]   -0.414 bias    2.326\n",
      "iter 6581/30000  loss         0.225067  avg_L1_norm_grad         0.000291  w[0]   -0.414 bias    2.326\n",
      "iter 6600/30000  loss         0.224988  avg_L1_norm_grad         0.000291  w[0]   -0.414 bias    2.330\n",
      "iter 6601/30000  loss         0.224984  avg_L1_norm_grad         0.000291  w[0]   -0.414 bias    2.330\n",
      "iter 6620/30000  loss         0.224906  avg_L1_norm_grad         0.000290  w[0]   -0.414 bias    2.334\n",
      "iter 6621/30000  loss         0.224902  avg_L1_norm_grad         0.000290  w[0]   -0.414 bias    2.334\n",
      "iter 6640/30000  loss         0.224824  avg_L1_norm_grad         0.000289  w[0]   -0.414 bias    2.338\n",
      "iter 6641/30000  loss         0.224820  avg_L1_norm_grad         0.000289  w[0]   -0.414 bias    2.338\n",
      "iter 6660/30000  loss         0.224742  avg_L1_norm_grad         0.000289  w[0]   -0.414 bias    2.342\n",
      "iter 6661/30000  loss         0.224738  avg_L1_norm_grad         0.000289  w[0]   -0.414 bias    2.342\n",
      "iter 6680/30000  loss         0.224661  avg_L1_norm_grad         0.000288  w[0]   -0.415 bias    2.346\n",
      "iter 6681/30000  loss         0.224657  avg_L1_norm_grad         0.000288  w[0]   -0.415 bias    2.346\n",
      "iter 6700/30000  loss         0.224580  avg_L1_norm_grad         0.000287  w[0]   -0.415 bias    2.350\n",
      "iter 6701/30000  loss         0.224576  avg_L1_norm_grad         0.000287  w[0]   -0.415 bias    2.350\n",
      "iter 6720/30000  loss         0.224499  avg_L1_norm_grad         0.000287  w[0]   -0.415 bias    2.353\n",
      "iter 6721/30000  loss         0.224495  avg_L1_norm_grad         0.000287  w[0]   -0.415 bias    2.354\n",
      "iter 6740/30000  loss         0.224419  avg_L1_norm_grad         0.000286  w[0]   -0.415 bias    2.357\n",
      "iter 6741/30000  loss         0.224415  avg_L1_norm_grad         0.000286  w[0]   -0.415 bias    2.358\n",
      "iter 6760/30000  loss         0.224339  avg_L1_norm_grad         0.000285  w[0]   -0.415 bias    2.361\n",
      "iter 6761/30000  loss         0.224335  avg_L1_norm_grad         0.000285  w[0]   -0.415 bias    2.362\n",
      "iter 6780/30000  loss         0.224260  avg_L1_norm_grad         0.000285  w[0]   -0.415 bias    2.365\n",
      "iter 6781/30000  loss         0.224256  avg_L1_norm_grad         0.000285  w[0]   -0.415 bias    2.365\n",
      "iter 6800/30000  loss         0.224180  avg_L1_norm_grad         0.000284  w[0]   -0.416 bias    2.369\n",
      "iter 6801/30000  loss         0.224176  avg_L1_norm_grad         0.000284  w[0]   -0.416 bias    2.369\n",
      "iter 6820/30000  loss         0.224102  avg_L1_norm_grad         0.000283  w[0]   -0.416 bias    2.373\n",
      "iter 6821/30000  loss         0.224098  avg_L1_norm_grad         0.000283  w[0]   -0.416 bias    2.373\n",
      "iter 6840/30000  loss         0.224023  avg_L1_norm_grad         0.000283  w[0]   -0.416 bias    2.377\n",
      "iter 6841/30000  loss         0.224019  avg_L1_norm_grad         0.000283  w[0]   -0.416 bias    2.377\n",
      "iter 6860/30000  loss         0.223945  avg_L1_norm_grad         0.000282  w[0]   -0.416 bias    2.381\n",
      "iter 6861/30000  loss         0.223941  avg_L1_norm_grad         0.000282  w[0]   -0.416 bias    2.381\n",
      "iter 6880/30000  loss         0.223867  avg_L1_norm_grad         0.000282  w[0]   -0.416 bias    2.385\n",
      "iter 6881/30000  loss         0.223863  avg_L1_norm_grad         0.000282  w[0]   -0.416 bias    2.385\n",
      "iter 6900/30000  loss         0.223790  avg_L1_norm_grad         0.000281  w[0]   -0.416 bias    2.389\n",
      "iter 6901/30000  loss         0.223786  avg_L1_norm_grad         0.000281  w[0]   -0.416 bias    2.389\n",
      "iter 6920/30000  loss         0.223712  avg_L1_norm_grad         0.000280  w[0]   -0.417 bias    2.392\n",
      "iter 6921/30000  loss         0.223709  avg_L1_norm_grad         0.000280  w[0]   -0.417 bias    2.393\n",
      "iter 6940/30000  loss         0.223636  avg_L1_norm_grad         0.000280  w[0]   -0.417 bias    2.396\n",
      "iter 6941/30000  loss         0.223632  avg_L1_norm_grad         0.000280  w[0]   -0.417 bias    2.396\n",
      "iter 6960/30000  loss         0.223559  avg_L1_norm_grad         0.000279  w[0]   -0.417 bias    2.400\n",
      "iter 6961/30000  loss         0.223555  avg_L1_norm_grad         0.000279  w[0]   -0.417 bias    2.400\n",
      "iter 6980/30000  loss         0.223483  avg_L1_norm_grad         0.000279  w[0]   -0.417 bias    2.404\n",
      "iter 6981/30000  loss         0.223479  avg_L1_norm_grad         0.000279  w[0]   -0.417 bias    2.404\n",
      "iter 7000/30000  loss         0.223407  avg_L1_norm_grad         0.000278  w[0]   -0.417 bias    2.408\n",
      "iter 7001/30000  loss         0.223403  avg_L1_norm_grad         0.000278  w[0]   -0.417 bias    2.408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7020/30000  loss         0.223331  avg_L1_norm_grad         0.000277  w[0]   -0.417 bias    2.412\n",
      "iter 7021/30000  loss         0.223328  avg_L1_norm_grad         0.000277  w[0]   -0.417 bias    2.412\n",
      "iter 7040/30000  loss         0.223256  avg_L1_norm_grad         0.000277  w[0]   -0.418 bias    2.415\n",
      "iter 7041/30000  loss         0.223253  avg_L1_norm_grad         0.000277  w[0]   -0.418 bias    2.416\n",
      "iter 7060/30000  loss         0.223181  avg_L1_norm_grad         0.000276  w[0]   -0.418 bias    2.419\n",
      "iter 7061/30000  loss         0.223178  avg_L1_norm_grad         0.000276  w[0]   -0.418 bias    2.419\n",
      "iter 7080/30000  loss         0.223107  avg_L1_norm_grad         0.000276  w[0]   -0.418 bias    2.423\n",
      "iter 7081/30000  loss         0.223103  avg_L1_norm_grad         0.000276  w[0]   -0.418 bias    2.423\n",
      "iter 7100/30000  loss         0.223032  avg_L1_norm_grad         0.000275  w[0]   -0.418 bias    2.427\n",
      "iter 7101/30000  loss         0.223029  avg_L1_norm_grad         0.000275  w[0]   -0.418 bias    2.427\n",
      "iter 7120/30000  loss         0.222958  avg_L1_norm_grad         0.000275  w[0]   -0.418 bias    2.431\n",
      "iter 7121/30000  loss         0.222955  avg_L1_norm_grad         0.000274  w[0]   -0.418 bias    2.431\n",
      "iter 7140/30000  loss         0.222885  avg_L1_norm_grad         0.000274  w[0]   -0.418 bias    2.434\n",
      "iter 7141/30000  loss         0.222881  avg_L1_norm_grad         0.000274  w[0]   -0.418 bias    2.435\n",
      "iter 7160/30000  loss         0.222811  avg_L1_norm_grad         0.000273  w[0]   -0.419 bias    2.438\n",
      "iter 7161/30000  loss         0.222808  avg_L1_norm_grad         0.000273  w[0]   -0.419 bias    2.438\n",
      "iter 7180/30000  loss         0.222738  avg_L1_norm_grad         0.000273  w[0]   -0.419 bias    2.442\n",
      "iter 7181/30000  loss         0.222735  avg_L1_norm_grad         0.000273  w[0]   -0.419 bias    2.442\n",
      "iter 7200/30000  loss         0.222666  avg_L1_norm_grad         0.000272  w[0]   -0.419 bias    2.446\n",
      "iter 7201/30000  loss         0.222662  avg_L1_norm_grad         0.000272  w[0]   -0.419 bias    2.446\n",
      "iter 7220/30000  loss         0.222593  avg_L1_norm_grad         0.000272  w[0]   -0.419 bias    2.450\n",
      "iter 7221/30000  loss         0.222590  avg_L1_norm_grad         0.000272  w[0]   -0.419 bias    2.450\n",
      "iter 7240/30000  loss         0.222521  avg_L1_norm_grad         0.000271  w[0]   -0.419 bias    2.453\n",
      "iter 7241/30000  loss         0.222517  avg_L1_norm_grad         0.000271  w[0]   -0.419 bias    2.454\n",
      "iter 7260/30000  loss         0.222449  avg_L1_norm_grad         0.000271  w[0]   -0.419 bias    2.457\n",
      "iter 7261/30000  loss         0.222446  avg_L1_norm_grad         0.000270  w[0]   -0.419 bias    2.457\n",
      "iter 7280/30000  loss         0.222378  avg_L1_norm_grad         0.000270  w[0]   -0.420 bias    2.461\n",
      "iter 7281/30000  loss         0.222374  avg_L1_norm_grad         0.000270  w[0]   -0.420 bias    2.461\n",
      "iter 7300/30000  loss         0.222306  avg_L1_norm_grad         0.000269  w[0]   -0.420 bias    2.465\n",
      "iter 7301/30000  loss         0.222303  avg_L1_norm_grad         0.000269  w[0]   -0.420 bias    2.465\n",
      "iter 7320/30000  loss         0.222235  avg_L1_norm_grad         0.000269  w[0]   -0.420 bias    2.468\n",
      "iter 7321/30000  loss         0.222232  avg_L1_norm_grad         0.000269  w[0]   -0.420 bias    2.468\n",
      "iter 7340/30000  loss         0.222165  avg_L1_norm_grad         0.000268  w[0]   -0.420 bias    2.472\n",
      "iter 7341/30000  loss         0.222161  avg_L1_norm_grad         0.000268  w[0]   -0.420 bias    2.472\n",
      "iter 7360/30000  loss         0.222094  avg_L1_norm_grad         0.000268  w[0]   -0.420 bias    2.476\n",
      "iter 7361/30000  loss         0.222091  avg_L1_norm_grad         0.000268  w[0]   -0.420 bias    2.476\n",
      "iter 7380/30000  loss         0.222024  avg_L1_norm_grad         0.000267  w[0]   -0.420 bias    2.479\n",
      "iter 7381/30000  loss         0.222020  avg_L1_norm_grad         0.000267  w[0]   -0.420 bias    2.480\n",
      "iter 7400/30000  loss         0.221954  avg_L1_norm_grad         0.000267  w[0]   -0.420 bias    2.483\n",
      "iter 7401/30000  loss         0.221951  avg_L1_norm_grad         0.000267  w[0]   -0.420 bias    2.483\n",
      "iter 7420/30000  loss         0.221885  avg_L1_norm_grad         0.000266  w[0]   -0.421 bias    2.487\n",
      "iter 7421/30000  loss         0.221881  avg_L1_norm_grad         0.000266  w[0]   -0.421 bias    2.487\n",
      "iter 7440/30000  loss         0.221815  avg_L1_norm_grad         0.000266  w[0]   -0.421 bias    2.491\n",
      "iter 7441/30000  loss         0.221812  avg_L1_norm_grad         0.000266  w[0]   -0.421 bias    2.491\n",
      "iter 7460/30000  loss         0.221746  avg_L1_norm_grad         0.000265  w[0]   -0.421 bias    2.494\n",
      "iter 7461/30000  loss         0.221743  avg_L1_norm_grad         0.000265  w[0]   -0.421 bias    2.494\n",
      "iter 7480/30000  loss         0.221677  avg_L1_norm_grad         0.000265  w[0]   -0.421 bias    2.498\n",
      "iter 7481/30000  loss         0.221674  avg_L1_norm_grad         0.000265  w[0]   -0.421 bias    2.498\n",
      "iter 7500/30000  loss         0.221609  avg_L1_norm_grad         0.000264  w[0]   -0.421 bias    2.502\n",
      "iter 7501/30000  loss         0.221606  avg_L1_norm_grad         0.000264  w[0]   -0.421 bias    2.502\n",
      "iter 7520/30000  loss         0.221541  avg_L1_norm_grad         0.000263  w[0]   -0.421 bias    2.505\n",
      "iter 7521/30000  loss         0.221537  avg_L1_norm_grad         0.000263  w[0]   -0.421 bias    2.505\n",
      "iter 7540/30000  loss         0.221473  avg_L1_norm_grad         0.000263  w[0]   -0.422 bias    2.509\n",
      "iter 7541/30000  loss         0.221469  avg_L1_norm_grad         0.000263  w[0]   -0.422 bias    2.509\n",
      "iter 7560/30000  loss         0.221405  avg_L1_norm_grad         0.000262  w[0]   -0.422 bias    2.513\n",
      "iter 7561/30000  loss         0.221402  avg_L1_norm_grad         0.000262  w[0]   -0.422 bias    2.513\n",
      "iter 7580/30000  loss         0.221338  avg_L1_norm_grad         0.000262  w[0]   -0.422 bias    2.516\n",
      "iter 7581/30000  loss         0.221334  avg_L1_norm_grad         0.000262  w[0]   -0.422 bias    2.516\n",
      "iter 7600/30000  loss         0.221271  avg_L1_norm_grad         0.000261  w[0]   -0.422 bias    2.520\n",
      "iter 7601/30000  loss         0.221267  avg_L1_norm_grad         0.000261  w[0]   -0.422 bias    2.520\n",
      "iter 7620/30000  loss         0.221204  avg_L1_norm_grad         0.000261  w[0]   -0.422 bias    2.524\n",
      "iter 7621/30000  loss         0.221200  avg_L1_norm_grad         0.000261  w[0]   -0.422 bias    2.524\n",
      "iter 7640/30000  loss         0.221137  avg_L1_norm_grad         0.000260  w[0]   -0.422 bias    2.527\n",
      "iter 7641/30000  loss         0.221134  avg_L1_norm_grad         0.000260  w[0]   -0.422 bias    2.527\n",
      "iter 7660/30000  loss         0.221071  avg_L1_norm_grad         0.000260  w[0]   -0.422 bias    2.531\n",
      "iter 7661/30000  loss         0.221067  avg_L1_norm_grad         0.000260  w[0]   -0.422 bias    2.531\n",
      "iter 7680/30000  loss         0.221005  avg_L1_norm_grad         0.000259  w[0]   -0.423 bias    2.534\n",
      "iter 7681/30000  loss         0.221001  avg_L1_norm_grad         0.000259  w[0]   -0.423 bias    2.535\n",
      "iter 7700/30000  loss         0.220939  avg_L1_norm_grad         0.000259  w[0]   -0.423 bias    2.538\n",
      "iter 7701/30000  loss         0.220936  avg_L1_norm_grad         0.000259  w[0]   -0.423 bias    2.538\n",
      "iter 7720/30000  loss         0.220873  avg_L1_norm_grad         0.000258  w[0]   -0.423 bias    2.542\n",
      "iter 7721/30000  loss         0.220870  avg_L1_norm_grad         0.000258  w[0]   -0.423 bias    2.542\n",
      "iter 7740/30000  loss         0.220808  avg_L1_norm_grad         0.000258  w[0]   -0.423 bias    2.545\n",
      "iter 7741/30000  loss         0.220805  avg_L1_norm_grad         0.000258  w[0]   -0.423 bias    2.545\n",
      "iter 7760/30000  loss         0.220743  avg_L1_norm_grad         0.000257  w[0]   -0.423 bias    2.549\n",
      "iter 7761/30000  loss         0.220740  avg_L1_norm_grad         0.000257  w[0]   -0.423 bias    2.549\n",
      "iter 7780/30000  loss         0.220678  avg_L1_norm_grad         0.000257  w[0]   -0.423 bias    2.552\n",
      "iter 7781/30000  loss         0.220675  avg_L1_norm_grad         0.000257  w[0]   -0.423 bias    2.553\n",
      "iter 7800/30000  loss         0.220613  avg_L1_norm_grad         0.000256  w[0]   -0.423 bias    2.556\n",
      "iter 7801/30000  loss         0.220610  avg_L1_norm_grad         0.000256  w[0]   -0.423 bias    2.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7820/30000  loss         0.220549  avg_L1_norm_grad         0.000256  w[0]   -0.424 bias    2.560\n",
      "iter 7821/30000  loss         0.220546  avg_L1_norm_grad         0.000256  w[0]   -0.424 bias    2.560\n",
      "iter 7840/30000  loss         0.220485  avg_L1_norm_grad         0.000255  w[0]   -0.424 bias    2.563\n",
      "iter 7841/30000  loss         0.220482  avg_L1_norm_grad         0.000255  w[0]   -0.424 bias    2.563\n",
      "iter 7860/30000  loss         0.220421  avg_L1_norm_grad         0.000255  w[0]   -0.424 bias    2.567\n",
      "iter 7861/30000  loss         0.220418  avg_L1_norm_grad         0.000255  w[0]   -0.424 bias    2.567\n",
      "iter 7880/30000  loss         0.220358  avg_L1_norm_grad         0.000254  w[0]   -0.424 bias    2.570\n",
      "iter 7881/30000  loss         0.220354  avg_L1_norm_grad         0.000254  w[0]   -0.424 bias    2.570\n",
      "iter 7900/30000  loss         0.220294  avg_L1_norm_grad         0.000254  w[0]   -0.424 bias    2.574\n",
      "iter 7901/30000  loss         0.220291  avg_L1_norm_grad         0.000254  w[0]   -0.424 bias    2.574\n",
      "iter 7920/30000  loss         0.220231  avg_L1_norm_grad         0.000253  w[0]   -0.424 bias    2.577\n",
      "iter 7921/30000  loss         0.220228  avg_L1_norm_grad         0.000253  w[0]   -0.424 bias    2.578\n",
      "iter 7940/30000  loss         0.220168  avg_L1_norm_grad         0.000253  w[0]   -0.424 bias    2.581\n",
      "iter 7941/30000  loss         0.220165  avg_L1_norm_grad         0.000253  w[0]   -0.424 bias    2.581\n",
      "iter 7960/30000  loss         0.220106  avg_L1_norm_grad         0.000252  w[0]   -0.425 bias    2.584\n",
      "iter 7961/30000  loss         0.220102  avg_L1_norm_grad         0.000252  w[0]   -0.425 bias    2.585\n",
      "iter 7980/30000  loss         0.220043  avg_L1_norm_grad         0.000252  w[0]   -0.425 bias    2.588\n",
      "iter 7981/30000  loss         0.220040  avg_L1_norm_grad         0.000252  w[0]   -0.425 bias    2.588\n",
      "iter 8000/30000  loss         0.219981  avg_L1_norm_grad         0.000251  w[0]   -0.425 bias    2.591\n",
      "iter 8001/30000  loss         0.219978  avg_L1_norm_grad         0.000251  w[0]   -0.425 bias    2.592\n",
      "iter 8020/30000  loss         0.219919  avg_L1_norm_grad         0.000251  w[0]   -0.425 bias    2.595\n",
      "iter 8021/30000  loss         0.219916  avg_L1_norm_grad         0.000251  w[0]   -0.425 bias    2.595\n",
      "iter 8040/30000  loss         0.219857  avg_L1_norm_grad         0.000250  w[0]   -0.425 bias    2.599\n",
      "iter 8041/30000  loss         0.219854  avg_L1_norm_grad         0.000250  w[0]   -0.425 bias    2.599\n",
      "iter 8060/30000  loss         0.219796  avg_L1_norm_grad         0.000250  w[0]   -0.425 bias    2.602\n",
      "iter 8061/30000  loss         0.219793  avg_L1_norm_grad         0.000250  w[0]   -0.425 bias    2.602\n",
      "iter 8080/30000  loss         0.219735  avg_L1_norm_grad         0.000249  w[0]   -0.425 bias    2.606\n",
      "iter 8081/30000  loss         0.219732  avg_L1_norm_grad         0.000249  w[0]   -0.425 bias    2.606\n",
      "iter 8100/30000  loss         0.219674  avg_L1_norm_grad         0.000249  w[0]   -0.425 bias    2.609\n",
      "iter 8101/30000  loss         0.219671  avg_L1_norm_grad         0.000249  w[0]   -0.425 bias    2.609\n",
      "iter 8120/30000  loss         0.219613  avg_L1_norm_grad         0.000249  w[0]   -0.426 bias    2.613\n",
      "iter 8121/30000  loss         0.219610  avg_L1_norm_grad         0.000249  w[0]   -0.426 bias    2.613\n",
      "iter 8140/30000  loss         0.219552  avg_L1_norm_grad         0.000248  w[0]   -0.426 bias    2.616\n",
      "iter 8141/30000  loss         0.219549  avg_L1_norm_grad         0.000248  w[0]   -0.426 bias    2.616\n",
      "iter 8160/30000  loss         0.219492  avg_L1_norm_grad         0.000248  w[0]   -0.426 bias    2.620\n",
      "iter 8161/30000  loss         0.219489  avg_L1_norm_grad         0.000248  w[0]   -0.426 bias    2.620\n",
      "iter 8180/30000  loss         0.219432  avg_L1_norm_grad         0.000247  w[0]   -0.426 bias    2.623\n",
      "iter 8181/30000  loss         0.219429  avg_L1_norm_grad         0.000247  w[0]   -0.426 bias    2.623\n",
      "iter 8200/30000  loss         0.219372  avg_L1_norm_grad         0.000247  w[0]   -0.426 bias    2.626\n",
      "iter 8201/30000  loss         0.219369  avg_L1_norm_grad         0.000247  w[0]   -0.426 bias    2.627\n",
      "iter 8220/30000  loss         0.219312  avg_L1_norm_grad         0.000246  w[0]   -0.426 bias    2.630\n",
      "iter 8221/30000  loss         0.219309  avg_L1_norm_grad         0.000246  w[0]   -0.426 bias    2.630\n",
      "iter 8240/30000  loss         0.219253  avg_L1_norm_grad         0.000246  w[0]   -0.426 bias    2.633\n",
      "iter 8241/30000  loss         0.219250  avg_L1_norm_grad         0.000246  w[0]   -0.426 bias    2.634\n",
      "iter 8260/30000  loss         0.219193  avg_L1_norm_grad         0.000245  w[0]   -0.427 bias    2.637\n",
      "iter 8261/30000  loss         0.219191  avg_L1_norm_grad         0.000245  w[0]   -0.427 bias    2.637\n",
      "iter 8280/30000  loss         0.219134  avg_L1_norm_grad         0.000245  w[0]   -0.427 bias    2.640\n",
      "iter 8281/30000  loss         0.219131  avg_L1_norm_grad         0.000245  w[0]   -0.427 bias    2.640\n",
      "iter 8300/30000  loss         0.219076  avg_L1_norm_grad         0.000244  w[0]   -0.427 bias    2.644\n",
      "iter 8301/30000  loss         0.219073  avg_L1_norm_grad         0.000244  w[0]   -0.427 bias    2.644\n",
      "iter 8320/30000  loss         0.219017  avg_L1_norm_grad         0.000244  w[0]   -0.427 bias    2.647\n",
      "iter 8321/30000  loss         0.219014  avg_L1_norm_grad         0.000244  w[0]   -0.427 bias    2.647\n",
      "iter 8340/30000  loss         0.218959  avg_L1_norm_grad         0.000244  w[0]   -0.427 bias    2.651\n",
      "iter 8341/30000  loss         0.218956  avg_L1_norm_grad         0.000243  w[0]   -0.427 bias    2.651\n",
      "iter 8360/30000  loss         0.218900  avg_L1_norm_grad         0.000243  w[0]   -0.427 bias    2.654\n",
      "iter 8361/30000  loss         0.218898  avg_L1_norm_grad         0.000243  w[0]   -0.427 bias    2.654\n",
      "iter 8380/30000  loss         0.218842  avg_L1_norm_grad         0.000243  w[0]   -0.427 bias    2.657\n",
      "iter 8381/30000  loss         0.218840  avg_L1_norm_grad         0.000243  w[0]   -0.427 bias    2.658\n",
      "iter 8400/30000  loss         0.218785  avg_L1_norm_grad         0.000242  w[0]   -0.427 bias    2.661\n",
      "iter 8401/30000  loss         0.218782  avg_L1_norm_grad         0.000242  w[0]   -0.427 bias    2.661\n",
      "iter 8420/30000  loss         0.218727  avg_L1_norm_grad         0.000242  w[0]   -0.428 bias    2.664\n",
      "iter 8421/30000  loss         0.218724  avg_L1_norm_grad         0.000242  w[0]   -0.428 bias    2.664\n",
      "iter 8440/30000  loss         0.218670  avg_L1_norm_grad         0.000241  w[0]   -0.428 bias    2.668\n",
      "iter 8441/30000  loss         0.218667  avg_L1_norm_grad         0.000241  w[0]   -0.428 bias    2.668\n",
      "iter 8460/30000  loss         0.218613  avg_L1_norm_grad         0.000241  w[0]   -0.428 bias    2.671\n",
      "iter 8461/30000  loss         0.218610  avg_L1_norm_grad         0.000241  w[0]   -0.428 bias    2.671\n",
      "iter 8480/30000  loss         0.218556  avg_L1_norm_grad         0.000240  w[0]   -0.428 bias    2.674\n",
      "iter 8481/30000  loss         0.218553  avg_L1_norm_grad         0.000240  w[0]   -0.428 bias    2.675\n",
      "iter 8500/30000  loss         0.218499  avg_L1_norm_grad         0.000240  w[0]   -0.428 bias    2.678\n",
      "iter 8501/30000  loss         0.218496  avg_L1_norm_grad         0.000240  w[0]   -0.428 bias    2.678\n",
      "iter 8520/30000  loss         0.218442  avg_L1_norm_grad         0.000240  w[0]   -0.428 bias    2.681\n",
      "iter 8521/30000  loss         0.218440  avg_L1_norm_grad         0.000240  w[0]   -0.428 bias    2.681\n",
      "iter 8540/30000  loss         0.218386  avg_L1_norm_grad         0.000239  w[0]   -0.428 bias    2.685\n",
      "iter 8541/30000  loss         0.218383  avg_L1_norm_grad         0.000239  w[0]   -0.428 bias    2.685\n",
      "iter 8560/30000  loss         0.218330  avg_L1_norm_grad         0.000239  w[0]   -0.428 bias    2.688\n",
      "iter 8561/30000  loss         0.218327  avg_L1_norm_grad         0.000239  w[0]   -0.428 bias    2.688\n",
      "iter 8580/30000  loss         0.218274  avg_L1_norm_grad         0.000238  w[0]   -0.429 bias    2.691\n",
      "iter 8581/30000  loss         0.218271  avg_L1_norm_grad         0.000238  w[0]   -0.429 bias    2.692\n",
      "iter 8600/30000  loss         0.218218  avg_L1_norm_grad         0.000238  w[0]   -0.429 bias    2.695\n",
      "iter 8601/30000  loss         0.218216  avg_L1_norm_grad         0.000238  w[0]   -0.429 bias    2.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8620/30000  loss         0.218163  avg_L1_norm_grad         0.000237  w[0]   -0.429 bias    2.698\n",
      "iter 8621/30000  loss         0.218160  avg_L1_norm_grad         0.000237  w[0]   -0.429 bias    2.698\n",
      "iter 8640/30000  loss         0.218107  avg_L1_norm_grad         0.000237  w[0]   -0.429 bias    2.701\n",
      "iter 8641/30000  loss         0.218105  avg_L1_norm_grad         0.000237  w[0]   -0.429 bias    2.702\n",
      "iter 8660/30000  loss         0.218052  avg_L1_norm_grad         0.000237  w[0]   -0.429 bias    2.705\n",
      "iter 8661/30000  loss         0.218050  avg_L1_norm_grad         0.000237  w[0]   -0.429 bias    2.705\n",
      "iter 8680/30000  loss         0.217997  avg_L1_norm_grad         0.000236  w[0]   -0.429 bias    2.708\n",
      "iter 8681/30000  loss         0.217995  avg_L1_norm_grad         0.000236  w[0]   -0.429 bias    2.708\n",
      "iter 8700/30000  loss         0.217943  avg_L1_norm_grad         0.000236  w[0]   -0.429 bias    2.712\n",
      "iter 8701/30000  loss         0.217940  avg_L1_norm_grad         0.000236  w[0]   -0.429 bias    2.712\n",
      "iter 8720/30000  loss         0.217888  avg_L1_norm_grad         0.000235  w[0]   -0.429 bias    2.715\n",
      "iter 8721/30000  loss         0.217885  avg_L1_norm_grad         0.000235  w[0]   -0.429 bias    2.715\n",
      "iter 8740/30000  loss         0.217834  avg_L1_norm_grad         0.000235  w[0]   -0.430 bias    2.718\n",
      "iter 8741/30000  loss         0.217831  avg_L1_norm_grad         0.000235  w[0]   -0.430 bias    2.718\n",
      "iter 8760/30000  loss         0.217779  avg_L1_norm_grad         0.000235  w[0]   -0.430 bias    2.721\n",
      "iter 8761/30000  loss         0.217777  avg_L1_norm_grad         0.000234  w[0]   -0.430 bias    2.722\n",
      "iter 8780/30000  loss         0.217725  avg_L1_norm_grad         0.000234  w[0]   -0.430 bias    2.725\n",
      "iter 8781/30000  loss         0.217723  avg_L1_norm_grad         0.000234  w[0]   -0.430 bias    2.725\n",
      "iter 8800/30000  loss         0.217672  avg_L1_norm_grad         0.000234  w[0]   -0.430 bias    2.728\n",
      "iter 8801/30000  loss         0.217669  avg_L1_norm_grad         0.000234  w[0]   -0.430 bias    2.728\n",
      "iter 8820/30000  loss         0.217618  avg_L1_norm_grad         0.000233  w[0]   -0.430 bias    2.731\n",
      "iter 8821/30000  loss         0.217615  avg_L1_norm_grad         0.000233  w[0]   -0.430 bias    2.732\n",
      "iter 8840/30000  loss         0.217564  avg_L1_norm_grad         0.000233  w[0]   -0.430 bias    2.735\n",
      "iter 8841/30000  loss         0.217562  avg_L1_norm_grad         0.000233  w[0]   -0.430 bias    2.735\n",
      "iter 8860/30000  loss         0.217511  avg_L1_norm_grad         0.000232  w[0]   -0.430 bias    2.738\n",
      "iter 8861/30000  loss         0.217509  avg_L1_norm_grad         0.000232  w[0]   -0.430 bias    2.738\n",
      "iter 8880/30000  loss         0.217458  avg_L1_norm_grad         0.000232  w[0]   -0.430 bias    2.741\n",
      "iter 8881/30000  loss         0.217455  avg_L1_norm_grad         0.000232  w[0]   -0.430 bias    2.742\n",
      "iter 8900/30000  loss         0.217405  avg_L1_norm_grad         0.000232  w[0]   -0.430 bias    2.745\n",
      "iter 8901/30000  loss         0.217403  avg_L1_norm_grad         0.000232  w[0]   -0.430 bias    2.745\n",
      "iter 8920/30000  loss         0.217352  avg_L1_norm_grad         0.000231  w[0]   -0.431 bias    2.748\n",
      "iter 8921/30000  loss         0.217350  avg_L1_norm_grad         0.000231  w[0]   -0.431 bias    2.748\n",
      "iter 8940/30000  loss         0.217300  avg_L1_norm_grad         0.000231  w[0]   -0.431 bias    2.751\n",
      "iter 8941/30000  loss         0.217297  avg_L1_norm_grad         0.000231  w[0]   -0.431 bias    2.751\n",
      "iter 8960/30000  loss         0.217248  avg_L1_norm_grad         0.000230  w[0]   -0.431 bias    2.754\n",
      "iter 8961/30000  loss         0.217245  avg_L1_norm_grad         0.000230  w[0]   -0.431 bias    2.755\n",
      "iter 8980/30000  loss         0.217195  avg_L1_norm_grad         0.000230  w[0]   -0.431 bias    2.758\n",
      "iter 8981/30000  loss         0.217193  avg_L1_norm_grad         0.000230  w[0]   -0.431 bias    2.758\n",
      "iter 9000/30000  loss         0.217143  avg_L1_norm_grad         0.000230  w[0]   -0.431 bias    2.761\n",
      "iter 9001/30000  loss         0.217141  avg_L1_norm_grad         0.000230  w[0]   -0.431 bias    2.761\n",
      "iter 9020/30000  loss         0.217092  avg_L1_norm_grad         0.000229  w[0]   -0.431 bias    2.764\n",
      "iter 9021/30000  loss         0.217089  avg_L1_norm_grad         0.000229  w[0]   -0.431 bias    2.764\n",
      "iter 9040/30000  loss         0.217040  avg_L1_norm_grad         0.000229  w[0]   -0.431 bias    2.768\n",
      "iter 9041/30000  loss         0.217037  avg_L1_norm_grad         0.000229  w[0]   -0.431 bias    2.768\n",
      "iter 9060/30000  loss         0.216988  avg_L1_norm_grad         0.000228  w[0]   -0.431 bias    2.771\n",
      "iter 9061/30000  loss         0.216986  avg_L1_norm_grad         0.000228  w[0]   -0.431 bias    2.771\n",
      "iter 9080/30000  loss         0.216937  avg_L1_norm_grad         0.000228  w[0]   -0.431 bias    2.774\n",
      "iter 9081/30000  loss         0.216934  avg_L1_norm_grad         0.000228  w[0]   -0.431 bias    2.774\n",
      "iter 9100/30000  loss         0.216886  avg_L1_norm_grad         0.000228  w[0]   -0.432 bias    2.777\n",
      "iter 9101/30000  loss         0.216883  avg_L1_norm_grad         0.000228  w[0]   -0.432 bias    2.777\n",
      "iter 9120/30000  loss         0.216835  avg_L1_norm_grad         0.000227  w[0]   -0.432 bias    2.781\n",
      "iter 9121/30000  loss         0.216832  avg_L1_norm_grad         0.000227  w[0]   -0.432 bias    2.781\n",
      "iter 9140/30000  loss         0.216784  avg_L1_norm_grad         0.000227  w[0]   -0.432 bias    2.784\n",
      "iter 9141/30000  loss         0.216782  avg_L1_norm_grad         0.000227  w[0]   -0.432 bias    2.784\n",
      "iter 9160/30000  loss         0.216734  avg_L1_norm_grad         0.000226  w[0]   -0.432 bias    2.787\n",
      "iter 9161/30000  loss         0.216731  avg_L1_norm_grad         0.000226  w[0]   -0.432 bias    2.787\n",
      "iter 9180/30000  loss         0.216683  avg_L1_norm_grad         0.000226  w[0]   -0.432 bias    2.790\n",
      "iter 9181/30000  loss         0.216681  avg_L1_norm_grad         0.000226  w[0]   -0.432 bias    2.790\n",
      "iter 9200/30000  loss         0.216633  avg_L1_norm_grad         0.000226  w[0]   -0.432 bias    2.793\n",
      "iter 9201/30000  loss         0.216630  avg_L1_norm_grad         0.000226  w[0]   -0.432 bias    2.794\n",
      "iter 9220/30000  loss         0.216583  avg_L1_norm_grad         0.000225  w[0]   -0.432 bias    2.797\n",
      "iter 9221/30000  loss         0.216580  avg_L1_norm_grad         0.000225  w[0]   -0.432 bias    2.797\n",
      "iter 9240/30000  loss         0.216533  avg_L1_norm_grad         0.000225  w[0]   -0.432 bias    2.800\n",
      "iter 9241/30000  loss         0.216530  avg_L1_norm_grad         0.000225  w[0]   -0.432 bias    2.800\n",
      "iter 9260/30000  loss         0.216483  avg_L1_norm_grad         0.000225  w[0]   -0.432 bias    2.803\n",
      "iter 9261/30000  loss         0.216480  avg_L1_norm_grad         0.000225  w[0]   -0.432 bias    2.803\n",
      "iter 9280/30000  loss         0.216433  avg_L1_norm_grad         0.000224  w[0]   -0.433 bias    2.806\n",
      "iter 9281/30000  loss         0.216431  avg_L1_norm_grad         0.000224  w[0]   -0.433 bias    2.806\n",
      "iter 9300/30000  loss         0.216384  avg_L1_norm_grad         0.000224  w[0]   -0.433 bias    2.809\n",
      "iter 9301/30000  loss         0.216381  avg_L1_norm_grad         0.000224  w[0]   -0.433 bias    2.810\n",
      "iter 9320/30000  loss         0.216334  avg_L1_norm_grad         0.000223  w[0]   -0.433 bias    2.813\n",
      "iter 9321/30000  loss         0.216332  avg_L1_norm_grad         0.000223  w[0]   -0.433 bias    2.813\n",
      "iter 9340/30000  loss         0.216285  avg_L1_norm_grad         0.000223  w[0]   -0.433 bias    2.816\n",
      "iter 9341/30000  loss         0.216283  avg_L1_norm_grad         0.000223  w[0]   -0.433 bias    2.816\n",
      "iter 9360/30000  loss         0.216236  avg_L1_norm_grad         0.000223  w[0]   -0.433 bias    2.819\n",
      "iter 9361/30000  loss         0.216234  avg_L1_norm_grad         0.000223  w[0]   -0.433 bias    2.819\n",
      "iter 9380/30000  loss         0.216188  avg_L1_norm_grad         0.000222  w[0]   -0.433 bias    2.822\n",
      "iter 9381/30000  loss         0.216185  avg_L1_norm_grad         0.000222  w[0]   -0.433 bias    2.822\n",
      "iter 9400/30000  loss         0.216139  avg_L1_norm_grad         0.000222  w[0]   -0.433 bias    2.825\n",
      "iter 9401/30000  loss         0.216136  avg_L1_norm_grad         0.000222  w[0]   -0.433 bias    2.826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9420/30000  loss         0.216090  avg_L1_norm_grad         0.000222  w[0]   -0.433 bias    2.829\n",
      "iter 9421/30000  loss         0.216088  avg_L1_norm_grad         0.000222  w[0]   -0.433 bias    2.829\n",
      "iter 9440/30000  loss         0.216042  avg_L1_norm_grad         0.000221  w[0]   -0.433 bias    2.832\n",
      "iter 9441/30000  loss         0.216040  avg_L1_norm_grad         0.000221  w[0]   -0.433 bias    2.832\n",
      "iter 9460/30000  loss         0.215994  avg_L1_norm_grad         0.000221  w[0]   -0.434 bias    2.835\n",
      "iter 9461/30000  loss         0.215991  avg_L1_norm_grad         0.000221  w[0]   -0.434 bias    2.835\n",
      "iter 9480/30000  loss         0.215946  avg_L1_norm_grad         0.000221  w[0]   -0.434 bias    2.838\n",
      "iter 9481/30000  loss         0.215943  avg_L1_norm_grad         0.000221  w[0]   -0.434 bias    2.838\n",
      "iter 9500/30000  loss         0.215898  avg_L1_norm_grad         0.000220  w[0]   -0.434 bias    2.841\n",
      "iter 9501/30000  loss         0.215896  avg_L1_norm_grad         0.000220  w[0]   -0.434 bias    2.841\n",
      "iter 9520/30000  loss         0.215850  avg_L1_norm_grad         0.000220  w[0]   -0.434 bias    2.844\n",
      "iter 9521/30000  loss         0.215848  avg_L1_norm_grad         0.000220  w[0]   -0.434 bias    2.844\n",
      "iter 9540/30000  loss         0.215803  avg_L1_norm_grad         0.000219  w[0]   -0.434 bias    2.847\n",
      "iter 9541/30000  loss         0.215800  avg_L1_norm_grad         0.000219  w[0]   -0.434 bias    2.848\n",
      "iter 9560/30000  loss         0.215755  avg_L1_norm_grad         0.000219  w[0]   -0.434 bias    2.851\n",
      "iter 9561/30000  loss         0.215753  avg_L1_norm_grad         0.000219  w[0]   -0.434 bias    2.851\n",
      "iter 9580/30000  loss         0.215708  avg_L1_norm_grad         0.000219  w[0]   -0.434 bias    2.854\n",
      "iter 9581/30000  loss         0.215706  avg_L1_norm_grad         0.000219  w[0]   -0.434 bias    2.854\n",
      "iter 9600/30000  loss         0.215661  avg_L1_norm_grad         0.000218  w[0]   -0.434 bias    2.857\n",
      "iter 9601/30000  loss         0.215659  avg_L1_norm_grad         0.000218  w[0]   -0.434 bias    2.857\n",
      "iter 9620/30000  loss         0.215614  avg_L1_norm_grad         0.000218  w[0]   -0.434 bias    2.860\n",
      "iter 9621/30000  loss         0.215612  avg_L1_norm_grad         0.000218  w[0]   -0.434 bias    2.860\n",
      "iter 9640/30000  loss         0.215567  avg_L1_norm_grad         0.000218  w[0]   -0.435 bias    2.863\n",
      "iter 9641/30000  loss         0.215565  avg_L1_norm_grad         0.000218  w[0]   -0.435 bias    2.863\n",
      "iter 9660/30000  loss         0.215521  avg_L1_norm_grad         0.000217  w[0]   -0.435 bias    2.866\n",
      "iter 9661/30000  loss         0.215518  avg_L1_norm_grad         0.000217  w[0]   -0.435 bias    2.866\n",
      "iter 9680/30000  loss         0.215474  avg_L1_norm_grad         0.000217  w[0]   -0.435 bias    2.869\n",
      "iter 9681/30000  loss         0.215472  avg_L1_norm_grad         0.000217  w[0]   -0.435 bias    2.869\n",
      "iter 9700/30000  loss         0.215428  avg_L1_norm_grad         0.000217  w[0]   -0.435 bias    2.872\n",
      "iter 9701/30000  loss         0.215425  avg_L1_norm_grad         0.000217  w[0]   -0.435 bias    2.873\n",
      "iter 9720/30000  loss         0.215381  avg_L1_norm_grad         0.000216  w[0]   -0.435 bias    2.876\n",
      "iter 9721/30000  loss         0.215379  avg_L1_norm_grad         0.000216  w[0]   -0.435 bias    2.876\n",
      "iter 9740/30000  loss         0.215335  avg_L1_norm_grad         0.000216  w[0]   -0.435 bias    2.879\n",
      "iter 9741/30000  loss         0.215333  avg_L1_norm_grad         0.000216  w[0]   -0.435 bias    2.879\n",
      "iter 9760/30000  loss         0.215289  avg_L1_norm_grad         0.000216  w[0]   -0.435 bias    2.882\n",
      "iter 9761/30000  loss         0.215287  avg_L1_norm_grad         0.000216  w[0]   -0.435 bias    2.882\n",
      "iter 9780/30000  loss         0.215244  avg_L1_norm_grad         0.000215  w[0]   -0.435 bias    2.885\n",
      "iter 9781/30000  loss         0.215241  avg_L1_norm_grad         0.000215  w[0]   -0.435 bias    2.885\n",
      "iter 9800/30000  loss         0.215198  avg_L1_norm_grad         0.000215  w[0]   -0.435 bias    2.888\n",
      "iter 9801/30000  loss         0.215196  avg_L1_norm_grad         0.000215  w[0]   -0.435 bias    2.888\n",
      "iter 9820/30000  loss         0.215153  avg_L1_norm_grad         0.000215  w[0]   -0.435 bias    2.891\n",
      "iter 9821/30000  loss         0.215150  avg_L1_norm_grad         0.000215  w[0]   -0.435 bias    2.891\n",
      "iter 9840/30000  loss         0.215107  avg_L1_norm_grad         0.000214  w[0]   -0.436 bias    2.894\n",
      "iter 9841/30000  loss         0.215105  avg_L1_norm_grad         0.000214  w[0]   -0.436 bias    2.894\n",
      "iter 9860/30000  loss         0.215062  avg_L1_norm_grad         0.000214  w[0]   -0.436 bias    2.897\n",
      "iter 9861/30000  loss         0.215060  avg_L1_norm_grad         0.000214  w[0]   -0.436 bias    2.897\n",
      "iter 9880/30000  loss         0.215017  avg_L1_norm_grad         0.000214  w[0]   -0.436 bias    2.900\n",
      "iter 9881/30000  loss         0.215015  avg_L1_norm_grad         0.000214  w[0]   -0.436 bias    2.900\n",
      "iter 9900/30000  loss         0.214972  avg_L1_norm_grad         0.000213  w[0]   -0.436 bias    2.903\n",
      "iter 9901/30000  loss         0.214970  avg_L1_norm_grad         0.000213  w[0]   -0.436 bias    2.903\n",
      "iter 9920/30000  loss         0.214927  avg_L1_norm_grad         0.000213  w[0]   -0.436 bias    2.906\n",
      "iter 9921/30000  loss         0.214925  avg_L1_norm_grad         0.000213  w[0]   -0.436 bias    2.907\n",
      "iter 9940/30000  loss         0.214883  avg_L1_norm_grad         0.000213  w[0]   -0.436 bias    2.909\n",
      "iter 9941/30000  loss         0.214880  avg_L1_norm_grad         0.000213  w[0]   -0.436 bias    2.910\n",
      "iter 9960/30000  loss         0.214838  avg_L1_norm_grad         0.000212  w[0]   -0.436 bias    2.912\n",
      "iter 9961/30000  loss         0.214836  avg_L1_norm_grad         0.000212  w[0]   -0.436 bias    2.913\n",
      "iter 9980/30000  loss         0.214794  avg_L1_norm_grad         0.000212  w[0]   -0.436 bias    2.916\n",
      "iter 9981/30000  loss         0.214792  avg_L1_norm_grad         0.000212  w[0]   -0.436 bias    2.916\n",
      "iter 10000/30000  loss         0.214750  avg_L1_norm_grad         0.000212  w[0]   -0.436 bias    2.919\n",
      "iter 10001/30000  loss         0.214747  avg_L1_norm_grad         0.000212  w[0]   -0.436 bias    2.919\n",
      "iter 10020/30000  loss         0.214705  avg_L1_norm_grad         0.000211  w[0]   -0.436 bias    2.922\n",
      "iter 10021/30000  loss         0.214703  avg_L1_norm_grad         0.000211  w[0]   -0.436 bias    2.922\n",
      "iter 10040/30000  loss         0.214662  avg_L1_norm_grad         0.000211  w[0]   -0.437 bias    2.925\n",
      "iter 10041/30000  loss         0.214659  avg_L1_norm_grad         0.000211  w[0]   -0.437 bias    2.925\n",
      "iter 10060/30000  loss         0.214618  avg_L1_norm_grad         0.000211  w[0]   -0.437 bias    2.928\n",
      "iter 10061/30000  loss         0.214616  avg_L1_norm_grad         0.000211  w[0]   -0.437 bias    2.928\n",
      "iter 10080/30000  loss         0.214574  avg_L1_norm_grad         0.000210  w[0]   -0.437 bias    2.931\n",
      "iter 10081/30000  loss         0.214572  avg_L1_norm_grad         0.000210  w[0]   -0.437 bias    2.931\n",
      "iter 10100/30000  loss         0.214530  avg_L1_norm_grad         0.000210  w[0]   -0.437 bias    2.934\n",
      "iter 10101/30000  loss         0.214528  avg_L1_norm_grad         0.000210  w[0]   -0.437 bias    2.934\n",
      "iter 10120/30000  loss         0.214487  avg_L1_norm_grad         0.000210  w[0]   -0.437 bias    2.937\n",
      "iter 10121/30000  loss         0.214485  avg_L1_norm_grad         0.000210  w[0]   -0.437 bias    2.937\n",
      "iter 10140/30000  loss         0.214444  avg_L1_norm_grad         0.000209  w[0]   -0.437 bias    2.940\n",
      "iter 10141/30000  loss         0.214442  avg_L1_norm_grad         0.000209  w[0]   -0.437 bias    2.940\n",
      "iter 10160/30000  loss         0.214401  avg_L1_norm_grad         0.000209  w[0]   -0.437 bias    2.943\n",
      "iter 10161/30000  loss         0.214398  avg_L1_norm_grad         0.000209  w[0]   -0.437 bias    2.943\n",
      "iter 10180/30000  loss         0.214358  avg_L1_norm_grad         0.000209  w[0]   -0.437 bias    2.946\n",
      "iter 10181/30000  loss         0.214355  avg_L1_norm_grad         0.000209  w[0]   -0.437 bias    2.946\n",
      "iter 10200/30000  loss         0.214315  avg_L1_norm_grad         0.000208  w[0]   -0.437 bias    2.949\n",
      "iter 10201/30000  loss         0.214313  avg_L1_norm_grad         0.000208  w[0]   -0.437 bias    2.949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10220/30000  loss         0.214272  avg_L1_norm_grad         0.000208  w[0]   -0.437 bias    2.952\n",
      "iter 10221/30000  loss         0.214270  avg_L1_norm_grad         0.000208  w[0]   -0.437 bias    2.952\n",
      "iter 10240/30000  loss         0.214229  avg_L1_norm_grad         0.000208  w[0]   -0.438 bias    2.955\n",
      "iter 10241/30000  loss         0.214227  avg_L1_norm_grad         0.000208  w[0]   -0.438 bias    2.955\n",
      "iter 10260/30000  loss         0.214187  avg_L1_norm_grad         0.000207  w[0]   -0.438 bias    2.958\n",
      "iter 10261/30000  loss         0.214185  avg_L1_norm_grad         0.000207  w[0]   -0.438 bias    2.958\n",
      "iter 10280/30000  loss         0.214145  avg_L1_norm_grad         0.000207  w[0]   -0.438 bias    2.961\n",
      "iter 10281/30000  loss         0.214142  avg_L1_norm_grad         0.000207  w[0]   -0.438 bias    2.961\n",
      "iter 10300/30000  loss         0.214102  avg_L1_norm_grad         0.000207  w[0]   -0.438 bias    2.964\n",
      "iter 10301/30000  loss         0.214100  avg_L1_norm_grad         0.000207  w[0]   -0.438 bias    2.964\n",
      "iter 10320/30000  loss         0.214060  avg_L1_norm_grad         0.000206  w[0]   -0.438 bias    2.967\n",
      "iter 10321/30000  loss         0.214058  avg_L1_norm_grad         0.000206  w[0]   -0.438 bias    2.967\n",
      "iter 10340/30000  loss         0.214018  avg_L1_norm_grad         0.000206  w[0]   -0.438 bias    2.970\n",
      "iter 10341/30000  loss         0.214016  avg_L1_norm_grad         0.000206  w[0]   -0.438 bias    2.970\n",
      "iter 10360/30000  loss         0.213976  avg_L1_norm_grad         0.000206  w[0]   -0.438 bias    2.973\n",
      "iter 10361/30000  loss         0.213974  avg_L1_norm_grad         0.000206  w[0]   -0.438 bias    2.973\n",
      "iter 10380/30000  loss         0.213935  avg_L1_norm_grad         0.000205  w[0]   -0.438 bias    2.976\n",
      "iter 10381/30000  loss         0.213933  avg_L1_norm_grad         0.000205  w[0]   -0.438 bias    2.976\n",
      "iter 10400/30000  loss         0.213893  avg_L1_norm_grad         0.000205  w[0]   -0.438 bias    2.979\n",
      "iter 10401/30000  loss         0.213891  avg_L1_norm_grad         0.000205  w[0]   -0.438 bias    2.979\n",
      "iter 10420/30000  loss         0.213852  avg_L1_norm_grad         0.000205  w[0]   -0.438 bias    2.982\n",
      "iter 10421/30000  loss         0.213849  avg_L1_norm_grad         0.000205  w[0]   -0.438 bias    2.982\n",
      "iter 10440/30000  loss         0.213810  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.985\n",
      "iter 10441/30000  loss         0.213808  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.985\n",
      "iter 10460/30000  loss         0.213769  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.987\n",
      "iter 10461/30000  loss         0.213767  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.988\n",
      "iter 10480/30000  loss         0.213728  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.990\n",
      "iter 10481/30000  loss         0.213726  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.991\n",
      "iter 10500/30000  loss         0.213687  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.993\n",
      "iter 10501/30000  loss         0.213685  avg_L1_norm_grad         0.000204  w[0]   -0.439 bias    2.994\n",
      "iter 10520/30000  loss         0.213646  avg_L1_norm_grad         0.000203  w[0]   -0.439 bias    2.996\n",
      "iter 10521/30000  loss         0.213644  avg_L1_norm_grad         0.000203  w[0]   -0.439 bias    2.996\n",
      "iter 10540/30000  loss         0.213605  avg_L1_norm_grad         0.000203  w[0]   -0.439 bias    2.999\n",
      "iter 10541/30000  loss         0.213603  avg_L1_norm_grad         0.000203  w[0]   -0.439 bias    2.999\n",
      "iter 10560/30000  loss         0.213565  avg_L1_norm_grad         0.000203  w[0]   -0.439 bias    3.002\n",
      "iter 10561/30000  loss         0.213563  avg_L1_norm_grad         0.000203  w[0]   -0.439 bias    3.002\n",
      "iter 10580/30000  loss         0.213524  avg_L1_norm_grad         0.000202  w[0]   -0.439 bias    3.005\n",
      "iter 10581/30000  loss         0.213522  avg_L1_norm_grad         0.000202  w[0]   -0.439 bias    3.005\n",
      "iter 10600/30000  loss         0.213484  avg_L1_norm_grad         0.000202  w[0]   -0.439 bias    3.008\n",
      "iter 10601/30000  loss         0.213482  avg_L1_norm_grad         0.000202  w[0]   -0.439 bias    3.008\n",
      "iter 10620/30000  loss         0.213443  avg_L1_norm_grad         0.000202  w[0]   -0.439 bias    3.011\n",
      "iter 10621/30000  loss         0.213441  avg_L1_norm_grad         0.000202  w[0]   -0.439 bias    3.011\n",
      "iter 10640/30000  loss         0.213403  avg_L1_norm_grad         0.000201  w[0]   -0.439 bias    3.014\n",
      "iter 10641/30000  loss         0.213401  avg_L1_norm_grad         0.000201  w[0]   -0.439 bias    3.014\n",
      "iter 10660/30000  loss         0.213363  avg_L1_norm_grad         0.000201  w[0]   -0.440 bias    3.017\n",
      "iter 10661/30000  loss         0.213361  avg_L1_norm_grad         0.000201  w[0]   -0.440 bias    3.017\n",
      "iter 10680/30000  loss         0.213323  avg_L1_norm_grad         0.000201  w[0]   -0.440 bias    3.020\n",
      "iter 10681/30000  loss         0.213321  avg_L1_norm_grad         0.000201  w[0]   -0.440 bias    3.020\n",
      "iter 10700/30000  loss         0.213284  avg_L1_norm_grad         0.000201  w[0]   -0.440 bias    3.023\n",
      "iter 10701/30000  loss         0.213282  avg_L1_norm_grad         0.000201  w[0]   -0.440 bias    3.023\n",
      "iter 10720/30000  loss         0.213244  avg_L1_norm_grad         0.000200  w[0]   -0.440 bias    3.026\n",
      "iter 10721/30000  loss         0.213242  avg_L1_norm_grad         0.000200  w[0]   -0.440 bias    3.026\n",
      "iter 10740/30000  loss         0.213204  avg_L1_norm_grad         0.000200  w[0]   -0.440 bias    3.028\n",
      "iter 10741/30000  loss         0.213202  avg_L1_norm_grad         0.000200  w[0]   -0.440 bias    3.029\n",
      "iter 10760/30000  loss         0.213165  avg_L1_norm_grad         0.000200  w[0]   -0.440 bias    3.031\n",
      "iter 10761/30000  loss         0.213163  avg_L1_norm_grad         0.000200  w[0]   -0.440 bias    3.031\n",
      "iter 10780/30000  loss         0.213126  avg_L1_norm_grad         0.000199  w[0]   -0.440 bias    3.034\n",
      "iter 10781/30000  loss         0.213124  avg_L1_norm_grad         0.000199  w[0]   -0.440 bias    3.034\n",
      "iter 10800/30000  loss         0.213086  avg_L1_norm_grad         0.000199  w[0]   -0.440 bias    3.037\n",
      "iter 10801/30000  loss         0.213084  avg_L1_norm_grad         0.000199  w[0]   -0.440 bias    3.037\n",
      "iter 10820/30000  loss         0.213047  avg_L1_norm_grad         0.000199  w[0]   -0.440 bias    3.040\n",
      "iter 10821/30000  loss         0.213045  avg_L1_norm_grad         0.000199  w[0]   -0.440 bias    3.040\n",
      "iter 10840/30000  loss         0.213008  avg_L1_norm_grad         0.000198  w[0]   -0.440 bias    3.043\n",
      "iter 10841/30000  loss         0.213006  avg_L1_norm_grad         0.000198  w[0]   -0.440 bias    3.043\n",
      "iter 10860/30000  loss         0.212969  avg_L1_norm_grad         0.000198  w[0]   -0.440 bias    3.046\n",
      "iter 10861/30000  loss         0.212967  avg_L1_norm_grad         0.000198  w[0]   -0.440 bias    3.046\n",
      "iter 10880/30000  loss         0.212931  avg_L1_norm_grad         0.000198  w[0]   -0.441 bias    3.049\n",
      "iter 10881/30000  loss         0.212929  avg_L1_norm_grad         0.000198  w[0]   -0.441 bias    3.049\n",
      "iter 10900/30000  loss         0.212892  avg_L1_norm_grad         0.000198  w[0]   -0.441 bias    3.052\n",
      "iter 10901/30000  loss         0.212890  avg_L1_norm_grad         0.000198  w[0]   -0.441 bias    3.052\n",
      "iter 10920/30000  loss         0.212853  avg_L1_norm_grad         0.000197  w[0]   -0.441 bias    3.054\n",
      "iter 10921/30000  loss         0.212852  avg_L1_norm_grad         0.000197  w[0]   -0.441 bias    3.055\n",
      "iter 10940/30000  loss         0.212815  avg_L1_norm_grad         0.000197  w[0]   -0.441 bias    3.057\n",
      "iter 10941/30000  loss         0.212813  avg_L1_norm_grad         0.000197  w[0]   -0.441 bias    3.057\n",
      "iter 10960/30000  loss         0.212777  avg_L1_norm_grad         0.000197  w[0]   -0.441 bias    3.060\n",
      "iter 10961/30000  loss         0.212775  avg_L1_norm_grad         0.000197  w[0]   -0.441 bias    3.060\n",
      "iter 10980/30000  loss         0.212738  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.063\n",
      "iter 10981/30000  loss         0.212737  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.063\n",
      "iter 11000/30000  loss         0.212700  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.066\n",
      "iter 11001/30000  loss         0.212698  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11020/30000  loss         0.212662  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.069\n",
      "iter 11021/30000  loss         0.212660  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.069\n",
      "iter 11040/30000  loss         0.212624  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.072\n",
      "iter 11041/30000  loss         0.212623  avg_L1_norm_grad         0.000196  w[0]   -0.441 bias    3.072\n",
      "iter 11060/30000  loss         0.212587  avg_L1_norm_grad         0.000195  w[0]   -0.441 bias    3.074\n",
      "iter 11061/30000  loss         0.212585  avg_L1_norm_grad         0.000195  w[0]   -0.441 bias    3.075\n",
      "iter 11080/30000  loss         0.212549  avg_L1_norm_grad         0.000195  w[0]   -0.441 bias    3.077\n",
      "iter 11081/30000  loss         0.212547  avg_L1_norm_grad         0.000195  w[0]   -0.441 bias    3.077\n",
      "iter 11100/30000  loss         0.212511  avg_L1_norm_grad         0.000195  w[0]   -0.442 bias    3.080\n",
      "iter 11101/30000  loss         0.212510  avg_L1_norm_grad         0.000195  w[0]   -0.442 bias    3.080\n",
      "iter 11120/30000  loss         0.212474  avg_L1_norm_grad         0.000195  w[0]   -0.442 bias    3.083\n",
      "iter 11121/30000  loss         0.212472  avg_L1_norm_grad         0.000195  w[0]   -0.442 bias    3.083\n",
      "iter 11140/30000  loss         0.212437  avg_L1_norm_grad         0.000194  w[0]   -0.442 bias    3.086\n",
      "iter 11141/30000  loss         0.212435  avg_L1_norm_grad         0.000194  w[0]   -0.442 bias    3.086\n",
      "iter 11160/30000  loss         0.212399  avg_L1_norm_grad         0.000194  w[0]   -0.442 bias    3.089\n",
      "iter 11161/30000  loss         0.212398  avg_L1_norm_grad         0.000194  w[0]   -0.442 bias    3.089\n",
      "iter 11180/30000  loss         0.212362  avg_L1_norm_grad         0.000194  w[0]   -0.442 bias    3.091\n",
      "iter 11181/30000  loss         0.212360  avg_L1_norm_grad         0.000194  w[0]   -0.442 bias    3.091\n",
      "iter 11200/30000  loss         0.212325  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.094\n",
      "iter 11201/30000  loss         0.212323  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.094\n",
      "iter 11220/30000  loss         0.212288  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.097\n",
      "iter 11221/30000  loss         0.212287  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.097\n",
      "iter 11240/30000  loss         0.212252  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.100\n",
      "iter 11241/30000  loss         0.212250  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.100\n",
      "iter 11260/30000  loss         0.212215  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.103\n",
      "iter 11261/30000  loss         0.212213  avg_L1_norm_grad         0.000193  w[0]   -0.442 bias    3.103\n",
      "iter 11280/30000  loss         0.212178  avg_L1_norm_grad         0.000192  w[0]   -0.442 bias    3.105\n",
      "iter 11281/30000  loss         0.212176  avg_L1_norm_grad         0.000192  w[0]   -0.442 bias    3.106\n",
      "iter 11300/30000  loss         0.212142  avg_L1_norm_grad         0.000192  w[0]   -0.442 bias    3.108\n",
      "iter 11301/30000  loss         0.212140  avg_L1_norm_grad         0.000192  w[0]   -0.442 bias    3.108\n",
      "iter 11320/30000  loss         0.212105  avg_L1_norm_grad         0.000192  w[0]   -0.442 bias    3.111\n",
      "iter 11321/30000  loss         0.212103  avg_L1_norm_grad         0.000192  w[0]   -0.442 bias    3.111\n",
      "iter 11340/30000  loss         0.212069  avg_L1_norm_grad         0.000192  w[0]   -0.443 bias    3.114\n",
      "iter 11341/30000  loss         0.212067  avg_L1_norm_grad         0.000191  w[0]   -0.443 bias    3.114\n",
      "iter 11360/30000  loss         0.212033  avg_L1_norm_grad         0.000191  w[0]   -0.443 bias    3.117\n",
      "iter 11361/30000  loss         0.212031  avg_L1_norm_grad         0.000191  w[0]   -0.443 bias    3.117\n",
      "iter 11380/30000  loss         0.211997  avg_L1_norm_grad         0.000191  w[0]   -0.443 bias    3.119\n",
      "iter 11381/30000  loss         0.211995  avg_L1_norm_grad         0.000191  w[0]   -0.443 bias    3.120\n",
      "iter 11400/30000  loss         0.211961  avg_L1_norm_grad         0.000191  w[0]   -0.443 bias    3.122\n",
      "iter 11401/30000  loss         0.211959  avg_L1_norm_grad         0.000191  w[0]   -0.443 bias    3.122\n",
      "iter 11420/30000  loss         0.211925  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.125\n",
      "iter 11421/30000  loss         0.211923  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.125\n",
      "iter 11440/30000  loss         0.211889  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.128\n",
      "iter 11441/30000  loss         0.211887  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.128\n",
      "iter 11460/30000  loss         0.211853  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.131\n",
      "iter 11461/30000  loss         0.211851  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.131\n",
      "iter 11480/30000  loss         0.211818  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.133\n",
      "iter 11481/30000  loss         0.211816  avg_L1_norm_grad         0.000190  w[0]   -0.443 bias    3.133\n",
      "iter 11500/30000  loss         0.211782  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.136\n",
      "iter 11501/30000  loss         0.211780  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.136\n",
      "iter 11520/30000  loss         0.211747  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.139\n",
      "iter 11521/30000  loss         0.211745  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.139\n",
      "iter 11540/30000  loss         0.211711  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.142\n",
      "iter 11541/30000  loss         0.211710  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.142\n",
      "iter 11560/30000  loss         0.211676  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.144\n",
      "iter 11561/30000  loss         0.211674  avg_L1_norm_grad         0.000189  w[0]   -0.443 bias    3.144\n",
      "iter 11580/30000  loss         0.211641  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.147\n",
      "iter 11581/30000  loss         0.211639  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.147\n",
      "iter 11600/30000  loss         0.211606  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.150\n",
      "iter 11601/30000  loss         0.211604  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.150\n",
      "iter 11620/30000  loss         0.211571  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.153\n",
      "iter 11621/30000  loss         0.211569  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.153\n",
      "iter 11640/30000  loss         0.211536  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.155\n",
      "iter 11641/30000  loss         0.211534  avg_L1_norm_grad         0.000188  w[0]   -0.444 bias    3.155\n",
      "iter 11660/30000  loss         0.211501  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.158\n",
      "iter 11661/30000  loss         0.211500  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.158\n",
      "iter 11680/30000  loss         0.211467  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.161\n",
      "iter 11681/30000  loss         0.211465  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.161\n",
      "iter 11700/30000  loss         0.211432  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.164\n",
      "iter 11701/30000  loss         0.211431  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.164\n",
      "iter 11720/30000  loss         0.211398  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.166\n",
      "iter 11721/30000  loss         0.211396  avg_L1_norm_grad         0.000187  w[0]   -0.444 bias    3.166\n",
      "iter 11740/30000  loss         0.211363  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.169\n",
      "iter 11741/30000  loss         0.211362  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.169\n",
      "iter 11760/30000  loss         0.211329  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.172\n",
      "iter 11761/30000  loss         0.211327  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.172\n",
      "iter 11780/30000  loss         0.211295  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.174\n",
      "iter 11781/30000  loss         0.211293  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.175\n",
      "iter 11800/30000  loss         0.211261  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.177\n",
      "iter 11801/30000  loss         0.211259  avg_L1_norm_grad         0.000186  w[0]   -0.444 bias    3.177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11820/30000  loss         0.211227  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.180\n",
      "iter 11821/30000  loss         0.211225  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.180\n",
      "iter 11840/30000  loss         0.211193  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.183\n",
      "iter 11841/30000  loss         0.211191  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.183\n",
      "iter 11860/30000  loss         0.211159  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.185\n",
      "iter 11861/30000  loss         0.211158  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.185\n",
      "iter 11880/30000  loss         0.211126  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.188\n",
      "iter 11881/30000  loss         0.211124  avg_L1_norm_grad         0.000185  w[0]   -0.445 bias    3.188\n",
      "iter 11900/30000  loss         0.211092  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.191\n",
      "iter 11901/30000  loss         0.211090  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.191\n",
      "iter 11920/30000  loss         0.211058  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.193\n",
      "iter 11921/30000  loss         0.211057  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.194\n",
      "iter 11940/30000  loss         0.211025  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.196\n",
      "iter 11941/30000  loss         0.211023  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.196\n",
      "iter 11960/30000  loss         0.210992  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.199\n",
      "iter 11961/30000  loss         0.210990  avg_L1_norm_grad         0.000184  w[0]   -0.445 bias    3.199\n",
      "iter 11980/30000  loss         0.210958  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.201\n",
      "iter 11981/30000  loss         0.210957  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.202\n",
      "iter 12000/30000  loss         0.210925  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.204\n",
      "iter 12001/30000  loss         0.210923  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.204\n",
      "iter 12020/30000  loss         0.210892  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.207\n",
      "iter 12021/30000  loss         0.210890  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.207\n",
      "iter 12040/30000  loss         0.210859  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.210\n",
      "iter 12041/30000  loss         0.210857  avg_L1_norm_grad         0.000183  w[0]   -0.445 bias    3.210\n",
      "iter 12060/30000  loss         0.210826  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.212\n",
      "iter 12061/30000  loss         0.210824  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.212\n",
      "iter 12080/30000  loss         0.210793  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.215\n",
      "iter 12081/30000  loss         0.210792  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.215\n",
      "iter 12100/30000  loss         0.210760  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.218\n",
      "iter 12101/30000  loss         0.210759  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.218\n",
      "iter 12120/30000  loss         0.210728  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.220\n",
      "iter 12121/30000  loss         0.210726  avg_L1_norm_grad         0.000182  w[0]   -0.446 bias    3.220\n",
      "iter 12140/30000  loss         0.210695  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.223\n",
      "iter 12141/30000  loss         0.210694  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.223\n",
      "iter 12160/30000  loss         0.210663  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.226\n",
      "iter 12161/30000  loss         0.210661  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.226\n",
      "iter 12180/30000  loss         0.210630  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.228\n",
      "iter 12181/30000  loss         0.210629  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.228\n",
      "iter 12200/30000  loss         0.210598  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.231\n",
      "iter 12201/30000  loss         0.210596  avg_L1_norm_grad         0.000181  w[0]   -0.446 bias    3.231\n",
      "iter 12220/30000  loss         0.210566  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.233\n",
      "iter 12221/30000  loss         0.210564  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.234\n",
      "iter 12240/30000  loss         0.210534  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.236\n",
      "iter 12241/30000  loss         0.210532  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.236\n",
      "iter 12260/30000  loss         0.210502  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.239\n",
      "iter 12261/30000  loss         0.210500  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.239\n",
      "iter 12280/30000  loss         0.210470  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.241\n",
      "iter 12281/30000  loss         0.210468  avg_L1_norm_grad         0.000180  w[0]   -0.446 bias    3.242\n",
      "iter 12300/30000  loss         0.210438  avg_L1_norm_grad         0.000179  w[0]   -0.446 bias    3.244\n",
      "iter 12301/30000  loss         0.210436  avg_L1_norm_grad         0.000179  w[0]   -0.446 bias    3.244\n",
      "iter 12320/30000  loss         0.210406  avg_L1_norm_grad         0.000179  w[0]   -0.447 bias    3.247\n",
      "iter 12321/30000  loss         0.210404  avg_L1_norm_grad         0.000179  w[0]   -0.447 bias    3.247\n",
      "iter 12340/30000  loss         0.210374  avg_L1_norm_grad         0.000179  w[0]   -0.447 bias    3.249\n",
      "iter 12341/30000  loss         0.210373  avg_L1_norm_grad         0.000179  w[0]   -0.447 bias    3.249\n",
      "iter 12360/30000  loss         0.210343  avg_L1_norm_grad         0.000179  w[0]   -0.447 bias    3.252\n",
      "iter 12361/30000  loss         0.210341  avg_L1_norm_grad         0.000179  w[0]   -0.447 bias    3.252\n",
      "iter 12380/30000  loss         0.210311  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.255\n",
      "iter 12381/30000  loss         0.210309  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.255\n",
      "iter 12400/30000  loss         0.210280  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.257\n",
      "iter 12401/30000  loss         0.210278  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.257\n",
      "iter 12420/30000  loss         0.210248  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.260\n",
      "iter 12421/30000  loss         0.210247  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.260\n",
      "iter 12440/30000  loss         0.210217  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.262\n",
      "iter 12441/30000  loss         0.210215  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.263\n",
      "iter 12460/30000  loss         0.210186  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.265\n",
      "iter 12461/30000  loss         0.210184  avg_L1_norm_grad         0.000178  w[0]   -0.447 bias    3.265\n",
      "iter 12480/30000  loss         0.210154  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.268\n",
      "iter 12481/30000  loss         0.210153  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.268\n",
      "iter 12500/30000  loss         0.210123  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.270\n",
      "iter 12501/30000  loss         0.210122  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.270\n",
      "iter 12520/30000  loss         0.210092  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.273\n",
      "iter 12521/30000  loss         0.210091  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.273\n",
      "iter 12540/30000  loss         0.210061  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.275\n",
      "iter 12541/30000  loss         0.210060  avg_L1_norm_grad         0.000177  w[0]   -0.447 bias    3.276\n",
      "iter 12560/30000  loss         0.210031  avg_L1_norm_grad         0.000176  w[0]   -0.447 bias    3.278\n",
      "iter 12561/30000  loss         0.210029  avg_L1_norm_grad         0.000176  w[0]   -0.447 bias    3.278\n",
      "iter 12580/30000  loss         0.210000  avg_L1_norm_grad         0.000176  w[0]   -0.448 bias    3.281\n",
      "iter 12581/30000  loss         0.209998  avg_L1_norm_grad         0.000176  w[0]   -0.448 bias    3.281\n",
      "iter 12600/30000  loss         0.209969  avg_L1_norm_grad         0.000176  w[0]   -0.448 bias    3.283\n",
      "iter 12601/30000  loss         0.209968  avg_L1_norm_grad         0.000176  w[0]   -0.448 bias    3.283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12620/30000  loss         0.209939  avg_L1_norm_grad         0.000176  w[0]   -0.448 bias    3.286\n",
      "iter 12621/30000  loss         0.209937  avg_L1_norm_grad         0.000176  w[0]   -0.448 bias    3.286\n",
      "iter 12640/30000  loss         0.209908  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.288\n",
      "iter 12641/30000  loss         0.209906  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.289\n",
      "iter 12660/30000  loss         0.209878  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.291\n",
      "iter 12661/30000  loss         0.209876  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.291\n",
      "iter 12680/30000  loss         0.209847  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.294\n",
      "iter 12681/30000  loss         0.209846  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.294\n",
      "iter 12700/30000  loss         0.209817  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.296\n",
      "iter 12701/30000  loss         0.209815  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.296\n",
      "iter 12720/30000  loss         0.209787  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.299\n",
      "iter 12721/30000  loss         0.209785  avg_L1_norm_grad         0.000175  w[0]   -0.448 bias    3.299\n",
      "iter 12740/30000  loss         0.209757  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.301\n",
      "iter 12741/30000  loss         0.209755  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.301\n",
      "iter 12760/30000  loss         0.209726  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.304\n",
      "iter 12761/30000  loss         0.209725  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.304\n",
      "iter 12780/30000  loss         0.209696  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.306\n",
      "iter 12781/30000  loss         0.209695  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.307\n",
      "iter 12800/30000  loss         0.209667  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.309\n",
      "iter 12801/30000  loss         0.209665  avg_L1_norm_grad         0.000174  w[0]   -0.448 bias    3.309\n",
      "iter 12820/30000  loss         0.209637  avg_L1_norm_grad         0.000173  w[0]   -0.448 bias    3.312\n",
      "iter 12821/30000  loss         0.209635  avg_L1_norm_grad         0.000173  w[0]   -0.448 bias    3.312\n",
      "iter 12840/30000  loss         0.209607  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.314\n",
      "iter 12841/30000  loss         0.209606  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.314\n",
      "iter 12860/30000  loss         0.209577  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.317\n",
      "iter 12861/30000  loss         0.209576  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.317\n",
      "iter 12880/30000  loss         0.209548  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.319\n",
      "iter 12881/30000  loss         0.209546  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.319\n",
      "iter 12900/30000  loss         0.209518  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.322\n",
      "iter 12901/30000  loss         0.209517  avg_L1_norm_grad         0.000173  w[0]   -0.449 bias    3.322\n",
      "iter 12920/30000  loss         0.209489  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.324\n",
      "iter 12921/30000  loss         0.209487  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.325\n",
      "iter 12940/30000  loss         0.209459  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.327\n",
      "iter 12941/30000  loss         0.209458  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.327\n",
      "iter 12960/30000  loss         0.209430  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.329\n",
      "iter 12961/30000  loss         0.209429  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.330\n",
      "iter 12980/30000  loss         0.209401  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.332\n",
      "iter 12981/30000  loss         0.209399  avg_L1_norm_grad         0.000172  w[0]   -0.449 bias    3.332\n",
      "iter 13000/30000  loss         0.209372  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.335\n",
      "iter 13001/30000  loss         0.209370  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.335\n",
      "iter 13020/30000  loss         0.209343  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.337\n",
      "iter 13021/30000  loss         0.209341  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.337\n",
      "iter 13040/30000  loss         0.209313  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.340\n",
      "iter 13041/30000  loss         0.209312  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.340\n",
      "iter 13060/30000  loss         0.209285  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.342\n",
      "iter 13061/30000  loss         0.209283  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.342\n",
      "iter 13080/30000  loss         0.209256  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.345\n",
      "iter 13081/30000  loss         0.209254  avg_L1_norm_grad         0.000171  w[0]   -0.449 bias    3.345\n",
      "iter 13100/30000  loss         0.209227  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.347\n",
      "iter 13101/30000  loss         0.209225  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.347\n",
      "iter 13120/30000  loss         0.209198  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.350\n",
      "iter 13121/30000  loss         0.209197  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.350\n",
      "iter 13140/30000  loss         0.209169  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.352\n",
      "iter 13141/30000  loss         0.209168  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.352\n",
      "iter 13160/30000  loss         0.209141  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.355\n",
      "iter 13161/30000  loss         0.209139  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.355\n",
      "iter 13180/30000  loss         0.209112  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.357\n",
      "iter 13181/30000  loss         0.209111  avg_L1_norm_grad         0.000170  w[0]   -0.450 bias    3.357\n",
      "iter 13200/30000  loss         0.209084  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.360\n",
      "iter 13201/30000  loss         0.209082  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.360\n",
      "iter 13220/30000  loss         0.209056  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.362\n",
      "iter 13221/30000  loss         0.209054  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.362\n",
      "iter 13240/30000  loss         0.209027  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.365\n",
      "iter 13241/30000  loss         0.209026  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.365\n",
      "iter 13260/30000  loss         0.208999  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.367\n",
      "iter 13261/30000  loss         0.208998  avg_L1_norm_grad         0.000169  w[0]   -0.450 bias    3.367\n",
      "iter 13280/30000  loss         0.208971  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.370\n",
      "iter 13281/30000  loss         0.208969  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.370\n",
      "iter 13300/30000  loss         0.208943  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.372\n",
      "iter 13301/30000  loss         0.208941  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.372\n",
      "iter 13320/30000  loss         0.208915  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.375\n",
      "iter 13321/30000  loss         0.208913  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.375\n",
      "iter 13340/30000  loss         0.208887  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.377\n",
      "iter 13341/30000  loss         0.208885  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.377\n",
      "iter 13360/30000  loss         0.208859  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.380\n",
      "iter 13361/30000  loss         0.208857  avg_L1_norm_grad         0.000168  w[0]   -0.450 bias    3.380\n",
      "iter 13380/30000  loss         0.208831  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.382\n",
      "iter 13381/30000  loss         0.208830  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.382\n",
      "iter 13400/30000  loss         0.208803  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.385\n",
      "iter 13401/30000  loss         0.208802  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13420/30000  loss         0.208776  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.387\n",
      "iter 13421/30000  loss         0.208774  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.387\n",
      "iter 13440/30000  loss         0.208748  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.390\n",
      "iter 13441/30000  loss         0.208747  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.390\n",
      "iter 13460/30000  loss         0.208720  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.392\n",
      "iter 13461/30000  loss         0.208719  avg_L1_norm_grad         0.000167  w[0]   -0.451 bias    3.392\n",
      "iter 13480/30000  loss         0.208693  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.395\n",
      "iter 13481/30000  loss         0.208691  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.395\n",
      "iter 13500/30000  loss         0.208665  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.397\n",
      "iter 13501/30000  loss         0.208664  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.397\n",
      "iter 13520/30000  loss         0.208638  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.400\n",
      "iter 13521/30000  loss         0.208637  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.400\n",
      "iter 13540/30000  loss         0.208611  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.402\n",
      "iter 13541/30000  loss         0.208609  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.402\n",
      "iter 13560/30000  loss         0.208584  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.404\n",
      "iter 13561/30000  loss         0.208582  avg_L1_norm_grad         0.000166  w[0]   -0.451 bias    3.405\n",
      "iter 13580/30000  loss         0.208556  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.407\n",
      "iter 13581/30000  loss         0.208555  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.407\n",
      "iter 13600/30000  loss         0.208529  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.409\n",
      "iter 13601/30000  loss         0.208528  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.409\n",
      "iter 13620/30000  loss         0.208502  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.412\n",
      "iter 13621/30000  loss         0.208501  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.412\n",
      "iter 13640/30000  loss         0.208475  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.414\n",
      "iter 13641/30000  loss         0.208474  avg_L1_norm_grad         0.000165  w[0]   -0.451 bias    3.414\n",
      "iter 13660/30000  loss         0.208448  avg_L1_norm_grad         0.000165  w[0]   -0.452 bias    3.417\n",
      "iter 13661/30000  loss         0.208447  avg_L1_norm_grad         0.000165  w[0]   -0.452 bias    3.417\n",
      "iter 13680/30000  loss         0.208422  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.419\n",
      "iter 13681/30000  loss         0.208420  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.419\n",
      "iter 13700/30000  loss         0.208395  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.422\n",
      "iter 13701/30000  loss         0.208393  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.422\n",
      "iter 13720/30000  loss         0.208368  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.424\n",
      "iter 13721/30000  loss         0.208367  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.424\n",
      "iter 13740/30000  loss         0.208341  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.426\n",
      "iter 13741/30000  loss         0.208340  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.427\n",
      "iter 13760/30000  loss         0.208315  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.429\n",
      "iter 13761/30000  loss         0.208314  avg_L1_norm_grad         0.000164  w[0]   -0.452 bias    3.429\n",
      "iter 13780/30000  loss         0.208288  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.431\n",
      "iter 13781/30000  loss         0.208287  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.431\n",
      "iter 13800/30000  loss         0.208262  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.434\n",
      "iter 13801/30000  loss         0.208261  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.434\n",
      "iter 13820/30000  loss         0.208235  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.436\n",
      "iter 13821/30000  loss         0.208234  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.436\n",
      "iter 13840/30000  loss         0.208209  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.439\n",
      "iter 13841/30000  loss         0.208208  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.439\n",
      "iter 13860/30000  loss         0.208183  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.441\n",
      "iter 13861/30000  loss         0.208182  avg_L1_norm_grad         0.000163  w[0]   -0.452 bias    3.441\n",
      "iter 13880/30000  loss         0.208157  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.443\n",
      "iter 13881/30000  loss         0.208155  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.444\n",
      "iter 13900/30000  loss         0.208131  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.446\n",
      "iter 13901/30000  loss         0.208129  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.446\n",
      "iter 13920/30000  loss         0.208104  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.448\n",
      "iter 13921/30000  loss         0.208103  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.448\n",
      "iter 13940/30000  loss         0.208078  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.451\n",
      "iter 13941/30000  loss         0.208077  avg_L1_norm_grad         0.000162  w[0]   -0.452 bias    3.451\n",
      "iter 13960/30000  loss         0.208052  avg_L1_norm_grad         0.000162  w[0]   -0.453 bias    3.453\n",
      "iter 13961/30000  loss         0.208051  avg_L1_norm_grad         0.000162  w[0]   -0.453 bias    3.453\n",
      "iter 13980/30000  loss         0.208027  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.456\n",
      "iter 13981/30000  loss         0.208025  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.456\n",
      "iter 14000/30000  loss         0.208001  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.458\n",
      "iter 14001/30000  loss         0.207999  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.458\n",
      "iter 14020/30000  loss         0.207975  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.460\n",
      "iter 14021/30000  loss         0.207974  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.460\n",
      "iter 14040/30000  loss         0.207949  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.463\n",
      "iter 14041/30000  loss         0.207948  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.463\n",
      "iter 14060/30000  loss         0.207924  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.465\n",
      "iter 14061/30000  loss         0.207922  avg_L1_norm_grad         0.000161  w[0]   -0.453 bias    3.465\n",
      "iter 14080/30000  loss         0.207898  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.467\n",
      "iter 14081/30000  loss         0.207897  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.468\n",
      "iter 14100/30000  loss         0.207872  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.470\n",
      "iter 14101/30000  loss         0.207871  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.470\n",
      "iter 14120/30000  loss         0.207847  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.472\n",
      "iter 14121/30000  loss         0.207846  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.472\n",
      "iter 14140/30000  loss         0.207822  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.475\n",
      "iter 14141/30000  loss         0.207820  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.475\n",
      "iter 14160/30000  loss         0.207796  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.477\n",
      "iter 14161/30000  loss         0.207795  avg_L1_norm_grad         0.000160  w[0]   -0.453 bias    3.477\n",
      "iter 14180/30000  loss         0.207771  avg_L1_norm_grad         0.000159  w[0]   -0.453 bias    3.479\n",
      "iter 14181/30000  loss         0.207770  avg_L1_norm_grad         0.000159  w[0]   -0.453 bias    3.480\n",
      "iter 14200/30000  loss         0.207746  avg_L1_norm_grad         0.000159  w[0]   -0.453 bias    3.482\n",
      "iter 14201/30000  loss         0.207744  avg_L1_norm_grad         0.000159  w[0]   -0.453 bias    3.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14220/30000  loss         0.207721  avg_L1_norm_grad         0.000159  w[0]   -0.453 bias    3.484\n",
      "iter 14221/30000  loss         0.207719  avg_L1_norm_grad         0.000159  w[0]   -0.453 bias    3.484\n",
      "iter 14240/30000  loss         0.207695  avg_L1_norm_grad         0.000159  w[0]   -0.454 bias    3.487\n",
      "iter 14241/30000  loss         0.207694  avg_L1_norm_grad         0.000159  w[0]   -0.454 bias    3.487\n",
      "iter 14260/30000  loss         0.207670  avg_L1_norm_grad         0.000159  w[0]   -0.454 bias    3.489\n",
      "iter 14261/30000  loss         0.207669  avg_L1_norm_grad         0.000159  w[0]   -0.454 bias    3.489\n",
      "iter 14280/30000  loss         0.207645  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.491\n",
      "iter 14281/30000  loss         0.207644  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.491\n",
      "iter 14300/30000  loss         0.207620  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.494\n",
      "iter 14301/30000  loss         0.207619  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.494\n",
      "iter 14320/30000  loss         0.207595  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.496\n",
      "iter 14321/30000  loss         0.207594  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.496\n",
      "iter 14340/30000  loss         0.207571  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.498\n",
      "iter 14341/30000  loss         0.207569  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.498\n",
      "iter 14360/30000  loss         0.207546  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.501\n",
      "iter 14361/30000  loss         0.207545  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.501\n",
      "iter 14380/30000  loss         0.207521  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.503\n",
      "iter 14381/30000  loss         0.207520  avg_L1_norm_grad         0.000158  w[0]   -0.454 bias    3.503\n",
      "iter 14400/30000  loss         0.207497  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.505\n",
      "iter 14401/30000  loss         0.207495  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.506\n",
      "iter 14420/30000  loss         0.207472  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.508\n",
      "iter 14421/30000  loss         0.207471  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.508\n",
      "iter 14440/30000  loss         0.207447  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.510\n",
      "iter 14441/30000  loss         0.207446  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.510\n",
      "iter 14460/30000  loss         0.207423  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.512\n",
      "iter 14461/30000  loss         0.207422  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.513\n",
      "iter 14480/30000  loss         0.207398  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.515\n",
      "iter 14481/30000  loss         0.207397  avg_L1_norm_grad         0.000157  w[0]   -0.454 bias    3.515\n",
      "iter 14500/30000  loss         0.207374  avg_L1_norm_grad         0.000156  w[0]   -0.454 bias    3.517\n",
      "iter 14501/30000  loss         0.207373  avg_L1_norm_grad         0.000156  w[0]   -0.454 bias    3.517\n",
      "iter 14520/30000  loss         0.207350  avg_L1_norm_grad         0.000156  w[0]   -0.454 bias    3.520\n",
      "iter 14521/30000  loss         0.207349  avg_L1_norm_grad         0.000156  w[0]   -0.454 bias    3.520\n",
      "iter 14540/30000  loss         0.207325  avg_L1_norm_grad         0.000156  w[0]   -0.455 bias    3.522\n",
      "iter 14541/30000  loss         0.207324  avg_L1_norm_grad         0.000156  w[0]   -0.455 bias    3.522\n",
      "iter 14560/30000  loss         0.207301  avg_L1_norm_grad         0.000156  w[0]   -0.455 bias    3.524\n",
      "iter 14561/30000  loss         0.207300  avg_L1_norm_grad         0.000156  w[0]   -0.455 bias    3.524\n",
      "iter 14580/30000  loss         0.207277  avg_L1_norm_grad         0.000156  w[0]   -0.455 bias    3.527\n",
      "iter 14581/30000  loss         0.207276  avg_L1_norm_grad         0.000156  w[0]   -0.455 bias    3.527\n",
      "iter 14600/30000  loss         0.207253  avg_L1_norm_grad         0.000156  w[0]   -0.455 bias    3.529\n",
      "iter 14601/30000  loss         0.207252  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.529\n",
      "iter 14620/30000  loss         0.207229  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.531\n",
      "iter 14621/30000  loss         0.207228  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.531\n",
      "iter 14640/30000  loss         0.207205  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.533\n",
      "iter 14641/30000  loss         0.207204  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.534\n",
      "iter 14660/30000  loss         0.207181  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.536\n",
      "iter 14661/30000  loss         0.207180  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.536\n",
      "iter 14680/30000  loss         0.207157  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.538\n",
      "iter 14681/30000  loss         0.207156  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.538\n",
      "iter 14700/30000  loss         0.207133  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.540\n",
      "iter 14701/30000  loss         0.207132  avg_L1_norm_grad         0.000155  w[0]   -0.455 bias    3.541\n",
      "iter 14720/30000  loss         0.207110  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.543\n",
      "iter 14721/30000  loss         0.207108  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.543\n",
      "iter 14740/30000  loss         0.207086  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.545\n",
      "iter 14741/30000  loss         0.207085  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.545\n",
      "iter 14760/30000  loss         0.207062  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.547\n",
      "iter 14761/30000  loss         0.207061  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.547\n",
      "iter 14780/30000  loss         0.207039  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.550\n",
      "iter 14781/30000  loss         0.207037  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.550\n",
      "iter 14800/30000  loss         0.207015  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.552\n",
      "iter 14801/30000  loss         0.207014  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.552\n",
      "iter 14820/30000  loss         0.206991  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.554\n",
      "iter 14821/30000  loss         0.206990  avg_L1_norm_grad         0.000154  w[0]   -0.455 bias    3.554\n",
      "iter 14840/30000  loss         0.206968  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.557\n",
      "iter 14841/30000  loss         0.206967  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.557\n",
      "iter 14860/30000  loss         0.206945  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.559\n",
      "iter 14861/30000  loss         0.206943  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.559\n",
      "iter 14880/30000  loss         0.206921  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.561\n",
      "iter 14881/30000  loss         0.206920  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.561\n",
      "iter 14900/30000  loss         0.206898  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.563\n",
      "iter 14901/30000  loss         0.206897  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.564\n",
      "iter 14920/30000  loss         0.206875  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.566\n",
      "iter 14921/30000  loss         0.206874  avg_L1_norm_grad         0.000153  w[0]   -0.456 bias    3.566\n",
      "iter 14940/30000  loss         0.206852  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.568\n",
      "iter 14941/30000  loss         0.206850  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.568\n",
      "iter 14960/30000  loss         0.206828  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.570\n",
      "iter 14961/30000  loss         0.206827  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.570\n",
      "iter 14980/30000  loss         0.206805  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.573\n",
      "iter 14981/30000  loss         0.206804  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.573\n",
      "iter 15000/30000  loss         0.206782  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.575\n",
      "iter 15001/30000  loss         0.206781  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15020/30000  loss         0.206759  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.577\n",
      "iter 15021/30000  loss         0.206758  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.577\n",
      "iter 15040/30000  loss         0.206736  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.579\n",
      "iter 15041/30000  loss         0.206735  avg_L1_norm_grad         0.000152  w[0]   -0.456 bias    3.580\n",
      "iter 15060/30000  loss         0.206713  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.582\n",
      "iter 15061/30000  loss         0.206712  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.582\n",
      "iter 15080/30000  loss         0.206691  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.584\n",
      "iter 15081/30000  loss         0.206689  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.584\n",
      "iter 15100/30000  loss         0.206668  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.586\n",
      "iter 15101/30000  loss         0.206667  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.586\n",
      "iter 15120/30000  loss         0.206645  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.589\n",
      "iter 15121/30000  loss         0.206644  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.589\n",
      "iter 15140/30000  loss         0.206622  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.591\n",
      "iter 15141/30000  loss         0.206621  avg_L1_norm_grad         0.000151  w[0]   -0.456 bias    3.591\n",
      "iter 15160/30000  loss         0.206600  avg_L1_norm_grad         0.000151  w[0]   -0.457 bias    3.593\n",
      "iter 15161/30000  loss         0.206599  avg_L1_norm_grad         0.000151  w[0]   -0.457 bias    3.593\n",
      "iter 15180/30000  loss         0.206577  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.595\n",
      "iter 15181/30000  loss         0.206576  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.595\n",
      "iter 15200/30000  loss         0.206555  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.598\n",
      "iter 15201/30000  loss         0.206554  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.598\n",
      "iter 15220/30000  loss         0.206532  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.600\n",
      "iter 15221/30000  loss         0.206531  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.600\n",
      "iter 15240/30000  loss         0.206510  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.602\n",
      "iter 15241/30000  loss         0.206509  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.602\n",
      "iter 15260/30000  loss         0.206487  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.604\n",
      "iter 15261/30000  loss         0.206486  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.604\n",
      "iter 15280/30000  loss         0.206465  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.607\n",
      "iter 15281/30000  loss         0.206464  avg_L1_norm_grad         0.000150  w[0]   -0.457 bias    3.607\n",
      "iter 15300/30000  loss         0.206443  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.609\n",
      "iter 15301/30000  loss         0.206442  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.609\n",
      "iter 15320/30000  loss         0.206421  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.611\n",
      "iter 15321/30000  loss         0.206419  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.611\n",
      "iter 15340/30000  loss         0.206398  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.613\n",
      "iter 15341/30000  loss         0.206397  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.613\n",
      "iter 15360/30000  loss         0.206376  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.616\n",
      "iter 15361/30000  loss         0.206375  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.616\n",
      "iter 15380/30000  loss         0.206354  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.618\n",
      "iter 15381/30000  loss         0.206353  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.618\n",
      "iter 15400/30000  loss         0.206332  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.620\n",
      "iter 15401/30000  loss         0.206331  avg_L1_norm_grad         0.000149  w[0]   -0.457 bias    3.620\n",
      "iter 15420/30000  loss         0.206310  avg_L1_norm_grad         0.000148  w[0]   -0.457 bias    3.622\n",
      "iter 15421/30000  loss         0.206309  avg_L1_norm_grad         0.000148  w[0]   -0.457 bias    3.622\n",
      "iter 15440/30000  loss         0.206288  avg_L1_norm_grad         0.000148  w[0]   -0.457 bias    3.625\n",
      "iter 15441/30000  loss         0.206287  avg_L1_norm_grad         0.000148  w[0]   -0.457 bias    3.625\n",
      "iter 15460/30000  loss         0.206266  avg_L1_norm_grad         0.000148  w[0]   -0.457 bias    3.627\n",
      "iter 15461/30000  loss         0.206265  avg_L1_norm_grad         0.000148  w[0]   -0.457 bias    3.627\n",
      "iter 15480/30000  loss         0.206244  avg_L1_norm_grad         0.000148  w[0]   -0.458 bias    3.629\n",
      "iter 15481/30000  loss         0.206243  avg_L1_norm_grad         0.000148  w[0]   -0.458 bias    3.629\n",
      "iter 15500/30000  loss         0.206223  avg_L1_norm_grad         0.000148  w[0]   -0.458 bias    3.631\n",
      "iter 15501/30000  loss         0.206221  avg_L1_norm_grad         0.000148  w[0]   -0.458 bias    3.631\n",
      "iter 15520/30000  loss         0.206201  avg_L1_norm_grad         0.000148  w[0]   -0.458 bias    3.633\n",
      "iter 15521/30000  loss         0.206200  avg_L1_norm_grad         0.000148  w[0]   -0.458 bias    3.634\n",
      "iter 15540/30000  loss         0.206179  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.636\n",
      "iter 15541/30000  loss         0.206178  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.636\n",
      "iter 15560/30000  loss         0.206157  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.638\n",
      "iter 15561/30000  loss         0.206156  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.638\n",
      "iter 15580/30000  loss         0.206136  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.640\n",
      "iter 15581/30000  loss         0.206135  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.640\n",
      "iter 15600/30000  loss         0.206114  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.642\n",
      "iter 15601/30000  loss         0.206113  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.642\n",
      "iter 15620/30000  loss         0.206093  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.645\n",
      "iter 15621/30000  loss         0.206092  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.645\n",
      "iter 15640/30000  loss         0.206071  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.647\n",
      "iter 15641/30000  loss         0.206070  avg_L1_norm_grad         0.000147  w[0]   -0.458 bias    3.647\n",
      "iter 15660/30000  loss         0.206050  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.649\n",
      "iter 15661/30000  loss         0.206049  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.649\n",
      "iter 15680/30000  loss         0.206028  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.651\n",
      "iter 15681/30000  loss         0.206027  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.651\n",
      "iter 15700/30000  loss         0.206007  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.653\n",
      "iter 15701/30000  loss         0.206006  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.653\n",
      "iter 15720/30000  loss         0.205986  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.656\n",
      "iter 15721/30000  loss         0.205985  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.656\n",
      "iter 15740/30000  loss         0.205965  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.658\n",
      "iter 15741/30000  loss         0.205964  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.658\n",
      "iter 15760/30000  loss         0.205943  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.660\n",
      "iter 15761/30000  loss         0.205942  avg_L1_norm_grad         0.000146  w[0]   -0.458 bias    3.660\n",
      "iter 15780/30000  loss         0.205922  avg_L1_norm_grad         0.000145  w[0]   -0.458 bias    3.662\n",
      "iter 15781/30000  loss         0.205921  avg_L1_norm_grad         0.000145  w[0]   -0.458 bias    3.662\n",
      "iter 15800/30000  loss         0.205901  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.664\n",
      "iter 15801/30000  loss         0.205900  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15820/30000  loss         0.205880  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.667\n",
      "iter 15821/30000  loss         0.205879  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.667\n",
      "iter 15840/30000  loss         0.205859  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.669\n",
      "iter 15841/30000  loss         0.205858  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.669\n",
      "iter 15860/30000  loss         0.205838  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.671\n",
      "iter 15861/30000  loss         0.205837  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.671\n",
      "iter 15880/30000  loss         0.205817  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.673\n",
      "iter 15881/30000  loss         0.205816  avg_L1_norm_grad         0.000145  w[0]   -0.459 bias    3.673\n",
      "iter 15900/30000  loss         0.205796  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.675\n",
      "iter 15901/30000  loss         0.205795  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.675\n",
      "iter 15920/30000  loss         0.205775  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.677\n",
      "iter 15921/30000  loss         0.205774  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.678\n",
      "iter 15940/30000  loss         0.205755  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.680\n",
      "iter 15941/30000  loss         0.205754  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.680\n",
      "iter 15960/30000  loss         0.205734  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.682\n",
      "iter 15961/30000  loss         0.205733  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.682\n",
      "iter 15980/30000  loss         0.205713  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.684\n",
      "iter 15981/30000  loss         0.205712  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.684\n",
      "iter 16000/30000  loss         0.205692  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.686\n",
      "iter 16001/30000  loss         0.205691  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.686\n",
      "iter 16020/30000  loss         0.205672  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.688\n",
      "iter 16021/30000  loss         0.205671  avg_L1_norm_grad         0.000144  w[0]   -0.459 bias    3.688\n",
      "iter 16040/30000  loss         0.205651  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.691\n",
      "iter 16041/30000  loss         0.205650  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.691\n",
      "iter 16060/30000  loss         0.205631  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.693\n",
      "iter 16061/30000  loss         0.205630  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.693\n",
      "iter 16080/30000  loss         0.205610  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.695\n",
      "iter 16081/30000  loss         0.205609  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.695\n",
      "iter 16100/30000  loss         0.205590  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.697\n",
      "iter 16101/30000  loss         0.205589  avg_L1_norm_grad         0.000143  w[0]   -0.459 bias    3.697\n",
      "iter 16120/30000  loss         0.205569  avg_L1_norm_grad         0.000143  w[0]   -0.460 bias    3.699\n",
      "iter 16121/30000  loss         0.205568  avg_L1_norm_grad         0.000143  w[0]   -0.460 bias    3.699\n",
      "iter 16140/30000  loss         0.205549  avg_L1_norm_grad         0.000143  w[0]   -0.460 bias    3.701\n",
      "iter 16141/30000  loss         0.205548  avg_L1_norm_grad         0.000143  w[0]   -0.460 bias    3.701\n",
      "iter 16160/30000  loss         0.205529  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.704\n",
      "iter 16161/30000  loss         0.205528  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.704\n",
      "iter 16180/30000  loss         0.205508  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.706\n",
      "iter 16181/30000  loss         0.205507  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.706\n",
      "iter 16200/30000  loss         0.205488  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.708\n",
      "iter 16201/30000  loss         0.205487  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.708\n",
      "iter 16220/30000  loss         0.205468  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.710\n",
      "iter 16221/30000  loss         0.205467  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.710\n",
      "iter 16240/30000  loss         0.205448  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.712\n",
      "iter 16241/30000  loss         0.205447  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.712\n",
      "iter 16260/30000  loss         0.205428  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.714\n",
      "iter 16261/30000  loss         0.205427  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.714\n",
      "iter 16280/30000  loss         0.205408  avg_L1_norm_grad         0.000142  w[0]   -0.460 bias    3.716\n",
      "iter 16281/30000  loss         0.205407  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.717\n",
      "iter 16300/30000  loss         0.205388  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.719\n",
      "iter 16301/30000  loss         0.205387  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.719\n",
      "iter 16320/30000  loss         0.205368  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.721\n",
      "iter 16321/30000  loss         0.205367  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.721\n",
      "iter 16340/30000  loss         0.205348  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.723\n",
      "iter 16341/30000  loss         0.205347  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.723\n",
      "iter 16360/30000  loss         0.205328  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.725\n",
      "iter 16361/30000  loss         0.205327  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.725\n",
      "iter 16380/30000  loss         0.205308  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.727\n",
      "iter 16381/30000  loss         0.205307  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.727\n",
      "iter 16400/30000  loss         0.205288  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.729\n",
      "iter 16401/30000  loss         0.205287  avg_L1_norm_grad         0.000141  w[0]   -0.460 bias    3.729\n",
      "iter 16420/30000  loss         0.205268  avg_L1_norm_grad         0.000140  w[0]   -0.460 bias    3.731\n",
      "iter 16421/30000  loss         0.205267  avg_L1_norm_grad         0.000140  w[0]   -0.460 bias    3.731\n",
      "iter 16440/30000  loss         0.205249  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.733\n",
      "iter 16441/30000  loss         0.205248  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.734\n",
      "iter 16460/30000  loss         0.205229  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.736\n",
      "iter 16461/30000  loss         0.205228  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.736\n",
      "iter 16480/30000  loss         0.205209  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.738\n",
      "iter 16481/30000  loss         0.205208  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.738\n",
      "iter 16500/30000  loss         0.205190  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.740\n",
      "iter 16501/30000  loss         0.205189  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.740\n",
      "iter 16520/30000  loss         0.205170  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.742\n",
      "iter 16521/30000  loss         0.205169  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.742\n",
      "iter 16540/30000  loss         0.205151  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.744\n",
      "iter 16541/30000  loss         0.205150  avg_L1_norm_grad         0.000140  w[0]   -0.461 bias    3.744\n",
      "iter 16560/30000  loss         0.205131  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.746\n",
      "iter 16561/30000  loss         0.205130  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.746\n",
      "iter 16580/30000  loss         0.205112  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.748\n",
      "iter 16581/30000  loss         0.205111  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.748\n",
      "iter 16600/30000  loss         0.205092  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.750\n",
      "iter 16601/30000  loss         0.205091  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16620/30000  loss         0.205073  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.753\n",
      "iter 16621/30000  loss         0.205072  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.753\n",
      "iter 16640/30000  loss         0.205054  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.755\n",
      "iter 16641/30000  loss         0.205053  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.755\n",
      "iter 16660/30000  loss         0.205034  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.757\n",
      "iter 16661/30000  loss         0.205033  avg_L1_norm_grad         0.000139  w[0]   -0.461 bias    3.757\n",
      "iter 16680/30000  loss         0.205015  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.759\n",
      "iter 16681/30000  loss         0.205014  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.759\n",
      "iter 16700/30000  loss         0.204996  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.761\n",
      "iter 16701/30000  loss         0.204995  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.761\n",
      "iter 16720/30000  loss         0.204977  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.763\n",
      "iter 16721/30000  loss         0.204976  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.763\n",
      "iter 16740/30000  loss         0.204958  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.765\n",
      "iter 16741/30000  loss         0.204957  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.765\n",
      "iter 16760/30000  loss         0.204939  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.767\n",
      "iter 16761/30000  loss         0.204938  avg_L1_norm_grad         0.000138  w[0]   -0.461 bias    3.767\n",
      "iter 16780/30000  loss         0.204920  avg_L1_norm_grad         0.000138  w[0]   -0.462 bias    3.769\n",
      "iter 16781/30000  loss         0.204919  avg_L1_norm_grad         0.000138  w[0]   -0.462 bias    3.769\n",
      "iter 16800/30000  loss         0.204901  avg_L1_norm_grad         0.000138  w[0]   -0.462 bias    3.771\n",
      "iter 16801/30000  loss         0.204900  avg_L1_norm_grad         0.000138  w[0]   -0.462 bias    3.772\n",
      "iter 16820/30000  loss         0.204882  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.774\n",
      "iter 16821/30000  loss         0.204881  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.774\n",
      "iter 16840/30000  loss         0.204863  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.776\n",
      "iter 16841/30000  loss         0.204862  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.776\n",
      "iter 16860/30000  loss         0.204844  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.778\n",
      "iter 16861/30000  loss         0.204843  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.778\n",
      "iter 16880/30000  loss         0.204825  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.780\n",
      "iter 16881/30000  loss         0.204824  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.780\n",
      "iter 16900/30000  loss         0.204806  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.782\n",
      "iter 16901/30000  loss         0.204805  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.782\n",
      "iter 16920/30000  loss         0.204787  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.784\n",
      "iter 16921/30000  loss         0.204786  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.784\n",
      "iter 16940/30000  loss         0.204769  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.786\n",
      "iter 16941/30000  loss         0.204768  avg_L1_norm_grad         0.000137  w[0]   -0.462 bias    3.786\n",
      "iter 16960/30000  loss         0.204750  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.788\n",
      "iter 16961/30000  loss         0.204749  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.788\n",
      "iter 16980/30000  loss         0.204731  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.790\n",
      "iter 16981/30000  loss         0.204730  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.790\n",
      "iter 17000/30000  loss         0.204713  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.792\n",
      "iter 17001/30000  loss         0.204712  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.792\n",
      "iter 17020/30000  loss         0.204694  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.794\n",
      "iter 17021/30000  loss         0.204693  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.794\n",
      "iter 17040/30000  loss         0.204675  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.796\n",
      "iter 17041/30000  loss         0.204674  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.796\n",
      "iter 17060/30000  loss         0.204657  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.798\n",
      "iter 17061/30000  loss         0.204656  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.799\n",
      "iter 17080/30000  loss         0.204638  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.801\n",
      "iter 17081/30000  loss         0.204637  avg_L1_norm_grad         0.000136  w[0]   -0.462 bias    3.801\n",
      "iter 17100/30000  loss         0.204620  avg_L1_norm_grad         0.000135  w[0]   -0.462 bias    3.803\n",
      "iter 17101/30000  loss         0.204619  avg_L1_norm_grad         0.000135  w[0]   -0.462 bias    3.803\n",
      "iter 17120/30000  loss         0.204602  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.805\n",
      "iter 17121/30000  loss         0.204601  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.805\n",
      "iter 17140/30000  loss         0.204583  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.807\n",
      "iter 17141/30000  loss         0.204582  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.807\n",
      "iter 17160/30000  loss         0.204565  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.809\n",
      "iter 17161/30000  loss         0.204564  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.809\n",
      "iter 17180/30000  loss         0.204547  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.811\n",
      "iter 17181/30000  loss         0.204546  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.811\n",
      "iter 17200/30000  loss         0.204528  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.813\n",
      "iter 17201/30000  loss         0.204527  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.813\n",
      "iter 17220/30000  loss         0.204510  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.815\n",
      "iter 17221/30000  loss         0.204509  avg_L1_norm_grad         0.000135  w[0]   -0.463 bias    3.815\n",
      "iter 17240/30000  loss         0.204492  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.817\n",
      "iter 17241/30000  loss         0.204491  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.817\n",
      "iter 17260/30000  loss         0.204474  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.819\n",
      "iter 17261/30000  loss         0.204473  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.819\n",
      "iter 17280/30000  loss         0.204456  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.821\n",
      "iter 17281/30000  loss         0.204455  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.821\n",
      "iter 17300/30000  loss         0.204438  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.823\n",
      "iter 17301/30000  loss         0.204437  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.823\n",
      "iter 17320/30000  loss         0.204420  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.825\n",
      "iter 17321/30000  loss         0.204419  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.825\n",
      "iter 17340/30000  loss         0.204402  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.827\n",
      "iter 17341/30000  loss         0.204401  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.827\n",
      "iter 17360/30000  loss         0.204384  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.829\n",
      "iter 17361/30000  loss         0.204383  avg_L1_norm_grad         0.000134  w[0]   -0.463 bias    3.829\n",
      "iter 17380/30000  loss         0.204366  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.831\n",
      "iter 17381/30000  loss         0.204365  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.831\n",
      "iter 17400/30000  loss         0.204348  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.833\n",
      "iter 17401/30000  loss         0.204347  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17420/30000  loss         0.204330  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.835\n",
      "iter 17421/30000  loss         0.204329  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.835\n",
      "iter 17440/30000  loss         0.204312  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.837\n",
      "iter 17441/30000  loss         0.204311  avg_L1_norm_grad         0.000133  w[0]   -0.463 bias    3.837\n",
      "iter 17460/30000  loss         0.204294  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.839\n",
      "iter 17461/30000  loss         0.204293  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.839\n",
      "iter 17480/30000  loss         0.204277  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.841\n",
      "iter 17481/30000  loss         0.204276  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.842\n",
      "iter 17500/30000  loss         0.204259  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.843\n",
      "iter 17501/30000  loss         0.204258  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.844\n",
      "iter 17520/30000  loss         0.204241  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.845\n",
      "iter 17521/30000  loss         0.204240  avg_L1_norm_grad         0.000133  w[0]   -0.464 bias    3.846\n",
      "iter 17540/30000  loss         0.204224  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.847\n",
      "iter 17541/30000  loss         0.204223  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.848\n",
      "iter 17560/30000  loss         0.204206  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.849\n",
      "iter 17561/30000  loss         0.204205  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.850\n",
      "iter 17580/30000  loss         0.204188  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.852\n",
      "iter 17581/30000  loss         0.204187  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.852\n",
      "iter 17600/30000  loss         0.204171  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.854\n",
      "iter 17601/30000  loss         0.204170  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.854\n",
      "iter 17620/30000  loss         0.204153  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.856\n",
      "iter 17621/30000  loss         0.204152  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.856\n",
      "iter 17640/30000  loss         0.204136  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.858\n",
      "iter 17641/30000  loss         0.204135  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.858\n",
      "iter 17660/30000  loss         0.204118  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.860\n",
      "iter 17661/30000  loss         0.204118  avg_L1_norm_grad         0.000132  w[0]   -0.464 bias    3.860\n",
      "iter 17680/30000  loss         0.204101  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.862\n",
      "iter 17681/30000  loss         0.204100  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.862\n",
      "iter 17700/30000  loss         0.204084  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.864\n",
      "iter 17701/30000  loss         0.204083  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.864\n",
      "iter 17720/30000  loss         0.204066  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.866\n",
      "iter 17721/30000  loss         0.204065  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.866\n",
      "iter 17740/30000  loss         0.204049  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.868\n",
      "iter 17741/30000  loss         0.204048  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.868\n",
      "iter 17760/30000  loss         0.204032  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.870\n",
      "iter 17761/30000  loss         0.204031  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.870\n",
      "iter 17780/30000  loss         0.204015  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.872\n",
      "iter 17781/30000  loss         0.204014  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.872\n",
      "iter 17800/30000  loss         0.203997  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.874\n",
      "iter 17801/30000  loss         0.203996  avg_L1_norm_grad         0.000131  w[0]   -0.464 bias    3.874\n",
      "iter 17820/30000  loss         0.203980  avg_L1_norm_grad         0.000131  w[0]   -0.465 bias    3.876\n",
      "iter 17821/30000  loss         0.203979  avg_L1_norm_grad         0.000131  w[0]   -0.465 bias    3.876\n",
      "iter 17840/30000  loss         0.203963  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.878\n",
      "iter 17841/30000  loss         0.203962  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.878\n",
      "iter 17860/30000  loss         0.203946  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.880\n",
      "iter 17861/30000  loss         0.203945  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.880\n",
      "iter 17880/30000  loss         0.203929  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.882\n",
      "iter 17881/30000  loss         0.203928  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.882\n",
      "iter 17900/30000  loss         0.203912  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.884\n",
      "iter 17901/30000  loss         0.203911  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.884\n",
      "iter 17920/30000  loss         0.203895  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.886\n",
      "iter 17921/30000  loss         0.203894  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.886\n",
      "iter 17940/30000  loss         0.203878  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.888\n",
      "iter 17941/30000  loss         0.203877  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.888\n",
      "iter 17960/30000  loss         0.203861  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.890\n",
      "iter 17961/30000  loss         0.203860  avg_L1_norm_grad         0.000130  w[0]   -0.465 bias    3.890\n",
      "iter 17980/30000  loss         0.203844  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.891\n",
      "iter 17981/30000  loss         0.203843  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.892\n",
      "iter 18000/30000  loss         0.203827  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.893\n",
      "iter 18001/30000  loss         0.203826  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.894\n",
      "iter 18020/30000  loss         0.203810  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.895\n",
      "iter 18021/30000  loss         0.203810  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.896\n",
      "iter 18040/30000  loss         0.203794  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.897\n",
      "iter 18041/30000  loss         0.203793  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.898\n",
      "iter 18060/30000  loss         0.203777  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.899\n",
      "iter 18061/30000  loss         0.203776  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.900\n",
      "iter 18080/30000  loss         0.203760  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.901\n",
      "iter 18081/30000  loss         0.203759  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.901\n",
      "iter 18100/30000  loss         0.203743  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.903\n",
      "iter 18101/30000  loss         0.203743  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.903\n",
      "iter 18120/30000  loss         0.203727  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.905\n",
      "iter 18121/30000  loss         0.203726  avg_L1_norm_grad         0.000129  w[0]   -0.465 bias    3.905\n",
      "iter 18140/30000  loss         0.203710  avg_L1_norm_grad         0.000128  w[0]   -0.465 bias    3.907\n",
      "iter 18141/30000  loss         0.203709  avg_L1_norm_grad         0.000128  w[0]   -0.465 bias    3.907\n",
      "iter 18160/30000  loss         0.203693  avg_L1_norm_grad         0.000128  w[0]   -0.465 bias    3.909\n",
      "iter 18161/30000  loss         0.203693  avg_L1_norm_grad         0.000128  w[0]   -0.465 bias    3.909\n",
      "iter 18180/30000  loss         0.203677  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.911\n",
      "iter 18181/30000  loss         0.203676  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.911\n",
      "iter 18200/30000  loss         0.203660  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.913\n",
      "iter 18201/30000  loss         0.203660  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18220/30000  loss         0.203644  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.915\n",
      "iter 18221/30000  loss         0.203643  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.915\n",
      "iter 18240/30000  loss         0.203627  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.917\n",
      "iter 18241/30000  loss         0.203627  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.917\n",
      "iter 18260/30000  loss         0.203611  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.919\n",
      "iter 18261/30000  loss         0.203610  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.919\n",
      "iter 18280/30000  loss         0.203594  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.921\n",
      "iter 18281/30000  loss         0.203594  avg_L1_norm_grad         0.000128  w[0]   -0.466 bias    3.921\n",
      "iter 18300/30000  loss         0.203578  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.923\n",
      "iter 18301/30000  loss         0.203577  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.923\n",
      "iter 18320/30000  loss         0.203562  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.925\n",
      "iter 18321/30000  loss         0.203561  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.925\n",
      "iter 18340/30000  loss         0.203545  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.927\n",
      "iter 18341/30000  loss         0.203545  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.927\n",
      "iter 18360/30000  loss         0.203529  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.929\n",
      "iter 18361/30000  loss         0.203528  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.929\n",
      "iter 18380/30000  loss         0.203513  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.931\n",
      "iter 18381/30000  loss         0.203512  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.931\n",
      "iter 18400/30000  loss         0.203497  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.933\n",
      "iter 18401/30000  loss         0.203496  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.933\n",
      "iter 18420/30000  loss         0.203480  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.935\n",
      "iter 18421/30000  loss         0.203480  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.935\n",
      "iter 18440/30000  loss         0.203464  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.937\n",
      "iter 18441/30000  loss         0.203464  avg_L1_norm_grad         0.000127  w[0]   -0.466 bias    3.937\n",
      "iter 18460/30000  loss         0.203448  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.939\n",
      "iter 18461/30000  loss         0.203447  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.939\n",
      "iter 18480/30000  loss         0.203432  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.940\n",
      "iter 18481/30000  loss         0.203431  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.941\n",
      "iter 18500/30000  loss         0.203416  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.942\n",
      "iter 18501/30000  loss         0.203415  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.943\n",
      "iter 18520/30000  loss         0.203400  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.944\n",
      "iter 18521/30000  loss         0.203399  avg_L1_norm_grad         0.000126  w[0]   -0.466 bias    3.944\n",
      "iter 18540/30000  loss         0.203384  avg_L1_norm_grad         0.000126  w[0]   -0.467 bias    3.946\n",
      "iter 18541/30000  loss         0.203383  avg_L1_norm_grad         0.000126  w[0]   -0.467 bias    3.946\n",
      "iter 18560/30000  loss         0.203368  avg_L1_norm_grad         0.000126  w[0]   -0.467 bias    3.948\n",
      "iter 18561/30000  loss         0.203367  avg_L1_norm_grad         0.000126  w[0]   -0.467 bias    3.948\n",
      "iter 18580/30000  loss         0.203352  avg_L1_norm_grad         0.000126  w[0]   -0.467 bias    3.950\n",
      "iter 18581/30000  loss         0.203351  avg_L1_norm_grad         0.000126  w[0]   -0.467 bias    3.950\n",
      "iter 18600/30000  loss         0.203336  avg_L1_norm_grad         0.000126  w[0]   -0.467 bias    3.952\n",
      "iter 18601/30000  loss         0.203335  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.952\n",
      "iter 18620/30000  loss         0.203320  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.954\n",
      "iter 18621/30000  loss         0.203319  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.954\n",
      "iter 18640/30000  loss         0.203304  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.956\n",
      "iter 18641/30000  loss         0.203304  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.956\n",
      "iter 18660/30000  loss         0.203289  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.958\n",
      "iter 18661/30000  loss         0.203288  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.958\n",
      "iter 18680/30000  loss         0.203273  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.960\n",
      "iter 18681/30000  loss         0.203272  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.960\n",
      "iter 18700/30000  loss         0.203257  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.962\n",
      "iter 18701/30000  loss         0.203256  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.962\n",
      "iter 18720/30000  loss         0.203241  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.964\n",
      "iter 18721/30000  loss         0.203240  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.964\n",
      "iter 18740/30000  loss         0.203226  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.966\n",
      "iter 18741/30000  loss         0.203225  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.966\n",
      "iter 18760/30000  loss         0.203210  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.967\n",
      "iter 18761/30000  loss         0.203209  avg_L1_norm_grad         0.000125  w[0]   -0.467 bias    3.968\n",
      "iter 18780/30000  loss         0.203194  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.969\n",
      "iter 18781/30000  loss         0.203193  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.969\n",
      "iter 18800/30000  loss         0.203179  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.971\n",
      "iter 18801/30000  loss         0.203178  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.971\n",
      "iter 18820/30000  loss         0.203163  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.973\n",
      "iter 18821/30000  loss         0.203162  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.973\n",
      "iter 18840/30000  loss         0.203147  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.975\n",
      "iter 18841/30000  loss         0.203147  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.975\n",
      "iter 18860/30000  loss         0.203132  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.977\n",
      "iter 18861/30000  loss         0.203131  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.977\n",
      "iter 18880/30000  loss         0.203116  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.979\n",
      "iter 18881/30000  loss         0.203116  avg_L1_norm_grad         0.000124  w[0]   -0.467 bias    3.979\n",
      "iter 18900/30000  loss         0.203101  avg_L1_norm_grad         0.000124  w[0]   -0.468 bias    3.981\n",
      "iter 18901/30000  loss         0.203100  avg_L1_norm_grad         0.000124  w[0]   -0.468 bias    3.981\n",
      "iter 18920/30000  loss         0.203085  avg_L1_norm_grad         0.000124  w[0]   -0.468 bias    3.983\n",
      "iter 18921/30000  loss         0.203085  avg_L1_norm_grad         0.000124  w[0]   -0.468 bias    3.983\n",
      "iter 18940/30000  loss         0.203070  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.985\n",
      "iter 18941/30000  loss         0.203069  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.985\n",
      "iter 18960/30000  loss         0.203055  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.986\n",
      "iter 18961/30000  loss         0.203054  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.987\n",
      "iter 18980/30000  loss         0.203039  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.988\n",
      "iter 18981/30000  loss         0.203039  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.988\n",
      "iter 19000/30000  loss         0.203024  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.990\n",
      "iter 19001/30000  loss         0.203023  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19020/30000  loss         0.203009  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.992\n",
      "iter 19021/30000  loss         0.203008  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.992\n",
      "iter 19040/30000  loss         0.202993  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.994\n",
      "iter 19041/30000  loss         0.202993  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.994\n",
      "iter 19060/30000  loss         0.202978  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.996\n",
      "iter 19061/30000  loss         0.202977  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.996\n",
      "iter 19080/30000  loss         0.202963  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.998\n",
      "iter 19081/30000  loss         0.202962  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    3.998\n",
      "iter 19100/30000  loss         0.202948  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    4.000\n",
      "iter 19101/30000  loss         0.202947  avg_L1_norm_grad         0.000123  w[0]   -0.468 bias    4.000\n",
      "iter 19120/30000  loss         0.202933  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.002\n",
      "iter 19121/30000  loss         0.202932  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.002\n",
      "iter 19140/30000  loss         0.202917  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.004\n",
      "iter 19141/30000  loss         0.202917  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.004\n",
      "iter 19160/30000  loss         0.202902  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.005\n",
      "iter 19161/30000  loss         0.202902  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.005\n",
      "iter 19180/30000  loss         0.202887  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.007\n",
      "iter 19181/30000  loss         0.202887  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.007\n",
      "iter 19200/30000  loss         0.202872  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.009\n",
      "iter 19201/30000  loss         0.202871  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.009\n",
      "iter 19220/30000  loss         0.202857  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.011\n",
      "iter 19221/30000  loss         0.202856  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.011\n",
      "iter 19240/30000  loss         0.202842  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.013\n",
      "iter 19241/30000  loss         0.202841  avg_L1_norm_grad         0.000122  w[0]   -0.468 bias    4.013\n",
      "iter 19260/30000  loss         0.202827  avg_L1_norm_grad         0.000122  w[0]   -0.469 bias    4.015\n",
      "iter 19261/30000  loss         0.202827  avg_L1_norm_grad         0.000122  w[0]   -0.469 bias    4.015\n",
      "iter 19280/30000  loss         0.202812  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.017\n",
      "iter 19281/30000  loss         0.202812  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.017\n",
      "iter 19300/30000  loss         0.202797  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.019\n",
      "iter 19301/30000  loss         0.202797  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.019\n",
      "iter 19320/30000  loss         0.202783  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.020\n",
      "iter 19321/30000  loss         0.202782  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.020\n",
      "iter 19340/30000  loss         0.202768  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.022\n",
      "iter 19341/30000  loss         0.202767  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.022\n",
      "iter 19360/30000  loss         0.202753  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.024\n",
      "iter 19361/30000  loss         0.202752  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.024\n",
      "iter 19380/30000  loss         0.202738  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.026\n",
      "iter 19381/30000  loss         0.202737  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.026\n",
      "iter 19400/30000  loss         0.202723  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.028\n",
      "iter 19401/30000  loss         0.202723  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.028\n",
      "iter 19420/30000  loss         0.202709  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.030\n",
      "iter 19421/30000  loss         0.202708  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.030\n",
      "iter 19440/30000  loss         0.202694  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.032\n",
      "iter 19441/30000  loss         0.202693  avg_L1_norm_grad         0.000121  w[0]   -0.469 bias    4.032\n",
      "iter 19460/30000  loss         0.202679  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.033\n",
      "iter 19461/30000  loss         0.202678  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.034\n",
      "iter 19480/30000  loss         0.202665  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.035\n",
      "iter 19481/30000  loss         0.202664  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.035\n",
      "iter 19500/30000  loss         0.202650  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.037\n",
      "iter 19501/30000  loss         0.202649  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.037\n",
      "iter 19520/30000  loss         0.202635  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.039\n",
      "iter 19521/30000  loss         0.202635  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.039\n",
      "iter 19540/30000  loss         0.202621  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.041\n",
      "iter 19541/30000  loss         0.202620  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.041\n",
      "iter 19560/30000  loss         0.202606  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.043\n",
      "iter 19561/30000  loss         0.202605  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.043\n",
      "iter 19580/30000  loss         0.202592  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.045\n",
      "iter 19581/30000  loss         0.202591  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.045\n",
      "iter 19600/30000  loss         0.202577  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.046\n",
      "iter 19601/30000  loss         0.202576  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.046\n",
      "iter 19620/30000  loss         0.202563  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.048\n",
      "iter 19621/30000  loss         0.202562  avg_L1_norm_grad         0.000120  w[0]   -0.469 bias    4.048\n",
      "iter 19640/30000  loss         0.202548  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.050\n",
      "iter 19641/30000  loss         0.202547  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.050\n",
      "iter 19660/30000  loss         0.202534  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.052\n",
      "iter 19661/30000  loss         0.202533  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.052\n",
      "iter 19680/30000  loss         0.202519  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.054\n",
      "iter 19681/30000  loss         0.202519  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.054\n",
      "iter 19700/30000  loss         0.202505  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.056\n",
      "iter 19701/30000  loss         0.202504  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.056\n",
      "iter 19720/30000  loss         0.202491  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.057\n",
      "iter 19721/30000  loss         0.202490  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.058\n",
      "iter 19740/30000  loss         0.202476  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.059\n",
      "iter 19741/30000  loss         0.202476  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.059\n",
      "iter 19760/30000  loss         0.202462  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.061\n",
      "iter 19761/30000  loss         0.202461  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.061\n",
      "iter 19780/30000  loss         0.202448  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.063\n",
      "iter 19781/30000  loss         0.202447  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.063\n",
      "iter 19800/30000  loss         0.202434  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.065\n",
      "iter 19801/30000  loss         0.202433  avg_L1_norm_grad         0.000119  w[0]   -0.470 bias    4.065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19820/30000  loss         0.202419  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.067\n",
      "iter 19821/30000  loss         0.202419  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.067\n",
      "iter 19840/30000  loss         0.202405  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.068\n",
      "iter 19841/30000  loss         0.202404  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.069\n",
      "iter 19860/30000  loss         0.202391  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.070\n",
      "iter 19861/30000  loss         0.202390  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.070\n",
      "iter 19880/30000  loss         0.202377  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.072\n",
      "iter 19881/30000  loss         0.202376  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.072\n",
      "iter 19900/30000  loss         0.202363  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.074\n",
      "iter 19901/30000  loss         0.202362  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.074\n",
      "iter 19920/30000  loss         0.202349  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.076\n",
      "iter 19921/30000  loss         0.202348  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.076\n",
      "iter 19940/30000  loss         0.202335  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.078\n",
      "iter 19941/30000  loss         0.202334  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.078\n",
      "iter 19960/30000  loss         0.202321  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.079\n",
      "iter 19961/30000  loss         0.202320  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.079\n",
      "iter 19980/30000  loss         0.202307  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.081\n",
      "iter 19981/30000  loss         0.202306  avg_L1_norm_grad         0.000118  w[0]   -0.470 bias    4.081\n",
      "iter 20000/30000  loss         0.202293  avg_L1_norm_grad         0.000117  w[0]   -0.470 bias    4.083\n",
      "iter 20001/30000  loss         0.202292  avg_L1_norm_grad         0.000117  w[0]   -0.470 bias    4.083\n",
      "iter 20020/30000  loss         0.202279  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.085\n",
      "iter 20021/30000  loss         0.202278  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.085\n",
      "iter 20040/30000  loss         0.202265  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.087\n",
      "iter 20041/30000  loss         0.202264  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.087\n",
      "iter 20060/30000  loss         0.202251  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.088\n",
      "iter 20061/30000  loss         0.202250  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.089\n",
      "iter 20080/30000  loss         0.202237  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.090\n",
      "iter 20081/30000  loss         0.202236  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.090\n",
      "iter 20100/30000  loss         0.202223  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.092\n",
      "iter 20101/30000  loss         0.202222  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.092\n",
      "iter 20120/30000  loss         0.202209  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.094\n",
      "iter 20121/30000  loss         0.202209  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.094\n",
      "iter 20140/30000  loss         0.202195  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.096\n",
      "iter 20141/30000  loss         0.202195  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.096\n",
      "iter 20160/30000  loss         0.202182  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.097\n",
      "iter 20161/30000  loss         0.202181  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.098\n",
      "iter 20180/30000  loss         0.202168  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.099\n",
      "iter 20181/30000  loss         0.202167  avg_L1_norm_grad         0.000117  w[0]   -0.471 bias    4.099\n",
      "iter 20200/30000  loss         0.202154  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.101\n",
      "iter 20201/30000  loss         0.202153  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.101\n",
      "iter 20220/30000  loss         0.202140  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.103\n",
      "iter 20221/30000  loss         0.202140  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.103\n",
      "iter 20240/30000  loss         0.202127  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.105\n",
      "iter 20241/30000  loss         0.202126  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.105\n",
      "iter 20260/30000  loss         0.202113  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.106\n",
      "iter 20261/30000  loss         0.202112  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.107\n",
      "iter 20280/30000  loss         0.202099  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.108\n",
      "iter 20281/30000  loss         0.202099  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.108\n",
      "iter 20300/30000  loss         0.202086  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.110\n",
      "iter 20301/30000  loss         0.202085  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.110\n",
      "iter 20320/30000  loss         0.202072  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.112\n",
      "iter 20321/30000  loss         0.202072  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.112\n",
      "iter 20340/30000  loss         0.202059  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.114\n",
      "iter 20341/30000  loss         0.202058  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.114\n",
      "iter 20360/30000  loss         0.202045  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.115\n",
      "iter 20361/30000  loss         0.202044  avg_L1_norm_grad         0.000116  w[0]   -0.471 bias    4.116\n",
      "iter 20380/30000  loss         0.202032  avg_L1_norm_grad         0.000115  w[0]   -0.471 bias    4.117\n",
      "iter 20381/30000  loss         0.202031  avg_L1_norm_grad         0.000115  w[0]   -0.471 bias    4.117\n",
      "iter 20400/30000  loss         0.202018  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.119\n",
      "iter 20401/30000  loss         0.202017  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.119\n",
      "iter 20420/30000  loss         0.202005  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.121\n",
      "iter 20421/30000  loss         0.202004  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.121\n",
      "iter 20440/30000  loss         0.201991  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.123\n",
      "iter 20441/30000  loss         0.201991  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.123\n",
      "iter 20460/30000  loss         0.201978  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.124\n",
      "iter 20461/30000  loss         0.201977  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.124\n",
      "iter 20480/30000  loss         0.201964  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.126\n",
      "iter 20481/30000  loss         0.201964  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.126\n",
      "iter 20500/30000  loss         0.201951  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.128\n",
      "iter 20501/30000  loss         0.201950  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.128\n",
      "iter 20520/30000  loss         0.201938  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.130\n",
      "iter 20521/30000  loss         0.201937  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.130\n",
      "iter 20540/30000  loss         0.201924  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.131\n",
      "iter 20541/30000  loss         0.201924  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.132\n",
      "iter 20560/30000  loss         0.201911  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.133\n",
      "iter 20561/30000  loss         0.201910  avg_L1_norm_grad         0.000115  w[0]   -0.472 bias    4.133\n",
      "iter 20580/30000  loss         0.201898  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.135\n",
      "iter 20581/30000  loss         0.201897  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.135\n",
      "iter 20600/30000  loss         0.201885  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.137\n",
      "iter 20601/30000  loss         0.201884  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20620/30000  loss         0.201871  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.139\n",
      "iter 20621/30000  loss         0.201871  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.139\n",
      "iter 20640/30000  loss         0.201858  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.140\n",
      "iter 20641/30000  loss         0.201857  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.140\n",
      "iter 20660/30000  loss         0.201845  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.142\n",
      "iter 20661/30000  loss         0.201844  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.142\n",
      "iter 20680/30000  loss         0.201832  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.144\n",
      "iter 20681/30000  loss         0.201831  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.144\n",
      "iter 20700/30000  loss         0.201819  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.146\n",
      "iter 20701/30000  loss         0.201818  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.146\n",
      "iter 20720/30000  loss         0.201806  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.147\n",
      "iter 20721/30000  loss         0.201805  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.147\n",
      "iter 20740/30000  loss         0.201792  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.149\n",
      "iter 20741/30000  loss         0.201792  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.149\n",
      "iter 20760/30000  loss         0.201779  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.151\n",
      "iter 20761/30000  loss         0.201779  avg_L1_norm_grad         0.000114  w[0]   -0.472 bias    4.151\n",
      "iter 20780/30000  loss         0.201766  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.153\n",
      "iter 20781/30000  loss         0.201766  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.153\n",
      "iter 20800/30000  loss         0.201753  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.154\n",
      "iter 20801/30000  loss         0.201753  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.155\n",
      "iter 20820/30000  loss         0.201740  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.156\n",
      "iter 20821/30000  loss         0.201740  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.156\n",
      "iter 20840/30000  loss         0.201727  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.158\n",
      "iter 20841/30000  loss         0.201727  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.158\n",
      "iter 20860/30000  loss         0.201714  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.160\n",
      "iter 20861/30000  loss         0.201714  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.160\n",
      "iter 20880/30000  loss         0.201701  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.161\n",
      "iter 20881/30000  loss         0.201701  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.162\n",
      "iter 20900/30000  loss         0.201689  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.163\n",
      "iter 20901/30000  loss         0.201688  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.163\n",
      "iter 20920/30000  loss         0.201676  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.165\n",
      "iter 20921/30000  loss         0.201675  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.165\n",
      "iter 20940/30000  loss         0.201663  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.167\n",
      "iter 20941/30000  loss         0.201662  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.167\n",
      "iter 20960/30000  loss         0.201650  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.168\n",
      "iter 20961/30000  loss         0.201649  avg_L1_norm_grad         0.000113  w[0]   -0.473 bias    4.169\n",
      "iter 20980/30000  loss         0.201637  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.170\n",
      "iter 20981/30000  loss         0.201637  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.170\n",
      "iter 21000/30000  loss         0.201624  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.172\n",
      "iter 21001/30000  loss         0.201624  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.172\n",
      "iter 21020/30000  loss         0.201612  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.174\n",
      "iter 21021/30000  loss         0.201611  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.174\n",
      "iter 21040/30000  loss         0.201599  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.175\n",
      "iter 21041/30000  loss         0.201598  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.175\n",
      "iter 21060/30000  loss         0.201586  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.177\n",
      "iter 21061/30000  loss         0.201586  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.177\n",
      "iter 21080/30000  loss         0.201573  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.179\n",
      "iter 21081/30000  loss         0.201573  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.179\n",
      "iter 21100/30000  loss         0.201561  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.181\n",
      "iter 21101/30000  loss         0.201560  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.181\n",
      "iter 21120/30000  loss         0.201548  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.182\n",
      "iter 21121/30000  loss         0.201547  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.182\n",
      "iter 21140/30000  loss         0.201535  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.184\n",
      "iter 21141/30000  loss         0.201535  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.184\n",
      "iter 21160/30000  loss         0.201523  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.186\n",
      "iter 21161/30000  loss         0.201522  avg_L1_norm_grad         0.000112  w[0]   -0.473 bias    4.186\n",
      "iter 21180/30000  loss         0.201510  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.188\n",
      "iter 21181/30000  loss         0.201510  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.188\n",
      "iter 21200/30000  loss         0.201498  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.189\n",
      "iter 21201/30000  loss         0.201497  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.189\n",
      "iter 21220/30000  loss         0.201485  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.191\n",
      "iter 21221/30000  loss         0.201484  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.191\n",
      "iter 21240/30000  loss         0.201473  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.193\n",
      "iter 21241/30000  loss         0.201472  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.193\n",
      "iter 21260/30000  loss         0.201460  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.194\n",
      "iter 21261/30000  loss         0.201459  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.195\n",
      "iter 21280/30000  loss         0.201448  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.196\n",
      "iter 21281/30000  loss         0.201447  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.196\n",
      "iter 21300/30000  loss         0.201435  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.198\n",
      "iter 21301/30000  loss         0.201435  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.198\n",
      "iter 21320/30000  loss         0.201423  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.200\n",
      "iter 21321/30000  loss         0.201422  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.200\n",
      "iter 21340/30000  loss         0.201410  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.201\n",
      "iter 21341/30000  loss         0.201410  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.201\n",
      "iter 21360/30000  loss         0.201398  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.203\n",
      "iter 21361/30000  loss         0.201397  avg_L1_norm_grad         0.000111  w[0]   -0.474 bias    4.203\n",
      "iter 21380/30000  loss         0.201386  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.205\n",
      "iter 21381/30000  loss         0.201385  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.205\n",
      "iter 21400/30000  loss         0.201373  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.206\n",
      "iter 21401/30000  loss         0.201373  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21420/30000  loss         0.201361  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.208\n",
      "iter 21421/30000  loss         0.201360  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.208\n",
      "iter 21440/30000  loss         0.201348  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.210\n",
      "iter 21441/30000  loss         0.201348  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.210\n",
      "iter 21460/30000  loss         0.201336  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.212\n",
      "iter 21461/30000  loss         0.201336  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.212\n",
      "iter 21480/30000  loss         0.201324  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.213\n",
      "iter 21481/30000  loss         0.201323  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.213\n",
      "iter 21500/30000  loss         0.201312  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.215\n",
      "iter 21501/30000  loss         0.201311  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.215\n",
      "iter 21520/30000  loss         0.201299  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.217\n",
      "iter 21521/30000  loss         0.201299  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.217\n",
      "iter 21540/30000  loss         0.201287  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.218\n",
      "iter 21541/30000  loss         0.201287  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.219\n",
      "iter 21560/30000  loss         0.201275  avg_L1_norm_grad         0.000110  w[0]   -0.474 bias    4.220\n",
      "iter 21561/30000  loss         0.201274  avg_L1_norm_grad         0.000110  w[0]   -0.475 bias    4.220\n",
      "iter 21580/30000  loss         0.201263  avg_L1_norm_grad         0.000110  w[0]   -0.475 bias    4.222\n",
      "iter 21581/30000  loss         0.201262  avg_L1_norm_grad         0.000110  w[0]   -0.475 bias    4.222\n",
      "iter 21600/30000  loss         0.201251  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.224\n",
      "iter 21601/30000  loss         0.201250  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.224\n",
      "iter 21620/30000  loss         0.201239  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.225\n",
      "iter 21621/30000  loss         0.201238  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.225\n",
      "iter 21640/30000  loss         0.201227  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.227\n",
      "iter 21641/30000  loss         0.201226  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.227\n",
      "iter 21660/30000  loss         0.201214  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.229\n",
      "iter 21661/30000  loss         0.201214  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.229\n",
      "iter 21680/30000  loss         0.201202  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.230\n",
      "iter 21681/30000  loss         0.201202  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.230\n",
      "iter 21700/30000  loss         0.201190  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.232\n",
      "iter 21701/30000  loss         0.201190  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.232\n",
      "iter 21720/30000  loss         0.201178  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.234\n",
      "iter 21721/30000  loss         0.201178  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.234\n",
      "iter 21740/30000  loss         0.201166  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.235\n",
      "iter 21741/30000  loss         0.201166  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.236\n",
      "iter 21760/30000  loss         0.201154  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.237\n",
      "iter 21761/30000  loss         0.201154  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.237\n",
      "iter 21780/30000  loss         0.201142  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.239\n",
      "iter 21781/30000  loss         0.201142  avg_L1_norm_grad         0.000109  w[0]   -0.475 bias    4.239\n",
      "iter 21800/30000  loss         0.201130  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.240\n",
      "iter 21801/30000  loss         0.201130  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.241\n",
      "iter 21820/30000  loss         0.201119  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.242\n",
      "iter 21821/30000  loss         0.201118  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.242\n",
      "iter 21840/30000  loss         0.201107  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.244\n",
      "iter 21841/30000  loss         0.201106  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.244\n",
      "iter 21860/30000  loss         0.201095  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.246\n",
      "iter 21861/30000  loss         0.201094  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.246\n",
      "iter 21880/30000  loss         0.201083  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.247\n",
      "iter 21881/30000  loss         0.201082  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.247\n",
      "iter 21900/30000  loss         0.201071  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.249\n",
      "iter 21901/30000  loss         0.201070  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.249\n",
      "iter 21920/30000  loss         0.201059  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.251\n",
      "iter 21921/30000  loss         0.201059  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.251\n",
      "iter 21940/30000  loss         0.201047  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.252\n",
      "iter 21941/30000  loss         0.201047  avg_L1_norm_grad         0.000108  w[0]   -0.475 bias    4.252\n",
      "iter 21960/30000  loss         0.201036  avg_L1_norm_grad         0.000108  w[0]   -0.476 bias    4.254\n",
      "iter 21961/30000  loss         0.201035  avg_L1_norm_grad         0.000108  w[0]   -0.476 bias    4.254\n",
      "iter 21980/30000  loss         0.201024  avg_L1_norm_grad         0.000108  w[0]   -0.476 bias    4.256\n",
      "iter 21981/30000  loss         0.201023  avg_L1_norm_grad         0.000108  w[0]   -0.476 bias    4.256\n",
      "iter 22000/30000  loss         0.201012  avg_L1_norm_grad         0.000108  w[0]   -0.476 bias    4.257\n",
      "iter 22001/30000  loss         0.201012  avg_L1_norm_grad         0.000108  w[0]   -0.476 bias    4.257\n",
      "iter 22020/30000  loss         0.201000  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.259\n",
      "iter 22021/30000  loss         0.201000  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.259\n",
      "iter 22040/30000  loss         0.200989  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.261\n",
      "iter 22041/30000  loss         0.200988  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.261\n",
      "iter 22060/30000  loss         0.200977  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.262\n",
      "iter 22061/30000  loss         0.200976  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.262\n",
      "iter 22080/30000  loss         0.200965  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.264\n",
      "iter 22081/30000  loss         0.200965  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.264\n",
      "iter 22100/30000  loss         0.200954  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.266\n",
      "iter 22101/30000  loss         0.200953  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.266\n",
      "iter 22120/30000  loss         0.200942  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.267\n",
      "iter 22121/30000  loss         0.200942  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.267\n",
      "iter 22140/30000  loss         0.200931  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.269\n",
      "iter 22141/30000  loss         0.200930  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.269\n",
      "iter 22160/30000  loss         0.200919  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.271\n",
      "iter 22161/30000  loss         0.200918  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.271\n",
      "iter 22180/30000  loss         0.200907  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.272\n",
      "iter 22181/30000  loss         0.200907  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.272\n",
      "iter 22200/30000  loss         0.200896  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.274\n",
      "iter 22201/30000  loss         0.200895  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22220/30000  loss         0.200884  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.276\n",
      "iter 22221/30000  loss         0.200884  avg_L1_norm_grad         0.000107  w[0]   -0.476 bias    4.276\n",
      "iter 22240/30000  loss         0.200873  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.277\n",
      "iter 22241/30000  loss         0.200872  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.277\n",
      "iter 22260/30000  loss         0.200861  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.279\n",
      "iter 22261/30000  loss         0.200861  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.279\n",
      "iter 22280/30000  loss         0.200850  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.281\n",
      "iter 22281/30000  loss         0.200849  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.281\n",
      "iter 22300/30000  loss         0.200838  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.282\n",
      "iter 22301/30000  loss         0.200838  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.282\n",
      "iter 22320/30000  loss         0.200827  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.284\n",
      "iter 22321/30000  loss         0.200826  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.284\n",
      "iter 22340/30000  loss         0.200816  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.286\n",
      "iter 22341/30000  loss         0.200815  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.286\n",
      "iter 22360/30000  loss         0.200804  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.287\n",
      "iter 22361/30000  loss         0.200804  avg_L1_norm_grad         0.000106  w[0]   -0.476 bias    4.287\n",
      "iter 22380/30000  loss         0.200793  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.289\n",
      "iter 22381/30000  loss         0.200792  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.289\n",
      "iter 22400/30000  loss         0.200782  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.290\n",
      "iter 22401/30000  loss         0.200781  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.291\n",
      "iter 22420/30000  loss         0.200770  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.292\n",
      "iter 22421/30000  loss         0.200770  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.292\n",
      "iter 22440/30000  loss         0.200759  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.294\n",
      "iter 22441/30000  loss         0.200758  avg_L1_norm_grad         0.000106  w[0]   -0.477 bias    4.294\n",
      "iter 22460/30000  loss         0.200748  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.295\n",
      "iter 22461/30000  loss         0.200747  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.296\n",
      "iter 22480/30000  loss         0.200736  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.297\n",
      "iter 22481/30000  loss         0.200736  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.297\n",
      "iter 22500/30000  loss         0.200725  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.299\n",
      "iter 22501/30000  loss         0.200724  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.299\n",
      "iter 22520/30000  loss         0.200714  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.300\n",
      "iter 22521/30000  loss         0.200713  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.300\n",
      "iter 22540/30000  loss         0.200703  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.302\n",
      "iter 22541/30000  loss         0.200702  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.302\n",
      "iter 22560/30000  loss         0.200691  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.304\n",
      "iter 22561/30000  loss         0.200691  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.304\n",
      "iter 22580/30000  loss         0.200680  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.305\n",
      "iter 22581/30000  loss         0.200680  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.305\n",
      "iter 22600/30000  loss         0.200669  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.307\n",
      "iter 22601/30000  loss         0.200669  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.307\n",
      "iter 22620/30000  loss         0.200658  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.309\n",
      "iter 22621/30000  loss         0.200657  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.309\n",
      "iter 22640/30000  loss         0.200647  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.310\n",
      "iter 22641/30000  loss         0.200646  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.310\n",
      "iter 22660/30000  loss         0.200636  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.312\n",
      "iter 22661/30000  loss         0.200635  avg_L1_norm_grad         0.000105  w[0]   -0.477 bias    4.312\n",
      "iter 22680/30000  loss         0.200625  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.313\n",
      "iter 22681/30000  loss         0.200624  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.314\n",
      "iter 22700/30000  loss         0.200614  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.315\n",
      "iter 22701/30000  loss         0.200613  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.315\n",
      "iter 22720/30000  loss         0.200602  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.317\n",
      "iter 22721/30000  loss         0.200602  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.317\n",
      "iter 22740/30000  loss         0.200591  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.318\n",
      "iter 22741/30000  loss         0.200591  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.318\n",
      "iter 22760/30000  loss         0.200580  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.320\n",
      "iter 22761/30000  loss         0.200580  avg_L1_norm_grad         0.000104  w[0]   -0.477 bias    4.320\n",
      "iter 22780/30000  loss         0.200569  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.322\n",
      "iter 22781/30000  loss         0.200569  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.322\n",
      "iter 22800/30000  loss         0.200558  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.323\n",
      "iter 22801/30000  loss         0.200558  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.323\n",
      "iter 22820/30000  loss         0.200548  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.325\n",
      "iter 22821/30000  loss         0.200547  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.325\n",
      "iter 22840/30000  loss         0.200537  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.326\n",
      "iter 22841/30000  loss         0.200536  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.326\n",
      "iter 22860/30000  loss         0.200526  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.328\n",
      "iter 22861/30000  loss         0.200525  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.328\n",
      "iter 22880/30000  loss         0.200515  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.330\n",
      "iter 22881/30000  loss         0.200514  avg_L1_norm_grad         0.000104  w[0]   -0.478 bias    4.330\n",
      "iter 22900/30000  loss         0.200504  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.331\n",
      "iter 22901/30000  loss         0.200503  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.331\n",
      "iter 22920/30000  loss         0.200493  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.333\n",
      "iter 22921/30000  loss         0.200492  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.333\n",
      "iter 22940/30000  loss         0.200482  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.334\n",
      "iter 22941/30000  loss         0.200482  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.335\n",
      "iter 22960/30000  loss         0.200471  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.336\n",
      "iter 22961/30000  loss         0.200471  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.336\n",
      "iter 22980/30000  loss         0.200461  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.338\n",
      "iter 22981/30000  loss         0.200460  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.338\n",
      "iter 23000/30000  loss         0.200450  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.339\n",
      "iter 23001/30000  loss         0.200449  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23020/30000  loss         0.200439  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.341\n",
      "iter 23021/30000  loss         0.200438  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.341\n",
      "iter 23040/30000  loss         0.200428  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.343\n",
      "iter 23041/30000  loss         0.200428  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.343\n",
      "iter 23060/30000  loss         0.200417  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.344\n",
      "iter 23061/30000  loss         0.200417  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.344\n",
      "iter 23080/30000  loss         0.200407  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.346\n",
      "iter 23081/30000  loss         0.200406  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.346\n",
      "iter 23100/30000  loss         0.200396  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.347\n",
      "iter 23101/30000  loss         0.200395  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.347\n",
      "iter 23120/30000  loss         0.200385  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.349\n",
      "iter 23121/30000  loss         0.200385  avg_L1_norm_grad         0.000103  w[0]   -0.478 bias    4.349\n",
      "iter 23140/30000  loss         0.200375  avg_L1_norm_grad         0.000102  w[0]   -0.478 bias    4.351\n",
      "iter 23141/30000  loss         0.200374  avg_L1_norm_grad         0.000102  w[0]   -0.478 bias    4.351\n",
      "iter 23160/30000  loss         0.200364  avg_L1_norm_grad         0.000102  w[0]   -0.478 bias    4.352\n",
      "iter 23161/30000  loss         0.200363  avg_L1_norm_grad         0.000102  w[0]   -0.478 bias    4.352\n",
      "iter 23180/30000  loss         0.200353  avg_L1_norm_grad         0.000102  w[0]   -0.478 bias    4.354\n",
      "iter 23181/30000  loss         0.200353  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.354\n",
      "iter 23200/30000  loss         0.200343  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.355\n",
      "iter 23201/30000  loss         0.200342  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.355\n",
      "iter 23220/30000  loss         0.200332  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.357\n",
      "iter 23221/30000  loss         0.200332  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.357\n",
      "iter 23240/30000  loss         0.200322  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.359\n",
      "iter 23241/30000  loss         0.200321  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.359\n",
      "iter 23260/30000  loss         0.200311  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.360\n",
      "iter 23261/30000  loss         0.200310  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.360\n",
      "iter 23280/30000  loss         0.200300  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.362\n",
      "iter 23281/30000  loss         0.200300  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.362\n",
      "iter 23300/30000  loss         0.200290  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.363\n",
      "iter 23301/30000  loss         0.200289  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.363\n",
      "iter 23320/30000  loss         0.200279  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.365\n",
      "iter 23321/30000  loss         0.200279  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.365\n",
      "iter 23340/30000  loss         0.200269  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.367\n",
      "iter 23341/30000  loss         0.200268  avg_L1_norm_grad         0.000102  w[0]   -0.479 bias    4.367\n",
      "iter 23360/30000  loss         0.200258  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.368\n",
      "iter 23361/30000  loss         0.200258  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.368\n",
      "iter 23380/30000  loss         0.200248  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.370\n",
      "iter 23381/30000  loss         0.200247  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.370\n",
      "iter 23400/30000  loss         0.200237  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.371\n",
      "iter 23401/30000  loss         0.200237  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.371\n",
      "iter 23420/30000  loss         0.200227  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.373\n",
      "iter 23421/30000  loss         0.200227  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.373\n",
      "iter 23440/30000  loss         0.200217  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.374\n",
      "iter 23441/30000  loss         0.200216  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.375\n",
      "iter 23460/30000  loss         0.200206  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.376\n",
      "iter 23461/30000  loss         0.200206  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.376\n",
      "iter 23480/30000  loss         0.200196  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.378\n",
      "iter 23481/30000  loss         0.200195  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.378\n",
      "iter 23500/30000  loss         0.200185  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.379\n",
      "iter 23501/30000  loss         0.200185  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.379\n",
      "iter 23520/30000  loss         0.200175  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.381\n",
      "iter 23521/30000  loss         0.200175  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.381\n",
      "iter 23540/30000  loss         0.200165  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.382\n",
      "iter 23541/30000  loss         0.200164  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.382\n",
      "iter 23560/30000  loss         0.200154  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.384\n",
      "iter 23561/30000  loss         0.200154  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.384\n",
      "iter 23580/30000  loss         0.200144  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.385\n",
      "iter 23581/30000  loss         0.200144  avg_L1_norm_grad         0.000101  w[0]   -0.479 bias    4.386\n",
      "iter 23600/30000  loss         0.200134  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.387\n",
      "iter 23601/30000  loss         0.200133  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.387\n",
      "iter 23620/30000  loss         0.200124  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.389\n",
      "iter 23621/30000  loss         0.200123  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.389\n",
      "iter 23640/30000  loss         0.200113  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.390\n",
      "iter 23641/30000  loss         0.200113  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.390\n",
      "iter 23660/30000  loss         0.200103  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.392\n",
      "iter 23661/30000  loss         0.200103  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.392\n",
      "iter 23680/30000  loss         0.200093  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.393\n",
      "iter 23681/30000  loss         0.200092  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.393\n",
      "iter 23700/30000  loss         0.200083  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.395\n",
      "iter 23701/30000  loss         0.200082  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.395\n",
      "iter 23720/30000  loss         0.200073  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.396\n",
      "iter 23721/30000  loss         0.200072  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.397\n",
      "iter 23740/30000  loss         0.200062  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.398\n",
      "iter 23741/30000  loss         0.200062  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.398\n",
      "iter 23760/30000  loss         0.200052  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.400\n",
      "iter 23761/30000  loss         0.200052  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.400\n",
      "iter 23780/30000  loss         0.200042  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.401\n",
      "iter 23781/30000  loss         0.200042  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.401\n",
      "iter 23800/30000  loss         0.200032  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.403\n",
      "iter 23801/30000  loss         0.200032  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23820/30000  loss         0.200022  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.404\n",
      "iter 23821/30000  loss         0.200021  avg_L1_norm_grad         0.000100  w[0]   -0.480 bias    4.404\n",
      "iter 23840/30000  loss         0.200012  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.406\n",
      "iter 23841/30000  loss         0.200011  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.406\n",
      "iter 23860/30000  loss         0.200002  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.407\n",
      "iter 23861/30000  loss         0.200001  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.407\n",
      "iter 23880/30000  loss         0.199992  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.409\n",
      "iter 23881/30000  loss         0.199991  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.409\n",
      "iter 23900/30000  loss         0.199982  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.410\n",
      "iter 23901/30000  loss         0.199981  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.411\n",
      "iter 23920/30000  loss         0.199972  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.412\n",
      "iter 23921/30000  loss         0.199971  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.412\n",
      "iter 23940/30000  loss         0.199962  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.414\n",
      "iter 23941/30000  loss         0.199961  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.414\n",
      "iter 23960/30000  loss         0.199952  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.415\n",
      "iter 23961/30000  loss         0.199951  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.415\n",
      "iter 23980/30000  loss         0.199942  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.417\n",
      "iter 23981/30000  loss         0.199941  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.417\n",
      "iter 24000/30000  loss         0.199932  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.418\n",
      "iter 24001/30000  loss         0.199931  avg_L1_norm_grad         0.000099  w[0]   -0.480 bias    4.418\n",
      "iter 24020/30000  loss         0.199922  avg_L1_norm_grad         0.000099  w[0]   -0.481 bias    4.420\n",
      "iter 24021/30000  loss         0.199921  avg_L1_norm_grad         0.000099  w[0]   -0.481 bias    4.420\n",
      "iter 24040/30000  loss         0.199912  avg_L1_norm_grad         0.000099  w[0]   -0.481 bias    4.421\n",
      "iter 24041/30000  loss         0.199911  avg_L1_norm_grad         0.000099  w[0]   -0.481 bias    4.421\n",
      "iter 24060/30000  loss         0.199902  avg_L1_norm_grad         0.000099  w[0]   -0.481 bias    4.423\n",
      "iter 24061/30000  loss         0.199901  avg_L1_norm_grad         0.000099  w[0]   -0.481 bias    4.423\n",
      "iter 24080/30000  loss         0.199892  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.424\n",
      "iter 24081/30000  loss         0.199892  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.425\n",
      "iter 24100/30000  loss         0.199882  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.426\n",
      "iter 24101/30000  loss         0.199882  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.426\n",
      "iter 24120/30000  loss         0.199872  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.428\n",
      "iter 24121/30000  loss         0.199872  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.428\n",
      "iter 24140/30000  loss         0.199863  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.429\n",
      "iter 24141/30000  loss         0.199862  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.429\n",
      "iter 24160/30000  loss         0.199853  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.431\n",
      "iter 24161/30000  loss         0.199852  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.431\n",
      "iter 24180/30000  loss         0.199843  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.432\n",
      "iter 24181/30000  loss         0.199842  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.432\n",
      "iter 24200/30000  loss         0.199833  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.434\n",
      "iter 24201/30000  loss         0.199833  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.434\n",
      "iter 24220/30000  loss         0.199823  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.435\n",
      "iter 24221/30000  loss         0.199823  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.435\n",
      "iter 24240/30000  loss         0.199814  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.437\n",
      "iter 24241/30000  loss         0.199813  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.437\n",
      "iter 24260/30000  loss         0.199804  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.438\n",
      "iter 24261/30000  loss         0.199803  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.438\n",
      "iter 24280/30000  loss         0.199794  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.440\n",
      "iter 24281/30000  loss         0.199794  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.440\n",
      "iter 24300/30000  loss         0.199784  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.441\n",
      "iter 24301/30000  loss         0.199784  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.441\n",
      "iter 24320/30000  loss         0.199775  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.443\n",
      "iter 24321/30000  loss         0.199774  avg_L1_norm_grad         0.000098  w[0]   -0.481 bias    4.443\n",
      "iter 24340/30000  loss         0.199765  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.444\n",
      "iter 24341/30000  loss         0.199764  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.444\n",
      "iter 24360/30000  loss         0.199755  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.446\n",
      "iter 24361/30000  loss         0.199755  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.446\n",
      "iter 24380/30000  loss         0.199746  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.447\n",
      "iter 24381/30000  loss         0.199745  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.448\n",
      "iter 24400/30000  loss         0.199736  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.449\n",
      "iter 24401/30000  loss         0.199736  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.449\n",
      "iter 24420/30000  loss         0.199726  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.451\n",
      "iter 24421/30000  loss         0.199726  avg_L1_norm_grad         0.000097  w[0]   -0.481 bias    4.451\n",
      "iter 24440/30000  loss         0.199717  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.452\n",
      "iter 24441/30000  loss         0.199716  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.452\n",
      "iter 24460/30000  loss         0.199707  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.454\n",
      "iter 24461/30000  loss         0.199707  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.454\n",
      "iter 24480/30000  loss         0.199698  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.455\n",
      "iter 24481/30000  loss         0.199697  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.455\n",
      "iter 24500/30000  loss         0.199688  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.457\n",
      "iter 24501/30000  loss         0.199688  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.457\n",
      "iter 24520/30000  loss         0.199678  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.458\n",
      "iter 24521/30000  loss         0.199678  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.458\n",
      "iter 24540/30000  loss         0.199669  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.460\n",
      "iter 24541/30000  loss         0.199668  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.460\n",
      "iter 24560/30000  loss         0.199659  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.461\n",
      "iter 24561/30000  loss         0.199659  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.461\n",
      "iter 24580/30000  loss         0.199650  avg_L1_norm_grad         0.000097  w[0]   -0.482 bias    4.463\n",
      "iter 24581/30000  loss         0.199649  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.463\n",
      "iter 24600/30000  loss         0.199640  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.464\n",
      "iter 24601/30000  loss         0.199640  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24620/30000  loss         0.199631  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.466\n",
      "iter 24621/30000  loss         0.199630  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.466\n",
      "iter 24640/30000  loss         0.199621  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.467\n",
      "iter 24641/30000  loss         0.199621  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.467\n",
      "iter 24660/30000  loss         0.199612  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.469\n",
      "iter 24661/30000  loss         0.199612  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.469\n",
      "iter 24680/30000  loss         0.199603  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.470\n",
      "iter 24681/30000  loss         0.199602  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.470\n",
      "iter 24700/30000  loss         0.199593  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.472\n",
      "iter 24701/30000  loss         0.199593  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.472\n",
      "iter 24720/30000  loss         0.199584  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.473\n",
      "iter 24721/30000  loss         0.199583  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.473\n",
      "iter 24740/30000  loss         0.199574  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.475\n",
      "iter 24741/30000  loss         0.199574  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.475\n",
      "iter 24760/30000  loss         0.199565  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.476\n",
      "iter 24761/30000  loss         0.199565  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.476\n",
      "iter 24780/30000  loss         0.199556  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.478\n",
      "iter 24781/30000  loss         0.199555  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.478\n",
      "iter 24800/30000  loss         0.199546  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.479\n",
      "iter 24801/30000  loss         0.199546  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.479\n",
      "iter 24820/30000  loss         0.199537  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.481\n",
      "iter 24821/30000  loss         0.199537  avg_L1_norm_grad         0.000096  w[0]   -0.482 bias    4.481\n",
      "iter 24840/30000  loss         0.199528  avg_L1_norm_grad         0.000095  w[0]   -0.482 bias    4.482\n",
      "iter 24841/30000  loss         0.199527  avg_L1_norm_grad         0.000095  w[0]   -0.482 bias    4.482\n",
      "iter 24860/30000  loss         0.199518  avg_L1_norm_grad         0.000095  w[0]   -0.482 bias    4.484\n",
      "iter 24861/30000  loss         0.199518  avg_L1_norm_grad         0.000095  w[0]   -0.482 bias    4.484\n",
      "iter 24880/30000  loss         0.199509  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.485\n",
      "iter 24881/30000  loss         0.199509  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.485\n",
      "iter 24900/30000  loss         0.199500  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.487\n",
      "iter 24901/30000  loss         0.199499  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.487\n",
      "iter 24920/30000  loss         0.199491  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.488\n",
      "iter 24921/30000  loss         0.199490  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.488\n",
      "iter 24940/30000  loss         0.199481  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.490\n",
      "iter 24941/30000  loss         0.199481  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.490\n",
      "iter 24960/30000  loss         0.199472  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.491\n",
      "iter 24961/30000  loss         0.199472  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.491\n",
      "iter 24980/30000  loss         0.199463  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.493\n",
      "iter 24981/30000  loss         0.199462  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.493\n",
      "iter 25000/30000  loss         0.199454  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.494\n",
      "iter 25001/30000  loss         0.199453  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.494\n",
      "iter 25020/30000  loss         0.199445  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.496\n",
      "iter 25021/30000  loss         0.199444  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.496\n",
      "iter 25040/30000  loss         0.199435  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.497\n",
      "iter 25041/30000  loss         0.199435  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.497\n",
      "iter 25060/30000  loss         0.199426  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.499\n",
      "iter 25061/30000  loss         0.199426  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.499\n",
      "iter 25080/30000  loss         0.199417  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.500\n",
      "iter 25081/30000  loss         0.199417  avg_L1_norm_grad         0.000095  w[0]   -0.483 bias    4.500\n",
      "iter 25100/30000  loss         0.199408  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.502\n",
      "iter 25101/30000  loss         0.199408  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.502\n",
      "iter 25120/30000  loss         0.199399  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.503\n",
      "iter 25121/30000  loss         0.199398  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.503\n",
      "iter 25140/30000  loss         0.199390  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.505\n",
      "iter 25141/30000  loss         0.199389  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.505\n",
      "iter 25160/30000  loss         0.199381  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.506\n",
      "iter 25161/30000  loss         0.199380  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.506\n",
      "iter 25180/30000  loss         0.199372  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.508\n",
      "iter 25181/30000  loss         0.199371  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.508\n",
      "iter 25200/30000  loss         0.199363  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.509\n",
      "iter 25201/30000  loss         0.199362  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.509\n",
      "iter 25220/30000  loss         0.199354  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.511\n",
      "iter 25221/30000  loss         0.199353  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.511\n",
      "iter 25240/30000  loss         0.199345  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.512\n",
      "iter 25241/30000  loss         0.199344  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.512\n",
      "iter 25260/30000  loss         0.199336  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.514\n",
      "iter 25261/30000  loss         0.199335  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.514\n",
      "iter 25280/30000  loss         0.199327  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.515\n",
      "iter 25281/30000  loss         0.199326  avg_L1_norm_grad         0.000094  w[0]   -0.483 bias    4.515\n",
      "iter 25300/30000  loss         0.199318  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.516\n",
      "iter 25301/30000  loss         0.199317  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.517\n",
      "iter 25320/30000  loss         0.199309  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.518\n",
      "iter 25321/30000  loss         0.199308  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.518\n",
      "iter 25340/30000  loss         0.199300  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.519\n",
      "iter 25341/30000  loss         0.199299  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.520\n",
      "iter 25360/30000  loss         0.199291  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.521\n",
      "iter 25361/30000  loss         0.199290  avg_L1_norm_grad         0.000094  w[0]   -0.484 bias    4.521\n",
      "iter 25380/30000  loss         0.199282  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.522\n",
      "iter 25381/30000  loss         0.199281  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.522\n",
      "iter 25400/30000  loss         0.199273  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.524\n",
      "iter 25401/30000  loss         0.199272  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25420/30000  loss         0.199264  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.525\n",
      "iter 25421/30000  loss         0.199263  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.525\n",
      "iter 25440/30000  loss         0.199255  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.527\n",
      "iter 25441/30000  loss         0.199255  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.527\n",
      "iter 25460/30000  loss         0.199246  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.528\n",
      "iter 25461/30000  loss         0.199246  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.528\n",
      "iter 25480/30000  loss         0.199237  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.530\n",
      "iter 25481/30000  loss         0.199237  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.530\n",
      "iter 25500/30000  loss         0.199228  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.531\n",
      "iter 25501/30000  loss         0.199228  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.531\n",
      "iter 25520/30000  loss         0.199220  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.533\n",
      "iter 25521/30000  loss         0.199219  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.533\n",
      "iter 25540/30000  loss         0.199211  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.534\n",
      "iter 25541/30000  loss         0.199210  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.534\n",
      "iter 25560/30000  loss         0.199202  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.536\n",
      "iter 25561/30000  loss         0.199202  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.536\n",
      "iter 25580/30000  loss         0.199193  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.537\n",
      "iter 25581/30000  loss         0.199193  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.537\n",
      "iter 25600/30000  loss         0.199184  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.539\n",
      "iter 25601/30000  loss         0.199184  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.539\n",
      "iter 25620/30000  loss         0.199176  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.540\n",
      "iter 25621/30000  loss         0.199175  avg_L1_norm_grad         0.000093  w[0]   -0.484 bias    4.540\n",
      "iter 25640/30000  loss         0.199167  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.541\n",
      "iter 25641/30000  loss         0.199167  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.541\n",
      "iter 25660/30000  loss         0.199158  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.543\n",
      "iter 25661/30000  loss         0.199158  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.543\n",
      "iter 25680/30000  loss         0.199149  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.544\n",
      "iter 25681/30000  loss         0.199149  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.544\n",
      "iter 25700/30000  loss         0.199141  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.546\n",
      "iter 25701/30000  loss         0.199140  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.546\n",
      "iter 25720/30000  loss         0.199132  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.547\n",
      "iter 25721/30000  loss         0.199132  avg_L1_norm_grad         0.000092  w[0]   -0.484 bias    4.547\n",
      "iter 25740/30000  loss         0.199123  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.549\n",
      "iter 25741/30000  loss         0.199123  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.549\n",
      "iter 25760/30000  loss         0.199115  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.550\n",
      "iter 25761/30000  loss         0.199114  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.550\n",
      "iter 25780/30000  loss         0.199106  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.552\n",
      "iter 25781/30000  loss         0.199106  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.552\n",
      "iter 25800/30000  loss         0.199097  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.553\n",
      "iter 25801/30000  loss         0.199097  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.553\n",
      "iter 25820/30000  loss         0.199089  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.554\n",
      "iter 25821/30000  loss         0.199088  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.555\n",
      "iter 25840/30000  loss         0.199080  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.556\n",
      "iter 25841/30000  loss         0.199080  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.556\n",
      "iter 25860/30000  loss         0.199072  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.557\n",
      "iter 25861/30000  loss         0.199071  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.557\n",
      "iter 25880/30000  loss         0.199063  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.559\n",
      "iter 25881/30000  loss         0.199063  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.559\n",
      "iter 25900/30000  loss         0.199054  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.560\n",
      "iter 25901/30000  loss         0.199054  avg_L1_norm_grad         0.000092  w[0]   -0.485 bias    4.560\n",
      "iter 25920/30000  loss         0.199046  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.562\n",
      "iter 25921/30000  loss         0.199045  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.562\n",
      "iter 25940/30000  loss         0.199037  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.563\n",
      "iter 25941/30000  loss         0.199037  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.563\n",
      "iter 25960/30000  loss         0.199029  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.565\n",
      "iter 25961/30000  loss         0.199028  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.565\n",
      "iter 25980/30000  loss         0.199020  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.566\n",
      "iter 25981/30000  loss         0.199020  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.566\n",
      "iter 26000/30000  loss         0.199012  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.567\n",
      "iter 26001/30000  loss         0.199011  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.568\n",
      "iter 26020/30000  loss         0.199003  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.569\n",
      "iter 26021/30000  loss         0.199003  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.569\n",
      "iter 26040/30000  loss         0.198995  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.570\n",
      "iter 26041/30000  loss         0.198994  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.570\n",
      "iter 26060/30000  loss         0.198986  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.572\n",
      "iter 26061/30000  loss         0.198986  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.572\n",
      "iter 26080/30000  loss         0.198978  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.573\n",
      "iter 26081/30000  loss         0.198977  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.573\n",
      "iter 26100/30000  loss         0.198969  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.575\n",
      "iter 26101/30000  loss         0.198969  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.575\n",
      "iter 26120/30000  loss         0.198961  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.576\n",
      "iter 26121/30000  loss         0.198961  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.576\n",
      "iter 26140/30000  loss         0.198953  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.578\n",
      "iter 26141/30000  loss         0.198952  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.578\n",
      "iter 26160/30000  loss         0.198944  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.579\n",
      "iter 26161/30000  loss         0.198944  avg_L1_norm_grad         0.000091  w[0]   -0.485 bias    4.579\n",
      "iter 26180/30000  loss         0.198936  avg_L1_norm_grad         0.000091  w[0]   -0.486 bias    4.580\n",
      "iter 26181/30000  loss         0.198935  avg_L1_norm_grad         0.000091  w[0]   -0.486 bias    4.580\n",
      "iter 26200/30000  loss         0.198927  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.582\n",
      "iter 26201/30000  loss         0.198927  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26220/30000  loss         0.198919  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.583\n",
      "iter 26221/30000  loss         0.198919  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.583\n",
      "iter 26240/30000  loss         0.198911  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.585\n",
      "iter 26241/30000  loss         0.198910  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.585\n",
      "iter 26260/30000  loss         0.198902  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.586\n",
      "iter 26261/30000  loss         0.198902  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.586\n",
      "iter 26280/30000  loss         0.198894  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.588\n",
      "iter 26281/30000  loss         0.198894  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.588\n",
      "iter 26300/30000  loss         0.198886  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.589\n",
      "iter 26301/30000  loss         0.198885  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.589\n",
      "iter 26320/30000  loss         0.198877  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.590\n",
      "iter 26321/30000  loss         0.198877  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.590\n",
      "iter 26340/30000  loss         0.198869  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.592\n",
      "iter 26341/30000  loss         0.198869  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.592\n",
      "iter 26360/30000  loss         0.198861  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.593\n",
      "iter 26361/30000  loss         0.198860  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.593\n",
      "iter 26380/30000  loss         0.198852  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.595\n",
      "iter 26381/30000  loss         0.198852  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.595\n",
      "iter 26400/30000  loss         0.198844  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.596\n",
      "iter 26401/30000  loss         0.198844  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.596\n",
      "iter 26420/30000  loss         0.198836  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.597\n",
      "iter 26421/30000  loss         0.198836  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.598\n",
      "iter 26440/30000  loss         0.198828  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.599\n",
      "iter 26441/30000  loss         0.198827  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.599\n",
      "iter 26460/30000  loss         0.198820  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.600\n",
      "iter 26461/30000  loss         0.198819  avg_L1_norm_grad         0.000090  w[0]   -0.486 bias    4.600\n",
      "iter 26480/30000  loss         0.198811  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.602\n",
      "iter 26481/30000  loss         0.198811  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.602\n",
      "iter 26500/30000  loss         0.198803  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.603\n",
      "iter 26501/30000  loss         0.198803  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.603\n",
      "iter 26520/30000  loss         0.198795  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.605\n",
      "iter 26521/30000  loss         0.198795  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.605\n",
      "iter 26540/30000  loss         0.198787  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.606\n",
      "iter 26541/30000  loss         0.198786  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.606\n",
      "iter 26560/30000  loss         0.198779  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.607\n",
      "iter 26561/30000  loss         0.198778  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.607\n",
      "iter 26580/30000  loss         0.198770  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.609\n",
      "iter 26581/30000  loss         0.198770  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.609\n",
      "iter 26600/30000  loss         0.198762  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.610\n",
      "iter 26601/30000  loss         0.198762  avg_L1_norm_grad         0.000089  w[0]   -0.486 bias    4.610\n",
      "iter 26620/30000  loss         0.198754  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.612\n",
      "iter 26621/30000  loss         0.198754  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.612\n",
      "iter 26640/30000  loss         0.198746  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.613\n",
      "iter 26641/30000  loss         0.198746  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.613\n",
      "iter 26660/30000  loss         0.198738  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.614\n",
      "iter 26661/30000  loss         0.198738  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.614\n",
      "iter 26680/30000  loss         0.198730  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.616\n",
      "iter 26681/30000  loss         0.198730  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.616\n",
      "iter 26700/30000  loss         0.198722  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.617\n",
      "iter 26701/30000  loss         0.198721  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.617\n",
      "iter 26720/30000  loss         0.198714  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.619\n",
      "iter 26721/30000  loss         0.198713  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.619\n",
      "iter 26740/30000  loss         0.198706  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.620\n",
      "iter 26741/30000  loss         0.198705  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.620\n",
      "iter 26760/30000  loss         0.198698  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.621\n",
      "iter 26761/30000  loss         0.198697  avg_L1_norm_grad         0.000089  w[0]   -0.487 bias    4.621\n",
      "iter 26780/30000  loss         0.198690  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.623\n",
      "iter 26781/30000  loss         0.198689  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.623\n",
      "iter 26800/30000  loss         0.198682  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.624\n",
      "iter 26801/30000  loss         0.198681  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.624\n",
      "iter 26820/30000  loss         0.198674  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.626\n",
      "iter 26821/30000  loss         0.198673  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.626\n",
      "iter 26840/30000  loss         0.198666  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.627\n",
      "iter 26841/30000  loss         0.198665  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.627\n",
      "iter 26860/30000  loss         0.198658  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.628\n",
      "iter 26861/30000  loss         0.198657  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.628\n",
      "iter 26880/30000  loss         0.198650  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.630\n",
      "iter 26881/30000  loss         0.198649  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.630\n",
      "iter 26900/30000  loss         0.198642  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.631\n",
      "iter 26901/30000  loss         0.198641  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.631\n",
      "iter 26920/30000  loss         0.198634  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.633\n",
      "iter 26921/30000  loss         0.198634  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.633\n",
      "iter 26940/30000  loss         0.198626  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.634\n",
      "iter 26941/30000  loss         0.198626  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.634\n",
      "iter 26960/30000  loss         0.198618  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.635\n",
      "iter 26961/30000  loss         0.198618  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.635\n",
      "iter 26980/30000  loss         0.198610  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.637\n",
      "iter 26981/30000  loss         0.198610  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.637\n",
      "iter 27000/30000  loss         0.198602  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.638\n",
      "iter 27001/30000  loss         0.198602  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27020/30000  loss         0.198594  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.639\n",
      "iter 27021/30000  loss         0.198594  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.640\n",
      "iter 27040/30000  loss         0.198587  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.641\n",
      "iter 27041/30000  loss         0.198586  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.641\n",
      "iter 27060/30000  loss         0.198579  avg_L1_norm_grad         0.000088  w[0]   -0.487 bias    4.642\n",
      "iter 27061/30000  loss         0.198578  avg_L1_norm_grad         0.000088  w[0]   -0.488 bias    4.642\n",
      "iter 27080/30000  loss         0.198571  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.644\n",
      "iter 27081/30000  loss         0.198570  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.644\n",
      "iter 27100/30000  loss         0.198563  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.645\n",
      "iter 27101/30000  loss         0.198563  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.645\n",
      "iter 27120/30000  loss         0.198555  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.646\n",
      "iter 27121/30000  loss         0.198555  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.646\n",
      "iter 27140/30000  loss         0.198547  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.648\n",
      "iter 27141/30000  loss         0.198547  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.648\n",
      "iter 27160/30000  loss         0.198540  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.649\n",
      "iter 27161/30000  loss         0.198539  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.649\n",
      "iter 27180/30000  loss         0.198532  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.651\n",
      "iter 27181/30000  loss         0.198531  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.651\n",
      "iter 27200/30000  loss         0.198524  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.652\n",
      "iter 27201/30000  loss         0.198524  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.652\n",
      "iter 27220/30000  loss         0.198516  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.653\n",
      "iter 27221/30000  loss         0.198516  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.653\n",
      "iter 27240/30000  loss         0.198509  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.655\n",
      "iter 27241/30000  loss         0.198508  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.655\n",
      "iter 27260/30000  loss         0.198501  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.656\n",
      "iter 27261/30000  loss         0.198500  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.656\n",
      "iter 27280/30000  loss         0.198493  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.657\n",
      "iter 27281/30000  loss         0.198493  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.657\n",
      "iter 27300/30000  loss         0.198485  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.659\n",
      "iter 27301/30000  loss         0.198485  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.659\n",
      "iter 27320/30000  loss         0.198478  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.660\n",
      "iter 27321/30000  loss         0.198477  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.660\n",
      "iter 27340/30000  loss         0.198470  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.662\n",
      "iter 27341/30000  loss         0.198470  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.662\n",
      "iter 27360/30000  loss         0.198462  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.663\n",
      "iter 27361/30000  loss         0.198462  avg_L1_norm_grad         0.000087  w[0]   -0.488 bias    4.663\n",
      "iter 27380/30000  loss         0.198455  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.664\n",
      "iter 27381/30000  loss         0.198454  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.664\n",
      "iter 27400/30000  loss         0.198447  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.666\n",
      "iter 27401/30000  loss         0.198447  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.666\n",
      "iter 27420/30000  loss         0.198439  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.667\n",
      "iter 27421/30000  loss         0.198439  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.667\n",
      "iter 27440/30000  loss         0.198432  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.668\n",
      "iter 27441/30000  loss         0.198431  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.668\n",
      "iter 27460/30000  loss         0.198424  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.670\n",
      "iter 27461/30000  loss         0.198424  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.670\n",
      "iter 27480/30000  loss         0.198416  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.671\n",
      "iter 27481/30000  loss         0.198416  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.671\n",
      "iter 27500/30000  loss         0.198409  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.672\n",
      "iter 27501/30000  loss         0.198408  avg_L1_norm_grad         0.000086  w[0]   -0.488 bias    4.673\n",
      "iter 27520/30000  loss         0.198401  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.674\n",
      "iter 27521/30000  loss         0.198401  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.674\n",
      "iter 27540/30000  loss         0.198394  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.675\n",
      "iter 27541/30000  loss         0.198393  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.675\n",
      "iter 27560/30000  loss         0.198386  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.677\n",
      "iter 27561/30000  loss         0.198386  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.677\n",
      "iter 27580/30000  loss         0.198379  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.678\n",
      "iter 27581/30000  loss         0.198378  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.678\n",
      "iter 27600/30000  loss         0.198371  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.679\n",
      "iter 27601/30000  loss         0.198371  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.679\n",
      "iter 27620/30000  loss         0.198363  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.681\n",
      "iter 27621/30000  loss         0.198363  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.681\n",
      "iter 27640/30000  loss         0.198356  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.682\n",
      "iter 27641/30000  loss         0.198356  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.682\n",
      "iter 27660/30000  loss         0.198348  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.683\n",
      "iter 27661/30000  loss         0.198348  avg_L1_norm_grad         0.000086  w[0]   -0.489 bias    4.683\n",
      "iter 27680/30000  loss         0.198341  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.685\n",
      "iter 27681/30000  loss         0.198341  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.685\n",
      "iter 27700/30000  loss         0.198333  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.686\n",
      "iter 27701/30000  loss         0.198333  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.686\n",
      "iter 27720/30000  loss         0.198326  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.687\n",
      "iter 27721/30000  loss         0.198326  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.687\n",
      "iter 27740/30000  loss         0.198318  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.689\n",
      "iter 27741/30000  loss         0.198318  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.689\n",
      "iter 27760/30000  loss         0.198311  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.690\n",
      "iter 27761/30000  loss         0.198311  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.690\n",
      "iter 27780/30000  loss         0.198304  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.691\n",
      "iter 27781/30000  loss         0.198303  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.692\n",
      "iter 27800/30000  loss         0.198296  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.693\n",
      "iter 27801/30000  loss         0.198296  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27820/30000  loss         0.198289  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.694\n",
      "iter 27821/30000  loss         0.198288  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.694\n",
      "iter 27840/30000  loss         0.198281  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.695\n",
      "iter 27841/30000  loss         0.198281  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.696\n",
      "iter 27860/30000  loss         0.198274  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.697\n",
      "iter 27861/30000  loss         0.198273  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.697\n",
      "iter 27880/30000  loss         0.198266  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.698\n",
      "iter 27881/30000  loss         0.198266  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.698\n",
      "iter 27900/30000  loss         0.198259  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.700\n",
      "iter 27901/30000  loss         0.198259  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.700\n",
      "iter 27920/30000  loss         0.198252  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.701\n",
      "iter 27921/30000  loss         0.198251  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.701\n",
      "iter 27940/30000  loss         0.198244  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.702\n",
      "iter 27941/30000  loss         0.198244  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.702\n",
      "iter 27960/30000  loss         0.198237  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.704\n",
      "iter 27961/30000  loss         0.198237  avg_L1_norm_grad         0.000085  w[0]   -0.489 bias    4.704\n",
      "iter 27980/30000  loss         0.198230  avg_L1_norm_grad         0.000085  w[0]   -0.490 bias    4.705\n",
      "iter 27981/30000  loss         0.198229  avg_L1_norm_grad         0.000085  w[0]   -0.490 bias    4.705\n",
      "iter 28000/30000  loss         0.198222  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.706\n",
      "iter 28001/30000  loss         0.198222  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.706\n",
      "iter 28020/30000  loss         0.198215  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.708\n",
      "iter 28021/30000  loss         0.198215  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.708\n",
      "iter 28040/30000  loss         0.198208  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.709\n",
      "iter 28041/30000  loss         0.198207  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.709\n",
      "iter 28060/30000  loss         0.198200  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.710\n",
      "iter 28061/30000  loss         0.198200  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.710\n",
      "iter 28080/30000  loss         0.198193  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.712\n",
      "iter 28081/30000  loss         0.198193  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.712\n",
      "iter 28100/30000  loss         0.198186  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.713\n",
      "iter 28101/30000  loss         0.198186  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.713\n",
      "iter 28120/30000  loss         0.198179  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.714\n",
      "iter 28121/30000  loss         0.198178  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.714\n",
      "iter 28140/30000  loss         0.198171  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.716\n",
      "iter 28141/30000  loss         0.198171  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.716\n",
      "iter 28160/30000  loss         0.198164  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.717\n",
      "iter 28161/30000  loss         0.198164  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.717\n",
      "iter 28180/30000  loss         0.198157  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.718\n",
      "iter 28181/30000  loss         0.198157  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.718\n",
      "iter 28200/30000  loss         0.198150  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.720\n",
      "iter 28201/30000  loss         0.198149  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.720\n",
      "iter 28220/30000  loss         0.198142  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.721\n",
      "iter 28221/30000  loss         0.198142  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.721\n",
      "iter 28240/30000  loss         0.198135  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.722\n",
      "iter 28241/30000  loss         0.198135  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.722\n",
      "iter 28260/30000  loss         0.198128  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.724\n",
      "iter 28261/30000  loss         0.198128  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.724\n",
      "iter 28280/30000  loss         0.198121  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.725\n",
      "iter 28281/30000  loss         0.198121  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.725\n",
      "iter 28300/30000  loss         0.198114  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.726\n",
      "iter 28301/30000  loss         0.198113  avg_L1_norm_grad         0.000084  w[0]   -0.490 bias    4.726\n",
      "iter 28320/30000  loss         0.198107  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.728\n",
      "iter 28321/30000  loss         0.198106  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.728\n",
      "iter 28340/30000  loss         0.198099  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.729\n",
      "iter 28341/30000  loss         0.198099  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.729\n",
      "iter 28360/30000  loss         0.198092  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.730\n",
      "iter 28361/30000  loss         0.198092  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.730\n",
      "iter 28380/30000  loss         0.198085  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.731\n",
      "iter 28381/30000  loss         0.198085  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.732\n",
      "iter 28400/30000  loss         0.198078  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.733\n",
      "iter 28401/30000  loss         0.198078  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.733\n",
      "iter 28420/30000  loss         0.198071  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.734\n",
      "iter 28421/30000  loss         0.198071  avg_L1_norm_grad         0.000083  w[0]   -0.490 bias    4.734\n",
      "iter 28440/30000  loss         0.198064  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.735\n",
      "iter 28441/30000  loss         0.198063  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.736\n",
      "iter 28460/30000  loss         0.198057  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.737\n",
      "iter 28461/30000  loss         0.198056  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.737\n",
      "iter 28480/30000  loss         0.198050  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.738\n",
      "iter 28481/30000  loss         0.198049  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.738\n",
      "iter 28500/30000  loss         0.198043  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.739\n",
      "iter 28501/30000  loss         0.198042  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.739\n",
      "iter 28520/30000  loss         0.198036  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.741\n",
      "iter 28521/30000  loss         0.198035  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.741\n",
      "iter 28540/30000  loss         0.198028  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.742\n",
      "iter 28541/30000  loss         0.198028  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.742\n",
      "iter 28560/30000  loss         0.198021  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.743\n",
      "iter 28561/30000  loss         0.198021  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.743\n",
      "iter 28580/30000  loss         0.198014  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.745\n",
      "iter 28581/30000  loss         0.198014  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.745\n",
      "iter 28600/30000  loss         0.198007  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.746\n",
      "iter 28601/30000  loss         0.198007  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28620/30000  loss         0.198000  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.747\n",
      "iter 28621/30000  loss         0.198000  avg_L1_norm_grad         0.000083  w[0]   -0.491 bias    4.747\n",
      "iter 28640/30000  loss         0.197993  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.749\n",
      "iter 28641/30000  loss         0.197993  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.749\n",
      "iter 28660/30000  loss         0.197986  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.750\n",
      "iter 28661/30000  loss         0.197986  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.750\n",
      "iter 28680/30000  loss         0.197979  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.751\n",
      "iter 28681/30000  loss         0.197979  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.751\n",
      "iter 28700/30000  loss         0.197973  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.753\n",
      "iter 28701/30000  loss         0.197972  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.753\n",
      "iter 28720/30000  loss         0.197966  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.754\n",
      "iter 28721/30000  loss         0.197965  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.754\n",
      "iter 28740/30000  loss         0.197959  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.755\n",
      "iter 28741/30000  loss         0.197958  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.755\n",
      "iter 28760/30000  loss         0.197952  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.756\n",
      "iter 28761/30000  loss         0.197951  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.756\n",
      "iter 28780/30000  loss         0.197945  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.758\n",
      "iter 28781/30000  loss         0.197944  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.758\n",
      "iter 28800/30000  loss         0.197938  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.759\n",
      "iter 28801/30000  loss         0.197937  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.759\n",
      "iter 28820/30000  loss         0.197931  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.760\n",
      "iter 28821/30000  loss         0.197931  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.760\n",
      "iter 28840/30000  loss         0.197924  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.762\n",
      "iter 28841/30000  loss         0.197924  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.762\n",
      "iter 28860/30000  loss         0.197917  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.763\n",
      "iter 28861/30000  loss         0.197917  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.763\n",
      "iter 28880/30000  loss         0.197910  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.764\n",
      "iter 28881/30000  loss         0.197910  avg_L1_norm_grad         0.000082  w[0]   -0.491 bias    4.764\n",
      "iter 28900/30000  loss         0.197903  avg_L1_norm_grad         0.000082  w[0]   -0.492 bias    4.766\n",
      "iter 28901/30000  loss         0.197903  avg_L1_norm_grad         0.000082  w[0]   -0.492 bias    4.766\n",
      "iter 28920/30000  loss         0.197897  avg_L1_norm_grad         0.000082  w[0]   -0.492 bias    4.767\n",
      "iter 28921/30000  loss         0.197896  avg_L1_norm_grad         0.000082  w[0]   -0.492 bias    4.767\n",
      "iter 28940/30000  loss         0.197890  avg_L1_norm_grad         0.000082  w[0]   -0.492 bias    4.768\n",
      "iter 28941/30000  loss         0.197889  avg_L1_norm_grad         0.000082  w[0]   -0.492 bias    4.768\n",
      "iter 28960/30000  loss         0.197883  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.769\n",
      "iter 28961/30000  loss         0.197883  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.769\n",
      "iter 28980/30000  loss         0.197876  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.771\n",
      "iter 28981/30000  loss         0.197876  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.771\n",
      "iter 29000/30000  loss         0.197869  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.772\n",
      "iter 29001/30000  loss         0.197869  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.772\n",
      "iter 29020/30000  loss         0.197862  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.773\n",
      "iter 29021/30000  loss         0.197862  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.773\n",
      "iter 29040/30000  loss         0.197856  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.775\n",
      "iter 29041/30000  loss         0.197855  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.775\n",
      "iter 29060/30000  loss         0.197849  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.776\n",
      "iter 29061/30000  loss         0.197849  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.776\n",
      "iter 29080/30000  loss         0.197842  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.777\n",
      "iter 29081/30000  loss         0.197842  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.777\n",
      "iter 29100/30000  loss         0.197835  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.778\n",
      "iter 29101/30000  loss         0.197835  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.779\n",
      "iter 29120/30000  loss         0.197829  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.780\n",
      "iter 29121/30000  loss         0.197828  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.780\n",
      "iter 29140/30000  loss         0.197822  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.781\n",
      "iter 29141/30000  loss         0.197821  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.781\n",
      "iter 29160/30000  loss         0.197815  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.782\n",
      "iter 29161/30000  loss         0.197815  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.782\n",
      "iter 29180/30000  loss         0.197808  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.784\n",
      "iter 29181/30000  loss         0.197808  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.784\n",
      "iter 29200/30000  loss         0.197802  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.785\n",
      "iter 29201/30000  loss         0.197801  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.785\n",
      "iter 29220/30000  loss         0.197795  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.786\n",
      "iter 29221/30000  loss         0.197795  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.786\n",
      "iter 29240/30000  loss         0.197788  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.787\n",
      "iter 29241/30000  loss         0.197788  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.788\n",
      "iter 29260/30000  loss         0.197782  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.789\n",
      "iter 29261/30000  loss         0.197781  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.789\n",
      "iter 29280/30000  loss         0.197775  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.790\n",
      "iter 29281/30000  loss         0.197775  avg_L1_norm_grad         0.000081  w[0]   -0.492 bias    4.790\n",
      "iter 29300/30000  loss         0.197768  avg_L1_norm_grad         0.000080  w[0]   -0.492 bias    4.791\n",
      "iter 29301/30000  loss         0.197768  avg_L1_norm_grad         0.000080  w[0]   -0.492 bias    4.791\n",
      "iter 29320/30000  loss         0.197762  avg_L1_norm_grad         0.000080  w[0]   -0.492 bias    4.793\n",
      "iter 29321/30000  loss         0.197761  avg_L1_norm_grad         0.000080  w[0]   -0.492 bias    4.793\n",
      "iter 29340/30000  loss         0.197755  avg_L1_norm_grad         0.000080  w[0]   -0.492 bias    4.794\n",
      "iter 29341/30000  loss         0.197755  avg_L1_norm_grad         0.000080  w[0]   -0.492 bias    4.794\n",
      "iter 29360/30000  loss         0.197748  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.795\n",
      "iter 29361/30000  loss         0.197748  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.795\n",
      "iter 29380/30000  loss         0.197742  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.796\n",
      "iter 29381/30000  loss         0.197741  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.796\n",
      "iter 29400/30000  loss         0.197735  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.798\n",
      "iter 29401/30000  loss         0.197735  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29420/30000  loss         0.197728  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.799\n",
      "iter 29421/30000  loss         0.197728  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.799\n",
      "iter 29440/30000  loss         0.197722  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.800\n",
      "iter 29441/30000  loss         0.197721  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.800\n",
      "iter 29460/30000  loss         0.197715  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.802\n",
      "iter 29461/30000  loss         0.197715  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.802\n",
      "iter 29480/30000  loss         0.197709  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.803\n",
      "iter 29481/30000  loss         0.197708  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.803\n",
      "iter 29500/30000  loss         0.197702  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.804\n",
      "iter 29501/30000  loss         0.197702  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.804\n",
      "iter 29520/30000  loss         0.197695  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.805\n",
      "iter 29521/30000  loss         0.197695  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.805\n",
      "iter 29540/30000  loss         0.197689  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.807\n",
      "iter 29541/30000  loss         0.197689  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.807\n",
      "iter 29560/30000  loss         0.197682  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.808\n",
      "iter 29561/30000  loss         0.197682  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.808\n",
      "iter 29580/30000  loss         0.197676  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.809\n",
      "iter 29581/30000  loss         0.197676  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.809\n",
      "iter 29600/30000  loss         0.197669  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.810\n",
      "iter 29601/30000  loss         0.197669  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.810\n",
      "iter 29620/30000  loss         0.197663  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.812\n",
      "iter 29621/30000  loss         0.197662  avg_L1_norm_grad         0.000080  w[0]   -0.493 bias    4.812\n",
      "iter 29640/30000  loss         0.197656  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.813\n",
      "iter 29641/30000  loss         0.197656  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.813\n",
      "iter 29660/30000  loss         0.197650  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.814\n",
      "iter 29661/30000  loss         0.197649  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.814\n",
      "iter 29680/30000  loss         0.197643  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.815\n",
      "iter 29681/30000  loss         0.197643  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.816\n",
      "iter 29700/30000  loss         0.197637  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.817\n",
      "iter 29701/30000  loss         0.197636  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.817\n",
      "iter 29720/30000  loss         0.197630  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.818\n",
      "iter 29721/30000  loss         0.197630  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.818\n",
      "iter 29740/30000  loss         0.197624  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.819\n",
      "iter 29741/30000  loss         0.197624  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.819\n",
      "iter 29760/30000  loss         0.197617  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.821\n",
      "iter 29761/30000  loss         0.197617  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.821\n",
      "iter 29780/30000  loss         0.197611  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.822\n",
      "iter 29781/30000  loss         0.197611  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.822\n",
      "iter 29800/30000  loss         0.197605  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.823\n",
      "iter 29801/30000  loss         0.197604  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.823\n",
      "iter 29820/30000  loss         0.197598  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.824\n",
      "iter 29821/30000  loss         0.197598  avg_L1_norm_grad         0.000079  w[0]   -0.493 bias    4.824\n",
      "iter 29840/30000  loss         0.197592  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.826\n",
      "iter 29841/30000  loss         0.197591  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.826\n",
      "iter 29860/30000  loss         0.197585  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.827\n",
      "iter 29861/30000  loss         0.197585  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.827\n",
      "iter 29880/30000  loss         0.197579  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.828\n",
      "iter 29881/30000  loss         0.197579  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.828\n",
      "iter 29900/30000  loss         0.197572  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.829\n",
      "iter 29901/30000  loss         0.197572  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.829\n",
      "iter 29920/30000  loss         0.197566  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.831\n",
      "iter 29921/30000  loss         0.197566  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.831\n",
      "iter 29940/30000  loss         0.197560  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.832\n",
      "iter 29941/30000  loss         0.197559  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.832\n",
      "iter 29960/30000  loss         0.197553  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.833\n",
      "iter 29961/30000  loss         0.197553  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.833\n",
      "iter 29980/30000  loss         0.197547  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.834\n",
      "iter 29981/30000  loss         0.197547  avg_L1_norm_grad         0.000079  w[0]   -0.494 bias    4.834\n",
      "iter 30000/30000  loss         0.197541  avg_L1_norm_grad         0.000078  w[0]   -0.494 bias    4.836\n",
      "Done. Did NOT converge.\n"
     ]
    }
   ],
   "source": [
    "lrcomp = LogisticRegressionGradientDescent(alpha=1, step_size=0.1, init_w_recipe='zeros')\n",
    "lrcomp.fit(x_tr_comp_bw, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5814</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>354</td>\n",
       "      <td>5646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1\n",
       "Actual               \n",
       "0.0        5814   186\n",
       "1.0         354  5646"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lrcomp.predict(x_tr_comp_bw)\n",
    "y_actu = pd.Series(y_tr, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.4\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.031943  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         1.335703  avg_L1_norm_grad         0.073675  w[0]    0.000 bias    0.000\n",
      "iter    2/30000  loss         3.891628  avg_L1_norm_grad         0.118814  w[0]    0.000 bias    0.214\n",
      "iter    3/30000  loss         4.676634  avg_L1_norm_grad         0.096872  w[0]    0.000 bias   -0.062\n",
      "iter    4/30000  loss         0.746810  avg_L1_norm_grad         0.015438  w[0]    0.000 bias    0.218\n",
      "iter    5/30000  loss         0.677926  avg_L1_norm_grad         0.018846  w[0]    0.000 bias    0.231\n",
      "iter    6/30000  loss         0.789034  avg_L1_norm_grad         0.048012  w[0]    0.000 bias    0.310\n",
      "iter    7/30000  loss         1.693644  avg_L1_norm_grad         0.068707  w[0]    0.000 bias    0.227\n",
      "iter    8/30000  loss         1.680874  avg_L1_norm_grad         0.093375  w[0]    0.000 bias    0.434\n",
      "iter    9/30000  loss         3.390588  avg_L1_norm_grad         0.085429  w[0]    0.000 bias    0.233\n",
      "iter   10/30000  loss         0.613638  avg_L1_norm_grad         0.022488  w[0]    0.000 bias    0.484\n",
      "iter   11/30000  loss         0.657006  avg_L1_norm_grad         0.027176  w[0]    0.000 bias    0.463\n",
      "iter   12/30000  loss         0.774421  avg_L1_norm_grad         0.052884  w[0]    0.000 bias    0.560\n",
      "iter   13/30000  loss         1.489999  avg_L1_norm_grad         0.059409  w[0]    0.000 bias    0.460\n",
      "iter   14/30000  loss         0.926023  avg_L1_norm_grad         0.064513  w[0]    0.000 bias    0.643\n",
      "iter   15/30000  loss         1.819263  avg_L1_norm_grad         0.064767  w[0]    0.000 bias    0.512\n",
      "iter   16/30000  loss         0.692836  avg_L1_norm_grad         0.048175  w[0]    0.000 bias    0.709\n",
      "iter   17/30000  loss         1.151779  avg_L1_norm_grad         0.049896  w[0]    0.000 bias    0.618\n",
      "iter   18/30000  loss         0.704598  avg_L1_norm_grad         0.051580  w[0]    0.000 bias    0.773\n",
      "iter   19/30000  loss         1.212905  avg_L1_norm_grad         0.051972  w[0]    0.000 bias    0.672\n",
      "iter   20/30000  loss         0.634570  avg_L1_norm_grad         0.047149  w[0]    0.000 bias    0.831\n",
      "iter   21/30000  loss         1.030820  avg_L1_norm_grad         0.047222  w[0]    0.000 bias    0.739\n",
      "iter   40/30000  loss         0.318109  avg_L1_norm_grad         0.017833  w[0]    0.000 bias    1.177\n",
      "iter   41/30000  loss         0.337537  avg_L1_norm_grad         0.016603  w[0]    0.000 bias    1.145\n",
      "iter   60/30000  loss         0.246586  avg_L1_norm_grad         0.003733  w[0]    0.000 bias    1.333\n",
      "iter   61/30000  loss         0.245540  avg_L1_norm_grad         0.003190  w[0]    0.000 bias    1.331\n",
      "iter   80/30000  loss         0.230818  avg_L1_norm_grad         0.000826  w[0]    0.000 bias    1.455\n",
      "iter   81/30000  loss         0.230272  avg_L1_norm_grad         0.000788  w[0]    0.000 bias    1.460\n",
      "iter  100/30000  loss         0.221373  avg_L1_norm_grad         0.000682  w[0]    0.000 bias    1.563\n",
      "iter  101/30000  loss         0.220967  avg_L1_norm_grad         0.000677  w[0]    0.000 bias    1.568\n",
      "iter  120/30000  loss         0.214108  avg_L1_norm_grad         0.000611  w[0]    0.000 bias    1.656\n",
      "iter  121/30000  loss         0.213786  avg_L1_norm_grad         0.000608  w[0]    0.000 bias    1.661\n",
      "iter  140/30000  loss         0.208239  avg_L1_norm_grad         0.000557  w[0]    0.000 bias    1.739\n",
      "iter  141/30000  loss         0.207974  avg_L1_norm_grad         0.000555  w[0]    0.000 bias    1.743\n",
      "iter  160/30000  loss         0.203349  avg_L1_norm_grad         0.000514  w[0]    0.000 bias    1.813\n",
      "iter  161/30000  loss         0.203126  avg_L1_norm_grad         0.000512  w[0]    0.000 bias    1.816\n",
      "iter  180/30000  loss         0.199185  avg_L1_norm_grad         0.000478  w[0]    0.000 bias    1.879\n",
      "iter  181/30000  loss         0.198993  avg_L1_norm_grad         0.000476  w[0]    0.000 bias    1.882\n",
      "iter  200/30000  loss         0.195580  avg_L1_norm_grad         0.000447  w[0]    0.000 bias    1.938\n",
      "iter  201/30000  loss         0.195412  avg_L1_norm_grad         0.000446  w[0]    0.000 bias    1.941\n",
      "iter  220/30000  loss         0.192418  avg_L1_norm_grad         0.000422  w[0]    0.000 bias    1.993\n",
      "iter  221/30000  loss         0.192270  avg_L1_norm_grad         0.000420  w[0]    0.000 bias    1.995\n",
      "iter  240/30000  loss         0.189615  avg_L1_norm_grad         0.000399  w[0]    0.000 bias    2.042\n",
      "iter  241/30000  loss         0.189483  avg_L1_norm_grad         0.000398  w[0]    0.000 bias    2.044\n",
      "iter  260/30000  loss         0.187107  avg_L1_norm_grad         0.000379  w[0]    0.000 bias    2.087\n",
      "iter  261/30000  loss         0.186988  avg_L1_norm_grad         0.000378  w[0]    0.000 bias    2.090\n",
      "iter  280/30000  loss         0.184845  avg_L1_norm_grad         0.000362  w[0]    0.000 bias    2.129\n",
      "iter  281/30000  loss         0.184738  avg_L1_norm_grad         0.000361  w[0]    0.000 bias    2.131\n",
      "iter  300/30000  loss         0.182793  avg_L1_norm_grad         0.000346  w[0]    0.000 bias    2.168\n",
      "iter  301/30000  loss         0.182695  avg_L1_norm_grad         0.000345  w[0]    0.000 bias    2.170\n",
      "iter  320/30000  loss         0.180919  avg_L1_norm_grad         0.000331  w[0]    0.000 bias    2.203\n",
      "iter  321/30000  loss         0.180829  avg_L1_norm_grad         0.000331  w[0]    0.000 bias    2.205\n",
      "iter  340/30000  loss         0.179199  avg_L1_norm_grad         0.000319  w[0]    0.000 bias    2.237\n",
      "iter  341/30000  loss         0.179117  avg_L1_norm_grad         0.000318  w[0]    0.000 bias    2.238\n",
      "iter  360/30000  loss         0.177613  avg_L1_norm_grad         0.000307  w[0]    0.000 bias    2.267\n",
      "iter  361/30000  loss         0.177537  avg_L1_norm_grad         0.000306  w[0]    0.000 bias    2.269\n",
      "iter  380/30000  loss         0.176145  avg_L1_norm_grad         0.000296  w[0]    0.000 bias    2.296\n",
      "iter  381/30000  loss         0.176074  avg_L1_norm_grad         0.000295  w[0]    0.000 bias    2.297\n",
      "iter  400/30000  loss         0.174780  avg_L1_norm_grad         0.000286  w[0]    0.000 bias    2.323\n",
      "iter  401/30000  loss         0.174715  avg_L1_norm_grad         0.000285  w[0]    0.000 bias    2.324\n",
      "iter  420/30000  loss         0.173508  avg_L1_norm_grad         0.000276  w[0]    0.000 bias    2.348\n",
      "iter  421/30000  loss         0.173447  avg_L1_norm_grad         0.000276  w[0]    0.000 bias    2.349\n",
      "iter  440/30000  loss         0.172318  avg_L1_norm_grad         0.000268  w[0]    0.000 bias    2.372\n",
      "iter  441/30000  loss         0.172260  avg_L1_norm_grad         0.000267  w[0]    0.000 bias    2.373\n",
      "iter  460/30000  loss         0.171201  avg_L1_norm_grad         0.000260  w[0]    0.000 bias    2.394\n",
      "iter  461/30000  loss         0.171147  avg_L1_norm_grad         0.000260  w[0]    0.000 bias    2.395\n",
      "iter  480/30000  loss         0.170151  avg_L1_norm_grad         0.000253  w[0]    0.000 bias    2.415\n",
      "iter  481/30000  loss         0.170100  avg_L1_norm_grad         0.000252  w[0]    0.000 bias    2.416\n",
      "iter  500/30000  loss         0.169161  avg_L1_norm_grad         0.000246  w[0]    0.000 bias    2.434\n",
      "iter  501/30000  loss         0.169113  avg_L1_norm_grad         0.000246  w[0]    0.000 bias    2.435\n",
      "iter  520/30000  loss         0.168226  avg_L1_norm_grad         0.000240  w[0]    0.000 bias    2.453\n",
      "iter  521/30000  loss         0.168181  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    2.454\n",
      "iter  540/30000  loss         0.167340  avg_L1_norm_grad         0.000234  w[0]    0.000 bias    2.470\n",
      "iter  541/30000  loss         0.167297  avg_L1_norm_grad         0.000233  w[0]    0.000 bias    2.471\n",
      "iter  560/30000  loss         0.166500  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.487\n",
      "iter  561/30000  loss         0.166460  avg_L1_norm_grad         0.000228  w[0]    0.000 bias    2.487\n",
      "iter  580/30000  loss         0.165702  avg_L1_norm_grad         0.000223  w[0]    0.000 bias    2.502\n",
      "iter  581/30000  loss         0.165663  avg_L1_norm_grad         0.000223  w[0]    0.000 bias    2.503\n",
      "iter  600/30000  loss         0.164942  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.517\n",
      "iter  601/30000  loss         0.164905  avg_L1_norm_grad         0.000218  w[0]    0.000 bias    2.518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.164218  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.531\n",
      "iter  621/30000  loss         0.164182  avg_L1_norm_grad         0.000213  w[0]    0.000 bias    2.532\n",
      "iter  640/30000  loss         0.163526  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    2.544\n",
      "iter  641/30000  loss         0.163492  avg_L1_norm_grad         0.000209  w[0]    0.000 bias    2.545\n",
      "iter  660/30000  loss         0.162865  avg_L1_norm_grad         0.000205  w[0]    0.000 bias    2.557\n",
      "iter  661/30000  loss         0.162832  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    2.557\n",
      "iter  680/30000  loss         0.162231  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.569\n",
      "iter  681/30000  loss         0.162200  avg_L1_norm_grad         0.000200  w[0]    0.000 bias    2.569\n",
      "iter  700/30000  loss         0.161624  avg_L1_norm_grad         0.000197  w[0]    0.000 bias    2.580\n",
      "iter  701/30000  loss         0.161595  avg_L1_norm_grad         0.000196  w[0]    0.000 bias    2.581\n",
      "iter  720/30000  loss         0.161042  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.591\n",
      "iter  721/30000  loss         0.161013  avg_L1_norm_grad         0.000193  w[0]    0.000 bias    2.591\n",
      "iter  740/30000  loss         0.160482  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.601\n",
      "iter  741/30000  loss         0.160455  avg_L1_norm_grad         0.000189  w[0]    0.000 bias    2.602\n",
      "iter  760/30000  loss         0.159944  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.611\n",
      "iter  761/30000  loss         0.159918  avg_L1_norm_grad         0.000186  w[0]    0.000 bias    2.611\n",
      "iter  780/30000  loss         0.159426  avg_L1_norm_grad         0.000183  w[0]    0.000 bias    2.620\n",
      "iter  781/30000  loss         0.159401  avg_L1_norm_grad         0.000182  w[0]    0.000 bias    2.621\n",
      "iter  800/30000  loss         0.158927  avg_L1_norm_grad         0.000180  w[0]    0.000 bias    2.629\n",
      "iter  801/30000  loss         0.158903  avg_L1_norm_grad         0.000179  w[0]    0.000 bias    2.630\n",
      "iter  820/30000  loss         0.158446  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.638\n",
      "iter  821/30000  loss         0.158422  avg_L1_norm_grad         0.000176  w[0]    0.000 bias    2.638\n",
      "iter  840/30000  loss         0.157981  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    2.646\n",
      "iter  841/30000  loss         0.157958  avg_L1_norm_grad         0.000174  w[0]    0.000 bias    2.646\n",
      "iter  860/30000  loss         0.157532  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    2.654\n",
      "iter  861/30000  loss         0.157510  avg_L1_norm_grad         0.000171  w[0]    0.000 bias    2.654\n",
      "iter  880/30000  loss         0.157098  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    2.661\n",
      "iter  881/30000  loss         0.157077  avg_L1_norm_grad         0.000168  w[0]    0.000 bias    2.661\n",
      "iter  900/30000  loss         0.156678  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.668\n",
      "iter  901/30000  loss         0.156658  avg_L1_norm_grad         0.000166  w[0]    0.000 bias    2.668\n",
      "iter  920/30000  loss         0.156272  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    2.675\n",
      "iter  921/30000  loss         0.156252  avg_L1_norm_grad         0.000163  w[0]    0.000 bias    2.675\n",
      "iter  940/30000  loss         0.155878  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    2.681\n",
      "iter  941/30000  loss         0.155858  avg_L1_norm_grad         0.000161  w[0]    0.000 bias    2.682\n",
      "iter  960/30000  loss         0.155496  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    2.688\n",
      "iter  961/30000  loss         0.155477  avg_L1_norm_grad         0.000159  w[0]    0.000 bias    2.688\n",
      "iter  980/30000  loss         0.155125  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.694\n",
      "iter  981/30000  loss         0.155107  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.694\n",
      "iter 1000/30000  loss         0.154766  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    2.699\n",
      "iter 1001/30000  loss         0.154748  avg_L1_norm_grad         0.000154  w[0]    0.000 bias    2.700\n",
      "iter 1020/30000  loss         0.154417  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.705\n",
      "iter 1021/30000  loss         0.154399  avg_L1_norm_grad         0.000152  w[0]    0.000 bias    2.705\n",
      "iter 1040/30000  loss         0.154077  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    2.710\n",
      "iter 1041/30000  loss         0.154060  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    2.710\n",
      "iter 1060/30000  loss         0.153747  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.715\n",
      "iter 1061/30000  loss         0.153731  avg_L1_norm_grad         0.000148  w[0]    0.000 bias    2.715\n",
      "iter 1080/30000  loss         0.153426  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.720\n",
      "iter 1081/30000  loss         0.153410  avg_L1_norm_grad         0.000146  w[0]    0.000 bias    2.720\n",
      "iter 1100/30000  loss         0.153114  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    2.725\n",
      "iter 1101/30000  loss         0.153098  avg_L1_norm_grad         0.000145  w[0]    0.000 bias    2.725\n",
      "iter 1120/30000  loss         0.152809  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.729\n",
      "iter 1121/30000  loss         0.152794  avg_L1_norm_grad         0.000143  w[0]    0.000 bias    2.729\n",
      "iter 1140/30000  loss         0.152513  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.734\n",
      "iter 1141/30000  loss         0.152498  avg_L1_norm_grad         0.000141  w[0]    0.000 bias    2.734\n",
      "iter 1160/30000  loss         0.152224  avg_L1_norm_grad         0.000140  w[0]    0.000 bias    2.738\n",
      "iter 1161/30000  loss         0.152210  avg_L1_norm_grad         0.000139  w[0]    0.000 bias    2.738\n",
      "iter 1180/30000  loss         0.151942  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.742\n",
      "iter 1181/30000  loss         0.151928  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.742\n",
      "iter 1200/30000  loss         0.151668  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.746\n",
      "iter 1201/30000  loss         0.151654  avg_L1_norm_grad         0.000136  w[0]    0.000 bias    2.746\n",
      "iter 1220/30000  loss         0.151399  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    2.749\n",
      "iter 1221/30000  loss         0.151386  avg_L1_norm_grad         0.000135  w[0]    0.000 bias    2.749\n",
      "iter 1240/30000  loss         0.151138  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.753\n",
      "iter 1241/30000  loss         0.151125  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.753\n",
      "iter 1260/30000  loss         0.150882  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.756\n",
      "iter 1261/30000  loss         0.150869  avg_L1_norm_grad         0.000132  w[0]    0.000 bias    2.756\n",
      "iter 1280/30000  loss         0.150632  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.760\n",
      "iter 1281/30000  loss         0.150620  avg_L1_norm_grad         0.000130  w[0]    0.000 bias    2.760\n",
      "iter 1300/30000  loss         0.150388  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.763\n",
      "iter 1301/30000  loss         0.150376  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.763\n",
      "iter 1320/30000  loss         0.150150  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.766\n",
      "iter 1321/30000  loss         0.150138  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.766\n",
      "iter 1340/30000  loss         0.149916  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.769\n",
      "iter 1341/30000  loss         0.149905  avg_L1_norm_grad         0.000126  w[0]    0.000 bias    2.769\n",
      "iter 1360/30000  loss         0.149688  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.772\n",
      "iter 1361/30000  loss         0.149677  avg_L1_norm_grad         0.000125  w[0]    0.000 bias    2.772\n",
      "iter 1380/30000  loss         0.149465  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.774\n",
      "iter 1381/30000  loss         0.149453  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.775\n",
      "iter 1400/30000  loss         0.149246  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.777\n",
      "iter 1401/30000  loss         0.149235  avg_L1_norm_grad         0.000123  w[0]    0.000 bias    2.777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.149032  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.780\n",
      "iter 1421/30000  loss         0.149021  avg_L1_norm_grad         0.000122  w[0]    0.000 bias    2.780\n",
      "iter 1440/30000  loss         0.148822  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.782\n",
      "iter 1441/30000  loss         0.148812  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.782\n",
      "iter 1460/30000  loss         0.148617  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    2.785\n",
      "iter 1461/30000  loss         0.148606  avg_L1_norm_grad         0.000119  w[0]    0.000 bias    2.785\n",
      "iter 1480/30000  loss         0.148415  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    2.787\n",
      "iter 1481/30000  loss         0.148405  avg_L1_norm_grad         0.000118  w[0]    0.000 bias    2.787\n",
      "iter 1500/30000  loss         0.148218  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    2.789\n",
      "iter 1501/30000  loss         0.148208  avg_L1_norm_grad         0.000117  w[0]    0.000 bias    2.789\n",
      "iter 1520/30000  loss         0.148024  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    2.791\n",
      "iter 1521/30000  loss         0.148015  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    2.792\n",
      "iter 1540/30000  loss         0.147835  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    2.794\n",
      "iter 1541/30000  loss         0.147825  avg_L1_norm_grad         0.000115  w[0]    0.000 bias    2.794\n",
      "iter 1560/30000  loss         0.147648  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    2.796\n",
      "iter 1561/30000  loss         0.147639  avg_L1_norm_grad         0.000114  w[0]    0.000 bias    2.796\n",
      "iter 1580/30000  loss         0.147466  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    2.798\n",
      "iter 1581/30000  loss         0.147457  avg_L1_norm_grad         0.000113  w[0]    0.000 bias    2.798\n",
      "iter 1600/30000  loss         0.147286  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    2.799\n",
      "iter 1601/30000  loss         0.147278  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    2.800\n",
      "iter 1620/30000  loss         0.147110  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    2.801\n",
      "iter 1621/30000  loss         0.147102  avg_L1_norm_grad         0.000111  w[0]    0.000 bias    2.801\n",
      "iter 1640/30000  loss         0.146938  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.803\n",
      "iter 1641/30000  loss         0.146929  avg_L1_norm_grad         0.000110  w[0]    0.000 bias    2.803\n",
      "iter 1660/30000  loss         0.146768  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    2.805\n",
      "iter 1661/30000  loss         0.146760  avg_L1_norm_grad         0.000109  w[0]    0.000 bias    2.805\n",
      "iter 1680/30000  loss         0.146601  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    2.807\n",
      "iter 1681/30000  loss         0.146593  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    2.807\n",
      "iter 1700/30000  loss         0.146438  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    2.808\n",
      "iter 1701/30000  loss         0.146429  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    2.808\n",
      "iter 1720/30000  loss         0.146277  avg_L1_norm_grad         0.000107  w[0]    0.000 bias    2.810\n",
      "iter 1721/30000  loss         0.146269  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    2.810\n",
      "iter 1740/30000  loss         0.146119  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    2.811\n",
      "iter 1741/30000  loss         0.146111  avg_L1_norm_grad         0.000106  w[0]    0.000 bias    2.811\n",
      "iter 1760/30000  loss         0.145963  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.813\n",
      "iter 1761/30000  loss         0.145955  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.813\n",
      "iter 1780/30000  loss         0.145810  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    2.814\n",
      "iter 1781/30000  loss         0.145803  avg_L1_norm_grad         0.000104  w[0]    0.000 bias    2.814\n",
      "iter 1800/30000  loss         0.145660  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    2.816\n",
      "iter 1801/30000  loss         0.145652  avg_L1_norm_grad         0.000103  w[0]    0.000 bias    2.816\n",
      "iter 1820/30000  loss         0.145512  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    2.817\n",
      "iter 1821/30000  loss         0.145505  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    2.817\n",
      "iter 1840/30000  loss         0.145367  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    2.818\n",
      "iter 1841/30000  loss         0.145359  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    2.819\n",
      "iter 1860/30000  loss         0.145224  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    2.820\n",
      "iter 1861/30000  loss         0.145216  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    2.820\n",
      "iter 1880/30000  loss         0.145083  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    2.821\n",
      "iter 1881/30000  loss         0.145076  avg_L1_norm_grad         0.000100  w[0]    0.000 bias    2.821\n",
      "iter 1900/30000  loss         0.144944  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    2.822\n",
      "iter 1901/30000  loss         0.144937  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    2.822\n",
      "iter 1920/30000  loss         0.144808  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    2.823\n",
      "iter 1921/30000  loss         0.144801  avg_L1_norm_grad         0.000099  w[0]    0.000 bias    2.824\n",
      "iter 1940/30000  loss         0.144674  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    2.825\n",
      "iter 1941/30000  loss         0.144667  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    2.825\n",
      "iter 1960/30000  loss         0.144541  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.826\n",
      "iter 1961/30000  loss         0.144535  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.826\n",
      "iter 1980/30000  loss         0.144411  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.827\n",
      "iter 1981/30000  loss         0.144405  avg_L1_norm_grad         0.000097  w[0]    0.000 bias    2.827\n",
      "iter 2000/30000  loss         0.144283  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    2.828\n",
      "iter 2001/30000  loss         0.144276  avg_L1_norm_grad         0.000096  w[0]    0.000 bias    2.828\n",
      "iter 2020/30000  loss         0.144157  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    2.829\n",
      "iter 2021/30000  loss         0.144150  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    2.829\n",
      "iter 2040/30000  loss         0.144032  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    2.830\n",
      "iter 2041/30000  loss         0.144026  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    2.830\n",
      "iter 2060/30000  loss         0.143909  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    2.831\n",
      "iter 2061/30000  loss         0.143903  avg_L1_norm_grad         0.000094  w[0]    0.000 bias    2.831\n",
      "iter 2080/30000  loss         0.143789  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.832\n",
      "iter 2081/30000  loss         0.143783  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.832\n",
      "iter 2100/30000  loss         0.143669  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.833\n",
      "iter 2101/30000  loss         0.143664  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.833\n",
      "iter 2120/30000  loss         0.143552  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    2.834\n",
      "iter 2121/30000  loss         0.143546  avg_L1_norm_grad         0.000092  w[0]    0.000 bias    2.834\n",
      "iter 2140/30000  loss         0.143436  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    2.835\n",
      "iter 2141/30000  loss         0.143431  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    2.835\n",
      "iter 2160/30000  loss         0.143322  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    2.836\n",
      "iter 2161/30000  loss         0.143317  avg_L1_norm_grad         0.000091  w[0]    0.000 bias    2.836\n",
      "iter 2180/30000  loss         0.143210  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    2.836\n",
      "iter 2181/30000  loss         0.143204  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    2.837\n",
      "iter 2200/30000  loss         0.143099  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    2.837\n",
      "iter 2201/30000  loss         0.143093  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    2.837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.142989  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.838\n",
      "iter 2221/30000  loss         0.142984  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.838\n",
      "iter 2240/30000  loss         0.142881  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.839\n",
      "iter 2241/30000  loss         0.142876  avg_L1_norm_grad         0.000089  w[0]    0.000 bias    2.839\n",
      "iter 2260/30000  loss         0.142775  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    2.840\n",
      "iter 2261/30000  loss         0.142770  avg_L1_norm_grad         0.000088  w[0]    0.000 bias    2.840\n",
      "iter 2280/30000  loss         0.142670  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    2.840\n",
      "iter 2281/30000  loss         0.142665  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    2.841\n",
      "iter 2300/30000  loss         0.142566  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    2.841\n",
      "iter 2301/30000  loss         0.142561  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    2.841\n",
      "iter 2320/30000  loss         0.142464  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.842\n",
      "iter 2321/30000  loss         0.142459  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.842\n",
      "iter 2340/30000  loss         0.142363  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.843\n",
      "iter 2341/30000  loss         0.142358  avg_L1_norm_grad         0.000086  w[0]    0.000 bias    2.843\n",
      "iter 2360/30000  loss         0.142263  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    2.843\n",
      "iter 2361/30000  loss         0.142258  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    2.843\n",
      "iter 2380/30000  loss         0.142165  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    2.844\n",
      "iter 2381/30000  loss         0.142160  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    2.844\n",
      "iter 2400/30000  loss         0.142068  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    2.845\n",
      "iter 2401/30000  loss         0.142063  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    2.845\n",
      "iter 2420/30000  loss         0.141972  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    2.845\n",
      "iter 2421/30000  loss         0.141967  avg_L1_norm_grad         0.000084  w[0]    0.000 bias    2.845\n",
      "iter 2440/30000  loss         0.141877  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.846\n",
      "iter 2441/30000  loss         0.141872  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.846\n",
      "iter 2460/30000  loss         0.141784  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.847\n",
      "iter 2461/30000  loss         0.141779  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.847\n",
      "iter 2480/30000  loss         0.141691  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    2.847\n",
      "iter 2481/30000  loss         0.141687  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    2.847\n",
      "iter 2500/30000  loss         0.141600  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    2.848\n",
      "iter 2501/30000  loss         0.141595  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    2.848\n",
      "iter 2520/30000  loss         0.141510  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    2.848\n",
      "iter 2521/30000  loss         0.141505  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    2.848\n",
      "iter 2540/30000  loss         0.141421  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    2.849\n",
      "iter 2541/30000  loss         0.141417  avg_L1_norm_grad         0.000081  w[0]    0.000 bias    2.849\n",
      "iter 2560/30000  loss         0.141333  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.850\n",
      "iter 2561/30000  loss         0.141329  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.850\n",
      "iter 2580/30000  loss         0.141246  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.850\n",
      "iter 2581/30000  loss         0.141242  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.850\n",
      "iter 2600/30000  loss         0.141160  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.851\n",
      "iter 2601/30000  loss         0.141156  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.851\n",
      "iter 2620/30000  loss         0.141076  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    2.851\n",
      "iter 2621/30000  loss         0.141071  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    2.851\n",
      "iter 2640/30000  loss         0.140992  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    2.852\n",
      "iter 2641/30000  loss         0.140988  avg_L1_norm_grad         0.000079  w[0]    0.000 bias    2.852\n",
      "iter 2660/30000  loss         0.140909  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    2.852\n",
      "iter 2661/30000  loss         0.140905  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    2.852\n",
      "iter 2680/30000  loss         0.140827  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    2.853\n",
      "iter 2681/30000  loss         0.140823  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    2.853\n",
      "iter 2700/30000  loss         0.140746  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.853\n",
      "iter 2701/30000  loss         0.140742  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.853\n",
      "iter 2720/30000  loss         0.140666  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.854\n",
      "iter 2721/30000  loss         0.140662  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.854\n",
      "iter 2740/30000  loss         0.140587  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.854\n",
      "iter 2741/30000  loss         0.140583  avg_L1_norm_grad         0.000077  w[0]    0.000 bias    2.854\n",
      "iter 2760/30000  loss         0.140509  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    2.855\n",
      "iter 2761/30000  loss         0.140505  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    2.855\n",
      "iter 2780/30000  loss         0.140432  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    2.855\n",
      "iter 2781/30000  loss         0.140428  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    2.855\n",
      "iter 2800/30000  loss         0.140355  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    2.856\n",
      "iter 2801/30000  loss         0.140352  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    2.856\n",
      "iter 2820/30000  loss         0.140280  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    2.856\n",
      "iter 2821/30000  loss         0.140276  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    2.856\n",
      "iter 2840/30000  loss         0.140205  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    2.857\n",
      "iter 2841/30000  loss         0.140201  avg_L1_norm_grad         0.000075  w[0]    0.000 bias    2.857\n",
      "iter 2860/30000  loss         0.140131  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.857\n",
      "iter 2861/30000  loss         0.140127  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.857\n",
      "iter 2880/30000  loss         0.140058  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.857\n",
      "iter 2881/30000  loss         0.140054  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.857\n",
      "iter 2900/30000  loss         0.139986  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    2.858\n",
      "iter 2901/30000  loss         0.139982  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    2.858\n",
      "iter 2920/30000  loss         0.139914  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    2.858\n",
      "iter 2921/30000  loss         0.139911  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    2.858\n",
      "iter 2940/30000  loss         0.139843  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    2.859\n",
      "iter 2941/30000  loss         0.139840  avg_L1_norm_grad         0.000073  w[0]    0.000 bias    2.859\n",
      "iter 2960/30000  loss         0.139773  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.859\n",
      "iter 2961/30000  loss         0.139770  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.859\n",
      "iter 2980/30000  loss         0.139704  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.859\n",
      "iter 2981/30000  loss         0.139701  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.859\n",
      "iter 3000/30000  loss         0.139636  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.860\n",
      "iter 3001/30000  loss         0.139632  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.139568  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.860\n",
      "iter 3021/30000  loss         0.139565  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.860\n",
      "iter 3040/30000  loss         0.139501  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.861\n",
      "iter 3041/30000  loss         0.139497  avg_L1_norm_grad         0.000071  w[0]    0.000 bias    2.861\n",
      "iter 3060/30000  loss         0.139434  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.861\n",
      "iter 3061/30000  loss         0.139431  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.861\n",
      "iter 3080/30000  loss         0.139369  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.861\n",
      "iter 3081/30000  loss         0.139365  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.861\n",
      "iter 3100/30000  loss         0.139304  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.862\n",
      "iter 3101/30000  loss         0.139300  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.862\n",
      "iter 3120/30000  loss         0.139239  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.862\n",
      "iter 3121/30000  loss         0.139236  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.862\n",
      "iter 3140/30000  loss         0.139176  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.862\n",
      "iter 3141/30000  loss         0.139172  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.862\n",
      "iter 3160/30000  loss         0.139113  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.863\n",
      "iter 3161/30000  loss         0.139109  avg_L1_norm_grad         0.000069  w[0]    0.000 bias    2.863\n",
      "iter 3180/30000  loss         0.139050  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.863\n",
      "iter 3181/30000  loss         0.139047  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.863\n",
      "iter 3200/30000  loss         0.138988  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.863\n",
      "iter 3201/30000  loss         0.138985  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.863\n",
      "iter 3220/30000  loss         0.138927  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.864\n",
      "iter 3221/30000  loss         0.138924  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.864\n",
      "iter 3240/30000  loss         0.138867  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.864\n",
      "iter 3241/30000  loss         0.138864  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.864\n",
      "iter 3260/30000  loss         0.138807  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.864\n",
      "iter 3261/30000  loss         0.138804  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.864\n",
      "iter 3280/30000  loss         0.138747  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.865\n",
      "iter 3281/30000  loss         0.138744  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.865\n",
      "iter 3300/30000  loss         0.138689  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.865\n",
      "iter 3301/30000  loss         0.138686  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.865\n",
      "iter 3320/30000  loss         0.138630  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    2.865\n",
      "iter 3321/30000  loss         0.138627  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    2.865\n",
      "iter 3340/30000  loss         0.138573  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    2.866\n",
      "iter 3341/30000  loss         0.138570  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    2.866\n",
      "iter 3360/30000  loss         0.138516  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    2.866\n",
      "iter 3361/30000  loss         0.138513  avg_L1_norm_grad         0.000066  w[0]    0.000 bias    2.866\n",
      "iter 3380/30000  loss         0.138459  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.866\n",
      "iter 3381/30000  loss         0.138456  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.866\n",
      "iter 3400/30000  loss         0.138403  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.867\n",
      "iter 3401/30000  loss         0.138400  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.867\n",
      "iter 3420/30000  loss         0.138348  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.867\n",
      "iter 3421/30000  loss         0.138345  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.867\n",
      "iter 3440/30000  loss         0.138293  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.867\n",
      "iter 3441/30000  loss         0.138290  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.867\n",
      "iter 3460/30000  loss         0.138238  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.867\n",
      "iter 3461/30000  loss         0.138236  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.867\n",
      "iter 3480/30000  loss         0.138184  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.868\n",
      "iter 3481/30000  loss         0.138182  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.868\n",
      "iter 3500/30000  loss         0.138131  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.868\n",
      "iter 3501/30000  loss         0.138128  avg_L1_norm_grad         0.000064  w[0]    0.000 bias    2.868\n",
      "iter 3520/30000  loss         0.138078  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.868\n",
      "iter 3521/30000  loss         0.138075  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.868\n",
      "iter 3540/30000  loss         0.138026  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.869\n",
      "iter 3541/30000  loss         0.138023  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.869\n",
      "iter 3560/30000  loss         0.137974  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.869\n",
      "iter 3561/30000  loss         0.137971  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.869\n",
      "iter 3580/30000  loss         0.137922  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.869\n",
      "iter 3581/30000  loss         0.137920  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.869\n",
      "iter 3600/30000  loss         0.137871  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.869\n",
      "iter 3601/30000  loss         0.137869  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.869\n",
      "iter 3620/30000  loss         0.137821  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.870\n",
      "iter 3621/30000  loss         0.137818  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.870\n",
      "iter 3640/30000  loss         0.137771  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.870\n",
      "iter 3641/30000  loss         0.137768  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.870\n",
      "iter 3660/30000  loss         0.137721  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.870\n",
      "iter 3661/30000  loss         0.137719  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.870\n",
      "iter 3680/30000  loss         0.137672  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.870\n",
      "iter 3681/30000  loss         0.137669  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.870\n",
      "iter 3700/30000  loss         0.137623  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.871\n",
      "iter 3701/30000  loss         0.137621  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.871\n",
      "iter 3720/30000  loss         0.137575  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.871\n",
      "iter 3721/30000  loss         0.137573  avg_L1_norm_grad         0.000061  w[0]    0.000 bias    2.871\n",
      "iter 3740/30000  loss         0.137527  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.871\n",
      "iter 3741/30000  loss         0.137525  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.871\n",
      "iter 3760/30000  loss         0.137480  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.871\n",
      "iter 3761/30000  loss         0.137477  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.872\n",
      "iter 3780/30000  loss         0.137433  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.872\n",
      "iter 3781/30000  loss         0.137430  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.872\n",
      "iter 3800/30000  loss         0.137386  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.872\n",
      "iter 3801/30000  loss         0.137384  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.137340  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.872\n",
      "iter 3821/30000  loss         0.137338  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.872\n",
      "iter 3840/30000  loss         0.137294  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.872\n",
      "iter 3841/30000  loss         0.137292  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.872\n",
      "iter 3860/30000  loss         0.137249  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.873\n",
      "iter 3861/30000  loss         0.137246  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.873\n",
      "iter 3880/30000  loss         0.137204  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.873\n",
      "iter 3881/30000  loss         0.137202  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.873\n",
      "iter 3900/30000  loss         0.137159  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.873\n",
      "iter 3901/30000  loss         0.137157  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.873\n",
      "iter 3920/30000  loss         0.137115  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.873\n",
      "iter 3921/30000  loss         0.137113  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.873\n",
      "iter 3940/30000  loss         0.137071  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.874\n",
      "iter 3941/30000  loss         0.137069  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.874\n",
      "iter 3960/30000  loss         0.137028  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.874\n",
      "iter 3961/30000  loss         0.137026  avg_L1_norm_grad         0.000058  w[0]    0.000 bias    2.874\n",
      "iter 3980/30000  loss         0.136985  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.874\n",
      "iter 3981/30000  loss         0.136982  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.874\n",
      "iter 4000/30000  loss         0.136942  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.874\n",
      "iter 4001/30000  loss         0.136940  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.874\n",
      "iter 4020/30000  loss         0.136899  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.875\n",
      "iter 4021/30000  loss         0.136897  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.875\n",
      "iter 4040/30000  loss         0.136857  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.875\n",
      "iter 4041/30000  loss         0.136855  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.875\n",
      "iter 4060/30000  loss         0.136816  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.875\n",
      "iter 4061/30000  loss         0.136814  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.875\n",
      "iter 4080/30000  loss         0.136775  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.875\n",
      "iter 4081/30000  loss         0.136772  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.875\n",
      "iter 4100/30000  loss         0.136734  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.875\n",
      "iter 4101/30000  loss         0.136732  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.875\n",
      "iter 4120/30000  loss         0.136693  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.876\n",
      "iter 4121/30000  loss         0.136691  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.876\n",
      "iter 4140/30000  loss         0.136653  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.876\n",
      "iter 4141/30000  loss         0.136651  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.876\n",
      "iter 4160/30000  loss         0.136613  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.876\n",
      "iter 4161/30000  loss         0.136611  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.876\n",
      "iter 4180/30000  loss         0.136573  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.876\n",
      "iter 4181/30000  loss         0.136571  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.876\n",
      "iter 4200/30000  loss         0.136534  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.877\n",
      "iter 4201/30000  loss         0.136532  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.877\n",
      "iter 4220/30000  loss         0.136495  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.877\n",
      "iter 4221/30000  loss         0.136493  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.877\n",
      "iter 4240/30000  loss         0.136456  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.877\n",
      "iter 4241/30000  loss         0.136454  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.877\n",
      "iter 4260/30000  loss         0.136418  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.877\n",
      "iter 4261/30000  loss         0.136416  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.877\n",
      "iter 4280/30000  loss         0.136380  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.877\n",
      "iter 4281/30000  loss         0.136378  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.877\n",
      "iter 4300/30000  loss         0.136342  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.878\n",
      "iter 4301/30000  loss         0.136340  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.878\n",
      "iter 4320/30000  loss         0.136305  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.878\n",
      "iter 4321/30000  loss         0.136303  avg_L1_norm_grad         0.000054  w[0]    0.000 bias    2.878\n",
      "iter 4340/30000  loss         0.136267  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.878\n",
      "iter 4341/30000  loss         0.136266  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.878\n",
      "iter 4360/30000  loss         0.136231  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.878\n",
      "iter 4361/30000  loss         0.136229  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.878\n",
      "iter 4380/30000  loss         0.136194  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.878\n",
      "iter 4381/30000  loss         0.136192  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.878\n",
      "iter 4400/30000  loss         0.136158  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.879\n",
      "iter 4401/30000  loss         0.136156  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.879\n",
      "iter 4420/30000  loss         0.136122  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.879\n",
      "iter 4421/30000  loss         0.136120  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.879\n",
      "iter 4440/30000  loss         0.136086  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.879\n",
      "iter 4441/30000  loss         0.136084  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.879\n",
      "iter 4460/30000  loss         0.136051  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.879\n",
      "iter 4461/30000  loss         0.136049  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.879\n",
      "iter 4480/30000  loss         0.136016  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.879\n",
      "iter 4481/30000  loss         0.136014  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.879\n",
      "iter 4500/30000  loss         0.135981  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.880\n",
      "iter 4501/30000  loss         0.135979  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.880\n",
      "iter 4520/30000  loss         0.135946  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.880\n",
      "iter 4521/30000  loss         0.135944  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.880\n",
      "iter 4540/30000  loss         0.135912  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.880\n",
      "iter 4541/30000  loss         0.135910  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.880\n",
      "iter 4560/30000  loss         0.135878  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.880\n",
      "iter 4561/30000  loss         0.135876  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.880\n",
      "iter 4580/30000  loss         0.135844  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.880\n",
      "iter 4581/30000  loss         0.135842  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.880\n",
      "iter 4600/30000  loss         0.135810  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.881\n",
      "iter 4601/30000  loss         0.135809  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4620/30000  loss         0.135777  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.881\n",
      "iter 4621/30000  loss         0.135775  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.881\n",
      "iter 4640/30000  loss         0.135744  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.881\n",
      "iter 4641/30000  loss         0.135742  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.881\n",
      "iter 4660/30000  loss         0.135711  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.881\n",
      "iter 4661/30000  loss         0.135710  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.881\n",
      "iter 4680/30000  loss         0.135679  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.881\n",
      "iter 4681/30000  loss         0.135677  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.881\n",
      "iter 4700/30000  loss         0.135647  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.881\n",
      "iter 4701/30000  loss         0.135645  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.881\n",
      "iter 4720/30000  loss         0.135614  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.882\n",
      "iter 4721/30000  loss         0.135613  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.882\n",
      "iter 4740/30000  loss         0.135583  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.882\n",
      "iter 4741/30000  loss         0.135581  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.882\n",
      "iter 4760/30000  loss         0.135551  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.882\n",
      "iter 4761/30000  loss         0.135550  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.882\n",
      "iter 4780/30000  loss         0.135520  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.882\n",
      "iter 4781/30000  loss         0.135518  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.882\n",
      "iter 4800/30000  loss         0.135489  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.882\n",
      "iter 4801/30000  loss         0.135487  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.882\n",
      "iter 4820/30000  loss         0.135458  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.883\n",
      "iter 4821/30000  loss         0.135456  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.883\n",
      "iter 4840/30000  loss         0.135427  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.883\n",
      "iter 4841/30000  loss         0.135426  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.883\n",
      "iter 4860/30000  loss         0.135397  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.883\n",
      "iter 4861/30000  loss         0.135395  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.883\n",
      "iter 4880/30000  loss         0.135367  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.883\n",
      "iter 4881/30000  loss         0.135365  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.883\n",
      "iter 4900/30000  loss         0.135337  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.883\n",
      "iter 4901/30000  loss         0.135335  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.883\n",
      "iter 4920/30000  loss         0.135307  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.883\n",
      "iter 4921/30000  loss         0.135306  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.883\n",
      "iter 4940/30000  loss         0.135277  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.884\n",
      "iter 4941/30000  loss         0.135276  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.884\n",
      "iter 4960/30000  loss         0.135248  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.884\n",
      "iter 4961/30000  loss         0.135247  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.884\n",
      "iter 4980/30000  loss         0.135219  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.884\n",
      "iter 4981/30000  loss         0.135218  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.884\n",
      "iter 5000/30000  loss         0.135190  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.884\n",
      "iter 5001/30000  loss         0.135189  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.884\n",
      "iter 5020/30000  loss         0.135162  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.884\n",
      "iter 5021/30000  loss         0.135160  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.884\n",
      "iter 5040/30000  loss         0.135133  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.884\n",
      "iter 5041/30000  loss         0.135132  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.885\n",
      "iter 5060/30000  loss         0.135105  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.885\n",
      "iter 5061/30000  loss         0.135103  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.885\n",
      "iter 5080/30000  loss         0.135077  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.885\n",
      "iter 5081/30000  loss         0.135075  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.885\n",
      "iter 5100/30000  loss         0.135049  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.885\n",
      "iter 5101/30000  loss         0.135048  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.885\n",
      "iter 5120/30000  loss         0.135021  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.885\n",
      "iter 5121/30000  loss         0.135020  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.885\n",
      "iter 5140/30000  loss         0.134994  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.885\n",
      "iter 5141/30000  loss         0.134992  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.885\n",
      "iter 5160/30000  loss         0.134967  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5161/30000  loss         0.134965  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5180/30000  loss         0.134939  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5181/30000  loss         0.134938  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5200/30000  loss         0.134913  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5201/30000  loss         0.134911  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5220/30000  loss         0.134886  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5221/30000  loss         0.134885  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5240/30000  loss         0.134859  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5241/30000  loss         0.134858  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.886\n",
      "iter 5260/30000  loss         0.134833  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.886\n",
      "iter 5261/30000  loss         0.134832  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.886\n",
      "iter 5280/30000  loss         0.134807  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5281/30000  loss         0.134806  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5300/30000  loss         0.134781  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5301/30000  loss         0.134780  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5320/30000  loss         0.134755  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5321/30000  loss         0.134754  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5340/30000  loss         0.134730  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5341/30000  loss         0.134728  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5360/30000  loss         0.134704  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5361/30000  loss         0.134703  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.887\n",
      "iter 5380/30000  loss         0.134679  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.887\n",
      "iter 5381/30000  loss         0.134678  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.887\n",
      "iter 5400/30000  loss         0.134654  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.887\n",
      "iter 5401/30000  loss         0.134653  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5420/30000  loss         0.134629  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5421/30000  loss         0.134628  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5440/30000  loss         0.134604  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5441/30000  loss         0.134603  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5460/30000  loss         0.134580  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5461/30000  loss         0.134579  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5480/30000  loss         0.134555  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5481/30000  loss         0.134554  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5500/30000  loss         0.134531  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5501/30000  loss         0.134530  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.888\n",
      "iter 5520/30000  loss         0.134507  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.888\n",
      "iter 5521/30000  loss         0.134506  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.888\n",
      "iter 5540/30000  loss         0.134483  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5541/30000  loss         0.134482  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5560/30000  loss         0.134460  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5561/30000  loss         0.134458  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5580/30000  loss         0.134436  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5581/30000  loss         0.134435  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5600/30000  loss         0.134413  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5601/30000  loss         0.134411  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5620/30000  loss         0.134389  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5621/30000  loss         0.134388  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5640/30000  loss         0.134366  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5641/30000  loss         0.134365  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.889\n",
      "iter 5660/30000  loss         0.134343  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5661/30000  loss         0.134342  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5680/30000  loss         0.134321  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5681/30000  loss         0.134319  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5700/30000  loss         0.134298  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5701/30000  loss         0.134297  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5720/30000  loss         0.134275  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5721/30000  loss         0.134274  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5740/30000  loss         0.134253  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5741/30000  loss         0.134252  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5760/30000  loss         0.134231  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5761/30000  loss         0.134230  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5780/30000  loss         0.134209  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5781/30000  loss         0.134208  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.890\n",
      "iter 5800/30000  loss         0.134187  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.891\n",
      "iter 5801/30000  loss         0.134186  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.891\n",
      "iter 5820/30000  loss         0.134165  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5821/30000  loss         0.134164  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5840/30000  loss         0.134144  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5841/30000  loss         0.134143  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5860/30000  loss         0.134122  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5861/30000  loss         0.134121  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5880/30000  loss         0.134101  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5881/30000  loss         0.134100  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5900/30000  loss         0.134080  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5901/30000  loss         0.134079  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5920/30000  loss         0.134059  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5921/30000  loss         0.134058  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.891\n",
      "iter 5940/30000  loss         0.134038  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.892\n",
      "iter 5941/30000  loss         0.134037  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.892\n",
      "iter 5960/30000  loss         0.134017  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 5961/30000  loss         0.134016  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 5980/30000  loss         0.133996  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 5981/30000  loss         0.133995  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 6000/30000  loss         0.133976  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 6001/30000  loss         0.133975  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 6020/30000  loss         0.133955  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 6021/30000  loss         0.133954  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 6040/30000  loss         0.133935  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 6041/30000  loss         0.133934  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.892\n",
      "iter 6060/30000  loss         0.133915  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.893\n",
      "iter 6061/30000  loss         0.133914  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.893\n",
      "iter 6080/30000  loss         0.133895  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.893\n",
      "iter 6081/30000  loss         0.133894  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.893\n",
      "iter 6100/30000  loss         0.133875  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.893\n",
      "iter 6101/30000  loss         0.133874  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.893\n",
      "iter 6120/30000  loss         0.133855  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6121/30000  loss         0.133854  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6140/30000  loss         0.133836  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6141/30000  loss         0.133835  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6160/30000  loss         0.133816  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6161/30000  loss         0.133815  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6180/30000  loss         0.133797  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6181/30000  loss         0.133796  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.893\n",
      "iter 6200/30000  loss         0.133778  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6201/30000  loss         0.133777  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6220/30000  loss         0.133759  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6221/30000  loss         0.133758  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6240/30000  loss         0.133740  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6241/30000  loss         0.133739  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6260/30000  loss         0.133721  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6261/30000  loss         0.133720  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6280/30000  loss         0.133702  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6281/30000  loss         0.133701  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.894\n",
      "iter 6300/30000  loss         0.133683  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.894\n",
      "iter 6301/30000  loss         0.133682  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.894\n",
      "iter 6320/30000  loss         0.133665  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.894\n",
      "iter 6321/30000  loss         0.133664  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.894\n",
      "iter 6340/30000  loss         0.133646  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.894\n",
      "iter 6341/30000  loss         0.133645  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.894\n",
      "iter 6360/30000  loss         0.133628  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6361/30000  loss         0.133627  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6380/30000  loss         0.133610  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6381/30000  loss         0.133609  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6400/30000  loss         0.133592  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6401/30000  loss         0.133591  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6420/30000  loss         0.133574  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6421/30000  loss         0.133573  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6440/30000  loss         0.133556  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6441/30000  loss         0.133555  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6460/30000  loss         0.133538  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6461/30000  loss         0.133537  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.895\n",
      "iter 6480/30000  loss         0.133521  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.895\n",
      "iter 6481/30000  loss         0.133520  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.895\n",
      "iter 6500/30000  loss         0.133503  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6501/30000  loss         0.133502  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6520/30000  loss         0.133486  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6521/30000  loss         0.133485  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6540/30000  loss         0.133468  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6541/30000  loss         0.133468  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6560/30000  loss         0.133451  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6561/30000  loss         0.133450  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6580/30000  loss         0.133434  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6581/30000  loss         0.133433  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6600/30000  loss         0.133417  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6601/30000  loss         0.133416  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6620/30000  loss         0.133400  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6621/30000  loss         0.133399  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.896\n",
      "iter 6640/30000  loss         0.133383  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.897\n",
      "iter 6641/30000  loss         0.133383  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.897\n",
      "iter 6660/30000  loss         0.133367  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6661/30000  loss         0.133366  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6680/30000  loss         0.133350  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6681/30000  loss         0.133349  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6700/30000  loss         0.133334  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6701/30000  loss         0.133333  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6720/30000  loss         0.133317  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6721/30000  loss         0.133316  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6740/30000  loss         0.133301  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6741/30000  loss         0.133300  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6760/30000  loss         0.133285  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6761/30000  loss         0.133284  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6780/30000  loss         0.133269  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6781/30000  loss         0.133268  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.897\n",
      "iter 6800/30000  loss         0.133253  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.898\n",
      "iter 6801/30000  loss         0.133252  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.898\n",
      "iter 6820/30000  loss         0.133237  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.898\n",
      "iter 6821/30000  loss         0.133236  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.898\n",
      "iter 6840/30000  loss         0.133221  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6841/30000  loss         0.133220  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6860/30000  loss         0.133205  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6861/30000  loss         0.133205  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6880/30000  loss         0.133190  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6881/30000  loss         0.133189  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6900/30000  loss         0.133174  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6901/30000  loss         0.133173  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6920/30000  loss         0.133159  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6921/30000  loss         0.133158  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6940/30000  loss         0.133143  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.898\n",
      "iter 6941/30000  loss         0.133143  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 6960/30000  loss         0.133128  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 6961/30000  loss         0.133127  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 6980/30000  loss         0.133113  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 6981/30000  loss         0.133112  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 7000/30000  loss         0.133098  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 7001/30000  loss         0.133097  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7020/30000  loss         0.133083  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 7021/30000  loss         0.133082  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 7040/30000  loss         0.133068  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.899\n",
      "iter 7041/30000  loss         0.133067  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.899\n",
      "iter 7060/30000  loss         0.133053  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.899\n",
      "iter 7061/30000  loss         0.133052  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.899\n",
      "iter 7080/30000  loss         0.133038  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.899\n",
      "iter 7081/30000  loss         0.133038  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.899\n",
      "iter 7100/30000  loss         0.133024  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7101/30000  loss         0.133023  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7120/30000  loss         0.133009  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7121/30000  loss         0.133009  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7140/30000  loss         0.132995  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7141/30000  loss         0.132994  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7160/30000  loss         0.132980  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7161/30000  loss         0.132980  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7180/30000  loss         0.132966  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7181/30000  loss         0.132965  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7200/30000  loss         0.132952  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7201/30000  loss         0.132951  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7220/30000  loss         0.132938  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7221/30000  loss         0.132937  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7240/30000  loss         0.132924  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7241/30000  loss         0.132923  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.900\n",
      "iter 7260/30000  loss         0.132910  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7261/30000  loss         0.132909  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7280/30000  loss         0.132896  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7281/30000  loss         0.132895  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7300/30000  loss         0.132882  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7301/30000  loss         0.132881  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7320/30000  loss         0.132868  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7321/30000  loss         0.132868  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7340/30000  loss         0.132855  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7341/30000  loss         0.132854  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7360/30000  loss         0.132841  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7361/30000  loss         0.132840  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7380/30000  loss         0.132828  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7381/30000  loss         0.132827  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7400/30000  loss         0.132814  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7401/30000  loss         0.132813  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7420/30000  loss         0.132801  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7421/30000  loss         0.132800  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.901\n",
      "iter 7440/30000  loss         0.132787  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.902\n",
      "iter 7441/30000  loss         0.132787  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.902\n",
      "iter 7460/30000  loss         0.132774  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.902\n",
      "iter 7461/30000  loss         0.132774  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.902\n",
      "iter 7480/30000  loss         0.132761  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7481/30000  loss         0.132761  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7500/30000  loss         0.132748  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7501/30000  loss         0.132747  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7520/30000  loss         0.132735  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7521/30000  loss         0.132734  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7540/30000  loss         0.132722  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7541/30000  loss         0.132722  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7560/30000  loss         0.132709  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7561/30000  loss         0.132709  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7580/30000  loss         0.132697  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7581/30000  loss         0.132696  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.902\n",
      "iter 7600/30000  loss         0.132684  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7601/30000  loss         0.132683  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7620/30000  loss         0.132671  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7621/30000  loss         0.132671  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7640/30000  loss         0.132659  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7641/30000  loss         0.132658  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7660/30000  loss         0.132646  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7661/30000  loss         0.132646  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7680/30000  loss         0.132634  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7681/30000  loss         0.132633  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.903\n",
      "iter 7700/30000  loss         0.132622  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7701/30000  loss         0.132621  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7720/30000  loss         0.132609  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7721/30000  loss         0.132609  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7740/30000  loss         0.132597  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7741/30000  loss         0.132596  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7760/30000  loss         0.132585  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7761/30000  loss         0.132584  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.903\n",
      "iter 7780/30000  loss         0.132573  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7781/30000  loss         0.132572  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7800/30000  loss         0.132561  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7801/30000  loss         0.132560  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7820/30000  loss         0.132549  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7821/30000  loss         0.132548  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7840/30000  loss         0.132537  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7841/30000  loss         0.132537  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7860/30000  loss         0.132525  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7861/30000  loss         0.132525  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7880/30000  loss         0.132514  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7881/30000  loss         0.132513  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7900/30000  loss         0.132502  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7901/30000  loss         0.132501  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7920/30000  loss         0.132490  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7921/30000  loss         0.132490  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.904\n",
      "iter 7940/30000  loss         0.132479  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.905\n",
      "iter 7941/30000  loss         0.132478  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.905\n",
      "iter 7960/30000  loss         0.132467  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 7961/30000  loss         0.132467  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 7980/30000  loss         0.132456  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 7981/30000  loss         0.132455  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8000/30000  loss         0.132444  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8001/30000  loss         0.132444  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8020/30000  loss         0.132433  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8021/30000  loss         0.132433  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8040/30000  loss         0.132422  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8041/30000  loss         0.132421  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8060/30000  loss         0.132411  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8061/30000  loss         0.132410  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8080/30000  loss         0.132400  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8081/30000  loss         0.132399  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8100/30000  loss         0.132389  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8101/30000  loss         0.132388  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.905\n",
      "iter 8120/30000  loss         0.132378  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8121/30000  loss         0.132377  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8140/30000  loss         0.132367  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8141/30000  loss         0.132366  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8160/30000  loss         0.132356  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8161/30000  loss         0.132355  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8180/30000  loss         0.132345  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8181/30000  loss         0.132344  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.906\n",
      "iter 8200/30000  loss         0.132334  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8201/30000  loss         0.132334  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8220/30000  loss         0.132323  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8221/30000  loss         0.132323  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8240/30000  loss         0.132313  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8241/30000  loss         0.132312  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8260/30000  loss         0.132302  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8261/30000  loss         0.132302  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8280/30000  loss         0.132292  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8281/30000  loss         0.132291  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.906\n",
      "iter 8300/30000  loss         0.132281  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8301/30000  loss         0.132281  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8320/30000  loss         0.132271  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8321/30000  loss         0.132270  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8340/30000  loss         0.132260  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8341/30000  loss         0.132260  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8360/30000  loss         0.132250  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8361/30000  loss         0.132250  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8380/30000  loss         0.132240  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8381/30000  loss         0.132239  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8400/30000  loss         0.132230  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8401/30000  loss         0.132229  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8420/30000  loss         0.132219  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8421/30000  loss         0.132219  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8440/30000  loss         0.132209  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8441/30000  loss         0.132209  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8460/30000  loss         0.132199  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8461/30000  loss         0.132199  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.907\n",
      "iter 8480/30000  loss         0.132189  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.907\n",
      "iter 8481/30000  loss         0.132189  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.907\n",
      "iter 8500/30000  loss         0.132179  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8501/30000  loss         0.132179  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8520/30000  loss         0.132170  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8521/30000  loss         0.132169  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8540/30000  loss         0.132160  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8541/30000  loss         0.132159  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8560/30000  loss         0.132150  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8561/30000  loss         0.132149  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8580/30000  loss         0.132140  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8581/30000  loss         0.132140  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8600/30000  loss         0.132131  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8601/30000  loss         0.132130  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8620/30000  loss         0.132121  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8621/30000  loss         0.132120  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8640/30000  loss         0.132111  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8641/30000  loss         0.132111  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8660/30000  loss         0.132102  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8661/30000  loss         0.132101  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.908\n",
      "iter 8680/30000  loss         0.132092  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8681/30000  loss         0.132092  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8700/30000  loss         0.132083  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8701/30000  loss         0.132082  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8720/30000  loss         0.132073  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8721/30000  loss         0.132073  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8740/30000  loss         0.132064  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8741/30000  loss         0.132064  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.909\n",
      "iter 8760/30000  loss         0.132055  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8761/30000  loss         0.132054  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8780/30000  loss         0.132046  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8781/30000  loss         0.132045  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8800/30000  loss         0.132036  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8801/30000  loss         0.132036  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8820/30000  loss         0.132027  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8821/30000  loss         0.132027  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8840/30000  loss         0.132018  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8841/30000  loss         0.132018  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8860/30000  loss         0.132009  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8861/30000  loss         0.132009  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.909\n",
      "iter 8880/30000  loss         0.132000  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8881/30000  loss         0.132000  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8900/30000  loss         0.131991  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8901/30000  loss         0.131991  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8920/30000  loss         0.131982  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8921/30000  loss         0.131982  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8940/30000  loss         0.131973  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8941/30000  loss         0.131973  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8960/30000  loss         0.131965  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8961/30000  loss         0.131964  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8980/30000  loss         0.131956  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 8981/30000  loss         0.131955  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 9000/30000  loss         0.131947  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 9001/30000  loss         0.131947  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 9020/30000  loss         0.131938  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 9021/30000  loss         0.131938  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 9040/30000  loss         0.131930  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 9041/30000  loss         0.131929  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.910\n",
      "iter 9060/30000  loss         0.131921  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.910\n",
      "iter 9061/30000  loss         0.131921  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.910\n",
      "iter 9080/30000  loss         0.131913  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9081/30000  loss         0.131912  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9100/30000  loss         0.131904  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9101/30000  loss         0.131904  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9120/30000  loss         0.131896  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9121/30000  loss         0.131895  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9140/30000  loss         0.131887  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9141/30000  loss         0.131887  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9160/30000  loss         0.131879  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9161/30000  loss         0.131878  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9180/30000  loss         0.131870  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9181/30000  loss         0.131870  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9200/30000  loss         0.131862  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9201/30000  loss         0.131862  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9220/30000  loss         0.131854  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9221/30000  loss         0.131853  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9240/30000  loss         0.131846  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9241/30000  loss         0.131845  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9260/30000  loss         0.131837  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9261/30000  loss         0.131837  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9280/30000  loss         0.131829  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9281/30000  loss         0.131829  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.911\n",
      "iter 9300/30000  loss         0.131821  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9301/30000  loss         0.131821  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9320/30000  loss         0.131813  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9321/30000  loss         0.131813  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9340/30000  loss         0.131805  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9341/30000  loss         0.131805  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9360/30000  loss         0.131797  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9361/30000  loss         0.131797  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.912\n",
      "iter 9380/30000  loss         0.131789  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9381/30000  loss         0.131789  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9400/30000  loss         0.131781  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9401/30000  loss         0.131781  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9420/30000  loss         0.131773  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9421/30000  loss         0.131773  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9440/30000  loss         0.131766  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9441/30000  loss         0.131765  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9460/30000  loss         0.131758  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9461/30000  loss         0.131758  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9480/30000  loss         0.131750  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9481/30000  loss         0.131750  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9500/30000  loss         0.131742  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9501/30000  loss         0.131742  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.912\n",
      "iter 9520/30000  loss         0.131735  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9521/30000  loss         0.131734  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9540/30000  loss         0.131727  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9541/30000  loss         0.131727  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9560/30000  loss         0.131720  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9561/30000  loss         0.131719  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9580/30000  loss         0.131712  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9581/30000  loss         0.131712  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9600/30000  loss         0.131704  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9601/30000  loss         0.131704  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9620/30000  loss         0.131697  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9621/30000  loss         0.131697  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9640/30000  loss         0.131690  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9641/30000  loss         0.131689  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9660/30000  loss         0.131682  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9661/30000  loss         0.131682  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9680/30000  loss         0.131675  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9681/30000  loss         0.131674  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9700/30000  loss         0.131667  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.913\n",
      "iter 9701/30000  loss         0.131667  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.913\n",
      "iter 9720/30000  loss         0.131660  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.913\n",
      "iter 9721/30000  loss         0.131660  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.913\n",
      "iter 9740/30000  loss         0.131653  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9741/30000  loss         0.131653  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9760/30000  loss         0.131646  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9761/30000  loss         0.131645  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9780/30000  loss         0.131638  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9781/30000  loss         0.131638  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9800/30000  loss         0.131631  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9801/30000  loss         0.131631  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9820/30000  loss         0.131624  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9821/30000  loss         0.131624  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9840/30000  loss         0.131617  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9841/30000  loss         0.131617  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9860/30000  loss         0.131610  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9861/30000  loss         0.131610  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9880/30000  loss         0.131603  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9881/30000  loss         0.131603  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9900/30000  loss         0.131596  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9901/30000  loss         0.131596  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9920/30000  loss         0.131589  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9921/30000  loss         0.131589  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9940/30000  loss         0.131582  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9941/30000  loss         0.131582  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.914\n",
      "iter 9960/30000  loss         0.131575  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 9961/30000  loss         0.131575  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 9980/30000  loss         0.131568  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 9981/30000  loss         0.131568  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 10000/30000  loss         0.131562  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 10001/30000  loss         0.131561  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 10020/30000  loss         0.131555  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 10021/30000  loss         0.131554  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 10040/30000  loss         0.131548  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 10041/30000  loss         0.131548  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.915\n",
      "iter 10060/30000  loss         0.131541  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10061/30000  loss         0.131541  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10080/30000  loss         0.131535  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10081/30000  loss         0.131534  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10100/30000  loss         0.131528  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10101/30000  loss         0.131528  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10120/30000  loss         0.131521  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10121/30000  loss         0.131521  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10140/30000  loss         0.131515  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10141/30000  loss         0.131514  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10160/30000  loss         0.131508  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10161/30000  loss         0.131508  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10180/30000  loss         0.131502  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10181/30000  loss         0.131501  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.915\n",
      "iter 10200/30000  loss         0.131495  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10201/30000  loss         0.131495  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10220/30000  loss         0.131489  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10221/30000  loss         0.131488  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10240/30000  loss         0.131482  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10241/30000  loss         0.131482  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10260/30000  loss         0.131476  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10261/30000  loss         0.131476  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10280/30000  loss         0.131469  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10281/30000  loss         0.131469  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10300/30000  loss         0.131463  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10301/30000  loss         0.131463  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10320/30000  loss         0.131457  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10321/30000  loss         0.131456  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10340/30000  loss         0.131451  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10341/30000  loss         0.131450  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10360/30000  loss         0.131444  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10361/30000  loss         0.131444  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10380/30000  loss         0.131438  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10381/30000  loss         0.131438  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10400/30000  loss         0.131432  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10401/30000  loss         0.131432  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10420/30000  loss         0.131426  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10421/30000  loss         0.131425  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.916\n",
      "iter 10440/30000  loss         0.131420  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10441/30000  loss         0.131419  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10460/30000  loss         0.131413  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10461/30000  loss         0.131413  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10480/30000  loss         0.131407  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10481/30000  loss         0.131407  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10500/30000  loss         0.131401  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10501/30000  loss         0.131401  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10520/30000  loss         0.131395  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10521/30000  loss         0.131395  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10540/30000  loss         0.131389  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10541/30000  loss         0.131389  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10560/30000  loss         0.131383  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10561/30000  loss         0.131383  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10580/30000  loss         0.131377  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10581/30000  loss         0.131377  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10600/30000  loss         0.131371  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10601/30000  loss         0.131371  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10620/30000  loss         0.131365  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10621/30000  loss         0.131365  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10640/30000  loss         0.131360  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10641/30000  loss         0.131359  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10660/30000  loss         0.131354  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10661/30000  loss         0.131353  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10680/30000  loss         0.131348  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10681/30000  loss         0.131348  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.917\n",
      "iter 10700/30000  loss         0.131342  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10701/30000  loss         0.131342  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10720/30000  loss         0.131336  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10721/30000  loss         0.131336  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10740/30000  loss         0.131331  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10741/30000  loss         0.131330  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10760/30000  loss         0.131325  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10761/30000  loss         0.131325  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10780/30000  loss         0.131319  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10781/30000  loss         0.131319  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10800/30000  loss         0.131314  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10801/30000  loss         0.131313  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10820/30000  loss         0.131308  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10821/30000  loss         0.131308  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.918\n",
      "iter 10840/30000  loss         0.131302  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10841/30000  loss         0.131302  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10860/30000  loss         0.131297  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10861/30000  loss         0.131297  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10880/30000  loss         0.131291  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10881/30000  loss         0.131291  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10900/30000  loss         0.131286  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10901/30000  loss         0.131286  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10920/30000  loss         0.131280  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10921/30000  loss         0.131280  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10940/30000  loss         0.131275  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10941/30000  loss         0.131275  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.918\n",
      "iter 10960/30000  loss         0.131269  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 10961/30000  loss         0.131269  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 10980/30000  loss         0.131264  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 10981/30000  loss         0.131264  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11000/30000  loss         0.131259  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11001/30000  loss         0.131258  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11020/30000  loss         0.131253  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11021/30000  loss         0.131253  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11040/30000  loss         0.131248  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11041/30000  loss         0.131248  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11060/30000  loss         0.131243  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11061/30000  loss         0.131242  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11080/30000  loss         0.131237  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11081/30000  loss         0.131237  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11100/30000  loss         0.131232  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11101/30000  loss         0.131232  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11120/30000  loss         0.131227  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11121/30000  loss         0.131226  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11140/30000  loss         0.131221  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11141/30000  loss         0.131221  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11160/30000  loss         0.131216  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11161/30000  loss         0.131216  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11180/30000  loss         0.131211  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11181/30000  loss         0.131211  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11200/30000  loss         0.131206  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11201/30000  loss         0.131206  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.919\n",
      "iter 11220/30000  loss         0.131201  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.920\n",
      "iter 11221/30000  loss         0.131201  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.920\n",
      "iter 11240/30000  loss         0.131196  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.920\n",
      "iter 11241/30000  loss         0.131195  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.920\n",
      "iter 11260/30000  loss         0.131191  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.920\n",
      "iter 11261/30000  loss         0.131190  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11280/30000  loss         0.131186  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11281/30000  loss         0.131185  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11300/30000  loss         0.131180  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11301/30000  loss         0.131180  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11320/30000  loss         0.131175  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11321/30000  loss         0.131175  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11340/30000  loss         0.131170  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11341/30000  loss         0.131170  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11360/30000  loss         0.131165  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11361/30000  loss         0.131165  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11380/30000  loss         0.131161  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11381/30000  loss         0.131160  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11400/30000  loss         0.131156  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11401/30000  loss         0.131155  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11420/30000  loss         0.131151  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11421/30000  loss         0.131150  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11440/30000  loss         0.131146  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11441/30000  loss         0.131146  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11460/30000  loss         0.131141  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11461/30000  loss         0.131141  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11480/30000  loss         0.131136  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11481/30000  loss         0.131136  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11500/30000  loss         0.131131  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11501/30000  loss         0.131131  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.920\n",
      "iter 11520/30000  loss         0.131126  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11521/30000  loss         0.131126  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11540/30000  loss         0.131122  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11541/30000  loss         0.131121  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11560/30000  loss         0.131117  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11561/30000  loss         0.131117  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11580/30000  loss         0.131112  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11581/30000  loss         0.131112  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11600/30000  loss         0.131107  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11601/30000  loss         0.131107  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11620/30000  loss         0.131103  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11621/30000  loss         0.131103  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11640/30000  loss         0.131098  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11641/30000  loss         0.131098  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11660/30000  loss         0.131093  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11661/30000  loss         0.131093  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11680/30000  loss         0.131089  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11681/30000  loss         0.131089  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11700/30000  loss         0.131084  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11701/30000  loss         0.131084  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.921\n",
      "iter 11720/30000  loss         0.131080  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11721/30000  loss         0.131079  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11740/30000  loss         0.131075  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11741/30000  loss         0.131075  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11760/30000  loss         0.131070  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11761/30000  loss         0.131070  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11780/30000  loss         0.131066  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11781/30000  loss         0.131066  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.921\n",
      "iter 11800/30000  loss         0.131061  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11801/30000  loss         0.131061  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11820/30000  loss         0.131057  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11821/30000  loss         0.131057  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11840/30000  loss         0.131052  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11841/30000  loss         0.131052  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11860/30000  loss         0.131048  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11861/30000  loss         0.131048  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11880/30000  loss         0.131044  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11881/30000  loss         0.131043  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11900/30000  loss         0.131039  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11901/30000  loss         0.131039  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11920/30000  loss         0.131035  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11921/30000  loss         0.131035  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11940/30000  loss         0.131030  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11941/30000  loss         0.131030  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11960/30000  loss         0.131026  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11961/30000  loss         0.131026  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11980/30000  loss         0.131022  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 11981/30000  loss         0.131021  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12000/30000  loss         0.131017  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12001/30000  loss         0.131017  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12020/30000  loss         0.131013  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12021/30000  loss         0.131013  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12040/30000  loss         0.131009  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12041/30000  loss         0.131009  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12060/30000  loss         0.131004  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12061/30000  loss         0.131004  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12080/30000  loss         0.131000  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12081/30000  loss         0.131000  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12100/30000  loss         0.130996  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12101/30000  loss         0.130996  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.922\n",
      "iter 12120/30000  loss         0.130992  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12121/30000  loss         0.130992  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12140/30000  loss         0.130988  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12141/30000  loss         0.130987  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12160/30000  loss         0.130983  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12161/30000  loss         0.130983  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12180/30000  loss         0.130979  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12181/30000  loss         0.130979  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12200/30000  loss         0.130975  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12201/30000  loss         0.130975  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.923\n",
      "iter 12220/30000  loss         0.130971  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12221/30000  loss         0.130971  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12240/30000  loss         0.130967  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12241/30000  loss         0.130967  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12260/30000  loss         0.130963  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12261/30000  loss         0.130963  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12280/30000  loss         0.130959  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12281/30000  loss         0.130959  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12300/30000  loss         0.130955  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12301/30000  loss         0.130955  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12320/30000  loss         0.130951  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12321/30000  loss         0.130950  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12340/30000  loss         0.130947  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12341/30000  loss         0.130946  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12360/30000  loss         0.130943  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12361/30000  loss         0.130942  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12380/30000  loss         0.130939  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12381/30000  loss         0.130938  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12400/30000  loss         0.130935  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12401/30000  loss         0.130935  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12420/30000  loss         0.130931  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12421/30000  loss         0.130931  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.923\n",
      "iter 12440/30000  loss         0.130927  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12441/30000  loss         0.130927  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12460/30000  loss         0.130923  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12461/30000  loss         0.130923  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12480/30000  loss         0.130919  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12481/30000  loss         0.130919  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12500/30000  loss         0.130915  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12501/30000  loss         0.130915  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12520/30000  loss         0.130911  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12521/30000  loss         0.130911  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12540/30000  loss         0.130907  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12541/30000  loss         0.130907  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12560/30000  loss         0.130904  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12561/30000  loss         0.130903  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12580/30000  loss         0.130900  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12581/30000  loss         0.130900  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12600/30000  loss         0.130896  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12601/30000  loss         0.130896  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12620/30000  loss         0.130892  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12621/30000  loss         0.130892  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12640/30000  loss         0.130888  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12641/30000  loss         0.130888  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12660/30000  loss         0.130885  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12661/30000  loss         0.130885  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12680/30000  loss         0.130881  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12681/30000  loss         0.130881  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12700/30000  loss         0.130877  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12701/30000  loss         0.130877  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12720/30000  loss         0.130874  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12721/30000  loss         0.130873  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.924\n",
      "iter 12740/30000  loss         0.130870  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.924\n",
      "iter 12741/30000  loss         0.130870  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.924\n",
      "iter 12760/30000  loss         0.130866  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.924\n",
      "iter 12761/30000  loss         0.130866  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.924\n",
      "iter 12780/30000  loss         0.130863  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12781/30000  loss         0.130862  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12800/30000  loss         0.130859  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12801/30000  loss         0.130859  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12820/30000  loss         0.130855  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12821/30000  loss         0.130855  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12840/30000  loss         0.130852  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12841/30000  loss         0.130851  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12860/30000  loss         0.130848  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12861/30000  loss         0.130848  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12880/30000  loss         0.130844  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12881/30000  loss         0.130844  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12900/30000  loss         0.130841  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12901/30000  loss         0.130841  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12920/30000  loss         0.130837  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12921/30000  loss         0.130837  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12940/30000  loss         0.130834  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12941/30000  loss         0.130834  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12960/30000  loss         0.130830  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12961/30000  loss         0.130830  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12980/30000  loss         0.130827  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 12981/30000  loss         0.130827  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13000/30000  loss         0.130823  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13001/30000  loss         0.130823  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13020/30000  loss         0.130820  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13021/30000  loss         0.130820  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13040/30000  loss         0.130816  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13041/30000  loss         0.130816  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13060/30000  loss         0.130813  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13061/30000  loss         0.130813  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13080/30000  loss         0.130809  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13081/30000  loss         0.130809  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13100/30000  loss         0.130806  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13101/30000  loss         0.130806  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13120/30000  loss         0.130803  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13121/30000  loss         0.130802  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13140/30000  loss         0.130799  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13141/30000  loss         0.130799  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.925\n",
      "iter 13160/30000  loss         0.130796  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13161/30000  loss         0.130796  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13180/30000  loss         0.130793  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13181/30000  loss         0.130792  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13200/30000  loss         0.130789  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13201/30000  loss         0.130789  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13220/30000  loss         0.130786  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13221/30000  loss         0.130786  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13240/30000  loss         0.130783  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13241/30000  loss         0.130782  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13260/30000  loss         0.130779  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13261/30000  loss         0.130779  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13280/30000  loss         0.130776  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13281/30000  loss         0.130776  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13300/30000  loss         0.130773  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13301/30000  loss         0.130772  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.926\n",
      "iter 13320/30000  loss         0.130769  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13321/30000  loss         0.130769  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13340/30000  loss         0.130766  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13341/30000  loss         0.130766  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13360/30000  loss         0.130763  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13361/30000  loss         0.130763  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13380/30000  loss         0.130760  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13381/30000  loss         0.130759  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13400/30000  loss         0.130756  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13401/30000  loss         0.130756  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13420/30000  loss         0.130753  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13421/30000  loss         0.130753  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13440/30000  loss         0.130750  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13441/30000  loss         0.130750  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13460/30000  loss         0.130747  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13461/30000  loss         0.130747  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13480/30000  loss         0.130744  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13481/30000  loss         0.130744  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13500/30000  loss         0.130741  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13501/30000  loss         0.130740  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13520/30000  loss         0.130737  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13521/30000  loss         0.130737  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.926\n",
      "iter 13540/30000  loss         0.130734  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13541/30000  loss         0.130734  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13560/30000  loss         0.130731  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13561/30000  loss         0.130731  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13580/30000  loss         0.130728  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13581/30000  loss         0.130728  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13600/30000  loss         0.130725  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13601/30000  loss         0.130725  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13620/30000  loss         0.130722  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13621/30000  loss         0.130722  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13640/30000  loss         0.130719  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13641/30000  loss         0.130719  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13660/30000  loss         0.130716  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13661/30000  loss         0.130716  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13680/30000  loss         0.130713  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13681/30000  loss         0.130713  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13700/30000  loss         0.130710  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13701/30000  loss         0.130710  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13720/30000  loss         0.130707  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13721/30000  loss         0.130707  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13740/30000  loss         0.130704  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13741/30000  loss         0.130704  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13760/30000  loss         0.130701  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13761/30000  loss         0.130701  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13780/30000  loss         0.130698  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13781/30000  loss         0.130698  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13800/30000  loss         0.130695  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13801/30000  loss         0.130695  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13820/30000  loss         0.130692  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13821/30000  loss         0.130692  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13840/30000  loss         0.130689  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13841/30000  loss         0.130689  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13860/30000  loss         0.130686  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13861/30000  loss         0.130686  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13880/30000  loss         0.130683  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13881/30000  loss         0.130683  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13900/30000  loss         0.130680  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13901/30000  loss         0.130680  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.927\n",
      "iter 13920/30000  loss         0.130677  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.927\n",
      "iter 13921/30000  loss         0.130677  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.927\n",
      "iter 13940/30000  loss         0.130674  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 13941/30000  loss         0.130674  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 13960/30000  loss         0.130672  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 13961/30000  loss         0.130671  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 13980/30000  loss         0.130669  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 13981/30000  loss         0.130669  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14000/30000  loss         0.130666  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14001/30000  loss         0.130666  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14020/30000  loss         0.130663  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14021/30000  loss         0.130663  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14040/30000  loss         0.130660  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14041/30000  loss         0.130660  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14060/30000  loss         0.130657  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14061/30000  loss         0.130657  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14080/30000  loss         0.130655  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14081/30000  loss         0.130654  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14100/30000  loss         0.130652  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14101/30000  loss         0.130652  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14120/30000  loss         0.130649  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14121/30000  loss         0.130649  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14140/30000  loss         0.130646  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14141/30000  loss         0.130646  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14160/30000  loss         0.130643  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14161/30000  loss         0.130643  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14180/30000  loss         0.130641  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14181/30000  loss         0.130641  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14200/30000  loss         0.130638  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14201/30000  loss         0.130638  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14220/30000  loss         0.130635  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14221/30000  loss         0.130635  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14240/30000  loss         0.130633  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14241/30000  loss         0.130632  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14260/30000  loss         0.130630  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14261/30000  loss         0.130630  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14280/30000  loss         0.130627  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14281/30000  loss         0.130627  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14300/30000  loss         0.130624  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14301/30000  loss         0.130624  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14320/30000  loss         0.130622  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14321/30000  loss         0.130622  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14340/30000  loss         0.130619  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14341/30000  loss         0.130619  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14360/30000  loss         0.130616  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14361/30000  loss         0.130616  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.928\n",
      "iter 14380/30000  loss         0.130614  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14381/30000  loss         0.130614  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14400/30000  loss         0.130611  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14401/30000  loss         0.130611  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14420/30000  loss         0.130609  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14421/30000  loss         0.130608  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14440/30000  loss         0.130606  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14441/30000  loss         0.130606  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14460/30000  loss         0.130603  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14461/30000  loss         0.130603  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14480/30000  loss         0.130601  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14481/30000  loss         0.130601  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14500/30000  loss         0.130598  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14501/30000  loss         0.130598  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14520/30000  loss         0.130596  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14521/30000  loss         0.130596  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14540/30000  loss         0.130593  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14541/30000  loss         0.130593  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14560/30000  loss         0.130591  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14561/30000  loss         0.130590  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14580/30000  loss         0.130588  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14581/30000  loss         0.130588  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.929\n",
      "iter 14600/30000  loss         0.130586  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14601/30000  loss         0.130585  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14620/30000  loss         0.130583  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14621/30000  loss         0.130583  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14640/30000  loss         0.130580  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14641/30000  loss         0.130580  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14660/30000  loss         0.130578  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14661/30000  loss         0.130578  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14680/30000  loss         0.130576  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14681/30000  loss         0.130575  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14700/30000  loss         0.130573  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14701/30000  loss         0.130573  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14720/30000  loss         0.130571  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14721/30000  loss         0.130570  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14740/30000  loss         0.130568  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14741/30000  loss         0.130568  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14760/30000  loss         0.130566  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14761/30000  loss         0.130566  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14780/30000  loss         0.130563  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14781/30000  loss         0.130563  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14800/30000  loss         0.130561  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14801/30000  loss         0.130561  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14820/30000  loss         0.130558  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14821/30000  loss         0.130558  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14840/30000  loss         0.130556  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14841/30000  loss         0.130556  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.929\n",
      "iter 14860/30000  loss         0.130554  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14861/30000  loss         0.130553  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14880/30000  loss         0.130551  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14881/30000  loss         0.130551  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14900/30000  loss         0.130549  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14901/30000  loss         0.130549  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14920/30000  loss         0.130546  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14921/30000  loss         0.130546  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14940/30000  loss         0.130544  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14941/30000  loss         0.130544  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14960/30000  loss         0.130542  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14961/30000  loss         0.130542  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14980/30000  loss         0.130539  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 14981/30000  loss         0.130539  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15000/30000  loss         0.130537  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15001/30000  loss         0.130537  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15020/30000  loss         0.130535  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15021/30000  loss         0.130535  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15040/30000  loss         0.130532  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15041/30000  loss         0.130532  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15060/30000  loss         0.130530  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15061/30000  loss         0.130530  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15080/30000  loss         0.130528  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15081/30000  loss         0.130528  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15100/30000  loss         0.130525  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15101/30000  loss         0.130525  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15120/30000  loss         0.130523  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15121/30000  loss         0.130523  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15140/30000  loss         0.130521  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15141/30000  loss         0.130521  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15160/30000  loss         0.130519  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15161/30000  loss         0.130519  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15180/30000  loss         0.130516  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15181/30000  loss         0.130516  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15200/30000  loss         0.130514  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15201/30000  loss         0.130514  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15220/30000  loss         0.130512  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15221/30000  loss         0.130512  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15240/30000  loss         0.130510  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15241/30000  loss         0.130510  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15260/30000  loss         0.130507  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15261/30000  loss         0.130507  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15280/30000  loss         0.130505  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15281/30000  loss         0.130505  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15300/30000  loss         0.130503  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15301/30000  loss         0.130503  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.930\n",
      "iter 15320/30000  loss         0.130501  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.930\n",
      "iter 15321/30000  loss         0.130501  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.930\n",
      "iter 15340/30000  loss         0.130499  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.930\n",
      "iter 15341/30000  loss         0.130499  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.930\n",
      "iter 15360/30000  loss         0.130496  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15361/30000  loss         0.130496  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15380/30000  loss         0.130494  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15381/30000  loss         0.130494  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15400/30000  loss         0.130492  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15401/30000  loss         0.130492  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15420/30000  loss         0.130490  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15421/30000  loss         0.130490  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15440/30000  loss         0.130488  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15441/30000  loss         0.130488  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15460/30000  loss         0.130486  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15461/30000  loss         0.130486  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15480/30000  loss         0.130484  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15481/30000  loss         0.130483  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15500/30000  loss         0.130481  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15501/30000  loss         0.130481  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15520/30000  loss         0.130479  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15521/30000  loss         0.130479  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15540/30000  loss         0.130477  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15541/30000  loss         0.130477  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15560/30000  loss         0.130475  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15561/30000  loss         0.130475  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15580/30000  loss         0.130473  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15581/30000  loss         0.130473  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15600/30000  loss         0.130471  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15601/30000  loss         0.130471  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15620/30000  loss         0.130469  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15621/30000  loss         0.130469  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15640/30000  loss         0.130467  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15641/30000  loss         0.130467  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15660/30000  loss         0.130465  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15661/30000  loss         0.130465  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15680/30000  loss         0.130463  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15681/30000  loss         0.130463  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15700/30000  loss         0.130461  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15701/30000  loss         0.130461  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15720/30000  loss         0.130459  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15721/30000  loss         0.130459  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15740/30000  loss         0.130457  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15741/30000  loss         0.130456  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15760/30000  loss         0.130455  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15761/30000  loss         0.130454  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15780/30000  loss         0.130453  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15781/30000  loss         0.130452  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15800/30000  loss         0.130451  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15801/30000  loss         0.130450  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15820/30000  loss         0.130449  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15821/30000  loss         0.130448  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15840/30000  loss         0.130447  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15841/30000  loss         0.130446  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15860/30000  loss         0.130445  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15861/30000  loss         0.130444  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15880/30000  loss         0.130443  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15881/30000  loss         0.130443  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15900/30000  loss         0.130441  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15901/30000  loss         0.130441  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.931\n",
      "iter 15920/30000  loss         0.130439  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 15921/30000  loss         0.130439  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 15940/30000  loss         0.130437  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 15941/30000  loss         0.130437  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 15960/30000  loss         0.130435  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 15961/30000  loss         0.130435  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 15980/30000  loss         0.130433  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 15981/30000  loss         0.130433  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16000/30000  loss         0.130431  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16001/30000  loss         0.130431  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16020/30000  loss         0.130429  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16021/30000  loss         0.130429  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16040/30000  loss         0.130427  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16041/30000  loss         0.130427  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16060/30000  loss         0.130425  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16061/30000  loss         0.130425  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16080/30000  loss         0.130423  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16081/30000  loss         0.130423  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16100/30000  loss         0.130421  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16101/30000  loss         0.130421  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.932\n",
      "iter 16120/30000  loss         0.130420  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16121/30000  loss         0.130419  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16140/30000  loss         0.130418  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16141/30000  loss         0.130418  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16160/30000  loss         0.130416  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16161/30000  loss         0.130416  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16180/30000  loss         0.130414  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16181/30000  loss         0.130414  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16200/30000  loss         0.130412  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16201/30000  loss         0.130412  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16220/30000  loss         0.130410  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16221/30000  loss         0.130410  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16240/30000  loss         0.130408  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16241/30000  loss         0.130408  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16260/30000  loss         0.130406  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16261/30000  loss         0.130406  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16280/30000  loss         0.130405  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16281/30000  loss         0.130405  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16300/30000  loss         0.130403  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16301/30000  loss         0.130403  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16320/30000  loss         0.130401  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16321/30000  loss         0.130401  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16340/30000  loss         0.130399  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16341/30000  loss         0.130399  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16360/30000  loss         0.130397  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16361/30000  loss         0.130397  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16380/30000  loss         0.130396  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16381/30000  loss         0.130396  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16400/30000  loss         0.130394  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16401/30000  loss         0.130394  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16420/30000  loss         0.130392  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16421/30000  loss         0.130392  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16440/30000  loss         0.130390  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16441/30000  loss         0.130390  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16460/30000  loss         0.130388  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16461/30000  loss         0.130388  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16480/30000  loss         0.130387  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16481/30000  loss         0.130387  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16500/30000  loss         0.130385  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16501/30000  loss         0.130385  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.932\n",
      "iter 16520/30000  loss         0.130383  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16521/30000  loss         0.130383  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16540/30000  loss         0.130381  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16541/30000  loss         0.130381  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16560/30000  loss         0.130380  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16561/30000  loss         0.130380  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16580/30000  loss         0.130378  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16581/30000  loss         0.130378  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16600/30000  loss         0.130376  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16601/30000  loss         0.130376  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16620/30000  loss         0.130375  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16621/30000  loss         0.130374  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16640/30000  loss         0.130373  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16641/30000  loss         0.130373  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16660/30000  loss         0.130371  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16661/30000  loss         0.130371  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16680/30000  loss         0.130369  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16681/30000  loss         0.130369  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16700/30000  loss         0.130368  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16701/30000  loss         0.130368  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16720/30000  loss         0.130366  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16721/30000  loss         0.130366  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16740/30000  loss         0.130364  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16741/30000  loss         0.130364  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16760/30000  loss         0.130363  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16761/30000  loss         0.130363  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16780/30000  loss         0.130361  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16781/30000  loss         0.130361  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16800/30000  loss         0.130359  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16801/30000  loss         0.130359  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16820/30000  loss         0.130358  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16821/30000  loss         0.130358  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16840/30000  loss         0.130356  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16841/30000  loss         0.130356  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16860/30000  loss         0.130354  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16861/30000  loss         0.130354  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16880/30000  loss         0.130353  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16881/30000  loss         0.130353  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16900/30000  loss         0.130351  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16901/30000  loss         0.130351  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16920/30000  loss         0.130350  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16921/30000  loss         0.130349  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16940/30000  loss         0.130348  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16941/30000  loss         0.130348  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16960/30000  loss         0.130346  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16961/30000  loss         0.130346  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16980/30000  loss         0.130345  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 16981/30000  loss         0.130345  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 17000/30000  loss         0.130343  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 17001/30000  loss         0.130343  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.933\n",
      "iter 17020/30000  loss         0.130341  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17021/30000  loss         0.130341  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17040/30000  loss         0.130340  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17041/30000  loss         0.130340  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17060/30000  loss         0.130338  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17061/30000  loss         0.130338  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17080/30000  loss         0.130337  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17081/30000  loss         0.130337  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17100/30000  loss         0.130335  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17101/30000  loss         0.130335  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17120/30000  loss         0.130334  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17121/30000  loss         0.130333  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17140/30000  loss         0.130332  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17141/30000  loss         0.130332  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17160/30000  loss         0.130330  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17161/30000  loss         0.130330  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17180/30000  loss         0.130329  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17181/30000  loss         0.130329  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.933\n",
      "iter 17200/30000  loss         0.130327  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17201/30000  loss         0.130327  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17220/30000  loss         0.130326  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17221/30000  loss         0.130326  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17240/30000  loss         0.130324  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17241/30000  loss         0.130324  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17260/30000  loss         0.130323  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17261/30000  loss         0.130323  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17280/30000  loss         0.130321  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17281/30000  loss         0.130321  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17300/30000  loss         0.130320  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17301/30000  loss         0.130320  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17320/30000  loss         0.130318  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17321/30000  loss         0.130318  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17340/30000  loss         0.130317  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17341/30000  loss         0.130317  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17360/30000  loss         0.130315  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17361/30000  loss         0.130315  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17380/30000  loss         0.130314  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17381/30000  loss         0.130314  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17400/30000  loss         0.130312  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17401/30000  loss         0.130312  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17420/30000  loss         0.130311  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17421/30000  loss         0.130311  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17440/30000  loss         0.130309  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17441/30000  loss         0.130309  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17460/30000  loss         0.130308  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17461/30000  loss         0.130308  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17480/30000  loss         0.130306  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17481/30000  loss         0.130306  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17500/30000  loss         0.130305  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17501/30000  loss         0.130305  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17520/30000  loss         0.130303  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17521/30000  loss         0.130303  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17540/30000  loss         0.130302  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17541/30000  loss         0.130302  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17560/30000  loss         0.130300  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17561/30000  loss         0.130300  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17580/30000  loss         0.130299  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17581/30000  loss         0.130299  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17600/30000  loss         0.130298  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17601/30000  loss         0.130298  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17620/30000  loss         0.130296  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17621/30000  loss         0.130296  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17640/30000  loss         0.130295  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17641/30000  loss         0.130295  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17660/30000  loss         0.130293  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17661/30000  loss         0.130293  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17680/30000  loss         0.130292  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17681/30000  loss         0.130292  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17700/30000  loss         0.130290  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17701/30000  loss         0.130290  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17720/30000  loss         0.130289  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17721/30000  loss         0.130289  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17740/30000  loss         0.130288  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17741/30000  loss         0.130288  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17760/30000  loss         0.130286  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17761/30000  loss         0.130286  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17780/30000  loss         0.130285  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17781/30000  loss         0.130285  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17800/30000  loss         0.130283  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17801/30000  loss         0.130283  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17820/30000  loss         0.130282  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17821/30000  loss         0.130282  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17840/30000  loss         0.130281  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17841/30000  loss         0.130281  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17860/30000  loss         0.130279  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17861/30000  loss         0.130279  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17880/30000  loss         0.130278  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17881/30000  loss         0.130278  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17900/30000  loss         0.130277  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17901/30000  loss         0.130277  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17920/30000  loss         0.130275  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17921/30000  loss         0.130275  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17940/30000  loss         0.130274  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17941/30000  loss         0.130274  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.934\n",
      "iter 17960/30000  loss         0.130273  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.935\n",
      "iter 17961/30000  loss         0.130272  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.935\n",
      "iter 17980/30000  loss         0.130271  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.935\n",
      "iter 17981/30000  loss         0.130271  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.935\n",
      "iter 18000/30000  loss         0.130270  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.935\n",
      "iter 18001/30000  loss         0.130270  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.935\n",
      "iter 18020/30000  loss         0.130269  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18021/30000  loss         0.130268  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18040/30000  loss         0.130267  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18041/30000  loss         0.130267  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18060/30000  loss         0.130266  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18061/30000  loss         0.130266  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18080/30000  loss         0.130265  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18081/30000  loss         0.130264  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18100/30000  loss         0.130263  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18101/30000  loss         0.130263  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18120/30000  loss         0.130262  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18121/30000  loss         0.130262  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18140/30000  loss         0.130261  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18141/30000  loss         0.130261  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18160/30000  loss         0.130259  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18161/30000  loss         0.130259  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18180/30000  loss         0.130258  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18181/30000  loss         0.130258  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18200/30000  loss         0.130257  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18201/30000  loss         0.130257  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18220/30000  loss         0.130255  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18221/30000  loss         0.130255  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18240/30000  loss         0.130254  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18241/30000  loss         0.130254  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18260/30000  loss         0.130253  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18261/30000  loss         0.130253  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18280/30000  loss         0.130252  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18281/30000  loss         0.130252  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18300/30000  loss         0.130250  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18301/30000  loss         0.130250  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18320/30000  loss         0.130249  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18321/30000  loss         0.130249  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18340/30000  loss         0.130248  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18341/30000  loss         0.130248  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18360/30000  loss         0.130247  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18361/30000  loss         0.130247  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18380/30000  loss         0.130245  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18381/30000  loss         0.130245  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18400/30000  loss         0.130244  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18401/30000  loss         0.130244  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18420/30000  loss         0.130243  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18421/30000  loss         0.130243  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18440/30000  loss         0.130242  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18441/30000  loss         0.130242  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18460/30000  loss         0.130240  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18461/30000  loss         0.130240  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18480/30000  loss         0.130239  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18481/30000  loss         0.130239  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18500/30000  loss         0.130238  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18501/30000  loss         0.130238  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18520/30000  loss         0.130237  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18521/30000  loss         0.130237  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18540/30000  loss         0.130235  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18541/30000  loss         0.130235  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18560/30000  loss         0.130234  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18561/30000  loss         0.130234  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18580/30000  loss         0.130233  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18581/30000  loss         0.130233  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18600/30000  loss         0.130232  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18601/30000  loss         0.130232  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18620/30000  loss         0.130231  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18621/30000  loss         0.130231  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18640/30000  loss         0.130229  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18641/30000  loss         0.130229  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18660/30000  loss         0.130228  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18661/30000  loss         0.130228  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18680/30000  loss         0.130227  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18681/30000  loss         0.130227  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18700/30000  loss         0.130226  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18701/30000  loss         0.130226  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18720/30000  loss         0.130225  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18721/30000  loss         0.130225  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18740/30000  loss         0.130224  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18741/30000  loss         0.130223  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18760/30000  loss         0.130222  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18761/30000  loss         0.130222  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18780/30000  loss         0.130221  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18781/30000  loss         0.130221  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18800/30000  loss         0.130220  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18801/30000  loss         0.130220  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18820/30000  loss         0.130219  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18821/30000  loss         0.130219  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.935\n",
      "iter 18840/30000  loss         0.130218  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18841/30000  loss         0.130218  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18860/30000  loss         0.130217  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18861/30000  loss         0.130217  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18880/30000  loss         0.130215  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18881/30000  loss         0.130215  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18900/30000  loss         0.130214  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18901/30000  loss         0.130214  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18920/30000  loss         0.130213  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18921/30000  loss         0.130213  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18940/30000  loss         0.130212  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18941/30000  loss         0.130212  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18960/30000  loss         0.130211  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18961/30000  loss         0.130211  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18980/30000  loss         0.130210  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 18981/30000  loss         0.130210  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19000/30000  loss         0.130209  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19001/30000  loss         0.130209  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19020/30000  loss         0.130208  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19021/30000  loss         0.130207  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19040/30000  loss         0.130206  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19041/30000  loss         0.130206  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19060/30000  loss         0.130205  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19061/30000  loss         0.130205  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19080/30000  loss         0.130204  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19081/30000  loss         0.130204  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19100/30000  loss         0.130203  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19101/30000  loss         0.130203  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19120/30000  loss         0.130202  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19121/30000  loss         0.130202  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.936\n",
      "iter 19140/30000  loss         0.130201  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19141/30000  loss         0.130201  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19160/30000  loss         0.130200  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19161/30000  loss         0.130200  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19180/30000  loss         0.130199  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19181/30000  loss         0.130199  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19200/30000  loss         0.130198  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19201/30000  loss         0.130198  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19220/30000  loss         0.130197  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19221/30000  loss         0.130196  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19240/30000  loss         0.130195  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19241/30000  loss         0.130195  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19260/30000  loss         0.130194  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19261/30000  loss         0.130194  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19280/30000  loss         0.130193  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19281/30000  loss         0.130193  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19300/30000  loss         0.130192  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19301/30000  loss         0.130192  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19320/30000  loss         0.130191  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19321/30000  loss         0.130191  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19340/30000  loss         0.130190  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19341/30000  loss         0.130190  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19360/30000  loss         0.130189  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19361/30000  loss         0.130189  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19380/30000  loss         0.130188  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19381/30000  loss         0.130188  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19400/30000  loss         0.130187  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19401/30000  loss         0.130187  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19420/30000  loss         0.130186  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19421/30000  loss         0.130186  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19440/30000  loss         0.130185  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19441/30000  loss         0.130185  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19460/30000  loss         0.130184  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19461/30000  loss         0.130184  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19480/30000  loss         0.130183  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19481/30000  loss         0.130183  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19500/30000  loss         0.130182  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19501/30000  loss         0.130182  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19520/30000  loss         0.130181  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19521/30000  loss         0.130181  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19540/30000  loss         0.130180  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19541/30000  loss         0.130180  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19560/30000  loss         0.130179  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19561/30000  loss         0.130179  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19580/30000  loss         0.130178  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19581/30000  loss         0.130178  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19600/30000  loss         0.130177  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19601/30000  loss         0.130177  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19620/30000  loss         0.130176  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19621/30000  loss         0.130176  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19640/30000  loss         0.130175  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19641/30000  loss         0.130175  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19660/30000  loss         0.130174  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19661/30000  loss         0.130174  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19680/30000  loss         0.130173  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19681/30000  loss         0.130173  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19700/30000  loss         0.130172  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19701/30000  loss         0.130172  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19720/30000  loss         0.130171  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19721/30000  loss         0.130171  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19740/30000  loss         0.130170  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19741/30000  loss         0.130170  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19760/30000  loss         0.130169  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19761/30000  loss         0.130169  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19780/30000  loss         0.130168  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19781/30000  loss         0.130168  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19800/30000  loss         0.130167  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19801/30000  loss         0.130167  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19820/30000  loss         0.130166  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19821/30000  loss         0.130166  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19840/30000  loss         0.130165  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19841/30000  loss         0.130165  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.936\n",
      "iter 19860/30000  loss         0.130164  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19861/30000  loss         0.130164  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19880/30000  loss         0.130163  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19881/30000  loss         0.130163  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19900/30000  loss         0.130162  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19901/30000  loss         0.130162  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19920/30000  loss         0.130161  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19921/30000  loss         0.130161  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19940/30000  loss         0.130160  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19941/30000  loss         0.130160  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19960/30000  loss         0.130159  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19961/30000  loss         0.130159  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19980/30000  loss         0.130158  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 19981/30000  loss         0.130158  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20000/30000  loss         0.130157  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20001/30000  loss         0.130157  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20020/30000  loss         0.130156  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20021/30000  loss         0.130156  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20040/30000  loss         0.130155  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20041/30000  loss         0.130155  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20060/30000  loss         0.130154  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20061/30000  loss         0.130154  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20080/30000  loss         0.130153  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20081/30000  loss         0.130153  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20100/30000  loss         0.130152  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20101/30000  loss         0.130152  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20120/30000  loss         0.130152  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20121/30000  loss         0.130152  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20140/30000  loss         0.130151  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20141/30000  loss         0.130151  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20160/30000  loss         0.130150  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20161/30000  loss         0.130150  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20180/30000  loss         0.130149  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20181/30000  loss         0.130149  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20200/30000  loss         0.130148  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20201/30000  loss         0.130148  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20220/30000  loss         0.130147  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20221/30000  loss         0.130147  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20240/30000  loss         0.130146  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20241/30000  loss         0.130146  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20260/30000  loss         0.130145  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20261/30000  loss         0.130145  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20280/30000  loss         0.130144  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20281/30000  loss         0.130144  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20300/30000  loss         0.130143  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20301/30000  loss         0.130143  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20320/30000  loss         0.130142  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20321/30000  loss         0.130142  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20340/30000  loss         0.130142  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20341/30000  loss         0.130142  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20360/30000  loss         0.130141  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20361/30000  loss         0.130141  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20380/30000  loss         0.130140  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20381/30000  loss         0.130140  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20400/30000  loss         0.130139  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20401/30000  loss         0.130139  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.937\n",
      "iter 20420/30000  loss         0.130138  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20421/30000  loss         0.130138  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20440/30000  loss         0.130137  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20441/30000  loss         0.130137  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20460/30000  loss         0.130136  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20461/30000  loss         0.130136  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20480/30000  loss         0.130135  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20481/30000  loss         0.130135  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20500/30000  loss         0.130135  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20501/30000  loss         0.130135  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20520/30000  loss         0.130134  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20521/30000  loss         0.130134  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20540/30000  loss         0.130133  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20541/30000  loss         0.130133  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20560/30000  loss         0.130132  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20561/30000  loss         0.130132  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20580/30000  loss         0.130131  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20581/30000  loss         0.130131  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20600/30000  loss         0.130130  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20601/30000  loss         0.130130  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20620/30000  loss         0.130129  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20621/30000  loss         0.130129  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20640/30000  loss         0.130129  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20641/30000  loss         0.130129  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20660/30000  loss         0.130128  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20661/30000  loss         0.130128  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20680/30000  loss         0.130127  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20681/30000  loss         0.130127  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20700/30000  loss         0.130126  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20701/30000  loss         0.130126  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20720/30000  loss         0.130125  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20721/30000  loss         0.130125  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20740/30000  loss         0.130124  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20741/30000  loss         0.130124  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20760/30000  loss         0.130124  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20761/30000  loss         0.130123  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20780/30000  loss         0.130123  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20781/30000  loss         0.130123  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20800/30000  loss         0.130122  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20801/30000  loss         0.130122  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20820/30000  loss         0.130121  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20821/30000  loss         0.130121  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20840/30000  loss         0.130120  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20841/30000  loss         0.130120  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20860/30000  loss         0.130119  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20861/30000  loss         0.130119  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20880/30000  loss         0.130119  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20881/30000  loss         0.130119  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20900/30000  loss         0.130118  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20901/30000  loss         0.130118  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20920/30000  loss         0.130117  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20921/30000  loss         0.130117  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20940/30000  loss         0.130116  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20941/30000  loss         0.130116  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20960/30000  loss         0.130115  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20961/30000  loss         0.130115  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20980/30000  loss         0.130115  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 20981/30000  loss         0.130115  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21000/30000  loss         0.130114  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21001/30000  loss         0.130114  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21020/30000  loss         0.130113  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21021/30000  loss         0.130113  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21040/30000  loss         0.130112  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21041/30000  loss         0.130112  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21060/30000  loss         0.130111  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21061/30000  loss         0.130111  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.937\n",
      "iter 21080/30000  loss         0.130111  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21081/30000  loss         0.130111  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21100/30000  loss         0.130110  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21101/30000  loss         0.130110  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21120/30000  loss         0.130109  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21121/30000  loss         0.130109  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21140/30000  loss         0.130108  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21141/30000  loss         0.130108  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21160/30000  loss         0.130107  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21161/30000  loss         0.130107  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21180/30000  loss         0.130107  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21181/30000  loss         0.130107  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21200/30000  loss         0.130106  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21201/30000  loss         0.130106  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21220/30000  loss         0.130105  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21221/30000  loss         0.130105  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21240/30000  loss         0.130104  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21241/30000  loss         0.130104  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21260/30000  loss         0.130104  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21261/30000  loss         0.130104  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21280/30000  loss         0.130103  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21281/30000  loss         0.130103  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21300/30000  loss         0.130102  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21301/30000  loss         0.130102  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21320/30000  loss         0.130101  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21321/30000  loss         0.130101  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21340/30000  loss         0.130101  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21341/30000  loss         0.130101  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21360/30000  loss         0.130100  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21361/30000  loss         0.130100  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21380/30000  loss         0.130099  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21381/30000  loss         0.130099  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21400/30000  loss         0.130098  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21401/30000  loss         0.130098  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21420/30000  loss         0.130098  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21421/30000  loss         0.130098  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21440/30000  loss         0.130097  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21441/30000  loss         0.130097  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21460/30000  loss         0.130096  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21461/30000  loss         0.130096  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21480/30000  loss         0.130095  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21481/30000  loss         0.130095  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21500/30000  loss         0.130095  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21501/30000  loss         0.130095  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21520/30000  loss         0.130094  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21521/30000  loss         0.130094  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21540/30000  loss         0.130093  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21541/30000  loss         0.130093  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21560/30000  loss         0.130092  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21561/30000  loss         0.130092  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21580/30000  loss         0.130092  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21581/30000  loss         0.130092  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21600/30000  loss         0.130091  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21601/30000  loss         0.130091  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21620/30000  loss         0.130090  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21621/30000  loss         0.130090  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21640/30000  loss         0.130090  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21641/30000  loss         0.130090  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21660/30000  loss         0.130089  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21661/30000  loss         0.130089  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21680/30000  loss         0.130088  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21681/30000  loss         0.130088  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21700/30000  loss         0.130087  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21701/30000  loss         0.130087  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21720/30000  loss         0.130087  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21721/30000  loss         0.130087  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21740/30000  loss         0.130086  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21741/30000  loss         0.130086  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21760/30000  loss         0.130085  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21761/30000  loss         0.130085  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21780/30000  loss         0.130085  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21781/30000  loss         0.130085  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21800/30000  loss         0.130084  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21801/30000  loss         0.130084  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21820/30000  loss         0.130083  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21821/30000  loss         0.130083  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21840/30000  loss         0.130082  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21841/30000  loss         0.130082  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21860/30000  loss         0.130082  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21861/30000  loss         0.130082  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21880/30000  loss         0.130081  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21881/30000  loss         0.130081  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.938\n",
      "iter 21900/30000  loss         0.130080  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21901/30000  loss         0.130080  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21920/30000  loss         0.130080  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21921/30000  loss         0.130080  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21940/30000  loss         0.130079  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21941/30000  loss         0.130079  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21960/30000  loss         0.130078  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21961/30000  loss         0.130078  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21980/30000  loss         0.130078  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 21981/30000  loss         0.130078  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22000/30000  loss         0.130077  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22001/30000  loss         0.130077  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22020/30000  loss         0.130076  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22021/30000  loss         0.130076  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22040/30000  loss         0.130076  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22041/30000  loss         0.130076  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22060/30000  loss         0.130075  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22061/30000  loss         0.130075  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22080/30000  loss         0.130074  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22081/30000  loss         0.130074  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22100/30000  loss         0.130074  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22101/30000  loss         0.130074  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22120/30000  loss         0.130073  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22121/30000  loss         0.130073  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22140/30000  loss         0.130072  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22141/30000  loss         0.130072  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22160/30000  loss         0.130072  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22161/30000  loss         0.130072  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22180/30000  loss         0.130071  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22181/30000  loss         0.130071  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22200/30000  loss         0.130070  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22201/30000  loss         0.130070  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22220/30000  loss         0.130070  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22221/30000  loss         0.130070  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22240/30000  loss         0.130069  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22241/30000  loss         0.130069  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22260/30000  loss         0.130068  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22261/30000  loss         0.130068  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22280/30000  loss         0.130068  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22281/30000  loss         0.130068  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22300/30000  loss         0.130067  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22301/30000  loss         0.130067  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22320/30000  loss         0.130066  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22321/30000  loss         0.130066  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22340/30000  loss         0.130066  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22341/30000  loss         0.130066  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22360/30000  loss         0.130065  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22361/30000  loss         0.130065  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22380/30000  loss         0.130065  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22381/30000  loss         0.130065  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22400/30000  loss         0.130064  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22401/30000  loss         0.130064  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22420/30000  loss         0.130063  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22421/30000  loss         0.130063  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22440/30000  loss         0.130063  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22441/30000  loss         0.130063  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22460/30000  loss         0.130062  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22461/30000  loss         0.130062  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22480/30000  loss         0.130061  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22481/30000  loss         0.130061  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22500/30000  loss         0.130061  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22501/30000  loss         0.130061  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22520/30000  loss         0.130060  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22521/30000  loss         0.130060  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22540/30000  loss         0.130060  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22541/30000  loss         0.130060  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22560/30000  loss         0.130059  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22561/30000  loss         0.130059  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22580/30000  loss         0.130058  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22581/30000  loss         0.130058  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22600/30000  loss         0.130058  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22601/30000  loss         0.130058  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22620/30000  loss         0.130057  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22621/30000  loss         0.130057  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22640/30000  loss         0.130056  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22641/30000  loss         0.130056  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.938\n",
      "iter 22660/30000  loss         0.130056  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22661/30000  loss         0.130056  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22680/30000  loss         0.130055  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22681/30000  loss         0.130055  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22700/30000  loss         0.130055  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22701/30000  loss         0.130055  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22720/30000  loss         0.130054  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22721/30000  loss         0.130054  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22740/30000  loss         0.130053  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22741/30000  loss         0.130053  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22760/30000  loss         0.130053  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22761/30000  loss         0.130053  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22780/30000  loss         0.130052  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22781/30000  loss         0.130052  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22800/30000  loss         0.130052  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22801/30000  loss         0.130052  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22820/30000  loss         0.130051  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22821/30000  loss         0.130051  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22840/30000  loss         0.130050  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22841/30000  loss         0.130050  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22860/30000  loss         0.130050  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22861/30000  loss         0.130050  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22880/30000  loss         0.130049  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22881/30000  loss         0.130049  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22900/30000  loss         0.130049  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22901/30000  loss         0.130049  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22920/30000  loss         0.130048  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22921/30000  loss         0.130048  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22940/30000  loss         0.130048  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22941/30000  loss         0.130048  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22960/30000  loss         0.130047  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22961/30000  loss         0.130047  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22980/30000  loss         0.130046  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 22981/30000  loss         0.130046  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23000/30000  loss         0.130046  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23001/30000  loss         0.130046  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23020/30000  loss         0.130045  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23021/30000  loss         0.130045  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23040/30000  loss         0.130045  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23041/30000  loss         0.130045  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23060/30000  loss         0.130044  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23061/30000  loss         0.130044  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23080/30000  loss         0.130044  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23081/30000  loss         0.130043  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23100/30000  loss         0.130043  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23101/30000  loss         0.130043  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23120/30000  loss         0.130042  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23121/30000  loss         0.130042  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23140/30000  loss         0.130042  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23141/30000  loss         0.130042  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23160/30000  loss         0.130041  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23161/30000  loss         0.130041  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23180/30000  loss         0.130041  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23181/30000  loss         0.130041  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23200/30000  loss         0.130040  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23201/30000  loss         0.130040  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23220/30000  loss         0.130040  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23221/30000  loss         0.130040  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23240/30000  loss         0.130039  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23241/30000  loss         0.130039  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23260/30000  loss         0.130038  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23261/30000  loss         0.130038  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23280/30000  loss         0.130038  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23281/30000  loss         0.130038  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23300/30000  loss         0.130037  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23301/30000  loss         0.130037  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23320/30000  loss         0.130037  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23321/30000  loss         0.130037  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23340/30000  loss         0.130036  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23341/30000  loss         0.130036  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23360/30000  loss         0.130036  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23361/30000  loss         0.130036  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23380/30000  loss         0.130035  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23381/30000  loss         0.130035  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23400/30000  loss         0.130035  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23401/30000  loss         0.130035  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23420/30000  loss         0.130034  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23421/30000  loss         0.130034  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23440/30000  loss         0.130034  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23441/30000  loss         0.130034  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23460/30000  loss         0.130033  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23461/30000  loss         0.130033  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23480/30000  loss         0.130033  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23481/30000  loss         0.130032  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23500/30000  loss         0.130032  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23501/30000  loss         0.130032  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23520/30000  loss         0.130031  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23521/30000  loss         0.130031  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23540/30000  loss         0.130031  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23541/30000  loss         0.130031  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23560/30000  loss         0.130030  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23561/30000  loss         0.130030  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23580/30000  loss         0.130030  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23581/30000  loss         0.130030  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23600/30000  loss         0.130029  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23601/30000  loss         0.130029  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23620/30000  loss         0.130029  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23621/30000  loss         0.130029  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.939\n",
      "iter 23640/30000  loss         0.130028  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23641/30000  loss         0.130028  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23660/30000  loss         0.130028  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23661/30000  loss         0.130028  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23680/30000  loss         0.130027  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23681/30000  loss         0.130027  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23700/30000  loss         0.130027  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23701/30000  loss         0.130027  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23720/30000  loss         0.130026  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23721/30000  loss         0.130026  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23740/30000  loss         0.130026  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23741/30000  loss         0.130026  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23760/30000  loss         0.130025  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23761/30000  loss         0.130025  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23780/30000  loss         0.130025  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23781/30000  loss         0.130025  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23800/30000  loss         0.130024  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23801/30000  loss         0.130024  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23820/30000  loss         0.130024  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23821/30000  loss         0.130024  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23840/30000  loss         0.130023  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23841/30000  loss         0.130023  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23860/30000  loss         0.130023  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23861/30000  loss         0.130023  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23880/30000  loss         0.130022  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23881/30000  loss         0.130022  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23900/30000  loss         0.130022  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23901/30000  loss         0.130022  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23920/30000  loss         0.130021  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23921/30000  loss         0.130021  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23940/30000  loss         0.130021  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23941/30000  loss         0.130021  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23960/30000  loss         0.130020  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23961/30000  loss         0.130020  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23980/30000  loss         0.130020  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 23981/30000  loss         0.130020  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24000/30000  loss         0.130019  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24001/30000  loss         0.130019  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24020/30000  loss         0.130019  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24021/30000  loss         0.130019  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24040/30000  loss         0.130018  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24041/30000  loss         0.130018  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24060/30000  loss         0.130018  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24061/30000  loss         0.130018  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24080/30000  loss         0.130017  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24081/30000  loss         0.130017  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24100/30000  loss         0.130017  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24101/30000  loss         0.130017  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24120/30000  loss         0.130016  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24121/30000  loss         0.130016  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24140/30000  loss         0.130016  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24141/30000  loss         0.130016  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24160/30000  loss         0.130015  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24161/30000  loss         0.130015  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24180/30000  loss         0.130015  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24181/30000  loss         0.130015  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24200/30000  loss         0.130014  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24201/30000  loss         0.130014  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24220/30000  loss         0.130014  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24221/30000  loss         0.130014  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24240/30000  loss         0.130013  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24241/30000  loss         0.130013  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24260/30000  loss         0.130013  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24261/30000  loss         0.130013  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24280/30000  loss         0.130012  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24281/30000  loss         0.130012  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24300/30000  loss         0.130012  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24301/30000  loss         0.130012  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24320/30000  loss         0.130012  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24321/30000  loss         0.130012  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24340/30000  loss         0.130011  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24341/30000  loss         0.130011  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24360/30000  loss         0.130011  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24361/30000  loss         0.130011  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24380/30000  loss         0.130010  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24381/30000  loss         0.130010  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24400/30000  loss         0.130010  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24401/30000  loss         0.130010  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24420/30000  loss         0.130009  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24421/30000  loss         0.130009  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24440/30000  loss         0.130009  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24441/30000  loss         0.130009  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24460/30000  loss         0.130008  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24461/30000  loss         0.130008  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24480/30000  loss         0.130008  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24481/30000  loss         0.130008  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24500/30000  loss         0.130007  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24501/30000  loss         0.130007  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24520/30000  loss         0.130007  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24521/30000  loss         0.130007  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24540/30000  loss         0.130006  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24541/30000  loss         0.130006  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24560/30000  loss         0.130006  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24561/30000  loss         0.130006  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24580/30000  loss         0.130006  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24581/30000  loss         0.130006  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24600/30000  loss         0.130005  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24601/30000  loss         0.130005  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24620/30000  loss         0.130005  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24621/30000  loss         0.130005  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24640/30000  loss         0.130004  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24641/30000  loss         0.130004  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24660/30000  loss         0.130004  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24661/30000  loss         0.130004  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24680/30000  loss         0.130003  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24681/30000  loss         0.130003  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24700/30000  loss         0.130003  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24701/30000  loss         0.130003  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24720/30000  loss         0.130002  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24721/30000  loss         0.130002  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24740/30000  loss         0.130002  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24741/30000  loss         0.130002  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24760/30000  loss         0.130002  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24761/30000  loss         0.130002  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24780/30000  loss         0.130001  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24781/30000  loss         0.130001  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24800/30000  loss         0.130001  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24801/30000  loss         0.130001  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.939\n",
      "iter 24820/30000  loss         0.130000  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24821/30000  loss         0.130000  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24840/30000  loss         0.130000  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24841/30000  loss         0.130000  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24860/30000  loss         0.129999  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24861/30000  loss         0.129999  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24880/30000  loss         0.129999  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24881/30000  loss         0.129999  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24900/30000  loss         0.129999  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24901/30000  loss         0.129999  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24920/30000  loss         0.129998  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24921/30000  loss         0.129998  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24940/30000  loss         0.129998  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24941/30000  loss         0.129998  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24960/30000  loss         0.129997  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24961/30000  loss         0.129997  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24980/30000  loss         0.129997  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 24981/30000  loss         0.129997  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25000/30000  loss         0.129996  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25001/30000  loss         0.129996  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25020/30000  loss         0.129996  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25021/30000  loss         0.129996  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25040/30000  loss         0.129996  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25041/30000  loss         0.129996  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25060/30000  loss         0.129995  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25061/30000  loss         0.129995  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25080/30000  loss         0.129995  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25081/30000  loss         0.129995  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25100/30000  loss         0.129994  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25101/30000  loss         0.129994  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25120/30000  loss         0.129994  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25121/30000  loss         0.129994  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25140/30000  loss         0.129994  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25141/30000  loss         0.129994  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25160/30000  loss         0.129993  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25161/30000  loss         0.129993  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25180/30000  loss         0.129993  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25181/30000  loss         0.129993  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25200/30000  loss         0.129992  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25201/30000  loss         0.129992  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25220/30000  loss         0.129992  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25221/30000  loss         0.129992  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25240/30000  loss         0.129992  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25241/30000  loss         0.129992  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25260/30000  loss         0.129991  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25261/30000  loss         0.129991  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25280/30000  loss         0.129991  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25281/30000  loss         0.129991  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25300/30000  loss         0.129990  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25301/30000  loss         0.129990  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25320/30000  loss         0.129990  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25321/30000  loss         0.129990  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25340/30000  loss         0.129990  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25341/30000  loss         0.129990  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25360/30000  loss         0.129989  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25361/30000  loss         0.129989  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25380/30000  loss         0.129989  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25381/30000  loss         0.129989  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25400/30000  loss         0.129988  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25401/30000  loss         0.129988  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25420/30000  loss         0.129988  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25421/30000  loss         0.129988  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25440/30000  loss         0.129988  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25441/30000  loss         0.129988  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25460/30000  loss         0.129987  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25461/30000  loss         0.129987  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25480/30000  loss         0.129987  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25481/30000  loss         0.129987  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25500/30000  loss         0.129986  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25501/30000  loss         0.129986  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25520/30000  loss         0.129986  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25521/30000  loss         0.129986  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25540/30000  loss         0.129986  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25541/30000  loss         0.129986  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25560/30000  loss         0.129985  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25561/30000  loss         0.129985  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25580/30000  loss         0.129985  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25581/30000  loss         0.129985  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25600/30000  loss         0.129984  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25601/30000  loss         0.129984  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25620/30000  loss         0.129984  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25621/30000  loss         0.129984  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25640/30000  loss         0.129984  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25641/30000  loss         0.129984  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25660/30000  loss         0.129983  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25661/30000  loss         0.129983  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25680/30000  loss         0.129983  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25681/30000  loss         0.129983  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25700/30000  loss         0.129983  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25701/30000  loss         0.129983  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25720/30000  loss         0.129982  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25721/30000  loss         0.129982  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.940\n",
      "iter 25740/30000  loss         0.129982  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25741/30000  loss         0.129982  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25760/30000  loss         0.129981  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25761/30000  loss         0.129981  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25780/30000  loss         0.129981  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25781/30000  loss         0.129981  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25800/30000  loss         0.129981  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25801/30000  loss         0.129981  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25820/30000  loss         0.129980  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25821/30000  loss         0.129980  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25840/30000  loss         0.129980  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25841/30000  loss         0.129980  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25860/30000  loss         0.129980  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25861/30000  loss         0.129980  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25880/30000  loss         0.129979  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25881/30000  loss         0.129979  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25900/30000  loss         0.129979  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25901/30000  loss         0.129979  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25920/30000  loss         0.129978  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25921/30000  loss         0.129978  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25940/30000  loss         0.129978  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25941/30000  loss         0.129978  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25960/30000  loss         0.129978  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25961/30000  loss         0.129978  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25980/30000  loss         0.129977  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 25981/30000  loss         0.129977  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26000/30000  loss         0.129977  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26001/30000  loss         0.129977  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26020/30000  loss         0.129977  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26021/30000  loss         0.129977  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26040/30000  loss         0.129976  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26041/30000  loss         0.129976  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26060/30000  loss         0.129976  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26061/30000  loss         0.129976  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26080/30000  loss         0.129976  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26081/30000  loss         0.129976  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26100/30000  loss         0.129975  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26101/30000  loss         0.129975  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26120/30000  loss         0.129975  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26121/30000  loss         0.129975  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26140/30000  loss         0.129975  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26141/30000  loss         0.129975  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26160/30000  loss         0.129974  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26161/30000  loss         0.129974  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26180/30000  loss         0.129974  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26181/30000  loss         0.129974  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26200/30000  loss         0.129973  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26201/30000  loss         0.129973  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26220/30000  loss         0.129973  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26221/30000  loss         0.129973  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26240/30000  loss         0.129973  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26241/30000  loss         0.129973  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26260/30000  loss         0.129972  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26261/30000  loss         0.129972  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26280/30000  loss         0.129972  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26281/30000  loss         0.129972  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26300/30000  loss         0.129972  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26301/30000  loss         0.129972  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26320/30000  loss         0.129971  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26321/30000  loss         0.129971  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26340/30000  loss         0.129971  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26341/30000  loss         0.129971  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26360/30000  loss         0.129971  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26361/30000  loss         0.129971  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26380/30000  loss         0.129970  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26381/30000  loss         0.129970  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26400/30000  loss         0.129970  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26401/30000  loss         0.129970  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26420/30000  loss         0.129970  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26421/30000  loss         0.129970  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26440/30000  loss         0.129969  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26441/30000  loss         0.129969  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26460/30000  loss         0.129969  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26461/30000  loss         0.129969  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26480/30000  loss         0.129969  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26481/30000  loss         0.129969  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26500/30000  loss         0.129968  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26501/30000  loss         0.129968  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26520/30000  loss         0.129968  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26521/30000  loss         0.129968  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26540/30000  loss         0.129968  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26541/30000  loss         0.129968  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26560/30000  loss         0.129967  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26561/30000  loss         0.129967  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26580/30000  loss         0.129967  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26581/30000  loss         0.129967  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26600/30000  loss         0.129967  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26601/30000  loss         0.129967  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26620/30000  loss         0.129966  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26621/30000  loss         0.129966  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26640/30000  loss         0.129966  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26641/30000  loss         0.129966  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26660/30000  loss         0.129966  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26661/30000  loss         0.129966  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26680/30000  loss         0.129965  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26681/30000  loss         0.129965  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26700/30000  loss         0.129965  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26701/30000  loss         0.129965  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26720/30000  loss         0.129965  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26721/30000  loss         0.129965  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26740/30000  loss         0.129964  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26741/30000  loss         0.129964  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26760/30000  loss         0.129964  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26761/30000  loss         0.129964  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26780/30000  loss         0.129964  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26781/30000  loss         0.129964  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26800/30000  loss         0.129963  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26801/30000  loss         0.129963  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26820/30000  loss         0.129963  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26821/30000  loss         0.129963  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26840/30000  loss         0.129963  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26841/30000  loss         0.129963  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26860/30000  loss         0.129963  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26861/30000  loss         0.129962  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26880/30000  loss         0.129962  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26881/30000  loss         0.129962  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26900/30000  loss         0.129962  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26901/30000  loss         0.129962  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26920/30000  loss         0.129962  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26921/30000  loss         0.129962  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26940/30000  loss         0.129961  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26941/30000  loss         0.129961  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26960/30000  loss         0.129961  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26961/30000  loss         0.129961  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26980/30000  loss         0.129961  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 26981/30000  loss         0.129961  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27000/30000  loss         0.129960  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27001/30000  loss         0.129960  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27020/30000  loss         0.129960  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27021/30000  loss         0.129960  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27040/30000  loss         0.129960  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27041/30000  loss         0.129960  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27060/30000  loss         0.129959  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27061/30000  loss         0.129959  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27080/30000  loss         0.129959  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27081/30000  loss         0.129959  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27100/30000  loss         0.129959  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27101/30000  loss         0.129959  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27120/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27121/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27140/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27141/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27160/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27161/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27180/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27181/30000  loss         0.129958  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27200/30000  loss         0.129957  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27201/30000  loss         0.129957  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27220/30000  loss         0.129957  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27221/30000  loss         0.129957  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27240/30000  loss         0.129957  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27241/30000  loss         0.129957  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27260/30000  loss         0.129956  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27261/30000  loss         0.129956  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27280/30000  loss         0.129956  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27281/30000  loss         0.129956  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27300/30000  loss         0.129956  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27301/30000  loss         0.129956  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27320/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27321/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27340/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27341/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27360/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27361/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27380/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27381/30000  loss         0.129955  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27400/30000  loss         0.129954  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27401/30000  loss         0.129954  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27420/30000  loss         0.129954  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27421/30000  loss         0.129954  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27440/30000  loss         0.129954  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27441/30000  loss         0.129954  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27460/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27461/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27480/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27481/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27500/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27501/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27520/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27521/30000  loss         0.129953  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27540/30000  loss         0.129952  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27541/30000  loss         0.129952  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27560/30000  loss         0.129952  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27561/30000  loss         0.129952  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27580/30000  loss         0.129952  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27581/30000  loss         0.129952  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27600/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27601/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27620/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27621/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27640/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27641/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27660/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27661/30000  loss         0.129951  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27680/30000  loss         0.129950  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27681/30000  loss         0.129950  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27700/30000  loss         0.129950  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27701/30000  loss         0.129950  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27720/30000  loss         0.129950  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27721/30000  loss         0.129950  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27740/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27741/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27760/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27761/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27780/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27781/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27800/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27801/30000  loss         0.129949  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27820/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27821/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27840/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27841/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27860/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27861/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27880/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27881/30000  loss         0.129948  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27900/30000  loss         0.129947  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27901/30000  loss         0.129947  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27920/30000  loss         0.129947  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27921/30000  loss         0.129947  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27940/30000  loss         0.129947  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27941/30000  loss         0.129947  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27960/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27961/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27980/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 27981/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28000/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28001/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28020/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28021/30000  loss         0.129946  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28040/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28041/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28060/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28061/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28080/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28081/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28100/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28101/30000  loss         0.129945  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28120/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28121/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28140/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28141/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28160/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28161/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28180/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28181/30000  loss         0.129944  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28200/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28201/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28220/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28221/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28240/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28241/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28260/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28261/30000  loss         0.129943  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28280/30000  loss         0.129942  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28281/30000  loss         0.129942  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28300/30000  loss         0.129942  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28301/30000  loss         0.129942  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.940\n",
      "iter 28320/30000  loss         0.129942  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28321/30000  loss         0.129942  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28340/30000  loss         0.129942  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28341/30000  loss         0.129941  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28360/30000  loss         0.129941  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28361/30000  loss         0.129941  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28380/30000  loss         0.129941  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28381/30000  loss         0.129941  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28400/30000  loss         0.129941  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28401/30000  loss         0.129941  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28420/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28421/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28440/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28441/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.940\n",
      "iter 28460/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28461/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28480/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28481/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28500/30000  loss         0.129940  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28501/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28520/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28521/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28540/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28541/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28560/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28561/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28580/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28581/30000  loss         0.129939  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28600/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28601/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28620/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28621/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28640/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28641/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28660/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28661/30000  loss         0.129938  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28680/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28681/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28700/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28701/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28720/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28721/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28740/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28741/30000  loss         0.129937  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28760/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28761/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28780/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28781/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28800/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28801/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28820/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28821/30000  loss         0.129936  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28840/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28841/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28860/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28861/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28880/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28881/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28900/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28901/30000  loss         0.129935  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28920/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28921/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28940/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28941/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28960/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28961/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28980/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 28981/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29000/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29001/30000  loss         0.129934  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29020/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29021/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29040/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29041/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29060/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29061/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29080/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29081/30000  loss         0.129933  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29100/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29101/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29120/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29121/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29140/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29141/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29160/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29161/30000  loss         0.129932  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29180/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29181/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29200/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29201/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29220/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29221/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29240/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29241/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29260/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29261/30000  loss         0.129931  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29280/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29281/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29300/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29301/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29320/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29321/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29340/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29341/30000  loss         0.129930  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29360/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29361/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29380/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29381/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29400/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29401/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29420/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29421/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29440/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29441/30000  loss         0.129929  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29460/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29461/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29480/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29481/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29500/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29501/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29520/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29521/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29540/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29541/30000  loss         0.129928  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29560/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29561/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29580/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29581/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29600/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29601/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29620/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29621/30000  loss         0.129927  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29640/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29641/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29660/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29661/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29680/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29681/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29700/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29701/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29720/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29721/30000  loss         0.129926  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29740/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29741/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29760/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29761/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29780/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29781/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29800/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29801/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29820/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29821/30000  loss         0.129925  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29840/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29841/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29860/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29861/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29880/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29881/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29900/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29901/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29920/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29921/30000  loss         0.129924  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29940/30000  loss         0.129923  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29941/30000  loss         0.129923  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29960/30000  loss         0.129923  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29961/30000  loss         0.129923  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29980/30000  loss         0.129923  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 29981/30000  loss         0.129923  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "iter 30000/30000  loss         0.129923  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.941\n",
      "Done. Did NOT converge.\n"
     ]
    }
   ],
   "source": [
    "lrt = LogisticRegressionGradientDescent(alpha=1, step_size=0.4, init_w_recipe='zeros')\n",
    "lrt.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>498</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>516</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0.0        498  478\n",
       "1.0        516  508"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lr.predict(x_te)\n",
    "y_actu = pd.Series(y_va, name='Actual')\n",
    "y_pred = pd.Series(y_hat, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 30000 iters of gradient descent with step_size 0.5\n",
      "iter    0/30000  loss         1.000000  avg_L1_norm_grad         0.031943  w[0]    0.000 bias    0.000\n",
      "iter    1/30000  loss         1.571866  avg_L1_norm_grad         0.078659  w[0]    0.000 bias    0.000\n",
      "iter    2/30000  loss         5.444314  avg_L1_norm_grad         0.120871  w[0]    0.000 bias    0.285\n",
      "iter    3/30000  loss         5.602124  avg_L1_norm_grad         0.097375  w[0]    0.000 bias   -0.070\n",
      "iter    4/30000  loss         0.868057  avg_L1_norm_grad         0.022085  w[0]    0.000 bias    0.282\n",
      "iter    5/30000  loss         0.880806  avg_L1_norm_grad         0.030923  w[0]    0.000 bias    0.274\n",
      "iter    6/30000  loss         1.463265  avg_L1_norm_grad         0.081640  w[0]    0.000 bias    0.416\n",
      "iter    7/30000  loss         3.976648  avg_L1_norm_grad         0.087309  w[0]    0.000 bias    0.205\n",
      "iter    8/30000  loss         0.931472  avg_L1_norm_grad         0.047598  w[0]    0.000 bias    0.525\n",
      "iter    9/30000  loss         1.699249  avg_L1_norm_grad         0.055907  w[0]    0.000 bias    0.421\n",
      "iter   10/30000  loss         1.397588  avg_L1_norm_grad         0.077610  w[0]    0.000 bias    0.644\n",
      "iter   11/30000  loss         3.134858  avg_L1_norm_grad         0.075050  w[0]    0.000 bias    0.442\n",
      "iter   12/30000  loss         0.746473  avg_L1_norm_grad         0.039454  w[0]    0.000 bias    0.725\n",
      "iter   13/30000  loss         1.170696  avg_L1_norm_grad         0.044232  w[0]    0.000 bias    0.640\n",
      "iter   14/30000  loss         1.043788  avg_L1_norm_grad         0.065843  w[0]    0.000 bias    0.819\n",
      "iter   15/30000  loss         2.297465  avg_L1_norm_grad         0.065688  w[0]    0.000 bias    0.651\n",
      "iter   16/30000  loss         0.756363  avg_L1_norm_grad         0.048664  w[0]    0.000 bias    0.902\n",
      "iter   17/30000  loss         1.395351  avg_L1_norm_grad         0.050480  w[0]    0.000 bias    0.785\n",
      "iter   18/30000  loss         0.812706  avg_L1_norm_grad         0.055376  w[0]    0.000 bias    0.981\n",
      "iter   19/30000  loss         1.626783  avg_L1_norm_grad         0.055604  w[0]    0.000 bias    0.842\n",
      "iter   20/30000  loss         0.708154  avg_L1_norm_grad         0.049216  w[0]    0.000 bias    1.055\n",
      "iter   21/30000  loss         1.304674  avg_L1_norm_grad         0.049249  w[0]    0.000 bias    0.932\n",
      "iter   40/30000  loss         0.335073  avg_L1_norm_grad         0.019697  w[0]    0.000 bias    1.458\n",
      "iter   41/30000  loss         0.374566  avg_L1_norm_grad         0.018307  w[0]    0.000 bias    1.409\n",
      "iter   60/30000  loss         0.252207  avg_L1_norm_grad         0.007848  w[0]    0.000 bias    1.602\n",
      "iter   61/30000  loss         0.253942  avg_L1_norm_grad         0.007182  w[0]    0.000 bias    1.585\n",
      "iter   80/30000  loss         0.230795  avg_L1_norm_grad         0.003875  w[0]    0.000 bias    1.700\n",
      "iter   81/30000  loss         0.230548  avg_L1_norm_grad         0.003509  w[0]    0.000 bias    1.693\n",
      "iter  100/30000  loss         0.221096  avg_L1_norm_grad         0.001713  w[0]    0.000 bias    1.777\n",
      "iter  101/30000  loss         0.220786  avg_L1_norm_grad         0.001529  w[0]    0.000 bias    1.776\n",
      "iter  120/30000  loss         0.215674  avg_L1_norm_grad         0.000679  w[0]    0.000 bias    1.843\n",
      "iter  121/30000  loss         0.215459  avg_L1_norm_grad         0.000613  w[0]    0.000 bias    1.845\n",
      "iter  140/30000  loss         0.211920  avg_L1_norm_grad         0.000424  w[0]    0.000 bias    1.901\n",
      "iter  141/30000  loss         0.211756  avg_L1_norm_grad         0.000408  w[0]    0.000 bias    1.904\n",
      "iter  160/30000  loss         0.208943  avg_L1_norm_grad         0.000370  w[0]    0.000 bias    1.952\n",
      "iter  161/30000  loss         0.208809  avg_L1_norm_grad         0.000368  w[0]    0.000 bias    1.955\n",
      "iter  180/30000  loss         0.206474  avg_L1_norm_grad         0.000338  w[0]    0.000 bias    1.997\n",
      "iter  181/30000  loss         0.206361  avg_L1_norm_grad         0.000336  w[0]    0.000 bias    1.999\n",
      "iter  200/30000  loss         0.204383  avg_L1_norm_grad         0.000312  w[0]    0.000 bias    2.037\n",
      "iter  201/30000  loss         0.204287  avg_L1_norm_grad         0.000311  w[0]    0.000 bias    2.039\n",
      "iter  220/30000  loss         0.202587  avg_L1_norm_grad         0.000290  w[0]    0.000 bias    2.072\n",
      "iter  221/30000  loss         0.202504  avg_L1_norm_grad         0.000289  w[0]    0.000 bias    2.074\n",
      "iter  240/30000  loss         0.201027  avg_L1_norm_grad         0.000271  w[0]    0.000 bias    2.104\n",
      "iter  241/30000  loss         0.200955  avg_L1_norm_grad         0.000270  w[0]    0.000 bias    2.105\n",
      "iter  260/30000  loss         0.199661  avg_L1_norm_grad         0.000254  w[0]    0.000 bias    2.132\n",
      "iter  261/30000  loss         0.199597  avg_L1_norm_grad         0.000254  w[0]    0.000 bias    2.133\n",
      "iter  280/30000  loss         0.198455  avg_L1_norm_grad         0.000240  w[0]    0.000 bias    2.157\n",
      "iter  281/30000  loss         0.198399  avg_L1_norm_grad         0.000239  w[0]    0.000 bias    2.159\n",
      "iter  300/30000  loss         0.197385  avg_L1_norm_grad         0.000227  w[0]    0.000 bias    2.180\n",
      "iter  301/30000  loss         0.197335  avg_L1_norm_grad         0.000226  w[0]    0.000 bias    2.181\n",
      "iter  320/30000  loss         0.196430  avg_L1_norm_grad         0.000215  w[0]    0.000 bias    2.201\n",
      "iter  321/30000  loss         0.196385  avg_L1_norm_grad         0.000214  w[0]    0.000 bias    2.201\n",
      "iter  340/30000  loss         0.195574  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    2.219\n",
      "iter  341/30000  loss         0.195534  avg_L1_norm_grad         0.000204  w[0]    0.000 bias    2.220\n",
      "iter  360/30000  loss         0.194803  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.235\n",
      "iter  361/30000  loss         0.194766  avg_L1_norm_grad         0.000194  w[0]    0.000 bias    2.236\n",
      "iter  380/30000  loss         0.194106  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.250\n",
      "iter  381/30000  loss         0.194072  avg_L1_norm_grad         0.000185  w[0]    0.000 bias    2.251\n",
      "iter  400/30000  loss         0.193473  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.264\n",
      "iter  401/30000  loss         0.193443  avg_L1_norm_grad         0.000177  w[0]    0.000 bias    2.265\n",
      "iter  420/30000  loss         0.192897  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.276\n",
      "iter  421/30000  loss         0.192870  avg_L1_norm_grad         0.000169  w[0]    0.000 bias    2.277\n",
      "iter  440/30000  loss         0.192372  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    2.287\n",
      "iter  441/30000  loss         0.192346  avg_L1_norm_grad         0.000162  w[0]    0.000 bias    2.288\n",
      "iter  460/30000  loss         0.191890  avg_L1_norm_grad         0.000156  w[0]    0.000 bias    2.297\n",
      "iter  461/30000  loss         0.191867  avg_L1_norm_grad         0.000155  w[0]    0.000 bias    2.298\n",
      "iter  480/30000  loss         0.191449  avg_L1_norm_grad         0.000150  w[0]    0.000 bias    2.306\n",
      "iter  481/30000  loss         0.191428  avg_L1_norm_grad         0.000149  w[0]    0.000 bias    2.307\n",
      "iter  500/30000  loss         0.191043  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.315\n",
      "iter  501/30000  loss         0.191023  avg_L1_norm_grad         0.000144  w[0]    0.000 bias    2.315\n",
      "iter  520/30000  loss         0.190668  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.322\n",
      "iter  521/30000  loss         0.190650  avg_L1_norm_grad         0.000138  w[0]    0.000 bias    2.322\n",
      "iter  540/30000  loss         0.190322  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.329\n",
      "iter  541/30000  loss         0.190306  avg_L1_norm_grad         0.000133  w[0]    0.000 bias    2.329\n",
      "iter  560/30000  loss         0.190002  avg_L1_norm_grad         0.000129  w[0]    0.000 bias    2.335\n",
      "iter  561/30000  loss         0.189987  avg_L1_norm_grad         0.000128  w[0]    0.000 bias    2.335\n",
      "iter  580/30000  loss         0.189706  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.340\n",
      "iter  581/30000  loss         0.189691  avg_L1_norm_grad         0.000124  w[0]    0.000 bias    2.341\n",
      "iter  600/30000  loss         0.189430  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.345\n",
      "iter  601/30000  loss         0.189417  avg_L1_norm_grad         0.000120  w[0]    0.000 bias    2.346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/30000  loss         0.189174  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    2.350\n",
      "iter  621/30000  loss         0.189161  avg_L1_norm_grad         0.000116  w[0]    0.000 bias    2.350\n",
      "iter  640/30000  loss         0.188935  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    2.354\n",
      "iter  641/30000  loss         0.188923  avg_L1_norm_grad         0.000112  w[0]    0.000 bias    2.354\n",
      "iter  660/30000  loss         0.188712  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    2.358\n",
      "iter  661/30000  loss         0.188701  avg_L1_norm_grad         0.000108  w[0]    0.000 bias    2.358\n",
      "iter  680/30000  loss         0.188504  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.361\n",
      "iter  681/30000  loss         0.188494  avg_L1_norm_grad         0.000105  w[0]    0.000 bias    2.361\n",
      "iter  700/30000  loss         0.188309  avg_L1_norm_grad         0.000102  w[0]    0.000 bias    2.364\n",
      "iter  701/30000  loss         0.188300  avg_L1_norm_grad         0.000101  w[0]    0.000 bias    2.364\n",
      "iter  720/30000  loss         0.188127  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    2.367\n",
      "iter  721/30000  loss         0.188118  avg_L1_norm_grad         0.000098  w[0]    0.000 bias    2.367\n",
      "iter  740/30000  loss         0.187956  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    2.369\n",
      "iter  741/30000  loss         0.187948  avg_L1_norm_grad         0.000095  w[0]    0.000 bias    2.369\n",
      "iter  760/30000  loss         0.187796  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.371\n",
      "iter  761/30000  loss         0.187788  avg_L1_norm_grad         0.000093  w[0]    0.000 bias    2.371\n",
      "iter  780/30000  loss         0.187645  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    2.373\n",
      "iter  781/30000  loss         0.187638  avg_L1_norm_grad         0.000090  w[0]    0.000 bias    2.373\n",
      "iter  800/30000  loss         0.187503  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    2.375\n",
      "iter  801/30000  loss         0.187496  avg_L1_norm_grad         0.000087  w[0]    0.000 bias    2.375\n",
      "iter  820/30000  loss         0.187370  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    2.376\n",
      "iter  821/30000  loss         0.187363  avg_L1_norm_grad         0.000085  w[0]    0.000 bias    2.377\n",
      "iter  840/30000  loss         0.187244  avg_L1_norm_grad         0.000083  w[0]    0.000 bias    2.378\n",
      "iter  841/30000  loss         0.187238  avg_L1_norm_grad         0.000082  w[0]    0.000 bias    2.378\n",
      "iter  860/30000  loss         0.187126  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.379\n",
      "iter  861/30000  loss         0.187120  avg_L1_norm_grad         0.000080  w[0]    0.000 bias    2.379\n",
      "iter  880/30000  loss         0.187014  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    2.380\n",
      "iter  881/30000  loss         0.187009  avg_L1_norm_grad         0.000078  w[0]    0.000 bias    2.380\n",
      "iter  900/30000  loss         0.186909  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    2.381\n",
      "iter  901/30000  loss         0.186904  avg_L1_norm_grad         0.000076  w[0]    0.000 bias    2.381\n",
      "iter  920/30000  loss         0.186809  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.382\n",
      "iter  921/30000  loss         0.186804  avg_L1_norm_grad         0.000074  w[0]    0.000 bias    2.382\n",
      "iter  940/30000  loss         0.186715  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.382\n",
      "iter  941/30000  loss         0.186710  avg_L1_norm_grad         0.000072  w[0]    0.000 bias    2.382\n",
      "iter  960/30000  loss         0.186625  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.383\n",
      "iter  961/30000  loss         0.186621  avg_L1_norm_grad         0.000070  w[0]    0.000 bias    2.383\n",
      "iter  980/30000  loss         0.186541  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.383\n",
      "iter  981/30000  loss         0.186537  avg_L1_norm_grad         0.000068  w[0]    0.000 bias    2.383\n",
      "iter 1000/30000  loss         0.186461  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.384\n",
      "iter 1001/30000  loss         0.186457  avg_L1_norm_grad         0.000067  w[0]    0.000 bias    2.384\n",
      "iter 1020/30000  loss         0.186385  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.384\n",
      "iter 1021/30000  loss         0.186381  avg_L1_norm_grad         0.000065  w[0]    0.000 bias    2.384\n",
      "iter 1040/30000  loss         0.186313  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.384\n",
      "iter 1041/30000  loss         0.186309  avg_L1_norm_grad         0.000063  w[0]    0.000 bias    2.384\n",
      "iter 1060/30000  loss         0.186245  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.385\n",
      "iter 1061/30000  loss         0.186241  avg_L1_norm_grad         0.000062  w[0]    0.000 bias    2.385\n",
      "iter 1080/30000  loss         0.186180  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.385\n",
      "iter 1081/30000  loss         0.186177  avg_L1_norm_grad         0.000060  w[0]    0.000 bias    2.385\n",
      "iter 1100/30000  loss         0.186118  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.385\n",
      "iter 1101/30000  loss         0.186115  avg_L1_norm_grad         0.000059  w[0]    0.000 bias    2.385\n",
      "iter 1120/30000  loss         0.186059  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.385\n",
      "iter 1121/30000  loss         0.186057  avg_L1_norm_grad         0.000057  w[0]    0.000 bias    2.385\n",
      "iter 1140/30000  loss         0.186004  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.385\n",
      "iter 1141/30000  loss         0.186001  avg_L1_norm_grad         0.000056  w[0]    0.000 bias    2.385\n",
      "iter 1160/30000  loss         0.185951  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.385\n",
      "iter 1161/30000  loss         0.185948  avg_L1_norm_grad         0.000055  w[0]    0.000 bias    2.385\n",
      "iter 1180/30000  loss         0.185900  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.385\n",
      "iter 1181/30000  loss         0.185898  avg_L1_norm_grad         0.000053  w[0]    0.000 bias    2.385\n",
      "iter 1200/30000  loss         0.185852  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.385\n",
      "iter 1201/30000  loss         0.185850  avg_L1_norm_grad         0.000052  w[0]    0.000 bias    2.385\n",
      "iter 1220/30000  loss         0.185806  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.384\n",
      "iter 1221/30000  loss         0.185804  avg_L1_norm_grad         0.000051  w[0]    0.000 bias    2.384\n",
      "iter 1240/30000  loss         0.185762  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.384\n",
      "iter 1241/30000  loss         0.185760  avg_L1_norm_grad         0.000050  w[0]    0.000 bias    2.384\n",
      "iter 1260/30000  loss         0.185721  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.384\n",
      "iter 1261/30000  loss         0.185719  avg_L1_norm_grad         0.000049  w[0]    0.000 bias    2.384\n",
      "iter 1280/30000  loss         0.185681  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.384\n",
      "iter 1281/30000  loss         0.185679  avg_L1_norm_grad         0.000048  w[0]    0.000 bias    2.384\n",
      "iter 1300/30000  loss         0.185643  avg_L1_norm_grad         0.000047  w[0]    0.000 bias    2.384\n",
      "iter 1301/30000  loss         0.185641  avg_L1_norm_grad         0.000046  w[0]    0.000 bias    2.384\n",
      "iter 1320/30000  loss         0.185607  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.383\n",
      "iter 1321/30000  loss         0.185605  avg_L1_norm_grad         0.000045  w[0]    0.000 bias    2.383\n",
      "iter 1340/30000  loss         0.185572  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.383\n",
      "iter 1341/30000  loss         0.185571  avg_L1_norm_grad         0.000044  w[0]    0.000 bias    2.383\n",
      "iter 1360/30000  loss         0.185539  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.383\n",
      "iter 1361/30000  loss         0.185538  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.383\n",
      "iter 1380/30000  loss         0.185508  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.383\n",
      "iter 1381/30000  loss         0.185506  avg_L1_norm_grad         0.000043  w[0]    0.000 bias    2.383\n",
      "iter 1400/30000  loss         0.185478  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.382\n",
      "iter 1401/30000  loss         0.185476  avg_L1_norm_grad         0.000042  w[0]    0.000 bias    2.382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/30000  loss         0.185449  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.382\n",
      "iter 1421/30000  loss         0.185448  avg_L1_norm_grad         0.000041  w[0]    0.000 bias    2.382\n",
      "iter 1440/30000  loss         0.185421  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.382\n",
      "iter 1441/30000  loss         0.185420  avg_L1_norm_grad         0.000040  w[0]    0.000 bias    2.382\n",
      "iter 1460/30000  loss         0.185395  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.381\n",
      "iter 1461/30000  loss         0.185394  avg_L1_norm_grad         0.000039  w[0]    0.000 bias    2.381\n",
      "iter 1480/30000  loss         0.185370  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.381\n",
      "iter 1481/30000  loss         0.185369  avg_L1_norm_grad         0.000038  w[0]    0.000 bias    2.381\n",
      "iter 1500/30000  loss         0.185346  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.381\n",
      "iter 1501/30000  loss         0.185345  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.381\n",
      "iter 1520/30000  loss         0.185323  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.380\n",
      "iter 1521/30000  loss         0.185322  avg_L1_norm_grad         0.000037  w[0]    0.000 bias    2.380\n",
      "iter 1540/30000  loss         0.185301  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.380\n",
      "iter 1541/30000  loss         0.185299  avg_L1_norm_grad         0.000036  w[0]    0.000 bias    2.380\n",
      "iter 1560/30000  loss         0.185279  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.380\n",
      "iter 1561/30000  loss         0.185278  avg_L1_norm_grad         0.000035  w[0]    0.000 bias    2.380\n",
      "iter 1580/30000  loss         0.185259  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.379\n",
      "iter 1581/30000  loss         0.185258  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.379\n",
      "iter 1600/30000  loss         0.185240  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.379\n",
      "iter 1601/30000  loss         0.185239  avg_L1_norm_grad         0.000034  w[0]    0.000 bias    2.379\n",
      "iter 1620/30000  loss         0.185221  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.379\n",
      "iter 1621/30000  loss         0.185220  avg_L1_norm_grad         0.000033  w[0]    0.000 bias    2.379\n",
      "iter 1640/30000  loss         0.185203  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.378\n",
      "iter 1641/30000  loss         0.185202  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.378\n",
      "iter 1660/30000  loss         0.185186  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.378\n",
      "iter 1661/30000  loss         0.185185  avg_L1_norm_grad         0.000032  w[0]    0.000 bias    2.378\n",
      "iter 1680/30000  loss         0.185169  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.378\n",
      "iter 1681/30000  loss         0.185169  avg_L1_norm_grad         0.000031  w[0]    0.000 bias    2.378\n",
      "iter 1700/30000  loss         0.185154  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.377\n",
      "iter 1701/30000  loss         0.185153  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.377\n",
      "iter 1720/30000  loss         0.185139  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.377\n",
      "iter 1721/30000  loss         0.185138  avg_L1_norm_grad         0.000030  w[0]    0.000 bias    2.377\n",
      "iter 1740/30000  loss         0.185124  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.377\n",
      "iter 1741/30000  loss         0.185123  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.377\n",
      "iter 1760/30000  loss         0.185110  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.376\n",
      "iter 1761/30000  loss         0.185109  avg_L1_norm_grad         0.000029  w[0]    0.000 bias    2.376\n",
      "iter 1780/30000  loss         0.185097  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.376\n",
      "iter 1781/30000  loss         0.185096  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.376\n",
      "iter 1800/30000  loss         0.185084  avg_L1_norm_grad         0.000028  w[0]    0.000 bias    2.376\n",
      "iter 1801/30000  loss         0.185083  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.376\n",
      "iter 1820/30000  loss         0.185071  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.375\n",
      "iter 1821/30000  loss         0.185071  avg_L1_norm_grad         0.000027  w[0]    0.000 bias    2.375\n",
      "iter 1840/30000  loss         0.185060  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.375\n",
      "iter 1841/30000  loss         0.185059  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.375\n",
      "iter 1860/30000  loss         0.185048  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.375\n",
      "iter 1861/30000  loss         0.185048  avg_L1_norm_grad         0.000026  w[0]    0.000 bias    2.375\n",
      "iter 1880/30000  loss         0.185037  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.375\n",
      "iter 1881/30000  loss         0.185037  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.375\n",
      "iter 1900/30000  loss         0.185027  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.374\n",
      "iter 1901/30000  loss         0.185026  avg_L1_norm_grad         0.000025  w[0]    0.000 bias    2.374\n",
      "iter 1920/30000  loss         0.185017  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.374\n",
      "iter 1921/30000  loss         0.185016  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.374\n",
      "iter 1940/30000  loss         0.185007  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.374\n",
      "iter 1941/30000  loss         0.185006  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.374\n",
      "iter 1960/30000  loss         0.184998  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.373\n",
      "iter 1961/30000  loss         0.184997  avg_L1_norm_grad         0.000024  w[0]    0.000 bias    2.373\n",
      "iter 1980/30000  loss         0.184989  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.373\n",
      "iter 1981/30000  loss         0.184988  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.373\n",
      "iter 2000/30000  loss         0.184980  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.373\n",
      "iter 2001/30000  loss         0.184979  avg_L1_norm_grad         0.000023  w[0]    0.000 bias    2.373\n",
      "iter 2020/30000  loss         0.184972  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.373\n",
      "iter 2021/30000  loss         0.184971  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.373\n",
      "iter 2040/30000  loss         0.184964  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.372\n",
      "iter 2041/30000  loss         0.184963  avg_L1_norm_grad         0.000022  w[0]    0.000 bias    2.372\n",
      "iter 2060/30000  loss         0.184956  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.372\n",
      "iter 2061/30000  loss         0.184955  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.372\n",
      "iter 2080/30000  loss         0.184948  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.372\n",
      "iter 2081/30000  loss         0.184948  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.372\n",
      "iter 2100/30000  loss         0.184941  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.371\n",
      "iter 2101/30000  loss         0.184941  avg_L1_norm_grad         0.000021  w[0]    0.000 bias    2.371\n",
      "iter 2120/30000  loss         0.184934  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.371\n",
      "iter 2121/30000  loss         0.184934  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.371\n",
      "iter 2140/30000  loss         0.184928  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.371\n",
      "iter 2141/30000  loss         0.184927  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.371\n",
      "iter 2160/30000  loss         0.184921  avg_L1_norm_grad         0.000020  w[0]    0.000 bias    2.371\n",
      "iter 2161/30000  loss         0.184921  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.371\n",
      "iter 2180/30000  loss         0.184915  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.371\n",
      "iter 2181/30000  loss         0.184915  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.371\n",
      "iter 2200/30000  loss         0.184909  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.370\n",
      "iter 2201/30000  loss         0.184909  avg_L1_norm_grad         0.000019  w[0]    0.000 bias    2.370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/30000  loss         0.184904  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.370\n",
      "iter 2221/30000  loss         0.184903  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.370\n",
      "iter 2240/30000  loss         0.184898  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.370\n",
      "iter 2241/30000  loss         0.184898  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.370\n",
      "iter 2260/30000  loss         0.184893  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.370\n",
      "iter 2261/30000  loss         0.184893  avg_L1_norm_grad         0.000018  w[0]    0.000 bias    2.370\n",
      "iter 2280/30000  loss         0.184888  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2281/30000  loss         0.184888  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2300/30000  loss         0.184883  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2301/30000  loss         0.184883  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2320/30000  loss         0.184878  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2321/30000  loss         0.184878  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2340/30000  loss         0.184874  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2341/30000  loss         0.184873  avg_L1_norm_grad         0.000017  w[0]    0.000 bias    2.369\n",
      "iter 2360/30000  loss         0.184869  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.369\n",
      "iter 2361/30000  loss         0.184869  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.369\n",
      "iter 2380/30000  loss         0.184865  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.368\n",
      "iter 2381/30000  loss         0.184865  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.368\n",
      "iter 2400/30000  loss         0.184861  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.368\n",
      "iter 2401/30000  loss         0.184861  avg_L1_norm_grad         0.000016  w[0]    0.000 bias    2.368\n",
      "iter 2420/30000  loss         0.184857  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.368\n",
      "iter 2421/30000  loss         0.184857  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.368\n",
      "iter 2440/30000  loss         0.184853  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.368\n",
      "iter 2441/30000  loss         0.184853  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.368\n",
      "iter 2460/30000  loss         0.184849  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.368\n",
      "iter 2461/30000  loss         0.184849  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.368\n",
      "iter 2480/30000  loss         0.184846  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.367\n",
      "iter 2481/30000  loss         0.184846  avg_L1_norm_grad         0.000015  w[0]    0.000 bias    2.367\n",
      "iter 2500/30000  loss         0.184843  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2501/30000  loss         0.184842  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2520/30000  loss         0.184839  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2521/30000  loss         0.184839  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2540/30000  loss         0.184836  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2541/30000  loss         0.184836  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2560/30000  loss         0.184833  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2561/30000  loss         0.184833  avg_L1_norm_grad         0.000014  w[0]    0.000 bias    2.367\n",
      "iter 2580/30000  loss         0.184830  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.367\n",
      "iter 2581/30000  loss         0.184830  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.367\n",
      "iter 2600/30000  loss         0.184827  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.366\n",
      "iter 2601/30000  loss         0.184827  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.366\n",
      "iter 2620/30000  loss         0.184824  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.366\n",
      "iter 2621/30000  loss         0.184824  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.366\n",
      "iter 2640/30000  loss         0.184822  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.366\n",
      "iter 2641/30000  loss         0.184822  avg_L1_norm_grad         0.000013  w[0]    0.000 bias    2.366\n",
      "iter 2660/30000  loss         0.184819  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2661/30000  loss         0.184819  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2680/30000  loss         0.184817  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2681/30000  loss         0.184817  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2700/30000  loss         0.184814  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2701/30000  loss         0.184814  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2720/30000  loss         0.184812  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2721/30000  loss         0.184812  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.366\n",
      "iter 2740/30000  loss         0.184810  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.365\n",
      "iter 2741/30000  loss         0.184810  avg_L1_norm_grad         0.000012  w[0]    0.000 bias    2.365\n",
      "iter 2760/30000  loss         0.184808  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2761/30000  loss         0.184808  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2780/30000  loss         0.184806  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2781/30000  loss         0.184806  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2800/30000  loss         0.184804  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2801/30000  loss         0.184804  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2820/30000  loss         0.184802  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2821/30000  loss         0.184802  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2840/30000  loss         0.184800  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2841/30000  loss         0.184800  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2860/30000  loss         0.184798  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2861/30000  loss         0.184798  avg_L1_norm_grad         0.000011  w[0]    0.000 bias    2.365\n",
      "iter 2880/30000  loss         0.184796  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.365\n",
      "iter 2881/30000  loss         0.184796  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.365\n",
      "iter 2900/30000  loss         0.184795  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2901/30000  loss         0.184794  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2920/30000  loss         0.184793  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2921/30000  loss         0.184793  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2940/30000  loss         0.184791  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2941/30000  loss         0.184791  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2960/30000  loss         0.184790  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2961/30000  loss         0.184790  avg_L1_norm_grad         0.000010  w[0]    0.000 bias    2.364\n",
      "iter 2980/30000  loss         0.184788  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 2981/30000  loss         0.184788  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3000/30000  loss         0.184787  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3001/30000  loss         0.184787  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/30000  loss         0.184786  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3021/30000  loss         0.184785  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3040/30000  loss         0.184784  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3041/30000  loss         0.184784  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3060/30000  loss         0.184783  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3061/30000  loss         0.184783  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.364\n",
      "iter 3080/30000  loss         0.184782  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.363\n",
      "iter 3081/30000  loss         0.184782  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.363\n",
      "iter 3100/30000  loss         0.184781  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.363\n",
      "iter 3101/30000  loss         0.184780  avg_L1_norm_grad         0.000009  w[0]    0.000 bias    2.363\n",
      "iter 3120/30000  loss         0.184779  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3121/30000  loss         0.184779  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3140/30000  loss         0.184778  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3141/30000  loss         0.184778  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3160/30000  loss         0.184777  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3161/30000  loss         0.184777  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3180/30000  loss         0.184776  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3181/30000  loss         0.184776  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3200/30000  loss         0.184775  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3201/30000  loss         0.184775  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3220/30000  loss         0.184774  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3221/30000  loss         0.184774  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3240/30000  loss         0.184773  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3241/30000  loss         0.184773  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3260/30000  loss         0.184772  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3261/30000  loss         0.184772  avg_L1_norm_grad         0.000008  w[0]    0.000 bias    2.363\n",
      "iter 3280/30000  loss         0.184771  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.363\n",
      "iter 3281/30000  loss         0.184771  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.363\n",
      "iter 3300/30000  loss         0.184770  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.363\n",
      "iter 3301/30000  loss         0.184770  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.363\n",
      "iter 3320/30000  loss         0.184770  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3321/30000  loss         0.184770  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3340/30000  loss         0.184769  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3341/30000  loss         0.184769  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3360/30000  loss         0.184768  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3361/30000  loss         0.184768  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3380/30000  loss         0.184767  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3381/30000  loss         0.184767  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3400/30000  loss         0.184767  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3401/30000  loss         0.184767  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3420/30000  loss         0.184766  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3421/30000  loss         0.184766  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3440/30000  loss         0.184765  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3441/30000  loss         0.184765  avg_L1_norm_grad         0.000007  w[0]    0.000 bias    2.362\n",
      "iter 3460/30000  loss         0.184765  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3461/30000  loss         0.184764  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3480/30000  loss         0.184764  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3481/30000  loss         0.184764  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3500/30000  loss         0.184763  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3501/30000  loss         0.184763  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3520/30000  loss         0.184763  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3521/30000  loss         0.184763  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3540/30000  loss         0.184762  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3541/30000  loss         0.184762  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3560/30000  loss         0.184762  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3561/30000  loss         0.184761  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3580/30000  loss         0.184761  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3581/30000  loss         0.184761  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3600/30000  loss         0.184760  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3601/30000  loss         0.184760  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.362\n",
      "iter 3620/30000  loss         0.184760  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.361\n",
      "iter 3621/30000  loss         0.184760  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.361\n",
      "iter 3640/30000  loss         0.184759  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.361\n",
      "iter 3641/30000  loss         0.184759  avg_L1_norm_grad         0.000006  w[0]    0.000 bias    2.361\n",
      "iter 3660/30000  loss         0.184759  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3661/30000  loss         0.184759  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3680/30000  loss         0.184758  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3681/30000  loss         0.184758  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3700/30000  loss         0.184758  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3701/30000  loss         0.184758  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3720/30000  loss         0.184758  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3721/30000  loss         0.184758  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3740/30000  loss         0.184757  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3741/30000  loss         0.184757  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3760/30000  loss         0.184757  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3761/30000  loss         0.184757  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3780/30000  loss         0.184756  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3781/30000  loss         0.184756  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3800/30000  loss         0.184756  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3801/30000  loss         0.184756  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/30000  loss         0.184756  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3821/30000  loss         0.184756  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3840/30000  loss         0.184755  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3841/30000  loss         0.184755  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3860/30000  loss         0.184755  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3861/30000  loss         0.184755  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3880/30000  loss         0.184755  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3881/30000  loss         0.184755  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3900/30000  loss         0.184754  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3901/30000  loss         0.184754  avg_L1_norm_grad         0.000005  w[0]    0.000 bias    2.361\n",
      "iter 3920/30000  loss         0.184754  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 3921/30000  loss         0.184754  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 3940/30000  loss         0.184754  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 3941/30000  loss         0.184754  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 3960/30000  loss         0.184753  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 3961/30000  loss         0.184753  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 3980/30000  loss         0.184753  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 3981/30000  loss         0.184753  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 4000/30000  loss         0.184753  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 4001/30000  loss         0.184753  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 4020/30000  loss         0.184753  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 4021/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.361\n",
      "iter 4040/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4041/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4060/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4061/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4080/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4081/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4100/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4101/30000  loss         0.184752  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4120/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4121/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4140/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4141/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4160/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4161/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4180/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4181/30000  loss         0.184751  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4200/30000  loss         0.184750  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4201/30000  loss         0.184750  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4220/30000  loss         0.184750  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4221/30000  loss         0.184750  avg_L1_norm_grad         0.000004  w[0]    0.000 bias    2.360\n",
      "iter 4240/30000  loss         0.184750  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4241/30000  loss         0.184750  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4260/30000  loss         0.184750  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4261/30000  loss         0.184750  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4280/30000  loss         0.184750  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4281/30000  loss         0.184750  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4300/30000  loss         0.184750  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4301/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4320/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4321/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4340/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4341/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4360/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4361/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4380/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4381/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4400/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4401/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4420/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4421/30000  loss         0.184749  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4440/30000  loss         0.184748  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "iter 4441/30000  loss         0.184748  avg_L1_norm_grad         0.000003  w[0]    0.000 bias    2.360\n",
      "Done. Converged after 4456 iterations.\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegressionGradientDescent(alpha=10, step_size=0.5, init_w_recipe='zeros')\n",
    "lr2.fit(x_tr, y_tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
